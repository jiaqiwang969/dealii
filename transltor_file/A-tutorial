include/deal.II-translator/A-tutorial/CMakeLists.txt
## ---------------------------------------------------------------------
##
## Copyright (C) 2012 - 2020 by the deal.II Authors
##
## This file is part of the deal.II library.
##
## The deal.II library is free software; you can use it, redistribute
## it, and/or modify it under the terms of the GNU Lesser General
## Public License as published by the Free Software Foundation; either
## version 2.1 of the License, or (at your option) any later version.
## The full text of the license can be found in the file LICENSE.md at
## the top level directory of deal.II.
##
## ---------------------------------------------------------------------



# Collect all of the directory names for the tutorial programs
FILE(GLOB _deal_ii_steps
  ${CMAKE_SOURCE_DIR}/examples/step-*
  )

# Also collect the names of all code gallery projects. To
# do so, find all 'author' files, then strip the last two
# levels of these paths.
#
# For unclear reasons, the glob returns these files as
# "/a/b/c/name//doc//author", so make sure we eat the
# double slashes in the second step
SET_IF_EMPTY(DEAL_II_CODE_GALLERY_DIRECTORY ${CMAKE_SOURCE_DIR}/code-gallery)
FILE(GLOB _code_gallery_names
     "${DEAL_II_CODE_GALLERY_DIRECTORY}/*/doc/author")
STRING(REGEX REPLACE "/+doc/+author" "" _code_gallery_names "${_code_gallery_names}")

#
# Define target for the tutorial. It depends on the
# file tutorial.h built via the next target below, as well
# as the various files we create from the tutorial
# directories below that. These dependencies are added
# below the respective targets.
#
# This file uses the DEAL_II_STEPS variable set in
# ../CMakeLists.txt.
#

ADD_CUSTOM_TARGET(tutorial)

#
# Describe how to build tutorial.h.
#
# First collect the various files this all depends on. This
# is simpler for the tutorials since the directory names
# are structured. For the code gallery, we have already
# determined the list of names of the relevant subdirectories
# (which may be empty), so we can create the list of files
# the following target depends on without a GLOB and instead
# using just a FOREACH loop.
#
# For tutorial programs, the steps.pl script will read
# tooltip, kind, and builds-on files. For the code gallery
# programs, the 'kind' is obvious and we only need the other
# two categories.
#

file(GLOB _deal_ii_steps_tooltip
  ${CMAKE_SOURCE_DIR}/examples/step-*/doc/tooltip
  )
FOREACH(_gallery ${_code_gallery_names})
  LIST(APPEND _deal_ii_code_gallery_tooltip
       ${_gallery}/doc/tooltip)
ENDFOREACH()
file(GLOB _deal_ii_steps_kind
  ${CMAKE_SOURCE_DIR}/examples/step-*/doc/kind
  )
file(GLOB _deal_ii_steps_buildson
  ${CMAKE_SOURCE_DIR}/examples/step-*/doc/builds-on
  )
FOREACH(_gallery ${_code_gallery_names})
  LIST(APPEND _deal_ii_code_gallery_buildson
       ${_gallery}/doc/builds-on)
ENDFOREACH()


ADD_CUSTOM_COMMAND(
  OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/tutorial.h
  COMMAND ${PERL_EXECUTABLE}
  ARGS
    ${CMAKE_SOURCE_DIR}/doc/doxygen/scripts/steps.pl
    ${CMAKE_CURRENT_SOURCE_DIR}/tutorial.h.in
    ${_deal_ii_steps}
    ${_code_gallery_names}
    > ${CMAKE_CURRENT_BINARY_DIR}/tutorial.h
  DEPENDS
    ${CMAKE_SOURCE_DIR}/doc/doxygen/scripts/steps.pl
    ${CMAKE_CURRENT_SOURCE_DIR}/tutorial.h.in
    ${_deal_ii_steps_tooltip}
    ${_deal_ii_steps_kind}
    ${_deal_ii_steps_buildson}
    ${_deal_ii_code_gallery_tooltip}
    ${_deal_ii_code_gallery_buildson}
  )
ADD_CUSTOM_TARGET(build_tutorial_h
  DEPENDS ${CMAKE_CURRENT_BINARY_DIR}/tutorial.h
    COMMENT
      "Building tutorial.h")
ADD_DEPENDENCIES(tutorial build_tutorial_h)


#
# Prepare the steps for documentation generation
#

FOREACH(_step ${_deal_ii_steps})
  GET_FILENAME_COMPONENT(_step "${_step}" NAME)

  IF(EXISTS "${CMAKE_SOURCE_DIR}/examples/${_step}/${_step}.cu")
    SET(_FILE_EXTENSION "cu")
  ELSE()
    SET(_FILE_EXTENSION "cc")
  ENDIF()

  ADD_CUSTOM_COMMAND(
    OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/${_step}.${_FILE_EXTENSION}
    COMMAND ${PERL_EXECUTABLE}
    ARGS
      ${CMAKE_SOURCE_DIR}/doc/doxygen/scripts/program2plain
      < ${CMAKE_SOURCE_DIR}/examples/${_step}/${_step}.${_FILE_EXTENSION}
      > ${CMAKE_CURRENT_BINARY_DIR}/${_step}.${_FILE_EXTENSION}
    DEPENDS
      ${CMAKE_SOURCE_DIR}/doc/doxygen/scripts/program2plain
      ${CMAKE_SOURCE_DIR}/examples/${_step}/${_step}.${_FILE_EXTENSION}
    VERBATIM
    )

  ADD_CUSTOM_COMMAND(
    OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/${_step}.h
    COMMAND ${PERL_EXECUTABLE}
    ARGS
      ${CMAKE_SOURCE_DIR}/doc/doxygen/scripts/make_step.pl
      ${_step} ${CMAKE_SOURCE_DIR}
      > ${CMAKE_CURRENT_BINARY_DIR}/${_step}.h
    WORKING_DIRECTORY
      ${CMAKE_CURRENT_BINARY_DIR}
    DEPENDS
      ${CMAKE_SOURCE_DIR}/doc/doxygen/scripts/make_step.pl
      ${CMAKE_SOURCE_DIR}/doc/doxygen/scripts/intro2toc
      ${CMAKE_SOURCE_DIR}/doc/doxygen/scripts/create_anchors
      ${CMAKE_SOURCE_DIR}/doc/doxygen/scripts/program2doxygen
      ${CMAKE_SOURCE_DIR}/examples/${_step}/${_step}.${_FILE_EXTENSION}
      ${CMAKE_SOURCE_DIR}/examples/${_step}/doc/intro.dox
      ${CMAKE_SOURCE_DIR}/examples/${_step}/doc/results.dox
    )

  ADD_CUSTOM_TARGET(tutorial_${_step}
    DEPENDS
      ${CMAKE_CURRENT_BINARY_DIR}/${_step}.h
      ${CMAKE_CURRENT_BINARY_DIR}/${_step}.${_FILE_EXTENSION}
      COMMENT
        "Building doxygen input file for tutorial program <${_step}>"
    )
  ADD_DEPENDENCIES(tutorial tutorial_${_step})
ENDFOREACH()
## ---------------------------------------------------------------------
##
## Copyright (C) 2012 - 2020 by the deal.II Authors
##
## This file is part of the deal.II library.
##
## The deal.II library is free software; you can use it, redistribute
## it, and/or modify it under the terms of the GNU Lesser General
## Public License as published by the Free Software Foundation; either
## version 2.1 of the License, or (at your option) any later version.
## The full text of the license can be found in the file LICENSE.md at
## the top level directory of deal.II.
##
## ---------------------------------------------------------------------



# Collect all of the directory names for the tutorial programs
FILE(GLOB _deal_ii_steps
  ${CMAKE_SOURCE_DIR}/examples/step-*
  )

# Also collect the names of all code gallery projects. To
# do so, find all 'author' files, then strip the last two
# levels of these paths.
#
# For unclear reasons, the glob returns these files as
# "/a/b/c/name//doc//author", so make sure we eat the
# double slashes in the second step
SET_IF_EMPTY(DEAL_II_CODE_GALLERY_DIRECTORY ${CMAKE_SOURCE_DIR}/code-gallery)
FILE(GLOB _code_gallery_names
     "${DEAL_II_CODE_GALLERY_DIRECTORY}/*/doc/author")
STRING(REGEX REPLACE "/+doc/+author" "" _code_gallery_names "${_code_gallery_names}")

#
# Define target for the tutorial. It depends on the
# file tutorial.h built via the next target below, as well
# as the various files we create from the tutorial
# directories below that. These dependencies are added
# below the respective targets.
#
# This file uses the DEAL_II_STEPS variable set in
# ../CMakeLists.txt.
#

ADD_CUSTOM_TARGET(tutorial)

#
# Describe how to build tutorial.h.
#
# First collect the various files this all depends on. This
# is simpler for the tutorials since the directory names
# are structured. For the code gallery, we have already
# determined the list of names of the relevant subdirectories
# (which may be empty), so we can create the list of files
# the following target depends on without a GLOB and instead
# using just a FOREACH loop.
#
# For tutorial programs, the steps.pl script will read
# tooltip, kind, and builds-on files. For the code gallery
# programs, the 'kind' is obvious and we only need the other
# two categories.
#

file(GLOB _deal_ii_steps_tooltip
  ${CMAKE_SOURCE_DIR}/examples/step-*/doc/tooltip
  )
FOREACH(_gallery ${_code_gallery_names})
  LIST(APPEND _deal_ii_code_gallery_tooltip
       ${_gallery}/doc/tooltip)
ENDFOREACH()
file(GLOB _deal_ii_steps_kind
  ${CMAKE_SOURCE_DIR}/examples/step-*/doc/kind
  )
file(GLOB _deal_ii_steps_buildson
  ${CMAKE_SOURCE_DIR}/examples/step-*/doc/builds-on
  )
FOREACH(_gallery ${_code_gallery_names})
  LIST(APPEND _deal_ii_code_gallery_buildson
       ${_gallery}/doc/builds-on)
ENDFOREACH()


ADD_CUSTOM_COMMAND(
  OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/tutorial.h
  COMMAND ${PERL_EXECUTABLE}
  ARGS
    ${CMAKE_SOURCE_DIR}/doc/doxygen/scripts/steps.pl
    ${CMAKE_CURRENT_SOURCE_DIR}/tutorial.h.in
    ${_deal_ii_steps}
    ${_code_gallery_names}
    > ${CMAKE_CURRENT_BINARY_DIR}/tutorial.h
  DEPENDS
    ${CMAKE_SOURCE_DIR}/doc/doxygen/scripts/steps.pl
    ${CMAKE_CURRENT_SOURCE_DIR}/tutorial.h.in
    ${_deal_ii_steps_tooltip}
    ${_deal_ii_steps_kind}
    ${_deal_ii_steps_buildson}
    ${_deal_ii_code_gallery_tooltip}
    ${_deal_ii_code_gallery_buildson}
  )
ADD_CUSTOM_TARGET(build_tutorial_h
  DEPENDS ${CMAKE_CURRENT_BINARY_DIR}/tutorial.h
    COMMENT
      "Building tutorial.h")
ADD_DEPENDENCIES(tutorial build_tutorial_h)


#
# Prepare the steps for documentation generation
#

FOREACH(_step ${_deal_ii_steps})
  GET_FILENAME_COMPONENT(_step "${_step}" NAME)

  IF(EXISTS "${CMAKE_SOURCE_DIR}/examples/${_step}/${_step}.cu")
    SET(_FILE_EXTENSION "cu")
  ELSE()
    SET(_FILE_EXTENSION "cc")
  ENDIF()

  ADD_CUSTOM_COMMAND(
    OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/${_step}.${_FILE_EXTENSION}
    COMMAND ${PERL_EXECUTABLE}
    ARGS
      ${CMAKE_SOURCE_DIR}/doc/doxygen/scripts/program2plain
      < ${CMAKE_SOURCE_DIR}/examples/${_step}/${_step}.${_FILE_EXTENSION}
      > ${CMAKE_CURRENT_BINARY_DIR}/${_step}.${_FILE_EXTENSION}
    DEPENDS
      ${CMAKE_SOURCE_DIR}/doc/doxygen/scripts/program2plain
      ${CMAKE_SOURCE_DIR}/examples/${_step}/${_step}.${_FILE_EXTENSION}
    VERBATIM
    )

  ADD_CUSTOM_COMMAND(
    OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/${_step}.h
    COMMAND ${PERL_EXECUTABLE}
    ARGS
      ${CMAKE_SOURCE_DIR}/doc/doxygen/scripts/make_step.pl
      ${_step} ${CMAKE_SOURCE_DIR}
      > ${CMAKE_CURRENT_BINARY_DIR}/${_step}.h
    WORKING_DIRECTORY
      ${CMAKE_CURRENT_BINARY_DIR}
    DEPENDS
      ${CMAKE_SOURCE_DIR}/doc/doxygen/scripts/make_step.pl
      ${CMAKE_SOURCE_DIR}/doc/doxygen/scripts/intro2toc
      ${CMAKE_SOURCE_DIR}/doc/doxygen/scripts/create_anchors
      ${CMAKE_SOURCE_DIR}/doc/doxygen/scripts/program2doxygen
      ${CMAKE_SOURCE_DIR}/examples/${_step}/${_step}.${_FILE_EXTENSION}
      ${CMAKE_SOURCE_DIR}/examples/${_step}/doc/intro.dox
      ${CMAKE_SOURCE_DIR}/examples/${_step}/doc/results.dox
    )

  ADD_CUSTOM_TARGET(tutorial_${_step}
    DEPENDS
      ${CMAKE_CURRENT_BINARY_DIR}/${_step}.h
      ${CMAKE_CURRENT_BINARY_DIR}/${_step}.${_FILE_EXTENSION}
      COMMENT
        "Building doxygen input file for tutorial program <${_step}>"
    )
  ADD_DEPENDENCIES(tutorial tutorial_${_step})
ENDFOREACH()


include/deal.II-translator/A-tutorial/step-10_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5]
* [1.x.6][1.x.7][1.x.8]
* 

* 
* This is a rather short example which only shows some aspects of usinghigher order mappings. By  [2.x.2] mapping [2.x.3]  we mean the transformationbetween the unit cell (i.e. the unit line, square, or cube) to thecells in real space. In all the previous examples, we have implicitlyused linear or d-linear mappings; you will not have noticed this atall, since this is what happens if you do not do anythingspecial. However, if your domain has curved boundaries, there arecases where the piecewise linear approximation of the boundary(i.e. by straight line segments) is not sufficient, and you want thatyour computational domain is an approximation to the real domain usingcurved boundaries as well. If the boundary approximation usespiecewise quadratic parabolas to approximate the true boundary, thenwe say that this is a quadratic or  [2.x.4]  approximation. If weuse piecewise graphs of cubic polynomials, then this is a  [2.x.5] approximation, and so on.
* 

* 
* For some differential equations, it is known that piecewise linearapproximations of the boundary, i.e.  [2.x.6]  mappings, are notsufficient if the boundary of the exact domain is curved. Examples are thebiharmonic equation using  [2.x.7]  elements, or the Eulerequations of gas dynamics on domains with curved reflective boundaries. In these cases,it is necessary to compute the integrals using a higher ordermapping. If we do not use such a higherorder mapping, the order of approximation of the boundary dominatesthe order of convergence of the entire numerical scheme, irrespectiveof the order of convergence of the discretization in the interior ofthe domain.
* 

* 
* Rather than demonstrating the use of higher order mappings with one ofthese more complicated examples, we do only a brief computation:calculating the value of  [2.x.8]  by twodifferent methods.
* 

* 
* The first method uses a triangulated approximation of the circle with unitradius and integrates a unit magnitude constant function ( [2.x.9] ) over it. Ofcourse, if the domain were the exact unit circle, then the area would be  [2.x.10] ,but since we only use an approximation by piecewise polynomial segments, thevalue of the area we integrate over is not exactly  [2.x.11] . However, it is knownthat as we refine the triangulation, a  [2.x.12]  mapping approximates the boundarywith an order  [2.x.13] , where  [2.x.14]  is the mesh size. We will check the valuesof the computed area of the circle and their convergence towards  [2.x.15]  undermesh refinement for different mappings. We will also find a convergencebehavior that is surprising at first, but has a good explanation.
* 

* 
* The second method works similarly, but this time does not use the areaof the triangulated unit circle, but rather its perimeter.  [2.x.16]  is thenapproximated by half of the perimeter, as we choose the radius equal to one.
* 

* 
*  [2.x.17]  This tutorial shows in essence how to choose a particularmapping for integrals, by attaching a particular geometry to thetriangulation (as had already been done in  [2.x.18] , for example) andthen passing a mapping argument to the FEValues class that is used forall integrals in deal.II. The geometry we choose is a circle, forwhich deal.II already has a class (SphericalManifold) that can beused. If you want to define your own geometry, for example because itis complicated and cannot be described by the classes alreadyavailable in deal.II, you will want to read through  [2.x.19] .
* 

*  [1.x.9] [1.x.10]
*  The first of the following include files are probably well-known by now and need no further explanation.
* 

* 
* [1.x.11]
* 
*  This include file is new. Even if we are not solving a PDE in this tutorial, we want to use a dummy finite element with zero degrees of freedoms provided by the FE_Nothing class.
* 

* 
* [1.x.12]
* 
*  The following header file is also new: in it, we declare the MappingQ class which we will use for polynomial mappings of arbitrary order:
* 

* 
* [1.x.13]
* 
*  And this again is C++:
* 

* 
* [1.x.14]
* 
*  The last step is as in previous programs:
* 

* 
* [1.x.15]
* 
*  Now, as we want to compute the value of  [2.x.20] , we have to compare to something. These are the first few digits of  [2.x.21] , which we define beforehand for later use. Since we would like to compute the difference between two numbers which are quite accurate, with the accuracy of the computed approximation to  [2.x.22]  being in the range of the number of digits which a double variable can hold, we rather declare the reference value as a  [2.x.23]  and give it a number of extra digits:
* 

* 
* [1.x.16]
* 
*  Then, the first task will be to generate some output. Since this program is so small, we do not employ object oriented techniques in it and do not declare classes (although, of course, we use the object oriented features of the library). Rather, we just pack the functionality into separate functions. We make these functions templates on the number of space dimensions to conform to usual practice when using deal.II, although we will only use them for two space dimensions and throw an exception when attempted to use for any other spatial dimension.   
*   The first of these functions just generates a triangulation of a circle (hyperball) and outputs the  [2.x.24]  mapping of its cells for different values of  [2.x.25] . Then, we refine the grid once and do so again.
* 

* 
* [1.x.17]
* 
*  So first generate a coarse triangulation of the circle and associate a suitable boundary description to it. By default,  [2.x.26]  attaches a SphericalManifold to the boundary (and uses FlatManifold for the interior) so we simply call that function and move on:
* 

* 
* [1.x.18]
* 
*  Then alternate between generating output on the current mesh for  [2.x.27] ,  [2.x.28] , and  [2.x.29]  mappings, and (at the end of the loop body) refining the mesh once globally.
* 

* 
* [1.x.19]
* 
*  For this, first set up an object describing the mapping. This is done using the MappingQ class, which takes as argument to the constructor the polynomial degree which it shall use.
* 

* 
* [1.x.20]
* 
*  As a side note, for a piecewise linear mapping, you could give a value of  [2.x.30]  to the constructor of MappingQ, but there is also a class MappingQ1 that achieves the same effect. Historically, it did a lot of things in a simpler way than MappingQ but is today just a wrapper around the latter. It is, however, still the class that is used implicitly in many places of the library if you do not specify another mapping explicitly.
* 

* 
*  
*   In order to actually write out the present grid with this mapping, we set up an object which we will use for output. We will generate Gnuplot output, which consists of a set of lines describing the mapped triangulation. By default, only one line is drawn for each face of the triangulation, but since we want to explicitly see the effect of the mapping, we want to have the faces in more detail. This can be done by passing the output object a structure which contains some flags. In the present case, since Gnuplot can only draw straight lines, we output a number of additional points on the faces so that each face is drawn by 30 small lines instead of only one. This is sufficient to give us the impression of seeing a curved line, rather than a set of straight lines.
* 

* 
* [1.x.21]
* 
*  Finally, generate a filename and a file for output:
* 

* 
* [1.x.22]
* 
*  Then write out the triangulation to this file. The last argument of the function is a pointer to a mapping object. This argument has a default value, and if no value is given a simple MappingQ1 object is taken, which we briefly described above. This would then result in a piecewise linear approximation of the true boundary in the output.
* 

* 
* [1.x.23]
* 
*  At the end of the loop, refine the mesh globally.
* 

* 
* [1.x.24]
* 
*  Now we proceed with the main part of the code, the approximation of  [2.x.31] . The area of a circle is of course given by  [2.x.32] , so having a circle of radius 1, the area represents just the number that is searched for. The numerical computation of the area is performed by integrating the constant function of value 1 over the whole computational domain, i.e. by computing the areas  [2.x.33] , where the sum extends over all quadrature points on all active cells in the triangulation, with  [2.x.34]  being the weight of quadrature point  [2.x.35] . The integrals on each cell are approximated by numerical quadrature, hence the only additional ingredient we need is to set up a FEValues object that provides the corresponding `JxW` values of each cell. (Note that `JxW` is meant to abbreviate [1.x.25]; since in numerical quadrature the two factors always occur at the same places, we only offer the combined quantity, rather than two separate ones.) We note that here we won't use the FEValues object in its original purpose, i.e. for the computation of values of basis functions of a specific finite element at certain quadrature points. Rather, we use it only to gain the `JxW` at the quadrature points, irrespective of the (dummy) finite element we will give to the constructor of the FEValues object. The actual finite element given to the FEValues object is not used at all, so we could give any.
* 

* 
* [1.x.26]
* 
*  For the numerical quadrature on all cells we employ a quadrature rule of sufficiently high degree. We choose QGauss that is of order 8 (4 points), to be sure that the errors due to numerical quadrature are of higher order than the order (maximal 6) that will occur due to the order of the approximation of the boundary, i.e. the order of the mappings employed. Note that the integrand, the Jacobian determinant, is not a polynomial function (rather, it is a rational one), so we do not use Gauss quadrature in order to get the exact value of the integral as done often in finite element computations, but could as well have used any quadrature formula of like order instead.
* 

* 
* [1.x.27]
* 
*  Now start by looping over polynomial mapping degrees=1..4:
* 

* 
* [1.x.28]
* 
*  First generate the triangulation, the boundary and the mapping object as already seen.
* 

* 
* [1.x.29]
* 
*  We now create a finite element. Unlike the rest of the example programs, we do not actually need to do any computations with shape functions; we only need the `JxW` values from an FEValues object. Hence we use the special finite element class FE_Nothing which has exactly zero degrees of freedom per cell (as the name implies, the local basis on each cell is the empty set). A more typical usage of FE_Nothing is shown in  [2.x.36] .
* 

* 
* [1.x.30]
* 
*  Likewise, we need to create a DoFHandler object. We do not actually use it, but it will provide us with `active_cell_iterators` that are needed to reinitialize the FEValues object on each cell of the triangulation.
* 

* 
* [1.x.31]
* 
*  Now we set up the FEValues object, giving the Mapping, the dummy finite element and the quadrature object to the constructor, together with the update flags asking for the `JxW` values at the quadrature points only. This tells the FEValues object that it needs not compute other quantities upon calling the  [2.x.37]  function, thus saving computation time.         
*   The most important difference in the construction of the FEValues object compared to previous example programs is that we pass a mapping object as first argument, which is to be used in the computation of the mapping from unit to real cell. In previous examples, this argument was omitted, resulting in the implicit use of an object of type MappingQ1.
* 

* 
* [1.x.32]
* 
*  We employ an object of the ConvergenceTable class to store all important data like the approximated values for  [2.x.38]  and the error with respect to the true value of  [2.x.39] . We will also use functions provided by the ConvergenceTable class to compute convergence rates of the approximations to  [2.x.40] .
* 

* 
* [1.x.33]
* 
*  Now we loop over several refinement steps of the triangulation.
* 

* 
* [1.x.34]
* 
*  In this loop we first add the number of active cells of the current triangulation to the table. This function automatically creates a table column with superscription `cells`, in case this column was not created before.
* 

* 
* [1.x.35]
* 
*  Then we distribute the degrees of freedom for the dummy finite element. Strictly speaking we do not need this function call in our special case but we call it to make the DoFHandler happy
* 
*  -  otherwise it would throw an assertion in the  [2.x.41]  function below.
* 

* 
* [1.x.36]
* 
*  We define the variable area as `long double` like we did for the `pi` variable before.
* 

* 
* [1.x.37]
* 
*  Now we loop over all cells, reinitialize the FEValues object for each cell, and add up all the `JxW` values for this cell to `area`...
* 

* 
* [1.x.38]
* 
*  ...and store the resulting area values and the errors in the table. We need a static cast to double as there is no add_value(string, long double) function implemented. Note that this also concerns the second call as the  [2.x.42]  function in the  [2.x.43]  namespace is overloaded on its argument types, so there exists a version taking and returning a  [2.x.44] , in contrast to the global namespace where only one such function is declared (which takes and returns a double).
* 

* 
* [1.x.39]
* 
*  We want to compute the convergence rates of the `error` column. Therefore we need to omit the other columns from the convergence rate evaluation before calling `evaluate_all_convergence_rates`
* 

* 
* [1.x.40]
* 
*  Finally we set the precision and scientific mode for output of some of the quantities...
* 

* 
* [1.x.41]
* 
*  ...and write the whole table to  [2.x.45] 
* 

* 
* [1.x.42]
* 
*  The following, second function also computes an approximation of  [2.x.46]  but this time via the perimeter  [2.x.47]  of the domain instead of the area. This function is only a variation of the previous function. So we will mainly give documentation for the differences.
* 

* 
* [1.x.43]
* 
*  We take the same order of quadrature but this time a `dim-1` dimensional quadrature as we will integrate over (boundary) lines rather than over cells.
* 

* 
* [1.x.44]
* 
*  We loop over all degrees, create the triangulation, the boundary, the mapping, the dummy finite element and the DoFHandler object as seen before.
* 

* 
* [1.x.45]
* 
*  Then we create a FEFaceValues object instead of a FEValues object as in the previous function. Again, we pass a mapping as first argument.
* 

* 
* [1.x.46]
* 
*  Now we run over all cells and over all faces of each cell. Only the contributions of the `JxW` values on boundary faces are added to the long double variable `perimeter`.
* 

* 
* [1.x.47]
* 
*  We reinit the FEFaceValues object with the cell iterator and the number of the face.
* 

* 
* [1.x.48]
* 
*  Then store the evaluated values in the table...
* 

* 
* [1.x.49]
* 
*  ...and end this function as we did in the previous one:
* 

* 
* [1.x.50]
* 
*  The following main function just calls the above functions in the order of their appearance. Apart from this, it looks just like the main functions of previous tutorial programs.
* 

* 
* [1.x.51]
* [1.x.52][1.x.53]
* 

* 
* The program performs two tasks, the first being to generate avisualization of the mapped domain, the second to compute pi by thetwo methods described. Let us first take a look at the generatedgraphics. They are generated in Gnuplot format, and can be viewed withthe commands
* [1.x.54]
* or using one of the other filenames. The second line makes sure thatthe aspect ratio of the generated output is actually 1:1, i.e. acircle is drawn as a circle on your screen, rather than as anellipse. The third line switches off the key in the graphic, as thatwill only print information (the filename) which is not that importantright now. Similarly, the fourth and fifth disable tick marks. The plotis then generated with a specific line width ("lw", here set to 4)and line type ("lt", here chosen by saying that the line should bedrawn using the RGB color "black").
* The following table shows the triangulated computational domain for  [2.x.48] , [2.x.49] , and  [2.x.50]  mappings, for the original coarse grid (left), and a onceuniformly refined grid (right).
*  [2.x.51] 
* These pictures show the obvious advantage of higher order mappings: theyapproximate the true boundary quite well also on rather coarse meshes. Todemonstrate this a little further, here is part of the upper right quartercircle of the coarse meshes with  [2.x.52]  and  [2.x.53]  mappings, where the dashedred line marks the actual circle:
*  [2.x.54] 
* Obviously the quadratic mapping approximates the boundary quite well,while for the cubic mapping the difference between approximated domainand true one is hardly visible already for the coarse grid. You canalso see that the mapping only changes something at the outerboundaries of the triangulation. In the interior, all lines are stillrepresented by linear functions, resulting in additional computationsonly on cells at the boundary. Higher order mappings are thereforeusually not noticeably slower than lower order ones, because theadditional computations are only performed on a small subset of allcells.
* 

* 
* The second purpose of the program was to compute the value of pi togood accuracy. This is the output of this part of the program:
* [1.x.55]
* 
* 

* 
* One of the immediate observations from the output is that in all cases thevalues converge quickly to the true value of [2.x.55] . Note that for the  [2.x.56]  mapping, we arealready in the regime of roundoff errors and the convergence rate levels off,which is already quite a lot. However, also note that for the  [2.x.57]  mapping,even on the finest grid the accuracy is significantly worse than on the coarsegrid for a  [2.x.58]  mapping!
* 

* 
* The last column of the output shows the convergence order, in powers of themesh width  [2.x.59] . In the introduction, we had stated that the convergence orderfor a  [2.x.60]  mapping should be  [2.x.61] . However, in the example shown, theorder is rather  [2.x.62] ! This at first surprising fact is explained by theproperties of the  [2.x.63]  mapping. At order [1.x.56], it uses support pointsthat are based on the [1.x.57]+1 point Gauss-Lobatto quadrature rule thatselects the support points in such a way that the quadrature rule converges atorder 2[1.x.58]. Even though these points are here only used for interpolationof a [1.x.59]th order polynomial, we get a superconvergence effect whennumerically evaluating the integral, resulting in the observed high order ofconvergence. (This effect is also discussed in detail in the followingpublication: A. Bonito, A. Demlow, and J. Owen: "A priori errorestimates for finite element approximations to eigenvalues andeigenfunctions of the Laplace-Beltrami operator", submitted, 2018.)
* 

* [1.x.60][1.x.61] [2.x.64] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-1_0.txt
[0.x.0]*
 [2.x.0] 
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18]
* [1.x.19][1.x.20][1.x.21]
* 

* [1.x.22][1.x.23]
* 

* Since this is the first tutorial program, let us comment first on howthis tutorial and the rest of the deal.II documentation is supposed towork. The documentation for deal.II comes essentially at threedifferent levels:
* 
*  - The tutorial: This is a collection of programs that shows how  deal.II is used in practice. It doesn't typically discuss individual  functions at the level of individual arguments, but rather wants to  give the big picture of how things work together. In other words, it  discusses "concepts": what are the building blocks of deal.II and  how are they used together in finite element programs.
* 
*  - The manual: This is the documentation of every single class and  every single (member) function in deal.II. You get there if, for  example, you click on the "Main page" or "Classes" tab at the top of  this page. This is the place where you would look up what the second  argument of  [2.x.1]  means,  to give just one slightly obscure example. You need this level of  documentation for when you know what you want to do, but forgot how  exactly the function was named, what its arguments are, or what it  returns. Note that you also get into the manual whenever you read  through the tutorial and click on any of the class or function  names, i.e. the tutorial contains a great many links into the manual  for whenever you need a more detailed description of a function or  class. On the other hand, the manual is not a good place to learn  deal.II since it gives you a microscopic view of things without  telling you how a function might fit into the bigger picture.
* 
*  - Modules: These are groups of classes and functions that work  together or have related functionality. If you click on the  "Modules" tab at the top of this page, you end up on a page that  lists a number of such groups. Each module discusses the underlying  principles of these classes; for example, the  [2.x.2]  module  talks about all sorts of different issues related to storing  sparsity patterns of matrices. This is documentation at an  intermediate level: they give you an overview of what's there in a  particular area. For example when you wonder what finite element  classes exist, you would take a look at the  [2.x.3]  module. The  modules are, of course, also cross-linked to the manual (and, at  times, to the tutorial); if you click on a class name, say on  Triangulation, would will also at the very top right under the class  name get a link to the modules this class is a member of if you want  to learn more about its context.
* Let's come back to the tutorial, since you are looking at the first program(or "step") of it. Each tutorial program is subdivided into the followingsections: [2.x.4]    [2.x.5]  [1.x.24] This is a discussion of what the program       does, including the mathematical model, and       what programming techniques are new compared to previous       tutorial programs.   [2.x.6]  [1.x.25] An extensively documented listing of the       source code. Here, we often document individual lines, or       blocks of code, and discuss what they do, how they do it, and       why. The comments frequently reference the introduction,       i.e. you have to understand [1.x.26] the program wants to achieve       (a goal discussed in the introduction) before you can       understand [1.x.27] it intends to get there.   [2.x.7]  [1.x.28] The output of the program, with comments and       interpretation. This section also frequently has a subsection       that gives suggestions on how to extend the program in various       direction; in the earlier programs, this is intended to give       you directions for little experiments designed to make your       familiar with deal.II, while in later programs it is more about       how to use more advanced numerical techniques.   [2.x.8]  [1.x.29] The source code stripped of       all comments. This is useful if you want to see the "big       picture" of the code, since the commented version of the       program has so much text in between that it is often difficult       to see the entire code of a single function on the screen at       once. [2.x.9] 
* The tutorials are not only meant to be static documentation, but youshould play with them. To this end, go to the [2.x.10]  directory (or whatever the number of thetutorial is that you're interested in) and type
* [1.x.30]
* The first command sets up the files that describe which include files thistutorial program depends on, how to compile it and how to run it. This commandshould find the installed deal.II libraries as well that were generated whenyou compiled and installed everything as described in the[1.x.31] file.If this command should fail to find the deal.II library, then you need toprovide the path to the installation using the command
* [1.x.32]
* instead.
* The second of the commands above compiles the sources into an executable, while thelast one executes it (strictly speaking,  [2.x.11]  will alsocompile the code if the executable doesn't exist yet, so you couldhave skipped the second command if you wanted). This is all that'sneeded to run the code and produce the output that is discussed in the"Results" section of the tutorial programs. This sequence needs to be repeatedin all of the tutorial directories you want to play with.
* When learning the library, you need to play with it and see whathappens. To this end, open the  [2.x.12] source file with your favorite editor and modify it in some way, save it andrun it as above. A few suggestions for possibly modifications are given at theend of the results section of this program, where we also provide a few linksto other useful pieces of information.
* 

* [1.x.33][1.x.34]
* 

* This and several of the other tutorial programs are also discussed anddemonstrated in [1.x.35] on deal.II and computational science. Inparticular, you can see the steps he executes to run this and otherprograms, and you will get a much better idea of the tools that can beused to work with deal.II. In particular, lectures 2 and 4 give an overview ofdeal.II and of the building blocks of any finite element code.( [2.x.13] 
* If you are not yet familiar with using Linux and running things on thecommand line, you may be interested in watching lectures 2.9 and 2.91.( [2.x.14] line and on what happens when compiling programs, respectively.
* Note that deal.II is actively developed, and in the course of thisdevelopment we occasionally rename or deprecate functions or classesthat are still referenced in these video lectures.  Forexample, the  [2.x.15]  code shown in video lecture 5 uses a classHyperShellBoundary which was replaced with SphericalManifold classlater on. Additionally, as of deal.II version 9.0,  [2.x.16] now automatically attaches a SphericalManifold to the Triangulation. Otherwisethe rest of the lecture material is relevant.
* [1.x.36][1.x.37]
* 

* Let's come back to  [2.x.17] , the current program.In this first example, we don't actually do very much, but show twotechniques: what is the syntax to generate triangulation objects, andsome elements of simple loops over all cells. We create two grids, onewhich is a regularly refined square (not very exciting, but a commonstarting grid for some problems), and one more geometric attempt: aring-shaped domain, which is refined towards the inner edge. Throughthis, you will get to know three things every finite element programwill have to have somewhere: An object of type Triangulation for themesh; a call to the GridGenerator functions to generate a mesh; andloops over all cells that involve iterators (iterators are ageneralization of pointers and are frequently used in the C++ standardlibrary; in the context of deal.II, the  [2.x.18]  module talksabout them).
* The program is otherwise small enough that it doesn't need a whole lotof introduction.
*  [2.x.19] 
* 

* [1.x.38][1.x.39]
* 

* If you are reading through this tutorial program, chances are that you areinterested in continuing to use deal.II for your own projects. Thus, you areabout to embark on an exercise in programming using a large-scale scientificcomputing library. Unless you are already an experienced user of large-scaleprogramming methods, this may be new territory for you &mdash; with all thenew rules that go along with it such as the fact that you will have to dealwith code written by others, that you may have to think about documenting yourown code because you may not remember what exactly it is doing a year down theroad (or because others will be using it as well), or coming up with ways totest that your program is doing the right thing. None of this is somethingthat we typically train mathematicians, engineers, or scientists in but thatis important when you start writing software of more than a few hundredlines. Remember: Producing software is not the same as just writing code.
* To make your life easier on this journey let us point to some resources thatare worthwhile browsing through before you start any large-scale programming:
* 
*  - The [1.x.40] has a good number of answers to questions about  particular aspects of deal.II, but also to more general questions such as "How  do I debug scientific computing codes?" or "Can I train myself to write code  that has fewer bugs?".
* 
*  - You will benefit from becoming a better programmer. An excellent  resource to this end is the book  [Code Complete](https://en.wikipedia.org/wiki/Code_Complete)  by Steve McConnell  [2.x.20]  . It's already  a few years old, with the last edition published in 2004, but it has  lost none of its appeal as a guide to good programming practices,  and some of the principal developers use it as a group reading  project with every generation of their research group members.
* 
*  - The [1.x.41]  that provides introductions to many topics that are important to dealing  with software, such as version control, make files, testing, etc. It is  specifically written for scientists and engineers, not for computer  scientists, and has a focus on short, practical lessons.
* 
*  - The [1.x.42] has a lot of resources (and interesting blog posts) that  cover many aspects of writing scientific software.
* 
*  - The [1.x.43] also has resources on software development, in  particular for parallel computing. In the "Events" section on  that site are recorded tutorials and webinars that cover many  interesting topics.
* 
*  - An article on [1.x.44] that gives an introduction to  many of the ways by which you can make sure you are an efficient  programmer writing programs that work.
* As a general recommendation: If you expect to spend more than a few dayswriting software in the future, do yourself the favor of learning tools thatcan make your life more productive, in particular debuggers and integrateddevelopment environments. ( [2.x.21] You will find that you will get the time spentlearning these tools back severalfold soon by being more productive!Several of the video lectures referenced above show how to use toolssuch as integrated development environments or debuggers.
* 

*  [1.x.45] [1.x.46]
*   [1.x.47]  [1.x.48]
* 

* 
*  The most fundamental class in the library is the Triangulation class, which is declared here:
* 

* 
* [1.x.49]
* 
*  Here are some functions to generate standard grids:
* 

* 
* [1.x.50]
* 
*  Output of grids in various graphics formats:
* 

* 
* [1.x.51]
* 
*  This is needed for C++ output:
* 

* 
* [1.x.52]
* 
*  And this for the declarations of the  [2.x.22]  and  [2.x.23]  functions:
* 

* 
* [1.x.53]
* 
*  The final step in importing deal.II is this: All deal.II functions and classes are in a namespace  [2.x.24] , to make sure they don't clash with symbols from other libraries you may want to use in conjunction with deal.II. One could use these functions and classes by prefixing every use of these names by  [2.x.25] , but that would quickly become cumbersome and annoying. Rather, we simply import the entire deal.II namespace for general use:
* 

* 
* [1.x.54]
* 
*   [1.x.55]  [1.x.56]
* 

* 
*  In the following, first function, we simply use the unit square as domain and produce a globally refined grid from it.
* 

* 
* [1.x.57]
* 
*  The first thing to do is to define an object for a triangulation of a two-dimensional domain:
* 

* 
* [1.x.58]
* 
*  Here and in many following cases, the string "<2>" after a class name indicates that this is an object that shall work in two space dimensions. Likewise, there are versions of the triangulation class that are working in one ("<1>") and three ("<3>") space dimensions. The way this works is through some template magic that we will investigate in some more detail in later example programs; there, we will also see how to write programs in an essentially dimension independent way.
* 

* 
*  Next, we want to fill the triangulation with a single cell for a square domain. The triangulation is the refined four times, to yield  [2.x.26]  cells in total:
* 

* 
* [1.x.59]
* 
*  Now we want to write a graphical representation of the mesh to an output file. The GridOut class of deal.II can do that in a number of different output formats; here, we choose scalable vector graphics (SVG) format that you can visualize using the web browser of your choice:
* 

* 
* [1.x.60]
* 
*   [1.x.61]  [1.x.62]
* 

* 
*  The grid in the following, second function is slightly more complicated in that we use a ring domain and refine the result once globally.
* 

* 
* [1.x.63]
* 
*  We start again by defining an object for a triangulation of a two-dimensional domain:
* 

* 
* [1.x.64]
* 
*  We then fill it with a ring domain. The center of the ring shall be the point (1,0), and inner and outer radius shall be 0.5 and 1. The number of circumferential cells could be adjusted automatically by this function, but we choose to set it explicitly to 10 as the last argument:
* 

* 
* [1.x.65]
* 
*  By default, the triangulation assumes that all boundaries are straight lines, and all cells are bi-linear quads or tri-linear hexes, and that they are defined by the cells of the coarse grid (which we just created). Unless we do something special, when new points need to be introduced the domain is assumed to be delineated by the straight lines of the coarse mesh, and new points will simply be in the middle of the surrounding ones. Here, however, we know that the domain is curved, and we would like to have the Triangulation place new points according to the underlying geometry. Fortunately, some good soul implemented an object which describes a spherical domain, of which the ring is a section; it only needs the center of the ring and automatically figures out how to instruct the Triangulation where to place the new points. The way this works in deal.II is that you tag parts of the triangulation you want to be curved with a number that is usually referred to as "manifold indicator" and then tell the triangulation to use a particular "manifold object" for all places with this manifold indicator. How exactly this works is not important at this point (you can read up on it in  [2.x.27]  and  [2.x.28] ). The functions in GridGenerator handle this for us in most circumstances: they attach the correct manifold to a domain so that when the triangulation is refined new cells are placed in the correct places. In the present case  [2.x.29]  attaches a SphericalManifold to all cells: this causes cells to be refined with calculations in spherical coordinates (so new cells have edges that are either radial or lie along concentric circles around the origin).   
*   By default (i.e., for a Triangulation created by hand or without a call to a GridGenerator function like  [2.x.30]  or  [2.x.31]  all cells and faces of the Triangulation have their manifold_id set to  [2.x.32]  which is the default if you want a manifold that produces straight edges, but you can change this number for individual cells and faces. In that case, the curved manifold thus associated with number zero will not apply to those parts with a non-zero manifold indicator, but other manifold description objects can be associated with those non-zero indicators. If no manifold description is associated with a particular manifold indicator, a manifold that produces straight edges is implied. (Manifold indicators are a slightly complicated topic; if you're confused about what exactly is happening here, you may want to look at the  [2.x.33]  "glossary entry on this topic".) Since the default chosen by  [2.x.34]  is reasonable we leave things alone.   
*   In order to demonstrate how to write a loop over all cells, we will refine the grid in five steps towards the inner circle of the domain:
* 

* 
* [1.x.66]
* 
*  Next, we need to loop over the active cells of the triangulation. You can think of a triangulation as a collection of cells. If it were an array, you would just get a pointer that you increment from one element to the next using the operator `++`. The cells of a triangulation aren't stored as a simple array, but the concept of an [1.x.67] generalizes how pointers work to arbitrary collections of objects (see [1.x.68] for more information). Typically, any container type in C++ will return an iterator pointing to the start of the collection with a method called `begin`, and an iterator point to 1 past the end of the collection with a method called `end`. We can increment an iterator `it` with the operator `++it`, dereference it to get the underlying data with `*it`, and check to see if we're done by comparing `it != collection.end()`.       
*   The second important piece is that we only need the active cells. Active cells are those that are not further refined, and the only ones that can be marked for further refinement. deal.II provides iterator categories that allow us to iterate over [1.x.69] cells (including the parent cells of active ones) or only over the active cells. Because we want the latter, we need to call the method  [2.x.35]        
*   Putting all of this together, we can loop over all the active cells of a triangulation with  [2.x.36]  In the initializer of this loop, we've used the `auto` keyword for the type of the iterator `it`. The `auto` keyword means that the type of the object being declared will be inferred from the context. This keyword is useful when the actual type names are long or possibly even redundant. If you're unsure of what the type is and want to look up what operations the result supports, you can go to the documentation for the method  [2.x.37]  In this case, the type of `it` is  [2.x.38]        
*   While the `auto` keyword can save us from having to type out long names of data types, we still have to type a lot of redundant declarations about the start and end iterator and how to increment it. Instead of doing that, we'll use [1.x.71], which wrap up all of the syntax shown above into a much shorter form:
* 

* 
* [1.x.72]
* 
* 

* 
*  [2.x.39]  See  [2.x.40]  for more information about the iterator classes used in deal.II, and  [2.x.41]  for more information about range-based for loops and the `auto` keyword.           
*   Next, we loop over all vertices of the cells. For that purpose we query an iterator over the vertex indices (in 2d, this is an array that contains the elements `{0,1,2,3}`, but since `cell->vertex_indices()` knows the dimension the cell lives in, the array so returned is correct in all dimensions and this enables this code to be correct whether we run it in 2d or 3d, i.e., it enables "dimension-independent programming"
* 
*  -  a big part of what we will discuss in  [2.x.42] ).
* 

* 
* [1.x.73]
* 
*  If this cell is at the inner boundary, then at least one of its vertices must sit on the inner ring and therefore have a radial distance from the center of exactly 0.5, up to floating point accuracy. So we compute this distance, and if we find a vertex with this property, we flag this cell for later refinement. We can then also break the loop over all vertices and move on to the next cell.               
*   Because the distance from the center is computed as a floating point number, we have to expect that whatever we compute is only accurate to within [round-off](https://en.wikipedia.org/wiki/Round-off_error). As a consequence, we can never expect to compare the distance with the inner radius by equality: A statement such as `if (distance_from_center == inner_radius)` will fail unless we get exceptionally lucky. Rather, we need to do this comparison with a certain tolerance, and the usual way to do this is to write it as `if  [2.x.43] 
* 
*  - inner_radius) <= tolerance)` where `tolerance` is some small number larger than round-off. The question is how to choose it: We could just pick, say, `1e-10`, but this is only appropriate if the objects we compare are of size one. If we had created a mesh with cells of size `1e+10`, then `1e-10` would be far lower than round-off and, as before, the comparison will only succeed if we get exceptionally lucky. Rather, it is almost always useful to make the tolerancerelative* to a typical "scale" of the objects being compared. Here, the "scale" would be the inner radius, or maybe the diameter of cells. We choose the former and set the tolerance equal to  [2.x.44]  times the inner radius of the annulus.
* 

* 
* [1.x.74]
* 
*  Now that we have marked all the cells that we want refined, we let the triangulation actually do this refinement. The function that does so owes its long name to the fact that one can also mark cells for coarsening, and the function does coarsening and refinement all at once:
* 

* 
* [1.x.75]
* 
*  Finally, after these five iterations of refinement, we want to again write the resulting mesh to a file, again in SVG format. This works just as above:
* 

* 
* [1.x.76]
* 
*   [1.x.77]  [1.x.78]
* 

* 
*  Finally, the main function. There isn't much to do here, only to call the two subfunctions, which produce the two grids.
* 

* 
* [1.x.79]
* [1.x.80][1.x.81]
* 

* Running the program produces graphics of two grids (grid-1.svg and grid-2.svg).You can open these with most every web browser
* 
*  -  in the simplest case,just open the current directory in your file system explorer and clickon the file. If you like working on the command line, you call yourweb browser with the file: `firefox grid-1.svg`, `google-chrome grid-1.svg`,or whatever the name of your browser is. If you do this, the two meshesshould look like this:
*  [2.x.45] 
* The left one, well, is not very exciting. The right one is &mdash; at least&mdash; unconventional. The pictures color-code the "refinement level" of eachcell: How many times did a coarse mesh cell have to be subdivided to obtainthe given cell. In the left image, this is boring since the mesh wasrefined globally a number of times, i.e., [1.x.82] cell wasrefined the same number of times.
* (While the second mesh is entirely artificial and made-up, andcertainly not very practical in applications, to everyone's surprise ithas found its way into the literature: see  [2.x.46] . Apparently it isgood for some things at least.)
* 

* [1.x.83][1.x.84]
* 

* [1.x.85][1.x.86]
* 

* This program obviously does not have a whole lot of functionality, butin particular the  [2.x.47]  function has a bunch ofplaces where you can play with it. For example, you could modify thecriterion by which we decide which cells to refine. An example wouldbe to change the condition to this:
* [1.x.87]
* This would refine all cells for which the  [2.x.48] -coordinate of the cell'scenter is greater than zero (the  [2.x.49] function that we call by dereferencing the  [2.x.50]  iteratorreturns a Point<2> object; subscripting  [2.x.51]  would givethe  [2.x.52] -coordinate, subscripting  [2.x.53]  the [2.x.54] -coordinate). By looking at the functions that TriaAccessorprovides, you can also use more complicated criteria for refinement.
* In general, what you can do with operations of the form`cell->something()` is a bit difficult to find in the documentationbecause `cell` is not a pointer but an iterator. The functions you cancall on a cell can be found in the documentation of the classes`TriaAccessor` (which has functions that can also be called on facesof cells or, more generally, all sorts of geometric objects thatappear in a triangulation), and `CellAccessor` (which adds a fewfunctions that are specific tocells*).
* A more thorough description of the whole iterator concept can be foundin the  [2.x.55]  documentation module.
* 

* [1.x.88][1.x.89]
* 

* Another possibility would be to generate meshes of entirely differentgeometries altogether. While for complex geometries there is no way aroundusing meshes obtained from mesh generators, there is a good number ofgeometries for which deal.II can create meshes using the functions in theGridGenerator namespace. Many of these geometries (such as the one used in thisexample program) contain cells with curved faces: put another way, we expect thenew vertices placed on the boundary to lie along a circle. deal.II handles complexgeometries with the Manifold class (and classes inheriting from it); in particular,the functions in GridGenerator corresponding to non-Cartesian grids (such as [2.x.56]  or  [2.x.57]  attach a Manifoldobject to the part of the triangulation that should be curved (SphericalManifoldand CylindricalManifold, respectively) and use another manifold on the parts thatshould be flat (FlatManifold). See the documentationof Manifold or the  [2.x.58]  "manifold module" for descriptions of the designphilosophy and interfaces of these classes. Take a look at what they provide andsee how they could be used in a program like this.
* We also discuss a variety of other ways to create and manipulate meshes (anddescribe the process of attaching Manifolds) in  [2.x.59] .
* 

* [1.x.90][1.x.91]
* 

* We close with a comment about modifying or writing programs with deal.II ingeneral. When you start working with tutorial programs or your ownapplications, you will find that mistakes happen: your program will containcode that either aborts the program right away or bugs that simply lead towrong results. In either case, you will find it extremely helpful to know howto work with a debugger: you may get by for a while by just putting debugoutput into your program, compiling it, and running it, but ultimately findingbugs with a debugger is much faster, much more convenient, and more reliablebecause you don't have to recompile the program all the time and because youcan inspect the values of variables and how they change.
* Rather than postponing learning how to use a debugger till you really can'tsee any other way to find a bug, here's the one piece ofadvice we will provide in this program: learn how to use a debugger as soon aspossible. It will be time well invested.( [2.x.60] Questions (FAQ) page linked to from the top-level [1.x.92] also provides a good numberof hints on debugging deal.II programs.
* 

* [1.x.93][1.x.94]
* 

* It is often useful to include meshes into your theses or publications.For this, it may not be very useful to color-code the cells byrefinement level, and to print the cell number onto each cell. Butit doesn't have to be that way
* 
*  -  the GridOut class allows setting flagsfor each possible output format (see the classes in the GridOutFlagsnamespace) that control how exactly a mesh is plotted. You can ofcourse also choose other output file formats such as VTK or VTU; thisis particularly useful for 3d meshes where a 2d format such as SVGis not particular useful because it fixes a particular viewpoint ontothe 3d object. As a consequence, you might want to explore otheroptions in the GridOut class.
* 

* [1.x.95][1.x.96] [2.x.61] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-11_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5]
* [1.x.6][1.x.7][1.x.8]
* 

* The problem we will be considering is the solution of Laplace's problem withNeumann boundary conditions only:[1.x.9]
* It is well known that if this problem is to have a solution, then the forcesneed to satisfy the compatibility condition[1.x.10]We will consider the special case that  [2.x.2]  is the circle of radius 1around the origin, and  [2.x.3] ,  [2.x.4] . This choice satisfies the compatibilitycondition.
* The compatibility condition allows a solution of the above equation, but itnevertheless retains an ambiguity: since only derivatives of the solutionappear in the equations, the solution is only determined up to a constant. Forthis reason, we have to pose another condition for the numerical solution,which fixes this constant.
* For this, there are various possibilities: [2.x.5]  [2.x.6]  Fix one node of the discretization to zero or any other fixed value.  This amounts to an additional condition  [2.x.7] . Although this is  common practice, it is not necessarily a good idea, since we know that the  solutions of Laplace's equation are only in  [2.x.8] , which does not allow for  the definition of point values because it is not a subset of the continuous  functions. Therefore, even though fixing one node is allowed for  discretized functions, it is not for continuous functions, and one can  often see this in a resulting error spike at this point in the numerical  solution.
*  [2.x.9]  Fixing the mean value over the domain to zero or any other value. This  is allowed on the continuous level, since  [2.x.10]   by Sobolev's inequality, and thus also on the discrete level since we  there only consider subsets of  [2.x.11] .
*  [2.x.12]  Fixing the mean value over the boundary of the domain to zero or any  other value. This is also allowed on the continuous level, since   [2.x.13] , again by Sobolev's  inequality. [2.x.14] We will choose the last possibility, since we want to demonstrate anothertechnique with it.
* While this describes the problem to be solved, we still have to figure out howto implement it. Basically, except for the additional mean value constraint,we have solved this problem several times, using Dirichlet boundary values,and we only need to drop the treatment of Dirichlet boundary nodes. The use ofhigher order mappings is also rather trivial and will be explained at thevarious places where we use it; in almost all conceivable cases, you will onlyconsider the objects describing mappings as a black box which you need notworry about, because their only uses seem to be to be passed to places deepinside the library where functions know how to handle them (i.e. in the [2.x.15]  classes and their descendants).
* The tricky point in this program is the use of the mean valueconstraint. Fortunately, there is a class in the library which knows how tohandle such constraints, and we have used it quite often already, withoutmentioning its generality. Note that if we assume that the boundary nodes arespaced equally along the boundary, then the mean value constraint[1.x.11]can be written as[1.x.12]where the sum shall run over all degree of freedom indices which are locatedon the boundary of the computational domain. Let us denote by  [2.x.16]  that indexon the boundary with the lowest number (or any other conveniently chosenindex), then the constraint can also be represented by[1.x.13]This, luckily, is exactly the form of constraints for which theAffineConstraints class was designed. Note that we have used thisclass in several previous examples for the representation of hanging nodesconstraints, which also have this form: there, the middle vertex shall havethe mean of the values of the adjacent vertices. In general, theAffineConstraints class is designed to handle affine constraintsof the form[1.x.14]where  [2.x.17]  denotes a matrix,  [2.x.18]  denotes a vector, and  [2.x.19]  the vector of nodalvalues. In this case, since  [2.x.20]  represents one homogeneous constraint,  [2.x.21]  isthe zero vector.
* In this example, the mean value along the boundary allows just such arepresentation, with  [2.x.22]  being a matrix with just one row (i.e. there is onlyone constraint). In the implementation, we will create an AffineConstraintsobject, add one constraint (i.e. add another row to the matrix) referring to thefirst boundary node  [2.x.23] , and insert the weights with which all the other nodescontribute, which in this example happens to be just  [2.x.24] .
* Later, we will use this object to eliminate the first boundary node from thelinear system of equations, reducing it to one which has a solution withoutthe ambiguity of the constant shift value. One of the problems of theimplementation will be that the explicit elimination of this node results in anumber of additional elements in the matrix, of which we do not know inadvance where they are located and how many additional entries will be in eachof the rows of the matrix. We will show how we can use an intermediate objectto work around this problem.
* But now on to the implementation of the program solving this problem...
* 

*  [1.x.15] [1.x.16]
*  As usual, the program starts with a rather long list of include files which you are probably already used to by now:
* 

* 
* [1.x.17]
* 
*  Just this one is new: it declares a class DynamicSparsityPattern, which we will use and explain further down below.
* 

* 
* [1.x.18]
* 
*  We will make use of the  [2.x.25]  algorithm of the C++ standard library, so we have to include the following file for its declaration:
* 

* 
* [1.x.19]
* 
*  The last step is as in all previous programs:
* 

* 
* [1.x.20]
* 
*  Then we declare a class which represents the solution of a Laplace problem. As this example program is based on  [2.x.26] , the class looks rather the same, with the sole structural difference that the functions  [2.x.27]  itself, and is thus called  [2.x.28] , and that the output function was dropped since the solution function is so boring that it is not worth being viewed.   
*   The only other noteworthy change is that the constructor takes a value representing the polynomial degree of the mapping to be used later on, and that it has another member variable representing exactly this mapping. In general, this variable will occur in real applications at the same places where the finite element is declared or used.
* 

* 
* [1.x.21]
* 
*  Construct such an object, by initializing the variables. Here, we use linear finite elements (the argument to the  [2.x.29]  variable denotes the polynomial degree), and mappings of given order. Print to screen what we are about to do.
* 

* 
* [1.x.22]
* 
*  The first task is to set up the variables for this problem. This includes generating a valid  [2.x.30]  object, as well as the sparsity patterns for the matrix, and the object representing the constraints that the mean value of the degrees of freedom on the boundary be zero.
* 

* 
* [1.x.23]
* 
*  The first task is trivial: generate an enumeration of the degrees of freedom, and initialize solution and right hand side vector to their correct sizes:
* 

* 
* [1.x.24]
* 
*  The next task is to construct the object representing the constraint that the mean value of the degrees of freedom on the boundary shall be zero. For this, we first want a list of those nodes that are actually at the boundary. The  [2.x.31]  namespace has a function that returns an IndexSet object that contains the indices of all those degrees of freedom that are at the boundary.     
*   Once we have this index set, we wanted to know which is the first index corresponding to a degree of freedom on the boundary. We need this because we wanted to constrain one of the nodes on the boundary by the values of all other DoFs on the boundary. To get the index of this "first" degree of freedom is easy enough using the IndexSet class:
* 

* 
* [1.x.25]
* 
*  Then generate a constraints object with just this one constraint. First clear all previous content (which might reside there from the previous computation on a once coarser grid), then add this one line constraining the  [2.x.32]  to the sum of other boundary DoFs each with weight
* 
*  - . Finally, close the constraints object, i.e. do some internal bookkeeping on it for faster processing of what is to come later:
* 

* 
* [1.x.26]
* 
*  Next task is to generate a sparsity pattern. This is indeed a tricky task here. Usually, we just call  [2.x.33]  and condense the result using the hanging node constraints. We have no hanging node constraints here (since we only refine globally in this example), but we have this global constraint on the boundary. This poses one severe problem in this context: the  [2.x.34]  class wants us to state beforehand the maximal number of entries per row, either for all rows or for each row separately. There are functions in the library which can tell you this number in case you just have hanging node constraints (namely  [2.x.35]  but how is this for the present case? The difficulty arises because the elimination of the constrained degree of freedom requires a number of additional entries in the matrix at places that are not so simple to determine. We would therefore have a problem had we to give a maximal number of entries per row here.     
*   Since this can be so difficult that no reasonable answer can be given that allows allocation of only a reasonable amount of memory, there is a class DynamicSparsityPattern, that can help us out here. It does not require that we know in advance how many entries rows could have, but allows just about any length. It is thus significantly more flexible in case you do not have good estimates of row lengths, however at the price that building up such a pattern is also significantly more expensive than building up a pattern for which you had information in advance. Nevertheless, as we have no other choice here, we'll just build such an object by initializing it with the dimensions of the matrix and calling another function  [2.x.36]  to get the sparsity pattern due to the differential operator, then condense it with the constraints object which adds those positions in the sparsity pattern that are required for the elimination of the constraint.
* 

* 
* [1.x.27]
* 
*  Finally, once we have the full pattern, we can initialize an object of type  [2.x.37]  from it and in turn initialize the matrix with it. Note that this is actually necessary, since the DynamicSparsityPattern is so inefficient compared to the  [2.x.38]  class due to the more flexible data structures it has to use, that we can impossibly base the sparse matrix class on it, but rather need an object of type  [2.x.39] , which we generate by copying from the intermediate object.     
*   As a further sidenote, you will notice that we do not explicitly have to  [2.x.40]  the sparsity pattern here. This, of course, is due to the fact that the  [2.x.41]  function generates a compressed object right from the start, to which you cannot add new entries anymore. The  [2.x.42]  call is therefore implicit in the  [2.x.43]  call.
* 

* 
* [1.x.28]
* 
*  The next function then assembles the linear system of equations, solves it, and evaluates the solution. This then makes three actions, and we will put them into eight true statements (excluding declaration of variables, and handling of temporary vectors). Thus, this function is something for the very lazy. Nevertheless, the functions called are rather powerful, and through them this function uses a good deal of the whole library. But let's look at each of the steps.
* 

* 
* [1.x.29]
* 
*  First, we have to assemble the matrix and the right hand side. In all previous examples, we have investigated various ways how to do this manually. However, since the Laplace matrix and simple right hand sides appear so frequently in applications, the library provides functions for actually doing this for you, i.e. they perform the loop over all cells, setting up the local matrices and vectors, and putting them together for the end result.     
*   The following are the two most commonly used ones: creation of the Laplace matrix and creation of a right hand side vector from body or boundary forces. They take the mapping object, the  [2.x.44]  object representing the degrees of freedom and the finite element in use, a quadrature formula to be used, and the output object. The function that creates a right hand side vector also has to take a function object describing the (continuous) right hand side function.     
*   Let us look at the way the matrix and body forces are integrated:
* 

* 
* [1.x.30]
* 
*  That's quite simple, right?     
*   Two remarks are in order, though: First, these functions are used in a lot of contexts. Maybe you want to create a Laplace or mass matrix for a vector values finite element; or you want to use the default Q1 mapping; or you want to assembled the matrix with a coefficient in the Laplace operator. For this reason, there are quite a large number of variants of these functions in the  [2.x.45]  and  [2.x.46]  namespaces. Whenever you need a slightly different version of these functions than the ones called above, it is certainly worthwhile to take a look at the documentation and to check whether something fits your needs.     
*   The second remark concerns the quadrature formula we use: we want to integrate over bilinear shape functions, so we know that we have to use at least an order two Gauss quadrature formula. On the other hand, we want the quadrature rule to have at least the order of the boundary approximation. Since the order of Gauss rule with  [2.x.47]  points is  [2.x.48] , and the order of the boundary approximation using polynomials of degree  [2.x.49]  is  [2.x.50] , we know that  [2.x.51] . Since r has to be an integer and (as mentioned above) has to be at least  [2.x.52] , this makes up for the formula above computing  [2.x.53] .     
*   Since the generation of the body force contributions to the right hand side vector was so simple, we do that all over again for the boundary forces as well: allocate a vector of the right size and call the right function. The boundary function has constant values, so we can generate an object from the library on the fly, and we use the same quadrature formula as above, but this time of lower dimension since we integrate over faces now instead of cells:
* 

* 
* [1.x.31]
* 
*  Then add the contributions from the boundary to those from the interior of the domain:
* 

* 
* [1.x.32]
* 
*  For assembling the right hand side, we had to use two different vector objects, and later add them together. The reason we had to do so is that the  [2.x.54]  and  [2.x.55]  functions first clear the output vector, rather than adding up their results to previous contents. This can reasonably be called a design flaw in the library made in its infancy, but unfortunately things are as they are for some time now and it is difficult to change such things that silently break existing code, so we have to live with that.
* 

* 
*  Now, the linear system is set up, so we can eliminate the one degree of freedom which we constrained to the other DoFs on the boundary for the mean value constraint from matrix and right hand side vector, and solve the system. After that, distribute the constraints again, which in this case means setting the constrained degree of freedom to its proper value
* 

* 
* [1.x.33]
* 
*  Finally, evaluate what we got as solution. As stated in the introduction, we are interested in the H1 semi-norm of the solution. Here, as well, we have a function in the library that does this, although in a slightly non-obvious way: the  [2.x.56]  function integrates the norm of the difference between a finite element function and a continuous function. If we therefore want the norm of a finite element field, we just put the continuous function to zero. Note that this function, just as so many other ones in the library as well, has at least two versions, one which takes a mapping as argument (which we make us of here), and the one which we have used in previous examples which implicitly uses  [2.x.57] .  Also note that we take a quadrature formula of one degree higher, in order to avoid superconvergence effects where the solution happens to be especially close to the exact solution at certain points (we don't know whether this might be the case here, but there are cases known of this, and we just want to make sure):
* 

* 
* [1.x.34]
* 
*  Then, the function just called returns its results as a vector of values each of which denotes the norm on one cell. To get the global norm, we do the following:
* 

* 
* [1.x.35]
* 
*  Last task
* 
*  -  generate output:
* 

* 
* [1.x.36]
* 
*  The following function solving the linear system of equations is copied from  [2.x.58]  and is explained there in some detail:
* 

* 
* [1.x.37]
* 
*  Next, we write the solution as well as the material ids to a VTU file. This is similar to what was done in many other tutorial programs. The new ingredient presented in this tutorial program is that we want to ensure that the data written to the file used for visualization is actually a faithful representation of what is used internally by deal.II. That is because most of the visualization data formats only represent cells by their vertex coordinates, but have no way of representing the curved boundaries that are used in deal.II when using higher order mappings
* 
*  -  in other words, what you see in the visualization tool is not actually what you are computing on. (The same, incidentally, is true when using higher order shape functions: Most visualization tools only render bilinear/trilinear representations. This is discussed in detail in  [2.x.59]    
*   So we need to ensure that a high-order representation is written to the file. We need to consider two particular topics. Firstly, we tell the DataOut object via the  [2.x.60]  that we intend to interpret the subdivisions of the elements as a high-order Lagrange polynomial rather than a collection of bilinear patches. Recent visualization programs, like ParaView version 5.5 or newer, can then render a high-order solution (see a [1.x.38] for more details). Secondly, we need to make sure that the mapping is passed to the  [2.x.61]  method. Finally, the DataOut class only prints curved faces for [1.x.39] cells by default, so we need to ensure that also inner cells are printed in a curved representation via the mapping.
* 

* 
* [1.x.40]
* 
*  Finally the main function controlling the different steps to be performed. Its content is rather straightforward, generating a triangulation of a circle, associating a boundary to it, and then doing several cycles on subsequently finer grids. Note again that we have put mesh refinement into the loop header; this may be something for a test program, but for real applications you should consider that this implies that the mesh is refined after the loop is executed the last time since the increment clause (the last part of the three-parted loop header) is executed before the comparison part (the second one), which may be rather costly if the mesh is already quite refined. In that case, you should arrange code such that the mesh is not further refined after the last loop run (or you should do it at the beginning of each run except for the first one).
* 

* 
* [1.x.41]
* 
*  After all the data is generated, write a table of results to the screen:
* 

* 
* [1.x.42]
* 
*  Finally the main function. It's structure is the same as that used in several of the previous examples, so probably needs no more explanation.
* 

* 
* [1.x.43]
* 
*  This is the main loop, doing the computations with mappings of linear through cubic mappings. Note that since we need the object of type  [2.x.62]  only once, we do not even name it, but create an unnamed such object and call the  [2.x.63]  function of it, subsequent to which it is immediately destroyed again.
* 

* 
* [1.x.44]
* [1.x.45][1.x.46]
* 

* This is what the program outputs:
* [1.x.47]
* As we expected, the convergence order for each of the differentmappings is clearly quadratic in the mesh size. What  [2.x.64] is [2.x.65] interesting, though, is that the error for a bilinear mapping(i.e. degree 1) is more than three times larger than that for thehigher order mappings; it is therefore clearly advantageous in thiscase to use a higher order mapping, not because it improves the orderof convergence but just to reduce the constant before the convergenceorder. On the other hand, using a cubic mapping only improves theresult further by insignificant amounts, except on very coarsegrids.
* We can also visualize the underlying meshes by using, for instance,ParaView. The image below shows initial meshes for different mappingdegrees:
*  [2.x.66] 
* Clearly, the effect is most pronounced when we go from the linear toquadratic mapping. This is also reflected in the error values givenin the table above. The effect of going from quadratic to cubic degreeis less dramatic, but still tangible owing to a more accuratedescription of the circular boundary.
* Next, let's look at the meshes after three global refinements
*  [2.x.67] 
* Here, the differences are much less visible, especially for higher ordermappings. Indeed, at this refinement level the error values reportedin the table are essentially identical between mappings of degrees twoand three.
* 

* [1.x.48][1.x.49] [2.x.68] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-12_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15]
*  [2.x.2] 
* [1.x.16]
* [1.x.17][1.x.18][1.x.19]
* 

* [1.x.20][1.x.21]
* 

* This example is devoted to the  [2.x.3] discontinuousGalerkin method [2.x.4] , or in short, the DG method. It includes the following topics. [2.x.5]    [2.x.6]  Discretization of the linear advection equation with the DG method.   [2.x.7]  Assembling of jump terms and other expressions on the interface between cells using FEInterfaceValues.   [2.x.8]  Assembling of the system matrix using the  [2.x.9]  [2.x.10] 
* The particular concern of this program are the loops of DG methods. These turnout to be especially complex, primarily because for the face terms, we have todistinguish the cases of boundary, regular interior faces and interior faceswith hanging nodes, respectively. The  [2.x.11]  handles thecomplexity on iterating over cells and faces and allows specifying "workers"for the different cell and face terms. The integration of face terms itself,including on adaptively refined faces, is done using the FEInterfaceValuesclass.
* [1.x.22][1.x.23]
* 

* The model problem solved in this example is the linear advection equation[1.x.24]subject to the boundary conditions[1.x.25]on the inflow part  [2.x.12]  of the boundary  [2.x.13] of the domain.  Here,  [2.x.14]  denotes avector field,  [2.x.15]  the (scalar) solutionfunction,  [2.x.16]  a boundary value function,[1.x.26]the inflow part of the boundary of the domain and  [2.x.17]  denotesthe unit outward normal to the boundary  [2.x.18] . This equation is theconservative version of the advection equation already considered in [2.x.19]  of this tutorial.
* 

* On each cell  [2.x.20] , we multiply by a test function  [2.x.21]  from the left and integrate by partsto get:[1.x.27]When summing this expression over all cells  [2.x.22] , the boundary integral is done overall internal and external faces and as such there are three cases: [2.x.23]  [2.x.24]  outer boundary on the inflow (we replace  [2.x.25]  by given  [2.x.26] ):   [2.x.27]  [2.x.28]  outer boundary on the outflow:   [2.x.29]  [2.x.30]  inner faces (integral from two sides turns into jump, we use the upwind velocity):   [2.x.31]  [2.x.32] 
* Here, the jump is defined as  [2.x.33] , where the superscripts referto the left ('+') and right ('-') values at the face. The upwind value [2.x.34]  is defined to be  [2.x.35]  if  [2.x.36]  and  [2.x.37]  otherwise.
* As a result, the mesh-dependent weak form reads:[1.x.28]Here,  [2.x.38]  is the set of all active cells of the triangulationand  [2.x.39]  is the set of all active interior faces. This formulationis known as the upwind discontinuous Galerkin method.
* In order to implement this bilinear form, we need to compute the cell terms(first sum) using the usual way to achieve integration on a cell, the interface terms (second sum) usingFEInterfaceValues, and the boundary terms (the other two terms).The summation of all those is done by  [2.x.40] 
* 

* 
* [1.x.29][1.x.30]
* 

* We solve the advection equation on [2.x.41]  with  [2.x.42] representing a circular counterclockwise flow field, and  [2.x.43] on  [2.x.44]  and  [2.x.45]  on  [2.x.46] .
* We solve on a sequence of meshes by refining the mesh adaptively by estimatingthe norm of the gradient on each cell. After solving on each mesh, we outputthe solution in vtk format and compute the  [2.x.47]  norm of the solution. Asthe exact solution is either 0 or 1, we can measure the magnitude of theovershoot of the numerical solution with this.
* 

*  [1.x.31] [1.x.32]
*  The first few files have already been covered in previous examples and will thus not be further commented on:
* 

* 
* [1.x.33]
* 
*  Here the discontinuous finite elements are defined. They are used in the same way as all other finite elements, though
* 
*  -  as you have seen in previous tutorial programs
* 
*  -  there isn't much user interaction with finite element classes at all: they are passed to  [2.x.48]  and  [2.x.49]  objects, and that is about it.
* 

* 
* [1.x.34]
* 
*  This header is needed for FEInterfaceValues to compute integrals on interfaces:
* 

* 
* [1.x.35]
* 
*  We are going to use the simplest possible solver, called Richardson iteration, that represents a simple defect correction. This, in combination with a block SSOR preconditioner (defined in precondition_block.h), that uses the special block matrix structure of system matrices arising from DG discretizations.
* 

* 
* [1.x.36]
* 
*  We are going to use gradients as refinement indicator.
* 

* 
* [1.x.37]
* 
*  Finally, the new include file for using the mesh_loop from the MeshWorker framework
* 

* 
* [1.x.38]
* 
*  Like in all programs, we finish this section by including the needed C++ headers and declaring we want to use objects in the dealii namespace without prefix.
* 

* 
* [1.x.39]
* 
*   [1.x.40]  [1.x.41]   
*   First, we define a class describing the inhomogeneous boundary data. Since only its values are used, we implement value_list(), but leave all other functions of Function undefined.
* 

* 
* [1.x.42]
* 
*  Given the flow direction, the inflow boundary of the unit square  [2.x.50]  are the right and the lower boundaries. We prescribe discontinuous boundary values 1 and 0 on the x-axis and value 0 on the right boundary. The values of this function on the outflow boundaries will not be used within the DG scheme.
* 

* 
* [1.x.43]
* 
*  Finally, a function that computes and returns the wind field  [2.x.51] . As explained in the introduction, we will use a rotational field around the origin in 2d. In 3d, we simply leave the  [2.x.52] -component unset (i.e., at zero), whereas the function can not be used in 1d in its current implementation:
* 

* 
* [1.x.44]
* 
*   [1.x.45]  [1.x.46]   
*   The following objects are the scratch and copy objects we use in the call to  [2.x.53]  The new object is the FEInterfaceValues object, that works similar to FEValues or FEFacesValues, except that it acts on an interface between two cells and allows us to assemble the interface terms in our weak form.
* 

* 
*  

* 
* [1.x.47]
* 
*   [1.x.48]  [1.x.49]   
*   After this preparations, we proceed with the main class of this program, called AdvectionProblem.   
*   This should all be pretty familiar to you. Interesting details will only come up in the implementation of the assemble function.
* 

* 
* [1.x.50]
* 
*  Furthermore we want to use DG elements.
* 

* 
* [1.x.51]
* 
*  The next four members represent the linear system to be solved.  [2.x.54]  are generated by  [2.x.55]  is computed in  [2.x.56]  is used to determine the location of nonzero elements in  [2.x.57] .
* 

* 
* [1.x.52]
* 
*  We start with the constructor. The 1 in the constructor call of  [2.x.58]  is the polynomial degree.
* 

* 
* [1.x.53]
* 
*  In the function that sets up the usual finite element data structures, we first need to distribute the DoFs.
* 

* 
* [1.x.54]
* 
*  We start by generating the sparsity pattern. To this end, we first fill an intermediate object of type DynamicSparsityPattern with the couplings appearing in the system. After building the pattern, this object is copied to  [2.x.59]  and can be discarded.
* 

* 
*  To build the sparsity pattern for DG discretizations, we can call the function analogue to  [2.x.60]  which is called  [2.x.61] 
* 

* 
* [1.x.55]
* 
*  Finally, we set up the structure of all components of the linear system.
* 

* 
* [1.x.56]
* 
*   [1.x.57]  [1.x.58]
* 

* 
*  Here we see the major difference to assembling by hand. Instead of writing loops over cells and faces, the logic is contained in the call to  [2.x.62]  and we only need to specify what should happen on each cell, each boundary face, and each interior face. These three tasks are handled by the lambda functions inside the function below.
* 

* 
*  

* 
* [1.x.59]
* 
*  This is the function that will be executed for each cell.
* 

* 
* [1.x.60]
* 
*  We solve a homogeneous equation, thus no right hand side shows up in the cell term.  What's left is integrating the matrix entries.
* 

* 
* [1.x.61]
* 
*  This is the function called for boundary faces and consists of a normal integration using FEFaceValues. New is the logic to decide if the term goes into the system matrix (outflow) or the right-hand side (inflow).
* 

* 
* [1.x.62]
* 
*  This is the function called on interior faces. The arguments specify cells, face and subface indices (for adaptive refinement). We just pass them along to the reinit() function of FEInterfaceValues.
* 

* 
* [1.x.63]
* 
*  The following lambda function will handle copying the data from the cell and face assembly into the global matrix and right-hand side.     
*   While we would not need an AffineConstraints object, because there are no hanging node constraints in DG discretizations, we use an empty object here as this allows us to use its `copy_local_to_global` functionality.
* 

* 
* [1.x.64]
* 
*  Here, we finally handle the assembly. We pass in ScratchData and CopyData objects, the lambda functions from above, an specify that we want to assemble interior faces once.
* 

* 
* [1.x.65]
* 
*   [1.x.66]  [1.x.67]   
*   For this simple problem we use the simplest possible solver, called Richardson iteration, that represents a simple defect correction. This, in combination with a block SSOR preconditioner, that uses the special block matrix structure of system matrices arising from DG discretizations. The size of these blocks are the number of DoFs per cell. Here, we use a SSOR preconditioning as we have not renumbered the DoFs according to the flow field. If the DoFs are renumbered in the downstream direction of the flow, then a block Gauss-Seidel preconditioner (see the PreconditionBlockSOR class with relaxation=1) does a much better job.
* 

* 
* [1.x.68]
* 
*  Here we create the preconditioner,
* 

* 
* [1.x.69]
* 
*  then assign the matrix to it and set the right block size:
* 

* 
* [1.x.70]
* 
*  After these preparations we are ready to start the linear solver.
* 

* 
* [1.x.71]
* 
*  We refine the grid according to a very simple refinement criterion, namely an approximation to the gradient of the solution. As here we consider the DG(1) method (i.e. we use piecewise bilinear shape functions) we could simply compute the gradients on each cell. But we do not want to base our refinement indicator on the gradients on each cell only, but want to base them also on jumps of the discontinuous solution function over faces between neighboring cells. The simplest way of doing that is to compute approximative gradients by difference quotients including the cell under consideration and its neighbors. This is done by the  [2.x.63]  class that computes the approximate gradients in a way similar to the  [2.x.64]  described in  [2.x.65]  of this tutorial. In fact, the  [2.x.66]  class was developed following the  [2.x.67]  class of  [2.x.68] . Relating to the discussion in  [2.x.69] , here we consider  [2.x.70] . Furthermore we note that we do not consider approximate second derivatives because solutions to the linear advection equation are in general not in  [2.x.71]  but only in  [2.x.72]  (or, to be more precise: in  [2.x.73] , i.e., the space of functions whose derivatives in direction  [2.x.74]  are square integrable).
* 

* 
* [1.x.72]
* 
*  The  [2.x.75]  class computes the gradients to float precision. This is sufficient as they are approximate and serve as refinement indicators only.
* 

* 
* [1.x.73]
* 
*  Now the approximate gradients are computed
* 

* 
* [1.x.74]
* 
*  and they are cell-wise scaled by the factor  [2.x.76] 
* 

* 
* [1.x.75]
* 
*  Finally they serve as refinement indicator.
* 

* 
* [1.x.76]
* 
*  The output of this program consists of a vtk file of the adaptively refined grids and the numerical solutions. Finally, we also compute the L-infinity norm of the solution using  [2.x.77] 
* 

* 
* [1.x.77]
* 
*  The following  [2.x.78]  function is similar to previous examples.
* 

* 
* [1.x.78]
* 
*  The following  [2.x.79]  function is similar to previous examples as well, and need not be commented on.
* 

* 
* [1.x.79]
* [1.x.80][1.x.81]
* 

* 
* The output of this program consist of the console output andsolutions in vtk format:
* [1.x.82]
* 
* We show the solutions on the initial mesh, the mesh after twoand after five adaptive refinement steps.
*  [2.x.80]  [2.x.81]  [2.x.82] 
* And finally we show a plot of a 3d computation.
*  [2.x.83] 
* 

* [1.x.83][1.x.84][1.x.85]
* 

* In this program we have used discontinuous elements. It is a legitimatequestion to ask why not simply use the normal, continuous ones. Of course, toeveryone with a background in numerical methods, the answer is obvious: thecontinuous Galerkin (cG) method is not stable for the transport equation,unless one specifically adds stabilization terms. The DG method, however,[1.x.86] stable. Illustrating this with the current program is not verydifficult; in fact, only the following minor modifications are necessary:
* 
*  - Change the element to FE_Q instead of FE_DGQ.
* 
*  - Add handling of hanging node constraints in exactly the same way as  [2.x.84] .
* 
*  - We need a different solver; the direct solver in  [2.x.85]  is a convenient  choice.An experienced deal.II user will be able to do this in less than 10 minutes.
* While the 2d solution has been shown above, containing a number of smallspikes at the interface that are, however, stable in height under meshrefinement, results look much different when using a continuous element:
*  [2.x.86] 
* In refinement iteration 5, the image can't be plotted in a reasonable way anymore as a 3d plot. We thus show a color plot with a range of  [2.x.87]  (thesolution values of the exact solution lie in  [2.x.88] , of course). In any case,it is clear that the continuous Galerkin solution exhibits oscillatorybehavior that gets worse and worse as the mesh is refined more and more.
* There are a number of strategies to stabilize the cG method, if one wants touse continuous elements for some reason. Discussing these methods is beyondthe scope of this tutorial program; an interested reader could, for example,take a look at  [2.x.89] .
* 

* 
* [1.x.87][1.x.88][1.x.89]
* 

* Given that the exact solution is known in this case, one interestingavenue for further extensions would be to confirm the order ofconvergence for this program. In the current case, the solution isnon-smooth, and so we can not expect to get a particularly high orderof convergence, even if we used higher order elements. But even if thesolution [1.x.90] smooth, the equation is not elliptic and so it is notimmediately clear that we should obtain a convergence order thatequals that of the optimal interpolation estimates (i.e. for examplethat we would get  [2.x.90]  convergence in the  [2.x.91]  norm by usingquadratic elements).
* In fact, for hyperbolic equations, theoretical predictions oftenindicate that the best one can hope for is an order one half below theinterpolation estimate. For example, for the streamline diffusionmethod (an alternative method to the DG method used here to stabilizethe solution of the transport equation), one can prove that forelements of degree  [2.x.92] , the order of convergence is  [2.x.93]  onarbitrary meshes. While the observed order is frequently  [2.x.94]  onuniformly refined meshes, one can construct so-called Peterson mesheson which the worse theoretical bound is actually attained. This shouldbe relatively simple to verify, for example using the [2.x.95]  function.
* A different direction is to observe that the solution of transport problemsoften has discontinuities and that therefore a mesh in which we [1.x.91]every cell in every coordinate direction may not be optimal. Rather, a betterstrategy would be to only cut cells in the direction parallel to thediscontinuity. This is called [1.x.92] and is thesubject of  [2.x.96] .
* 

* [1.x.93][1.x.94] [2.x.97] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-12b_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] ,  [2.x.2] ,  [2.x.3] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12]
* [1.x.13][1.x.14][1.x.15]
* 

* This is a variant of  [2.x.4]  with the only change that we are using theMeshWorker framework with the pre-made LocalIntegrator helper classes insteadof assembling the face terms using FEInterfaceValues.
* The details of this framework on how it is used in practice will be explainedas part of this tutorial program.
* [1.x.16][1.x.17]
* 

* The problem we solve here is the same as the one in  [2.x.5] .
* 

*  [1.x.18] [1.x.19]
*  The first few files have already been covered in previous examples and will thus not be further commented on:
* 

* 
* [1.x.20]
* 
*  Here the discontinuous finite elements are defined. They are used in the same way as all other finite elements, though
* 
*  -  as you have seen in previous tutorial programs
* 
*  -  there isn't much user interaction with finite element classes at all: they are passed to  [2.x.6]  and  [2.x.7]  objects, and that is about it.
* 

* 
* [1.x.21]
* 
*  We are going to use the simplest possible solver, called Richardson iteration, that represents a simple defect correction. This, in combination with a block SSOR preconditioner (defined in precondition_block.h), that uses the special block matrix structure of system matrices arising from DG discretizations.
* 

* 
* [1.x.22]
* 
*  We are going to use gradients as refinement indicator.
* 

* 
* [1.x.23]
* 
*  Here come the new include files for using the MeshWorker framework. The first contains the class  [2.x.8]  which provides local integrators with a mapping between local and global degrees of freedom. It stores the results of local integrals as well in its base class  [2.x.9]  In the second of these files, we find an object of type  [2.x.10]  which is mostly a wrapper around a group of FEValues objects. The file <tt>meshworker/simple.h</tt> contains classes assembling locally integrated data into a global system containing only a single matrix. Finally, we will need the file that runs the loop over all mesh cells and faces.
* 

* 
* [1.x.24]
* 
*  Like in all programs, we finish this section by including the needed C++ headers and declaring we want to use objects in the dealii namespace without prefix.
* 

* 
* [1.x.25]
* 
*   [1.x.26]  [1.x.27]   
*   First, we define a class describing the inhomogeneous boundary data. Since only its values are used, we implement value_list(), but leave all other functions of Function undefined.
* 

* 
* [1.x.28]
* 
*  Given the flow direction, the inflow boundary of the unit square  [2.x.11]  are the right and the lower boundaries. We prescribe discontinuous boundary values 1 and 0 on the x-axis and value 0 on the right boundary. The values of this function on the outflow boundaries will not be used within the DG scheme.
* 

* 
* [1.x.29]
* 
*  Finally, a function that computes and returns the wind field  [2.x.12] . As explained in the introduction, we will use a rotational field around the origin in 2d. In 3d, we simply leave the  [2.x.13] -component unset (i.e., at zero), whereas the function can not be used in 1d in its current implementation:
* 

* 
* [1.x.30]
* 
*   [1.x.31]  [1.x.32]   
*   After this preparations, we proceed with the main class of this program, called AdvectionProblem. It is basically the main class of  [2.x.14] . We do not have an AffineConstraints object, because there are no hanging node constraints in DG discretizations.
* 

* 
*  Major differences will only come up in the implementation of the assemble functions, since here, we not only need to cover the flux integrals over faces, we also use the MeshWorker interface to simplify the loops involved.
* 

* 
* [1.x.33]
* 
*  Furthermore we want to use DG elements of degree 1 (but this is only specified in the constructor). If you want to use a DG method of a different degree the whole program stays the same, only replace 1 in the constructor by the desired polynomial degree.
* 

* 
* [1.x.34]
* 
*  The next four members represent the linear system to be solved.  [2.x.15]  are generated by  [2.x.16]  is computed in  [2.x.17]  is used to determine the location of nonzero elements in  [2.x.18] .
* 

* 
* [1.x.35]
* 
*  Finally, we have to provide functions that assemble the cell, boundary, and inner face terms. Within the MeshWorker framework, the loop over all cells and much of the setup of operations will be done outside this class, so all we have to provide are these three operations. They will then work on intermediate objects for which first, we here define alias to the info objects handed to the local integration functions in order to make our life easier below.
* 

* 
* [1.x.36]
* 
*  The following three functions are then the ones that get called inside the generic loop over all cells and faces. They are the ones doing the actual integration.     
*   In our code below, these functions do not access member variables of the current class, so we can mark them as  [2.x.19]  and simply pass pointers to these functions to the MeshWorker framework. If, however, these functions would want to access member variables (or needed additional arguments beyond the ones specified below), we could use the facilities of lambda functions to provide the MeshWorker framework with objects that act as if they had the required number and types of arguments, but have in fact other arguments already bound.
* 

* 
* [1.x.37]
* 
*  We start with the constructor. The 1 in the constructor call of  [2.x.20]  is the polynomial degree.
* 

* 
* [1.x.38]
* 
*  In the function that sets up the usual finite element data structures, we first need to distribute the DoFs.
* 

* 
* [1.x.39]
* 
*  We start by generating the sparsity pattern. To this end, we first fill an intermediate object of type DynamicSparsityPattern with the couplings appearing in the system. After building the pattern, this object is copied to  [2.x.21]  and can be discarded.
* 

* 
*  To build the sparsity pattern for DG discretizations, we can call the function analogue to  [2.x.22]  which is called  [2.x.23] 
* 

* 
* [1.x.40]
* 
*  Finally, we set up the structure of all components of the linear system.
* 

* 
* [1.x.41]
* 
*   [1.x.42]  [1.x.43]
* 

* 
*  Here we see the major difference to assembling by hand. Instead of writing loops over cells and faces, we leave all this to the MeshWorker framework. In order to do so, we just have to define local integration functions and use one of the classes in namespace  [2.x.24]  to build the global system.
* 

* 
* [1.x.44]
* 
*  This is the magic object, which knows everything about the data structures and local integration.  This is the object doing the work in the function  [2.x.25]  which is implicitly called by  [2.x.26]  below. After the functions to which we provide pointers did the local integration, the  [2.x.27]  object distributes these into the global sparse matrix and the right hand side vector.
* 

* 
* [1.x.45]
* 
*  First, we initialize the quadrature formulae and the update flags in the worker base class. For quadrature, we play safe and use a QGauss formula with number of points one higher than the polynomial degree used. Since the quadratures for cells, boundary and interior faces can be selected independently, we have to hand over this value three times.
* 

* 
* [1.x.46]
* 
*  These are the types of values we need for integrating our system. They are added to the flags used on cells, boundary and interior faces, as well as interior neighbor faces, which is forced by the four  [2.x.28]  values.
* 

* 
* [1.x.47]
* 
*  After preparing all data in <tt>info_box</tt>, we initialize the FEValues objects in there.
* 

* 
* [1.x.48]
* 
*  The object created so far helps us do the local integration on each cell and face. Now, we need an object which receives the integrated (local) data and forwards them to the assembler.
* 

* 
* [1.x.49]
* 
*  Now, we have to create the assembler object and tell it, where to put the local data. These will be our system matrix and the right hand side.
* 

* 
* [1.x.50]
* 
*  Finally, the integration loop over all active cells (determined by the first argument, which is an active iterator).     
*   As noted in the discussion when declaring the local integration functions in the class declaration, the arguments expected by the assembling integrator class are not actually function pointers. Rather, they are objects that can be called like functions with a certain number of arguments. Consequently, we could also pass objects with appropriate operator() implementations here, or lambda functions if the local integrators were, for example, non-static member functions.
* 

* 
* [1.x.51]
* 
*   [1.x.52]  [1.x.53]
* 

* 
*  These are the functions given to the  [2.x.29]  called just above. They compute the local contributions to the system matrix and right hand side on cells and faces.
* 

* 
* [1.x.54]
* 
*  First, let us retrieve some of the objects used here from  [2.x.30]  Note that these objects can handle much more complex structures, thus the access here looks more complicated than might seem necessary.
* 

* 
* [1.x.55]
* 
*  With these objects, we continue local integration like always. First, we loop over the quadrature points and compute the advection vector in the current point.
* 

* 
* [1.x.56]
* 
*  We solve a homogeneous equation, thus no right hand side shows up in the cell term.  What's left is integrating the matrix entries.
* 

* 
* [1.x.57]
* 
*  Now the same for the boundary terms. Note that now we use FEValuesBase, the base class for both FEFaceValues and FESubfaceValues, in order to get access to normal vectors.
* 

* 
* [1.x.58]
* 
*  Finally, the interior face terms. The difference here is that we receive two info objects, one for each cell adjacent to the face and we assemble four matrices, one for each cell and two for coupling back and forth.
* 

* 
* [1.x.59]
* 
*  For quadrature points, weights, etc., we use the FEValuesBase object of the first argument.
* 

* 
* [1.x.60]
* 
*  For additional shape functions, we have to ask the neighbors FEValuesBase.
* 

* 
* [1.x.61]
* 
*  Then we get references to the four local matrices. The letters u and v refer to trial and test functions, respectively. The %numbers indicate the cells provided by info1 and info2. By convention, the two matrices in each info object refer to the test functions on the respective cell. The first matrix contains the interior couplings of that cell, while the second contains the couplings between cells.
* 

* 
* [1.x.62]
* 
*  Here, following the previous functions, we would have the local right hand side vectors. Fortunately, the interface terms only involve the solution and the right hand side does not receive any contributions.
* 

* 
*  

* 
* [1.x.63]
* 
*  This term we've already seen:
* 

* 
* [1.x.64]
* 
*  We additionally assemble the term  [2.x.31] ,
* 

* 
* [1.x.65]
* 
*  This one we've already seen, too:
* 

* 
* [1.x.66]
* 
*  And this is another new one:  [2.x.32] :
* 

* 
* [1.x.67]
* 
*   [1.x.68]  [1.x.69]   
*   For this simple problem we use the simplest possible solver, called Richardson iteration, that represents a simple defect correction. This, in combination with a block SSOR preconditioner, that uses the special block matrix structure of system matrices arising from DG discretizations. The size of these blocks are the number of DoFs per cell. Here, we use a SSOR preconditioning as we have not renumbered the DoFs according to the flow field. If the DoFs are renumbered in the downstream direction of the flow, then a block Gauss-Seidel preconditioner (see the PreconditionBlockSOR class with relaxation=1) does a much better job.
* 

* 
* [1.x.70]
* 
*  Here we create the preconditioner,
* 

* 
* [1.x.71]
* 
*  then assign the matrix to it and set the right block size:
* 

* 
* [1.x.72]
* 
*  After these preparations we are ready to start the linear solver.
* 

* 
* [1.x.73]
* 
*  We refine the grid according to a very simple refinement criterion, namely an approximation to the gradient of the solution. As here we consider the DG(1) method (i.e. we use piecewise bilinear shape functions) we could simply compute the gradients on each cell. But we do not want to base our refinement indicator on the gradients on each cell only, but want to base them also on jumps of the discontinuous solution function over faces between neighboring cells. The simplest way of doing that is to compute approximative gradients by difference quotients including the cell under consideration and its neighbors. This is done by the  [2.x.33]  class that computes the approximate gradients in a way similar to the  [2.x.34]  described in  [2.x.35]  of this tutorial. In fact, the  [2.x.36]  class was developed following the  [2.x.37]  class of  [2.x.38] . Relating to the discussion in  [2.x.39] , here we consider  [2.x.40] . Furthermore we note that we do not consider approximate second derivatives because solutions to the linear advection equation are in general not in  [2.x.41]  but only in  [2.x.42]  (or, to be more precise: in  [2.x.43] , i.e., the space of functions whose derivatives in direction  [2.x.44]  are square integrable).
* 

* 
* [1.x.74]
* 
*  The  [2.x.45]  class computes the gradients to float precision. This is sufficient as they are approximate and serve as refinement indicators only.
* 

* 
* [1.x.75]
* 
*  Now the approximate gradients are computed
* 

* 
* [1.x.76]
* 
*  and they are cell-wise scaled by the factor  [2.x.46] 
* 

* 
* [1.x.77]
* 
*  Finally they serve as refinement indicator.
* 

* 
* [1.x.78]
* 
*  The output of this program consists of eps-files of the adaptively refined grids and the numerical solutions given in gnuplot format.
* 

* 
* [1.x.79]
* 
*  First write the grid in eps format.
* 

* 
* [1.x.80]
* 
*  Then output the solution in gnuplot format.
* 

* 
* [1.x.81]
* 
*  The following  [2.x.47]  function is similar to previous examples.
* 

* 
* [1.x.82]
* 
*  The following  [2.x.48]  function is similar to previous examples as well, and need not be commented on.
* 

* 
* [1.x.83]
* [1.x.84][1.x.85]
* 

* 
* The output of this program is very similar to  [2.x.49]  and we are not repeating the output here.
* We show the solutions on the initial mesh, the mesh after twoand after five adaptive refinement steps.
*  [2.x.50]  [2.x.51]  [2.x.52] 
* 

* Then we show the final grid (after 5 refinement steps) and the solution again,this time with a nicer 3d rendering (obtained using the  [2.x.53] function and the VTK-based VisIt visualization program) that better shows thesharpness of the jump on the refined mesh and the over- and undershoots of thesolution along the interface:
*  [2.x.54]  [2.x.55] 
* 

* And finally we show a plot of a 3d computation.
*  [2.x.56] 
* 

* [1.x.86][1.x.87][1.x.88]
* 

* For ideas for further extensions, please see see  [2.x.57] .
* 

* [1.x.89][1.x.90] [2.x.58] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-13_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19]
* [1.x.20][1.x.21][1.x.22]
* 

* [1.x.23][1.x.24]
* 

* 
* In this example program, we will not so much be concerned withdescribing new ways how to use deal.II and its facilities, but ratherwith presenting methods of writing modular and extensible finiteelement programs. The main reason for this is the size and complexityof modern research software: applications implementing modern errorestimation concepts and adaptive solution methods tend to becomerather large. For example, when this program was written in 2002, thethree largest applications by the mainauthors of deal.II, are at the time of writing of this exampleprogram: [2.x.2]  [2.x.3]  a program for solving conservation hyperbolic equations by the     Discontinuous Galerkin Finite Element method: 33,775 lines of     code; [2.x.4]  a parameter estimation program: 28,980 lines of code; [2.x.5]  a wave equation solver: 21,020 lines of code. [2.x.6] 
* (The library proper
* 
*  - without example programs and test suite
* 
*  - has slightlymore than 150,000 lines of code as of spring 2002. It is of course severaltimes larger now.) The sizes of these applications are at the edge of whatone person, even an experienced programmer, can manage.
* 

* 
* The numbers above make one thing rather clear: monolithic programs thatare not broken up into smaller, mostly independent pieces have no wayof surviving, since even the author will quickly lose the overview ofthe various dependencies between different parts of a program. Onlydata encapsulation, for example using object oriented programmingmethods, and modularization by defining small but fixed interfaces canhelp structure data flow and mutual interdependencies. It is also anabsolute prerequisite if more than one person is developing a program,since otherwise confusion will quickly prevail as one developerwould need to know if another changed something about the internals ofa different module if they were not cleanly separated.
* 

* 
* In previous examples, you have seen how the library itself is brokenup into several complexes each building atop the underlying ones, butrelatively independent of the other ones: [2.x.7]  [2.x.8] the triangulation class complex, with associated iterator classes; [2.x.9] the finite element classes; [2.x.10] the DoFHandler class complex, with associated iterators, built on    the triangulation and finite element classes; [2.x.11] the classes implementing mappings between unit and real cells; [2.x.12] the FEValues class complex, built atop the finite elements and    mappings. [2.x.13] Besides these, and a large number of smaller classes, there are ofcourse the following "tool" modules: [2.x.14]  [2.x.15] output in various graphical formats; [2.x.16] linear algebra classes. [2.x.17] These complexes can also be found as a flow chart on the front page ofthe deal.II manual website.
* 

* 
* The goal of this program is now to give an example of how a relativelysimple finite element program could be structured such that we end upwith a set of modules that are as independent of each other aspossible. This allows to change the program at one end, without having toworry that it might break at the other, as long as we do not touch theinterface through which the two ends communicate. The interface inC++, of course, is the declaration of abstract base classes.
* 

* 
* Here, we will implement (again) a Laplace solver, although with anumber of differences compared to previous example programs: [2.x.18]  [2.x.19] The classes that implement the process of numerically solving the    equation are no more responsible for driving the process of    "solving-estimating error-refining-solving again", but we delegate    this to external functions. This allows first to use it as a    building block in a larger context, where the solution of a    Laplace equation might only be one part (for example, in a    nonlinear problem, where Laplace equations might have to be solved    in each nonlinear step). It would also allow to build a framework    around this class that would allow using solvers for other    equations (but with the same external interface) instead, in case    some techniques shall be evaluated for different types of partial    differential equations. [2.x.20] It splits the process of evaluating the computed solution to a    separate set of classes. The reason is that one is usually not    interested in the solution of a PDE per se, but rather in certain    aspects of it. For example, one might wish to compute the traction    at a certain boundary in elastic computations, or in the signal of    a seismic wave at a receiver position at a given    location. Sometimes, one might have an interest in several of    these aspects. Since the evaluation of a solution is something    that does not usually affect the process of solution, we split it    off into a separate module, to allow for the development of such    evaluation filters independently of the development of the solver    classes. [2.x.21] Separate the classes that implement mesh refinement from the    classes that compute the solution. [2.x.22] Separate the description of the test case with which we will    present the program, from the rest of the program. [2.x.23] Parallelize the assembly of linear systems using the WorkStream    facilities. This follows the extensive description that can be    found in the  [2.x.24]  "Parallel computing with multiple processors accessing shared memory"    documentation module. The implementation essentially follows what    has already been described in  [2.x.25] . [2.x.26] 
* 

* 
* The things the program does are not new. In fact, this is more like amelange of previous programs, cannibalizing various parts andfunctions from earlier examples. It is the way they are arranged inthis program that should be the focus of the reader, i.e. the softwaredesign techniques used in the program to achieve the goal ofimplementing the desired mathematical method. However, we muststress that software design is in part also a subjective matter:different persons have different programming backgrounds and havedifferent opinions about the "right" style of programming; thisprogram therefore expresses only what the author considers usefulpractice, and is not necessarily a style that you have to adopt inorder to write successful numerical software if you feel uncomfortablewith the chosen ways. It should serve as a case study, however,inspiring the reader with ideas to the desired end.
* 

* 
* Once you have worked through the program, you will remark that it isalready somewhat complex in its structure. Nevertheless, itonly has about 850 lines of code, without comments. In realapplications, there would of course be comments and classdocumentation, which would bring that to maybe 1200 lines. Yet, compared tothe applications listed above, this is still small, as they are 20 to25 times as large. For programs as large, a proper design right fromthe start is thus indispensable. Otherwise, it will have to beredesigned at one point in its life, once it becomes too large to bemanageable.
* 

* 
* Despite of this, all three programs listed above have undergone majorrevisions, or even rewrites. The wave program, for example, was onceentirely teared to parts when it was still significantly smaller, justto assemble it again in a more modular form. By that time, it hadbecome impossible to add functionality without affecting older partsof the code (the main problem with the code was the data flow: in timedependent application, the major concern is when to store data to diskand when to reload it again; if this is not done in an organizedfashion, then you end up with data released too early, loaded toolate, or not released at all). Although the present example programthus draws from several years of experience, it is certainly notwithout flaws in its design, and in particular might not be suited foran application where the objective is different. It should serve as aninspiration for writing your own application in a modular way, toavoid the pitfalls of too closely coupled codes.
* 

* 
* [1.x.25][1.x.26]
* 

* 
* What the program actually does is not even the main point of thisprogram, the structure of the program is more important. However, in afew words, a description would be: solve the Laplace equation for agiven right hand side such that the solution is the function [2.x.27] . The goal of thecomputation is to get the value of the solution at the point [2.x.28] , and to compare the accuracy withwhich we resolve this value for two refinement criteria, namely globalrefinement and refinement by the error indicator by Kelly et al. whichwe have already used in previous examples.
* 

* 
* The results will, as usual, be discussed in the respective section ofthis document. In doing so, we will find a slightly irritatingobservation about the relative performance of the two refinementcriteria. In a later example program, building atop this one, we willdevise a different method that should hopefully perform better thanthe techniques discussed here.
* 

* 
* So much now for all the theoretical and anecdotal background. The bestway of learning about a program is to look at it, so here it is:
* 

*  [1.x.27] [1.x.28]
*  As in all programs, we start with a list of include files from the library, and as usual they are in the standard order which is  [2.x.29] 
* 
*  -   [2.x.30] 
* 
*  -   [2.x.31] 
* 
*  -   [2.x.32] 
* 
*  -   [2.x.33] 
* 
*  -   [2.x.34]  (as each of these categories roughly builds upon previous ones), then C++ standard headers:
* 

* 
* [1.x.29]
* 
*  Now for the C++ standard headers:
* 

* 
* [1.x.30]
* 
*  The last step is as in all previous programs:
* 

* 
* [1.x.31]
* 
*   [1.x.32]  [1.x.33]
* 

* 
*  As for the program itself, we first define classes that evaluate the solutions of a Laplace equation. In fact, they can evaluate every kind of solution, as long as it is described by a  [2.x.35]  object, and a solution vector. We define them here first, even before the classes that actually generate the solution to be evaluated, since we need to declare an abstract base class that the solver classes can refer to.   
*   From an abstract point of view, we declare a pure base class that provides an evaluation operator() which will do the evaluation of the solution (whatever derived classes might consider an  [2.x.36] ). Since this is the only real function of this base class (except for some bookkeeping machinery), one usually terms such a class that only has an  [2.x.37]  a  [2.x.38]  in C++ terminology, since it is used just like a function object.   
*   Objects of this functor type will then later be passed to the solver object, which applies it to the solution just computed. The evaluation objects may then extract any quantity they like from the solution. The advantage of putting these evaluation functions into a separate hierarchy of classes is that by design they cannot use the internals of the solver object and are therefore independent of changes to the way the solver works. Furthermore, it is trivial to write another evaluation class without modifying the solver class, which speeds up programming (not being able to use internals of another class also means that you do not have to worry about them
* 
*  -  programming evaluators is usually a rather quickly done task), as well as compilation (if solver and evaluation classes are put into different files: the solver only needs to see the declaration of the abstract base class, and therefore does not need to be recompiled upon addition of a new evaluation class, or modification of an old one).  On a related note, you can reuse the evaluation classes for other projects, solving different equations.   
*   In order to improve separation of code into different modules, we put the evaluation classes into a namespace of their own. This makes it easier to actually solve different equations in the same program, by assembling it from existing building blocks. The reason for this is that classes for similar purposes tend to have the same name, although they were developed in different contexts. In order to be able to use them together in one program, it is necessary that they are placed in different namespaces. This we do here:
* 

* 
* [1.x.34]
* 
*  Now for the abstract base class of evaluation classes: its main purpose is to declare a pure virtual function  [2.x.39]  taking a  [2.x.40]  object, and the solution vector. In order to be able to use pointers to this base class only, it also has to declare a virtual destructor, which however does nothing. Besides this, it only provides for a little bit of bookkeeping: since we usually want to evaluate solutions on subsequent refinement levels, we store the number of the present refinement cycle, and provide a function to change this number.
* 

* 
* [1.x.35]
* 
*   [1.x.36]  [1.x.37]
* 

* 
*  The next thing is to implement actual evaluation classes. As noted in the introduction, we'd like to extract a point value from the solution, so the first class does this in its  [2.x.41] . The actual point is given to this class through the constructor, as well as a table object into which it will put its findings.     
*   Finding out the value of a finite element field at an arbitrary point is rather difficult, if we cannot rely on knowing the actual finite element used, since then we cannot, for example, interpolate between nodes. For simplicity, we therefore assume here that the point at which we want to evaluate the field is actually a node. If, in the process of evaluating the solution, we find that we did not encounter this point upon looping over all vertices, we then have to throw an exception in order to signal to the calling functions that something has gone wrong, rather than silently ignore this error.     
*   In the  [2.x.42]  example program, we have already seen how such an exception class can be declared, using the  [2.x.43]  macros. We use this mechanism here again.     
*   From this, the actual declaration of this class should be evident. Note that of course even if we do not list a destructor explicitly, an implicit destructor is generated from the compiler, and it is virtual just as the one of the base class.
* 

* 
* [1.x.38]
* 
*  As for the definition, the constructor is trivial, just taking data and storing it in object-local ones:
* 

* 
* [1.x.39]
* 
*  Now for the function that is mainly of interest in this class, the computation of the point value:
* 

* 
* [1.x.40]
* 
*  First allocate a variable that will hold the point value. Initialize it with a value that is clearly bogus, so that if we fail to set it to a reasonable value, we will note at once. This may not be necessary in a function as small as this one, since we can easily see all possible paths of execution here, but it proved to be helpful for more complex cases, and so we employ this strategy here as well.
* 

* 
* [1.x.41]
* 
*  Then loop over all cells and all their vertices, and check whether a vertex matches the evaluation point. If this is the case, then extract the point value, set a flag that we have found the point of interest, and exit the loop.
* 

* 
* [1.x.42]
* 
*  In order to extract the point value from the global solution vector, pick that component that belongs to the vertex of interest, and, in case the solution is vector-valued, take the first component of it:
* 

* 
* [1.x.43]
* 
*  Note that by this we have made an assumption that is not valid always and should be documented in the class declaration if this were code for a real application rather than a tutorial program: we assume that the finite element used for the solution we try to evaluate actually has degrees of freedom associated with vertices. This, for example, does not hold for discontinuous elements, were the support points for the shape functions happen to be located at the vertices, but are not associated with the vertices but rather with the cell interior, since association with vertices would imply continuity there. It would also not hold for edge oriented elements, and the like.                 
*   Ideally, we would check this at the beginning of the function, for example by a statement like <code>Assert (dof_handler.get_fe().dofs_per_vertex  [2.x.44]  0, ExcNotImplemented())</code>, which should make it quite clear what is going wrong when the exception is triggered. In this case, we omit it (which is indeed bad style), but knowing that that does not hurt here, since the statement  [2.x.45]  would fail if we asked it to give us the DoF index of a vertex if there were none.                 
*   We stress again that this restriction on the allowed finite elements should be stated in the class documentation.
* 

* 
*  Since we found the right point, we now set the respective flag and exit the innermost loop. The outer loop will also be terminated due to the set flag.
* 

* 
* [1.x.44]
* 
*  Finally, we'd like to make sure that we have indeed found the evaluation point, since if that were not so we could not give a reasonable value of the solution there and the rest of the computations were useless anyway. So make sure through the  [2.x.46]  macro already used in the  [2.x.47]  program that we have indeed found this point. If this is not so, the macro throws an exception of the type that is given to it as second argument, but compared to a straightforward  [2.x.48]  statement, it fills the exception object with a set of additional information, for example the source file and line number where the exception was generated, and the condition that failed. If you have a  [2.x.49]  clause in your main function (as this program has), you will catch all exceptions that are not caught somewhere in between and thus already handled, and this additional information will help you find out what happened and where it went wrong.
* 

* 
* [1.x.45]
* 
*  Note that we have used the  [2.x.50]  macro in other example programs as well. It differed from the  [2.x.51]  macro used here in that it simply aborts the program, rather than throwing an exception, and that it did so only in debug mode. It was the right macro to use to check about the size of vectors passed as arguments to functions, and the like.       
*   However, here the situation is different: whether we find the evaluation point or not may change from refinement to refinement (for example, if the four cells around point are coarsened away, then the point may vanish after refinement and coarsening). This is something that cannot be predicted from a few number of runs of the program in debug mode, but should be checked always, also in production runs. Thus the use of the  [2.x.52]  macro here.
* 

* 
*  Now, if we are sure that we have found the evaluation point, we can add the results into the table of results:
* 

* 
* [1.x.46]
* 
*   [1.x.47]  [1.x.48]
* 

* 
*  A different, maybe slightly odd kind of  [2.x.53]  of a solution is to output it to a file in a graphical format. Since in the evaluation functions we are given a  [2.x.54]  object and the solution vector, we have all we need to do this, so we can do it in an evaluation class. The reason for actually doing so instead of putting it into the class that computed the solution is that this way we have more flexibility: if we choose to only output certain aspects of it, or not output it at all. In any case, we do not need to modify the solver class, we just have to modify one of the modules out of which we build this program. This form of encapsulation, as above, helps us to keep each part of the program rather simple as the interfaces are kept simple, and no access to hidden data is possible.     
*   Since this class which generates the output is derived from the common  [2.x.55]  base class, its main interface is the  [2.x.56]  function. Furthermore, it has a constructor taking a string that will be used as the base part of the file name to which output will be sent (we will augment it by a number indicating the number of the refinement cycle
* 
*  -  the base class has this information at hand
* 
*  - , and a suffix), and the constructor also takes a value that indicates which format is requested, i.e. for which graphics program we shall generate output (from this we will then also generate the suffix of the filename to which we write).     
*   Regarding the output format, the DataOutBase namespace provides an enumeration field  [2.x.57]  which lists names for all supported output formats. At the time of writing of this program, the supported graphics formats are represented by the enum values  [2.x.58] ,  [2.x.59] ,  [2.x.60] ,  [2.x.61] , etc, but this list will certainly grow over time. Now, within various functions of that base class, you can use values of this type to get information about these graphics formats (for example the default suffix used for files of each format), and you can call a generic  [2.x.62]  function, which then branches to the  [2.x.63] , etc functions which we have used in previous examples already, based on the value of a second argument given to it denoting the required output format. This mechanism makes it simple to write an extensible program that can decide which output format to use at runtime, and it also makes it rather simple to write the program in a way such that it takes advantage of newly implemented output formats, without the need to change the application program.     
*   Of these two fields, the base name and the output format descriptor, the constructor takes values and stores them for later use by the actual evaluation function.
* 

* 
* [1.x.49]
* 
*  Following the description above, the function generating the actual output is now relatively straightforward. The only particularly interesting feature over previous example programs is the use of the  [2.x.64]  function, returning the usual suffix for files of a given format (e.g. ".eps" for encapsulated postscript files, ".gnuplot" for Gnuplot files), and of the generic  [2.x.65]  function with a second argument, which internally branches to the actual output functions for the different graphics formats, based on the value of the format descriptor passed as second argument.     
*   Also note that we have to prefix  [2.x.66]  to access a member variable of the template dependent base class. The reason here, and further down in the program is the same as the one described in the  [2.x.67]  example program (look for  [2.x.68]  there).
* 

* 
* [1.x.50]
* 
*   [1.x.51]  [1.x.52]
* 

* 
*  In practical applications, one would add here a list of other possible evaluation classes, representing quantities that one may be interested in. For this example, that much shall be sufficient, so we close the namespace.
* 

* 
* [1.x.53]
* 
*   [1.x.54]  [1.x.55]
* 

* 
*  After defining what we want to know of the solution, we should now care how to get at it. We will pack everything we need into a namespace of its own, for much the same reasons as for the evaluations above.   
*   Since we have discussed Laplace solvers already in considerable detail in previous examples, there is not much new stuff following. Rather, we have to a great extent cannibalized previous examples and put them, in slightly different form, into this example program. We will therefore mostly be concerned with discussing the differences to previous examples.   
*   Basically, as already said in the introduction, the lack of new stuff in this example is deliberate, as it is more to demonstrate software design practices, rather than mathematics. The emphasis in explanations below will therefore be more on the actual implementation.
* 

* 
* [1.x.56]
* 
*   [1.x.57]  [1.x.58]
* 

* 
*  In defining a Laplace solver, we start out by declaring an abstract base class, that has no functionality itself except for taking and storing a pointer to the triangulation to be used later.     
*   This base class is very general, and could as well be used for any other stationary problem. It provides declarations of functions that shall, in derived classes, solve a problem, postprocess the solution with a list of evaluation objects, and refine the grid, respectively. None of these functions actually does something itself in the base class.     
*   Due to the lack of actual functionality, the programming style of declaring very abstract base classes is similar to the style used in Smalltalk or Java programs, where all classes are derived from entirely abstract classes  [2.x.69] , even number representations. The author admits that he does not particularly like the use of such a style in C++, as it puts style over reason. Furthermore, it promotes the use of virtual functions for everything (for example, in Java, all functions are virtual per se), which, however, has proven to be rather inefficient in many applications where functions are often only accessing data, not doing computations, and therefore quickly return; the overhead of virtual functions can then be significant. The opinion of the author is to have abstract base classes wherever at least some part of the code of actual implementations can be shared and thus separated into the base class.     
*   Besides all these theoretical questions, we here have a good reason, which will become clearer to the reader below. Basically, we want to be able to have a family of different Laplace solvers that differ so much that no larger common subset of functionality could be found. We therefore just declare such an abstract base class, taking a pointer to a triangulation in the constructor and storing it henceforth. Since this triangulation will be used throughout all computations, we have to make sure that the triangulation is valid until it is last used. We do this by keeping a  [2.x.70]  to this triangulation, as explained in  [2.x.71] .     
*   Note that while the pointer itself is declared constant (i.e. throughout the lifetime of this object, the pointer points to the same object), it is not declared as a pointer to a constant triangulation. In fact, by this we allow that derived classes refine or coarsen the triangulation within the  [2.x.72]  function.     
*   Finally, we have a function  [2.x.73]  is only a tool for the driver functions to decide whether we want to go on with mesh refinement or not. It returns the number of degrees of freedom the present simulation has.
* 

* 
* [1.x.59]
* 
*  The implementation of the only two non-abstract functions is then rather boring:
* 

* 
* [1.x.60]
* 
*   [1.x.61]  [1.x.62]
* 

* 
*  Following now the main class that implements assembling the matrix of the linear system, solving it, and calling the postprocessor objects on the solution. It implements the  [2.x.74]  and  [2.x.75]  functions declared in the base class. It does not, however, implement the  [2.x.76]  method, as mesh refinement will be implemented in a number of derived classes.     
*   It also declares a new abstract virtual function,  [2.x.77] , that needs to be overloaded in subclasses. The reason is that we will implement two different classes that will implement different methods to assemble the right hand side vector. This function might also be interesting in cases where the right hand side depends not simply on a continuous function, but on something else as well, for example the solution of another discretized problem, etc. The latter happens frequently in non-linear problems.     
*   As we mentioned previously, the actual content of this class is not new, but a mixture of various techniques already used in previous examples. We will therefore not discuss them in detail, but refer the reader to these programs.     
*   Basically, in a few words, the constructor of this class takes pointers to a triangulation, a finite element, and a function object representing the boundary values. These are either passed down to the base class's constructor, or are stored and used to generate a  [2.x.78]  object later. Since finite elements and quadrature formula should match, it is also passed a quadrature object.     
*   The  [2.x.79]  sets up the data structures for the actual solution, calls the functions to assemble the linear system, and solves it.     
*   The  [2.x.80]  function finally takes an evaluation object and applies it to the computed solution.     
*   The  [2.x.81]  function finally implements the pure virtual function of the base class.
* 

* 
* [1.x.63]
* 
*  In the protected section of this class, we first have a number of member variables, of which the use should be clear from the previous examples:
* 

* 
* [1.x.64]
* 
*  Then we declare an abstract function that will be used to assemble the right hand side. As explained above, there are various cases for which this action differs strongly in what is necessary, so we defer this to derived classes:
* 

* 
* [1.x.65]
* 
*  Next, in the private section, we have a small class which represents an entire linear system, i.e. a matrix, a right hand side, and a solution vector, as well as the constraints that are applied to it, such as those due to hanging nodes. Its constructor initializes the various subobjects, and there is a function that implements a conjugate gradient method as solver.
* 

* 
* [1.x.66]
* 
*  Finally, there is a set of functions which will be used to assemble the actual system matrix. The main function of this group,  [2.x.82]  computes the matrix in parallel on multicore systems, using the following two helper functions. The mechanism for doing so is the same as in the  [2.x.83]  example program and follows the WorkStream concept outlined in  [2.x.84]  . The main function also calls the virtual function assembling the right hand side.
* 

* 
* [1.x.67]
* 
*  Now here comes the constructor of the class. It does not do much except store pointers to the objects given, and generate  [2.x.85]  object initialized with the given pointer to a triangulation. This causes the DoF handler to store that pointer, but does not already generate a finite element numbering (we only ask for that in the  [2.x.86]  function).
* 

* 
* [1.x.68]
* 
*  The destructor is simple, it only clears the information stored in the DoF handler object to release the memory.
* 

* 
* [1.x.69]
* 
*  The next function is the one which delegates the main work in solving the problem: it sets up the DoF handler object with the finite element given to the constructor of this object, the creates an object that denotes the linear system (i.e. the matrix, the right hand side vector, and the solution vector), calls the function to assemble it, and finally solves it:
* 

* 
* [1.x.70]
* 
*  As stated above, the  [2.x.87]  function takes an evaluation object, and applies it to the computed solution. This function may be called multiply, once for each evaluation of the solution which the user required.
* 

* 
* [1.x.71]
* 
*  The  [2.x.88]  function should be self-explanatory:
* 

* 
* [1.x.72]
* 
*  The following function assembles matrix and right hand side of the linear system to be solved in each step. We will do things in parallel at a couple of levels. First, note that we need to assemble both the matrix and the right hand side. These are independent operations, and we should do this in parallel. To this end, we use the concept of "tasks" that is discussed in the  [2.x.89]  documentation module. In essence, what we want to say "here is something that needs to be worked on, go do it whenever a CPU core is available", then do something else, and when we need the result of the first operation wait for its completion. At the second level, we want to assemble the matrix using the exact same strategy we have already used in  [2.x.90] , namely the WorkStream concept.     
*   While we could consider either assembling the right hand side or assembling the matrix as the thing to do in the background while doing the other, we will opt for the former approach simply because the call to  [2.x.91]  is so much simpler to write than the call to  [2.x.92]  with its many arguments. In any case, the code then looks like this to assemble the entire linear system:
* 

* 
* [1.x.73]
* 
*  The syntax above requires some explanation. There are multiple version of  [2.x.93]  that expect different arguments. In  [2.x.94] , we used one version that took a pair of iterators, a pair of pointers to member functions with very specific argument lists, a pointer or reference to the object on which these member functions have to work, and a scratch and copy data object. This is a bit restrictive since the member functions called this way have to have an argument list that exactly matches what  [2.x.95]  expects: the local assembly function needs to take an iterator, a scratch object and a copy object; and the copy-local-to-global function needs to take exactly a copy object. But, what if we want something that's slightly more general? For example, in the current program, the copy-local-to-global function needs to know which linear system object to write the local contributions into, i.e., it also has to take a  [2.x.96]  argument. That won't work with the approach using member function pointers.       
*   Fortunately, C++ offers a way out. These are called function objects. In essence, what  [2.x.97]  wants to do is not call a member function. It wants to call some function that takes an iterator, a scratch object and a copy object in the first case, and a copy object in the second case. Whether these are member functions, global functions, or something else, is really not of much concern to WorkStream. Consequently, there is a second version of the function that just takes function objects
* 
*  -  objects that have an  [2.x.98]  and that consequently can be called like functions, whatever they really represent. The typical way to generate such function objects is using a [1.x.74] that wraps the function call including the individual arguments with fixed values. All the arguments that are part of the outer function signature are specified as regular function arguments in the lambda function. The fixed values are passed into the lambda function using the capture list (`[...]`). It is possible to use a capture default or to list all the variables that are to be bound to the lambda explicitly. For the sake of clarity we decided to omit the capture default here, but that capture list could equally well be `[&]`, meaning that all used variables are copied into the lambda by reference.       
*   At this point, we have assembled the matrix and condensed it. The right hand side may or may not have been completely assembled, but we would like to condense the right hand side vector next. We can only do this if the assembly of this vector has finished, so we have to wait for the task to finish; in computer science, waiting for a task is typically called "joining" the task, explaining the name of the function we call below.       
*   Since that task may or may not have finished, and since we may have to wait for it to finish, we may as well try to pack other things that need to be done anyway into this gap. Consequently, we first interpolate boundary values before we wait for the right hand side. Of course, another possibility would have been to also interpolate the boundary values on a separate task since doing so is independent of the other things we have done in this function so far. Feel free to find the correct syntax to also create a task for this interpolation and start it at the top of this function, along with the assembly of the right hand side. (You will find that this is slightly more complicated since there are multiple versions of  [2.x.99]  and so simply taking the address  [2.x.100]  produces a set of overloaded functions that can't be passed to  [2.x.101]  right away
* 
*  -  you have to select which element of this overload set you want by casting the address expression to a function pointer type that is specific to the version of the function that you want to call on the task.)
* 

* 
* [1.x.75]
* 
*  Now that we have the complete linear system, we can also treat boundary values, which need to be eliminated from both the matrix and the right hand side:
* 

* 
* [1.x.76]
* 
*  The second half of this set of functions deals with the local assembly on each cell and copying local contributions into the global matrix object. This works in exactly the same way as described in  [2.x.102] :
* 

* 
* [1.x.77]
* 
*  Now for the functions that implement actions in the linear system class. First, the constructor initializes all data elements to their correct sizes, and sets up a number of additional data structures, such as constraints due to hanging nodes. Since setting up the hanging nodes and finding out about the nonzero elements of the matrix is independent, we do that in parallel (if the library was configured to use concurrency, at least; otherwise, the actions are performed sequentially). Note that we start only one thread, and do the second action in the main thread. Since only one task is generated, we don't use the  [2.x.103]  class here, but rather use the one created task object directly to wait for this particular task's exit.     
*   Note that taking up the address of the  [2.x.104]  function is a little tricky, since there are actually three of them, one for each supported space dimension. Taking addresses of overloaded functions is somewhat complicated in C++, since the address-of operator  [2.x.105]  in that case returns more like a set of values (the addresses of all functions with that name), and selecting the right one is then the next step. If the context dictates which one to take (for example by assigning to a function pointer of known type), then the compiler can do that by itself, but if this set of pointers shall be given as the argument to a function that takes a template, the compiler could choose all without having a preference for one. We therefore have to make it clear to the compiler which one we would like to have; for this, we could use a cast, but for more clarity, we assign it to a temporary  [2.x.106]  (short for <code>pointer to make_hanging_node_constraints</code>) with the right type, and using this pointer instead.
* 

* 
* [1.x.78]
* 
*  Start a side task then continue on the main thread
* 

* 
* [1.x.79]
* 
*  Wait for the side task to be done before going further
* 

* 
* [1.x.80]
* 
*  Finally initialize the matrix and right hand side vector
* 

* 
* [1.x.81]
* 
*  The second function of this class simply solves the linear system by a preconditioned conjugate gradient method. This has been extensively discussed before, so we don't dwell into it any more.
* 

* 
* [1.x.82]
* 
*   [1.x.83]  [1.x.84]
* 

* 
*  In the previous section, a base class for Laplace solvers was implemented, that lacked the functionality to assemble the right hand side vector, however, for reasons that were explained there. Now we implement a corresponding class that can do this for the case that the right hand side of a problem is given as a function object.     
*   The actions of the class are rather what you have seen already in previous examples already, so a brief explanation should suffice: the constructor takes the same data as does that of the underlying class (to which it passes all information) except for one function object that denotes the right hand side of the problem. A pointer to this object is stored (again as a  [2.x.107] , in order to make sure that the function object is not deleted as long as it is still used by this class).     
*   The only functional part of this class is the  [2.x.108]  method that does what its name suggests.
* 

* 
* [1.x.85]
* 
*  The constructor of this class basically does what it is announced to do above...
* 

* 
* [1.x.86]
* 
*  ... as does the  [2.x.109]  function. Since this is explained in several of the previous example programs, we leave it at that.
* 

* 
* [1.x.87]
* 
*   [1.x.88]  [1.x.89]
* 

* 
*  By now, all functions of the abstract base class except for the  [2.x.110]  function have been implemented. We will now have two classes that implement this function for the  [2.x.111]  class, one doing global refinement, one a form of local refinement.     
*   The first, doing global refinement, is rather simple: its main function just calls  [2.x.112] , which does all the work.     
*   Note that since the  [2.x.113]  base class of the  [2.x.114]  class is virtual, we have to declare a constructor that initializes the immediate base class as well as the abstract virtual one.     
*   Apart from this technical complication, the class is probably simple enough to be left without further comments.
* 

* 
* [1.x.90]
* 
*   [1.x.91]  [1.x.92]
* 

* 
*  The second class implementing refinement strategies uses the Kelly refinement indicator used in various example programs before. Since this indicator is already implemented in a class of its own inside the deal.II library, there is not much t do here except cal the function computing the indicator, then using it to select a number of cells for refinement and coarsening, and refinement the mesh accordingly.     
*   Again, this should now be sufficiently standard to allow the omission of further comments.
* 

* 
* [1.x.93]
* 
*   [1.x.94]  [1.x.95]
* 

* 
*  As this is one more academic example, we'd like to compare exact and computed solution against each other. For this, we need to declare function classes representing the exact solution (for comparison and for the Dirichlet boundary values), as well as a class that denotes the right hand side of the equation (this is simply the Laplace operator applied to the exact solution we'd like to recover).   
*   For this example, let us choose as exact solution the function  [2.x.115] . In more than two dimensions, simply repeat the sine-factor with  [2.x.116]  and so on. Given this, the following two classes are probably straightforward from the previous examples.
* 

* 
* [1.x.96]
* 
*   [1.x.97]  [1.x.98]
* 

* 
*  What is now missing are only the functions that actually select the various options, and run the simulation on successively finer grids to monitor the progress as the mesh is refined.   
*   This we do in the following function: it takes a solver object, and a list of postprocessing (evaluation) objects, and runs them with intermittent mesh refinement:
* 

* 
* [1.x.99]
* 
*  We will give an indicator of the step we are presently computing, in order to keep the user informed that something is still happening, and that the program is not in an endless loop. This is the head of this status line:
* 

* 
* [1.x.100]
* 
*  Then start a loop which only terminates once the number of degrees of freedom is larger than 20,000 (you may of course change this limit, if you need more
* 
*  -  or less
* 
*  -  accuracy from your program).
* 

* 
* [1.x.101]
* 
*  Then give the  [2.x.117]  indication for this iteration. Note that the  [2.x.118]  is needed to have the text actually appear on the screen, rather than only in some buffer that is only flushed the next time we issue an end-line.
* 

* 
* [1.x.102]
* 
*  Now solve the problem on the present grid, and run the evaluators on it. The long type name of iterators into the list is a little annoying, but could be shortened by an alias, if so desired.
* 

* 
* [1.x.103]
* 
*  Now check whether more iterations are required, or whether the loop shall be ended:
* 

* 
* [1.x.104]
* 
*  Finally end the line in which we displayed status reports:
* 

* 
* [1.x.105]
* 
*  The final function is one which takes the name of a solver (presently "kelly" and "global" are allowed), creates a solver object out of it using a coarse grid (in this case the ubiquitous unit square) and a finite element object (here the likewise ubiquitous bilinear one), and uses that solver to ask for the solution of the problem on a sequence of successively refined grids.   
*   The function also sets up two of evaluation functions, one evaluating the solution at the point (0.5,0.5), the other writing out the solution to a file.
* 

* 
* [1.x.106]
* 
*  First minor task: tell the user what is going to happen. Thus write a header line, and a line with all '-' characters of the same length as the first one right below.
* 

* 
* [1.x.107]
* 
*  Then set up triangulation, finite element, etc.
* 

* 
* [1.x.108]
* 
*  Create a solver object of the kind indicated by the argument to this function. If the name is not recognized, throw an exception! The respective solver object is stored in a  [2.x.119]  to avoid having to delete the pointer after use.
* 

* 
* [1.x.109]
* 
*  Next create a table object in which the values of the numerical solution at the point (0.5,0.5) will be stored, and create a respective evaluation object:
* 

* 
* [1.x.110]
* 
*  Also generate an evaluator which writes out the solution:
* 

* 
* [1.x.111]
* 
*  Take these two evaluation objects and put them in a list...
* 

* 
* [1.x.112]
* 
*  ... which we can then pass on to the function that actually runs the simulation on successively refined grids:
* 

* 
* [1.x.113]
* 
*  When this all is done, write out the results of the point evaluations:
* 

* 
* [1.x.114]
* 
*  And one blank line after all results:
* 

* 
* [1.x.115]
* 
*  There is not much to say about the main function. It follows the same pattern as in all previous examples, with attempts to catch thrown exceptions, and displaying as much information as possible if we should get some. The rest is self-explanatory.
* 

* 
* [1.x.116]
* [1.x.117][1.x.118]
* 

* 
* 

* The results of this program are not that interesting
* 
*  - after allits purpose was not to demonstrate some new mathematical idea, andalso not how to program with deal.II, but rather to use the materialwhich we have developed in the previous examples to form somethingwhich demonstrates a way to build modern finite element software in amodular and extensible way.
* 

* 
* Nevertheless, we of course show the results of the program. Offoremost interest is the point value computation, for which we hadimplemented the corresponding evaluation class. The results (i.e. theoutput) of the program looks as follows:
* [1.x.119]
* 
* 

* What surprises here is that the exact value is 1.59491554..., and thatit is apparently surprisingly complicated to compute the solution even toonly one per cent accuracy, although the solution is smooth (in factinfinitely often differentiable). This smoothness is shown in thegraphical output generated by the program, here coarse grid and thefirst 9 refinement steps of the Kelly refinement indicator:
* 

*  [2.x.120] 
* 

* While we're already at watching pictures, this is the eighth grid, asviewed from top:
* 

*  [2.x.121] 
* 

* However, we are not yet finished with evaluation the point valuecomputation. In fact, plotting the error [2.x.122]  for the tworefinement criteria yields the following picture:
* 

*  [2.x.123] 
* 

* 
* 

* What  [2.x.124] is [2.x.125]  disturbing about this picture is that not only is theadaptive mesh refinement not better than global refinement as onewould usually expect, it is even significantly worse since itsconvergence is irregular, preventing all extrapolation techniques whenusing the values of subsequent meshes! On the other hand, globalrefinement provides a perfect  [2.x.126]  or  [2.x.127] convergence history and provides every opportunity to even improve onthe point values by extrapolation. Global mesh refinement musttherefore be considered superior in this example! This is even moresurprising as the evaluation point is not somewhere in the left partwhere the mesh is coarse, but rather to the right and the adaptiverefinement should refine the mesh around the evaluation point as well.
* 

* 
* We thus close the discussion of this example program with a question:
* <p align="center">  <strong> [2.x.128] What is wrong with adaptivity if it is not better than  global refinement? [2.x.129] </strong>
* 

* 
* 

*  [2.x.130] Exercise at the end of this example: [2.x.131]  There is a simple reasonfor the bad and irregular behavior of the adapted mesh solutions. Itis simple to find out by looking at the mesh around the evaluationpoint in each of the steps
* 
*  - the data for this is in the output filesof the program. An exercise would therefore be to modify the meshrefinement routine such that the problem (once you remark it) isavoided. The second exercise is to check whether the results are thenbetter than global refinement, and if so if even a better order ofconvergence (in terms of the number of degrees of freedom) isachieved, or only by a better constant.
* 

* 
* ( [2.x.132] Very brief answers for the impatient: [2.x.133]  at steps with largererrors, the mesh is not regular at the point of evaluation, i.e. someof the adjacent cells have hanging nodes; this destroys somesuperapproximation effects of which the globally refined mesh canprofit. Answer 2: this quick hack
* [1.x.120]
* in the refinement function of the Kelly refinement class right beforeexecuting refinement would improve the results (exercise: what doesthe code do?), making them consistently better than globalrefinement. Behavior is still irregular, though, so no results aboutan order of convergence are possible.)
* 

* [1.x.121][1.x.122] [2.x.134] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-14_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34][1.x.35][1.x.36][1.x.37][1.x.38][1.x.39][1.x.40][1.x.41][1.x.42][1.x.43]
* [1.x.44][1.x.45][1.x.46]
* 

* [1.x.47][1.x.48]
* 

* The Heidelberg group of Professor Rolf Rannacher, to which the three initialauthors of the deal.II library belonged during their PhD time and partly alsoafterwards, has been involved with adaptivity and error estimation for finiteelement discretizations since the mid-1990ies. The main achievement is thedevelopment of error estimates for arbitrary functionals of the solution, andof optimal mesh refinement for its computation.
* We will not discuss the derivation of these concepts in too great detail, butwill implement the main ideas in the present example program. For a thoroughintroduction into the general idea, we refer to the seminal work of Becker andRannacher  [2.x.2] ,  [2.x.3] , and the overview article of the same authors inActa Numerica  [2.x.4] ; the first introduces the concept of errorestimation and adaptivity for general functional output for the Laplaceequation, while the second gives many examples of applications of theseconcepts to a large number of other, more complicated equations. Forapplications to individual types of equations, see also the publications byBecker  [2.x.5] ,  [2.x.6] , Kanschat  [2.x.7] ,  [2.x.8] , Suttmeier [2.x.9] ,  [2.x.10] ,  [2.x.11] ,  [2.x.12] , Bangerth  [2.x.13] , [2.x.14] ,  [2.x.15] ,  [2.x.16] , and Hartmann  [2.x.17] ,  [2.x.18] , [2.x.19] . All of these works, from the original introduction by Becker andRannacher to individual contributions to particular equations, have later beensummarized in a book by Bangerth and Rannacher that covers all of these topics,see  [2.x.20] .
* 

* The basic idea is the following: in applications, one is not usuallyinterested in the solution per se, but rather in certain aspects of it. Forexample, in simulations of flow problems, one may want to know the lift ordrag of a body immersed in the fluid; it is this quantity that we want to knowto best accuracy, and whether the rest of the solution of the describingequations is well resolved is not of primary interest. Likewise, in elasticityone might want to know about values of the stress at certain points to guesswhether maximal load values of joints are safe, for example. Or, in radiativetransfer problems, mean flux intensities are of interest.
* In all the cases just listed, it is the evaluation of a functional  [2.x.21]  ofthe solution which we are interested in, rather than the values of  [2.x.22] everywhere. Since the exact solution  [2.x.23]  is not available, but only itsnumerical approximation  [2.x.24] , it is sensible to ask whether the computedvalue  [2.x.25]  is within certain limits of the exact value  [2.x.26] , i.e. wewant to bound the error with respect to this functional,  [2.x.27] .
* For simplicity of exposition, we henceforth assume that both the quantity ofinterest  [2.x.28] , as well as the equation are linear, and we will in particularshow the derivation for the Laplace equation with homogeneous Dirichletboundary conditions, although the concept is much more general. For thisgeneral case, we refer to the references listed above.  The goal is to obtainbounds on the error,  [2.x.29] . For this, let us denote by  [2.x.30]  thesolution of a dual problem, defined as follows:[1.x.49]where  [2.x.31]  is the bilinear form associated with the differentialequation, and the test functions are chosen from the corresponding solutionspace. Then, taking as special test function  [2.x.32]  the error, we havethat[1.x.50]and we can, by Galerkin orthogonality, rewrite this as[1.x.51]where  [2.x.33]  can be chosen from the discrete test space inwhatever way we find convenient.
* Concretely, for Laplace's equation, the error identity reads[1.x.52]Because we want to use this formula not only to compute error, butalso to refine the mesh, we need to rewrite the expression above as asum over cells where each cell's contribution can then be used as anerror indicator for this cell.Thus, we split the scalar products into terms for each cell, andintegrate by parts on each of them:[1.x.53]
* Next we use that  [2.x.34] , and that for solutions of the Laplaceequation, the solution is smooth enough that  [2.x.35]  iscontinuous almost everywhere
* 
*  -  so the terms involving  [2.x.36]  on onecell cancels with that on its neighbor, where the normal vector has theopposite sign. (The same is not true for  [2.x.37] , though.)At the boundary of the domain, where there is no neighbor cellwith which this term could cancel, the weight  [2.x.38]  can be chosen aszero, and the whole term disappears.
* Thus, we have[1.x.54]
* In a final step, note that when taking the normal derivative of  [2.x.39] , we meanthe value of this quantity as taken from this side of the cell (for the usualLagrange elements, derivatives are not continuous across edges). We thenrewrite the above formula by exchanging half of the edge integral of cell  [2.x.40] with the neighbor cell  [2.x.41] , to obtain[1.x.55]
* Using that for the normal vectors on adjacent cells we have  [2.x.42] , we define the jump of thenormal derivative by[1.x.56]and get the final form after setting the discrete function  [2.x.43] , whichis by now still arbitrary, to the point interpolation of the dual solution, [2.x.44] :[1.x.57]
* 
* With this, we have obtained an exact representation of the error of the finiteelement discretization with respect to arbitrary (linear) functionals [2.x.45] . Its structure is a weighted form of a residual estimator, as both [2.x.46]  and  [2.x.47]  are cell and edge residuals that vanishon the exact solution, and  [2.x.48]  are weights indicating how important theresiduals on a certain cell is for the evaluation of the given functional.Furthermore, it is a cell-wise quantity, so we can use it as a mesh refinementcriterion. The question, is: how to evaluate it? After all, the evaluationrequires knowledge of the dual solution  [2.x.49] , which carries the informationabout the quantity we want to know to best accuracy.
* In some, very special cases, this dual solution is known. For example, if thefunctional  [2.x.50]  is the point evaluation,  [2.x.51] , thenthe dual solution has to satisfy[1.x.58]with the Dirac delta function on the right hand side, and the dual solution isthe Green's function with respect to the point  [2.x.52] . For simple geometries,this function is analytically known, and we could insert it into the errorrepresentation formula.
* However, we do not want to restrict ourselves to such special cases. Rather,we will compute the dual solution numerically, and approximate  [2.x.53]  by somenumerically obtained  [2.x.54] . We note that it is not sufficient to computethis approximation  [2.x.55]  using the same method as used for the primalsolution  [2.x.56] , since then  [2.x.57] , and the overall errorestimate would be zero. Rather, the approximation  [2.x.58]  has to be from alarger space than the primal finite element space. There are various ways toobtain such an approximation (see the cited literature), and we will choose tocompute it with a higher order finite element space. While this is certainlynot the most efficient way, it is simple since we already have all we need todo that in place, and it also allows for simple experimenting. For moreefficient methods, again refer to the given literature, in particular [2.x.59] ,  [2.x.60] .
* With this, we end the discussion of the mathematical side of this program andturn to the actual implementation.
* 

* 
*  [2.x.61]  There are two steps above that do not seem necessary if all youcare about is computing the error: namely, (i) the subtraction of [2.x.62]  from  [2.x.63] , and (ii) splitting the integral into a sum of cellsand integrating by parts on each. Indeed, neither of these two stepschange  [2.x.64]  at all, as we only ever consider identities above untilthe substitution of  [2.x.65]  by  [2.x.66] . In other words, if you careonly about [1.x.59]  [2.x.67] , then these stepsare not necessary. On the other hand, if you want to use the errorestimate also as a refinement criterion for each cell of the mesh,then it is necessary to (i) break the estimate into a sum of cells,and (ii) massage the formulas in such a way that each cell'scontributions have something to do with the local error. (While thecontortions above do not change the value of the [1.x.60]  [2.x.68] ,they change the values we compute for each cell  [2.x.69] .) To this end, wewant to write everything in the form "residual times dual weight"where a "residual" is something that goes to zero as the approximationbecomes  [2.x.70]  better and better. For example, the quantity  [2.x.71]  is not a residual, since it simply converges to the (normalcomponent of) the gradient of the exact solution. On the other hand, [2.x.72]  is a residual because it converges to  [2.x.73] . All of the steps we have taken above in developing the finalform of  [2.x.74]  have indeed had the goal of bringing the final formulainto a form where each term converges to zero as the discrete solution [2.x.75]  converges to  [2.x.76] . This then allows considering each cell'scontribution as an "error indicator" that also converges to zero
* 
*  -  asit should as the mesh is refined.
* 

* 
* [1.x.61][1.x.62]
* 

* The  [2.x.77]  example program builds heavily on the techniques already used inthe  [2.x.78]  program. Its implementation of the dual weighted residual errorestimator explained above is done by deriving a second class, properly called [2.x.79]  base class, and having a class( [2.x.80] ) that joins the two again and controls the solutionof the primal and dual problem, and then uses both to compute the errorindicator for mesh refinement.
* The program continues the modular concept of the previous example, byimplementing the dual functional, describing quantity of interest, by anabstract base class, and providing two different functionals which implementthis interface. Adding a different quantity of interest is thus simple.
* One of the more fundamental differences is the handling of data. A common caseis that you develop a program that solves a certain equation, and test it withdifferent right hand sides, different domains, different coefficients andboundary values, etc. Usually, these have to match, so that exact solutionsare known, or that their combination makes sense at all.
* We demonstrate a way how this can be achieved in a simple, yet very flexibleway. We will put everything that belongs to a certain setup into one class,and provide a little C++ mortar around it, so that entire setups (domains,coefficients, right hand sides, etc.) can be exchanged by only changingsomething in  [2.x.81] one [2.x.82]  place.
* Going this way a little further, we have also centralized all the otherparameters that describe how the program is to work in one place, such as theorder of the finite element, the maximal number of degrees of freedom, theevaluation objects that shall be executed on the computed solutions, and soon. This allows for simpler configuration of the program, and we will show ina later program how to use a library class that can handle setting theseparameters by reading an input file. The general aim is to reduce the placeswithin a program where one may have to look when wanting to change someparameter, as it has turned out in practice that one forgets where they are asprograms grow. Furthermore, putting all options describing what the programdoes in a certain run into a file (that can be stored with the results) helpsrepeatability of results more than if the various flags were set somewhere inthe program, where their exact values are forgotten after the next change tothis place.
* Unfortunately, the program has become rather long. While this admittedlyreduces its usefulness as an example program, we think that it is a very goodstarting point for development of a program for other kinds of problems,involving different equations than the Laplace equation treated here.Furthermore, it shows everything that we can show you about our way of aposteriori error estimation, and its structure should make it simple for youto adjust this method to other problems, other functionals, other geometries,coefficients, etc.
* The author believes that the present program is his masterpiece among theexample programs, regarding the mathematical complexity, as well as thesimplicity to add extensions. If you use this program as a basis for your ownprograms, we would kindly like to ask you to state this fact and the name ofthe author of the example program, Wolfgang Bangerth, in publications thatarise from that, of your program consists in a considerable part of theexample program.
* 

*  [1.x.63] [1.x.64]
*  Start out with well known things...
* 

* 
* [1.x.65]
* 
*  The last step is as in all previous programs:
* 

* 
* [1.x.66]
* 
*   [1.x.67]  [1.x.68]
* 

* 
*  As mentioned in the introduction, significant parts of the program have simply been taken over from the  [2.x.83]  example program. We therefore only comment on those things that are new.   
*   First, the framework for evaluation of solutions is unchanged, i.e. the base class is the same, and the class to evaluate the solution at a grid point is unchanged:
* 

* 
* [1.x.69]
* 
*   [1.x.70]  [1.x.71]
* 

* 
* [1.x.72]
* 
*   [1.x.73]  [1.x.74]
* 

* 
* [1.x.75]
* 
*   [1.x.76]  [1.x.77]
* 

* 
*  Besides the class implementing the evaluation of the solution at one point, we here provide one which evaluates the gradient at a grid point. Since in general the gradient of a finite element function is not continuous at a vertex, we have to be a little bit more careful here. What we do is to loop over all cells, even if we have found the point already on one cell, and use the mean value of the gradient at the vertex taken from all adjacent cells.     
*   Given the interface of the  [2.x.84]  class, the declaration of this class provides little surprise, and neither does the constructor:
* 

* 
* [1.x.78]
* 
*  The more interesting things happen inside the function doing the actual evaluation:
* 

* 
* [1.x.79]
* 
*  This time initialize the return value with something useful, since we will have to add up a number of contributions and take the mean value afterwards...
* 

* 
* [1.x.80]
* 
*  ...then have some objects of which the meaning will become clear below...
* 

* 
* [1.x.81]
* 
*  ...and next loop over all cells and their vertices, and count how often the vertex has been found:
* 

* 
* [1.x.82]
* 
*  Things are now no more as simple, since we can't get the gradient of the finite element field as before, where we simply had to pick one degree of freedom at a vertex.               
*   Rather, we have to evaluate the finite element field on this cell, and at a certain point. As you know, evaluating finite element fields at certain points is done through the  [2.x.85]  class, so we use that. The question is: the  [2.x.86]  object needs to be a given a quadrature formula and can then compute the values of finite element quantities at the quadrature points. Here, we don't want to do quadrature, we simply want to specify some points!               
*   Nevertheless, the same way is chosen: use a special quadrature rule with points at the vertices, since these are what we are interested in. The appropriate rule is the trapezoidal rule, so that is the reason why we used that one above.               
*   Thus: initialize the  [2.x.87]  object on this cell,
* 

* 
* [1.x.83]
* 
*  and extract the gradients of the solution vector at the vertices:
* 

* 
* [1.x.84]
* 
*  Now we have the gradients at all vertices, so pick out that one which belongs to the evaluation point (note that the order of vertices is not necessarily the same as that of the quadrature points):
* 

* 
* [1.x.85]
* 
*  Check that the evaluation point was indeed found,
* 

* 
* [1.x.86]
* 
*  and if so take the x-derivative of the gradient there as the value which we are interested in, and increase the counter indicating how often we have added to that variable:
* 

* 
* [1.x.87]
* 
*  Finally break out of the innermost loop iterating over the vertices of the present cell, since if we have found the evaluation point at one vertex it cannot be at a following vertex as well:
* 

* 
* [1.x.88]
* 
*  Now we have looped over all cells and vertices, so check whether the point was found:
* 

* 
* [1.x.89]
* 
*  We have simply summed up the contributions of all adjacent cells, so we still have to compute the mean value. Once this is done, report the status:
* 

* 
* [1.x.90]
* 
*   [1.x.91]  [1.x.92]
* 

* 
*  Since this program has a more difficult structure (it computed a dual solution in addition to a primal one), writing out the solution is no more done by an evaluation object since we want to write both solutions at once into one file, and that requires some more information than available to the evaluation classes.     
*   However, we also want to look at the grids generated. This again can be done with one such class. Its structure is analog to the  [2.x.88]  class of the previous example program, so we do not discuss it here in more detail. Furthermore, everything that is used here has already been used in previous example programs.
* 

* 
* [1.x.93]
* 
*   [1.x.94]  [1.x.95]
* 

* 
*  Next are the actual solver classes. Again, we discuss only the differences to the previous program.
* 

* 
* [1.x.96]
* 
*   [1.x.97]  [1.x.98]
* 

* 
*  This class is almost unchanged, with the exception that it declares two more functions:  [2.x.89]  will be used to generate output files from the actual solutions computed by derived classes, and the  [2.x.90]  function by which the testing framework sets the number of the refinement cycle to a local variable in this class; this number is later used to generate filenames for the solution output.
* 

* 
* [1.x.99]
* 
*   [1.x.100]  [1.x.101]
* 

* 
*  Likewise, the  [2.x.91]  class is entirely unchanged and will thus not be discussed.
* 

* 
* [1.x.102]
* 
*  The remainder of the class is essentially a copy of  [2.x.92]  as well, including the data structures and functions necessary to compute the linear system in parallel using the WorkStream framework:
* 

* 
* [1.x.103]
* 
*  The following few functions and constructors are verbatim copies taken from  [2.x.93] :
* 

* 
* [1.x.104]
* 
*  Now for the functions that implement actions in the linear system class. First, the constructor initializes all data elements to their correct sizes, and sets up a number of additional data structures, such as constraints due to hanging nodes. Since setting up the hanging nodes and finding out about the nonzero elements of the matrix is independent, we do that in parallel (if the library was configured to use concurrency, at least; otherwise, the actions are performed sequentially). Note that we start only one thread, and do the second action in the main thread. Since only one thread is generated, we don't use the  [2.x.94]  class here, but rather use the one created task object directly to wait for this particular task's exit. The approach is generally the same as the one we have used in  [2.x.95]  above.     
*   Note that taking the address of the  [2.x.96]  function is a little tricky, since there are actually three functions of this name, one for each supported space dimension. Taking addresses of overloaded functions is somewhat complicated in C++, since the address-of operator  [2.x.97]  in that case returns a set of values (the addresses of all functions with that name), and selecting the right one is then the next step. If the context dictates which one to take (for example by assigning to a function pointer of known type), then the compiler can do that by itself, but if this set of pointers shall be given as the argument to a function that takes a template, the compiler could choose all without having a preference for one. We therefore have to make it clear to the compiler which one we would like to have; for this, we could use a cast, but for more clarity, we assign it to a temporary  [2.x.98]  (short for <code>pointer to make_hanging_node_constraints</code>) with the right type, and using this pointer instead.
* 

* 
* [1.x.105]
* 
*  Start a side task then continue on the main thread
* 

* 
* [1.x.106]
* 
*  Wait for the side task to be done before going further
* 

* 
* [1.x.107]
* 
*   [1.x.108]  [1.x.109]
* 

* 
*  The  [2.x.99]  class is also mostly unchanged except for implementing the  [2.x.100]  function. We keep the  [2.x.101]  classes in this program, and they can then rely on the default implementation of this function which simply outputs the primal solution. The class implementing dual weighted error estimators will overload this function itself, to also output the dual solution.
* 

* 
* [1.x.110]
* 
*   [1.x.111]  [1.x.112]
* 

* 
*  For the following two classes, the same applies as for most of the above: the class is taken from the previous example as-is:
* 

* 
* [1.x.113]
* 
*   [1.x.114]  [1.x.115]
* 

* 
*  This class is a variant of the previous one, in that it allows to weight the refinement indicators we get from the library's Kelly indicator by some function. We include this class since the goal of this example program is to demonstrate automatic refinement criteria even for complex output quantities such as point values or stresses. If we did not solve a dual problem and compute the weights thereof, we would probably be tempted to give a hand-crafted weighting to the indicators to account for the fact that we are going to evaluate these quantities. This class accepts such a weighting function as argument to its constructor:
* 

* 
* [1.x.116]
* 
*  Now, here comes the main function, including the weighting:
* 

* 
* [1.x.117]
* 
*  First compute some residual based error indicators for all cells by a method already implemented in the library. What exactly we compute here is described in more detail in the documentation of that class.
* 

* 
* [1.x.118]
* 
*  Next weigh each entry in the vector of indicators by the value of the function given to the constructor, evaluated at the cell center. We need to write the result into the vector entry that corresponds to the current cell, which we can obtain by asking the cell what its index among all active cells is using  [2.x.102]  (In reality, this index is zero for the first cell we handle in the loop, one for the second cell, etc., and we could as well just keep track of this index using an integer counter; but using  [2.x.103]  makes this more explicit.)
* 

* 
* [1.x.119]
* 
*   [1.x.120]  [1.x.121]   
*   In this example program, we work with the same data sets as in the previous one, but as it may so happen that someone wants to run the program with different boundary values and right hand side functions, or on a different grid, we show a simple technique to do exactly that. For more clarity, we furthermore pack everything that has to do with equation data into a namespace of its own.   
*   The underlying assumption is that this is a research program, and that there we often have a number of test cases that consist of a domain, a right hand side, boundary values, possibly a specified coefficient, and a number of other parameters. They often vary all at the same time when shifting from one example to another. To make handling such sets of problem description parameters simple is the goal of the following.   
*   Basically, the idea is this: let us have a structure for each set of data, in which we pack everything that describes a test case: here, these are two subclasses, one called  [2.x.104]  for the boundary values of the exact solution, and one called  [2.x.105] , and then a way to generate the coarse grid. Since the solution of the previous example program looked like curved ridges, we use this name here for the enclosing class. Note that the names of the two inner classes have to be the same for all enclosing test case classes, and also that we have attached the dimension template argument to the enclosing class rather than to the inner ones, to make further processing simpler.  (From a language viewpoint, a namespace would be better to encapsulate these inner classes, rather than a structure. However, namespaces cannot be given as template arguments, so we use a structure to allow a second object to select from within its given argument. The enclosing structure, of course, has no member variables apart from the classes it declares, and a static function to generate the coarse mesh; it will in general never be instantiated.)   
*   The idea is then the following (this is the right time to also take a brief look at the code below): we can generate objects for boundary values and right hand side by simply giving the name of the outer class as a template argument to a class which we call here  [2.x.106] , and it then creates objects for the inner classes. In this case, to get all that characterizes the curved ridge solution, we would simply generate an instance of  [2.x.107] , and everything we need to know about the solution would be static member variables and functions of that object.   
*   This approach might seem like overkill in this case, but will become very handy once a certain set up is not only characterized by Dirichlet boundary values and a right hand side function, but in addition by material properties, Neumann values, different boundary descriptors, etc. In that case, the  [2.x.108]  class might consist of a dozen or more objects, and each descriptor class (like the  [2.x.109]  class below) would have to provide them. Then, you will be happy to be able to change from one set of data to another by only changing the template argument to the  [2.x.110]  class at one place, rather than at many.   
*   With this framework for different test cases, we are almost finished, but one thing remains: by now we can select statically, by changing one template argument, which data set to choose. In order to be able to do that dynamically, i.e. at run time, we need a base class. This we provide in the obvious way, see below, with virtual abstract functions. It forces us to introduce a second template parameter  [2.x.111]  which we need for the base class (which could be avoided using some template magic, but we omit that), but that's all.   
*   Adding new testcases is now simple, you don't have to touch the framework classes, only a structure like the  [2.x.112]  one is needed.
* 

* 
* [1.x.122]
* 
*   [1.x.123]  [1.x.124]
* 

* 
*  Based on the above description, the  [2.x.113]  class then looks as follows. To allow using the  [2.x.114]  class with this class, we derived from the  [2.x.115]  class.
* 

* 
* [1.x.125]
* 
*  And now for the derived class that takes the template argument as explained above.     
*   Here we pack the data elements into private variables, and allow access to them through the methods of the base class.
* 

* 
* [1.x.126]
* 
*  We have to provide definitions for the static member variables of the above class:
* 

* 
* [1.x.127]
* 
*  And definitions of the member functions:
* 

* 
* [1.x.128]
* 
*   [1.x.129]  [1.x.130]
* 

* 
*  The class that is used to describe the boundary values and right hand side of the  [2.x.116]  problem already used in the  [2.x.117]  example program is then like so:
* 

* 
* [1.x.131]
* 
*   [1.x.132]  [1.x.133]
* 

* 
*  This example program was written while giving practical courses for a lecture on adaptive finite element methods and duality based error estimates. For these courses, we had one exercise, which required to solve the Laplace equation with constant right hand side on a square domain with a square hole in the center, and zero boundary values. Since the implementation of the properties of this problem is so particularly simple here, lets do it. As the number of the exercise was 2.3, we take the liberty to retain this name for the class as well.
* 

* 
* [1.x.134]
* 
*  We need a class to denote the boundary values of the problem. In this case, this is simple: it's the zero function, so don't even declare a class, just an alias:
* 

* 
* [1.x.135]
* 
*  Second, a class that denotes the right hand side. Since they are constant, just subclass the corresponding class of the library and be done:
* 

* 
* [1.x.136]
* 
*  Finally a function to generate the coarse grid. This is somewhat more complicated here, see immediately below.
* 

* 
* [1.x.137]
* 
*  As stated above, the grid for this example is the square [-1,1]^2 with the square [-1/2,1/2]^2 as hole in it. We create the coarse grid as 4 times 4 cells with the middle four ones missing. To understand how exactly the mesh is going to look, it may be simplest to just look at the "Results" section of this tutorial program first. In general, if you'd like to understand more about creating meshes either from scratch by hand, as we do here, or using other techniques, you should take a look at  [2.x.118] .     
*   Of course, the example has an extension to 3d, but since this function cannot be written in a dimension independent way we choose not to implement this here, but rather only specialize the template for dim=2. If you compile the program for 3d, you'll get a message from the linker that this function is not implemented for 3d, and needs to be provided.     
*   For the creation of this geometry, the library has no predefined method. In this case, the geometry is still simple enough to do the creation by hand, rather than using a mesh generator.
* 

* 
* [1.x.138]
* 
*  We first define the space dimension, to allow those parts of the function that are actually dimension independent to use this variable. That makes it simpler if you later take this as a starting point to implement a 3d version of this mesh. The next step is then to have a list of vertices. Here, they are 24 (5 times 5, with the middle one omitted). It is probably best to draw a sketch here.
* 

* 
* [1.x.139]
* 
*  Next, we have to define the cells and the vertices they contain.
* 

* 
* [1.x.140]
* 
*  Again, we generate a C++ vector type from this, but this time by looping over the cells (yes, this is boring). Additionally, we set the material indicator to zero for all the cells:
* 

* 
* [1.x.141]
* 
*  Finally pass all this information to the library to generate a triangulation. The last parameter may be used to pass information about non-zero boundary indicators at certain faces of the triangulation to the library, but we don't want that here, so we give an empty object:
* 

* 
* [1.x.142]
* 
*  And since we want that the evaluation point (3/4,3/4) in this example is a grid point, we refine once globally:
* 

* 
* [1.x.143]
* 
*   [1.x.144]  [1.x.145]   
*   As you have now read through this framework, you may be wondering why we have not chosen to implement the classes implementing a certain setup (like the  [2.x.119]  class) directly as classes derived from  [2.x.120] . Indeed, we could have done very well so. The only reason is that then we would have to have member variables for the solution and right hand side classes in the  [2.x.121]  class, as well as member functions overloading the abstract functions of the base class giving access to these member variables. The  [2.x.122]  class has the sole reason to relieve us from the need to reiterate these member variables and functions that would be necessary in all such classes. In some way, the template mechanism here only provides a way to have default implementations for a number of functions that depend on external quantities and can thus not be provided using normal virtual functions, at least not without the help of templates.   
*   However, there might be good reasons to actually implement classes derived from  [2.x.123] , for example if the solution or right hand side classes require constructors that take arguments, which the  [2.x.124]  class cannot provide. In that case, subclassing is a worthwhile strategy. Other possibilities for special cases are to derive from  [2.x.125]  where  [2.x.126]  denotes a class, or even to explicitly specialize  [2.x.127] . The latter allows to transparently use the way the  [2.x.128]  class is used for other set-ups, but with special actions taken for special arguments.   
*   A final observation favoring the approach taken here is the following: we have found numerous times that when starting a project, the number of parameters (usually boundary values, right hand side, coarse grid, just as here) was small, and the number of test cases was small as well. One then starts out by handcoding them into a number of  [2.x.129]  statements. Over time, projects grow, and so does the number of test cases. The number of  [2.x.130]  statements grows with that, and their length as well, and one starts to find ways to consider impossible examples where domains, boundary values, and right hand sides do not fit together any more, and starts losing the overview over the whole structure. Encapsulating everything belonging to a certain test case into a structure of its own has proven worthwhile for this, as it keeps everything that belongs to one test case in one place. Furthermore, it allows to put these things all in one or more files that are only devoted to test cases and their data, without having to bring their actual implementation into contact with the rest of the program.
* 

* 
*  
*  
*  [1.x.146]  [1.x.147]
* 

* 
*  As with the other components of the program, we put everything we need to describe dual functionals into a namespace of its own, and define an abstract base class that provides the interface the class solving the dual problem needs for its work.   
*   We will then implement two such classes, for the evaluation of a point value and of the derivative of the solution at that point. For these functionals we already have the corresponding evaluation objects, so they are complementary.
* 

* 
* [1.x.148]
* 
*   [1.x.149]  [1.x.150]
* 

* 
*  First start with the base class for dual functionals. Since for linear problems the characteristics of the dual problem play a role only in the right hand side, we only need to provide for a function that assembles the right hand side for a given discretization:
* 

* 
* [1.x.151]
* 
*   [1.x.152]  [1.x.153]
* 

* 
*  As a first application, we consider the functional corresponding to the evaluation of the solution's value at a given point which again we assume to be a vertex. Apart from the constructor that takes and stores the evaluation point, this class consists only of the function that implements assembling the right hand side.
* 

* 
* [1.x.154]
* 
*  As for doing the main purpose of the class, assembling the right hand side, let us first consider what is necessary: The right hand side of the dual problem is a vector of values J(phi_i), where J is the error functional, and phi_i is the i-th shape function. Here, J is the evaluation at the point x0, i.e. J(phi_i)=phi_i(x0).     
*   Now, we have assumed that the evaluation point is a vertex. Thus, for the usual finite elements we might be using in this program, we can take for granted that at such a point exactly one shape function is nonzero, and in particular has the value one. Thus, we set the right hand side vector to all-zeros, then seek for the shape function associated with that point and set the corresponding value of the right hand side vector to one:
* 

* 
* [1.x.155]
* 
*  So, first set everything to zeros...
* 

* 
* [1.x.156]
* 
*  ...then loop over cells and find the evaluation point among the vertices (or very close to a vertex, which may happen due to floating point round-off):
* 

* 
* [1.x.157]
* 
*  Ok, found, so set corresponding entry, and leave function since we are finished:
* 

* 
* [1.x.158]
* 
*  Finally, a sanity check: if we somehow got here, then we must have missed the evaluation point, so raise an exception unconditionally:
* 

* 
* [1.x.159]
* 
*   [1.x.160]  [1.x.161]
* 

* 
*  As second application, we again consider the evaluation of the x-derivative of the solution at one point. Again, the declaration of the class, and the implementation of its constructor is not too interesting:
* 

* 
* [1.x.162]
* 
*  What is interesting is the implementation of this functional: here, J(phi_i)=d/dx phi_i(x0).     
*   We could, as in the implementation of the respective evaluation object take the average of the gradients of each shape function phi_i at this evaluation point. However, we take a slightly different approach: we simply take the average over all cells that surround this point. The question which cells  [2.x.131]  the evaluation point is made dependent on the mesh width by including those cells for which the distance of the cell's midpoint to the evaluation point is less than the cell's diameter.     
*   Taking the average of the gradient over the area/volume of these cells leads to a dual solution which is very close to the one which would result from the point evaluation of the gradient. It is simple to justify theoretically that this does not change the method significantly.
* 

* 
* [1.x.163]
* 
*  Again, first set all entries to zero:
* 

* 
* [1.x.164]
* 
*  Initialize a  [2.x.132]  object with a quadrature formula, have abbreviations for the number of quadrature points and shape functions...
* 

* 
* [1.x.165]
* 
*  ...and have two objects that are used to store the global indices of the degrees of freedom on a cell, and the values of the gradients of the shape functions at the quadrature points:
* 

* 
* [1.x.166]
* 
*  Finally have a variable in which we will sum up the area/volume of the cells over which we integrate, by integrating the unit functions on these cells:
* 

* 
* [1.x.167]
* 
*  Then start the loop over all cells, and select those cells which are close enough to the evaluation point:
* 

* 
* [1.x.168]
* 
*  If we have found such a cell, then initialize the  [2.x.133]  object and integrate the x-component of the gradient of each shape function, as well as the unit function for the total area/volume.
* 

* 
* [1.x.169]
* 
*  If we have the local contributions, distribute them to the global vector:
* 

* 
* [1.x.170]
* 
*  After we have looped over all cells, check whether we have found any at all, by making sure that their volume is non-zero. If not, then the results will be botched, as the right hand side should then still be zero, so throw an exception:
* 

* 
* [1.x.171]
* 
*  Finally, we have by now only integrated the gradients of the shape functions, not taking their mean value. We fix this by dividing by the measure of the volume over which we have integrated:
* 

* 
* [1.x.172]
* 
*   [1.x.173]  [1.x.174]
* 

* 
* [1.x.175]
* 
*   [1.x.176]  [1.x.177]
* 

* 
*  In the same way as the  [2.x.134]  class above, we now implement a  [2.x.135] . It has all the same features, the only difference is that it does not take a function object denoting a right hand side object, but now takes a  [2.x.136]  object that will assemble the right hand side vector of the dual problem. The rest of the class is rather trivial.     
*   Since both primal and dual solver will use the same triangulation, but different discretizations, it now becomes clear why we have made the  [2.x.137]  class a virtual one: since the final class will be derived from both  [2.x.138]  as well as  [2.x.139]  instances, would we not have marked the inheritance as virtual. Since in many applications the base class would store much more information than just the triangulation which needs to be shared between primal and dual solvers, we do not usually want to use two such base classes.
* 

* 
* [1.x.178]
* 
*   [1.x.179]  [1.x.180]
* 

* 
*  Here finally comes the main class of this program, the one that implements the dual weighted residual error estimator. It joins the primal and dual solver classes to use them for the computation of primal and dual solutions, and implements the error representation formula for use as error estimate and mesh refinement.     
*   The first few of the functions of this class are mostly overriders of the respective functions of the base class:
* 

* 
* [1.x.181]
* 
*  In the private section, we have two functions that are used to call the  [2.x.140]  functions of the primal and dual base classes. These two functions will be called in parallel by the  [2.x.141]  function of this class.
* 

* 
* [1.x.182]
* 
*  Then declare abbreviations for active cell iterators, to avoid that we have to write this lengthy name over and over again:
* 

* 
*  

* 
* [1.x.183]
* 
*  Next, declare a data type that we will us to store the contribution of faces to the error estimator. The idea is that we can compute the face terms from each of the two cells to this face, as they are the same when viewed from both sides. What we will do is to compute them only once, based on some rules explained below which of the two adjacent cells will be in charge to do so. We then store the contribution of each face in a map mapping faces to their values, and only collect the contributions for each cell by looping over the cells a second time and grabbing the values from the map.       
*   The data type of this map is declared here:
* 

* 
* [1.x.184]
* 
*  In the computation of the error estimates on cells and faces, we need a number of helper objects, such as  [2.x.142]  and  [2.x.143]  functions, but also temporary objects storing the values and gradients of primal and dual solutions, for example. These fields are needed in the three functions that do the integration on cells, and regular and irregular faces, respectively.       
*   There are three reasonable ways to provide these fields: first, as local variables in the function that needs them; second, as member variables of this class; third, as arguments passed to that function.       
*   These three alternatives all have drawbacks: the third that their number is not negligible and would make calling these functions a lengthy enterprise. The second has the drawback that it disallows parallelization, since the threads that will compute the error estimate have to have their own copies of these variables each, so member variables of the enclosing class will not work. The first approach, although straightforward, has a subtle but important drawback: we will call these functions over and over again, many thousands of times maybe; it now turns out that allocating vectors and other objects that need memory from the heap is an expensive business in terms of run-time, since memory allocation is expensive when several threads are involved. It is thus significantly better to allocate the memory only once, and recycle the objects as often as possible.       
*   What to do? Our answer is to use a variant of the third strategy. In fact, this is exactly what the WorkStream concept is supposed to do (we have already introduced it above, but see also  [2.x.144] ). To avoid that we have to give these functions a dozen or so arguments, we pack all these variables into two structures, one which is used for the computations on cells, the other doing them on the faces. Both are then joined into the WeightedResidualScratchData class that will serve as the "scratch data" class of the WorkStream concept:
* 

* 
* [1.x.185]
* 
*   [2.x.145]  generally wants both a scratch object and a copy object. Here, for reasons similar to what we had in  [2.x.146]  when discussing the computation of an approximation of the gradient, we don't actually need a "copy data" structure. Since WorkStream insists on having one of these, we just declare an empty structure that does nothing other than being there.
* 

* 
* [1.x.186]
* 
*  Regarding the evaluation of the error estimator, we have one driver function that uses  [2.x.147]  to call the second function on every cell:
* 

* 
* [1.x.187]
* 
*  Then we have functions that do the actual integration of the error representation formula. They will treat the terms on the cell interiors, on those faces that have no hanging nodes, and on those faces with hanging nodes, respectively:
* 

* 
* [1.x.188]
* 
*  In the implementation of this class, we first have the constructors of the  [2.x.148]  member classes, and the  [2.x.149]  constructor. They only initialize fields to their correct lengths, so we do not have to discuss them in too much detail:
* 

* 
* [1.x.189]
* 
*  The next five functions are boring, as they simply relay their work to the base classes. The first calls the primal and dual solvers in parallel, while postprocessing the solution and retrieving the number of degrees of freedom is done by the primal class.
* 

* 
* [1.x.190]
* 
*  Now, it is becoming more interesting: the  [2.x.150]  function asks the error estimator to compute the cell-wise error indicators, then uses their absolute values for mesh refinement.
* 

* 
* [1.x.191]
* 
*  First call the function that computes the cell-wise and global error:
* 

* 
* [1.x.192]
* 
*  Then note that marking cells for refinement or coarsening only works if all indicators are positive, to allow their comparison. Thus, drop the signs on all these indicators:
* 

* 
* [1.x.193]
* 
*  Finally, we can select between different strategies for refinement. The default here is to refine those cells with the largest error indicators that make up for a total of 80 per cent of the error, while we coarsen those with the smallest indicators that make up for the bottom 2 per cent of the error.
* 

* 
* [1.x.194]
* 
*  Since we want to output both the primal and the dual solution, we overload the  [2.x.151]  function. The only interesting feature of this function is that the primal and dual solutions are defined on different finite element spaces, which is not the format the  [2.x.152]  class expects. Thus, we have to transfer them to a common finite element space. Since we want the solutions only to see them qualitatively, we contend ourselves with interpolating the dual solution to the (smaller) primal space. For the interpolation, there is a library function, that takes a AffineConstraints object including the hanging node constraints. The rest is standard.
* 

* 
* [1.x.195]
* 
*  Add the data vectors for which we want output. Add them both, the  [2.x.153]  functions can handle as many data vectors as you wish to write to output:
* 

* 
* [1.x.196]
* 
*   [1.x.197]  [1.x.198]
* 

* 
*   [1.x.199]  [1.x.200]     
*   As for the actual computation of error estimates, let's start with the function that drives all this, i.e. calls those functions that actually do the work, and finally collects the results.
* 

* 
* [1.x.201]
* 
*  The first task in computing the error is to set up vectors that denote the primal solution, and the weights (z-z_h)=(z-I_hz), both in the finite element space for which we have computed the dual solution. For this, we have to interpolate the primal solution to the dual finite element space, and to subtract the interpolation of the computed dual solution to the primal finite element space. Fortunately, the library provides functions for the interpolation into larger or smaller finite element spaces, so this is mostly obvious.       
*   First, let's do that for the primal solution: it is cell-wise interpolated into the finite element space in which we have solved the dual problem: But, again as in the  [2.x.154]  function we first need to create an AffineConstraints object including the hanging node constraints, but this time of the dual finite element space.
* 

* 
* [1.x.202]
* 
*  Then for computing the interpolation of the numerically approximated dual solution z into the finite element space of the primal solution and subtracting it from z: use the  [2.x.155]  function, that gives (z-I_hz) in the element space of the dual solution.
* 

* 
* [1.x.203]
* 
*  Note that this could probably have been more efficient since those constraints have been used previously when assembling matrix and right hand side for the primal problem and writing out the dual solution. We leave the optimization of the program in this respect as an exercise.
* 

* 
*  Having computed the dual weights we now proceed with computing the cell and face residuals of the primal solution. First we set up a map between face iterators and their jump term contributions of faces to the error estimator. The reason is that we compute the jump terms only once, from one side of the face, and want to collect them only afterwards when looping over all cells a second time.       
*   We initialize this map already with a value of
* 
*  - e20 for all faces, since this value will stand out in the results if something should go wrong and we fail to compute the value for a face for some reason. Secondly, this initialization already makes the  [2.x.156]  object allocate all objects it may possibly need. This is important since we will write into this structure from parallel threads, and doing so would not be thread-safe if the map needed to allocate memory and thereby reshape its data structures. In other words, the initial initialization relieves us from the necessity to synchronize the threads through a mutex each time they write to (and modify the structure of) this map.
* 

* 
* [1.x.204]
* 
*  Then hand it all off to  [2.x.157]  to compute the estimators for all cells in parallel:
* 

* 
* [1.x.205]
* 
*  Once the error contributions are computed, sum them up. For this, note that the cell terms are already set, and that only the edge terms need to be collected. Thus, loop over all cells and their faces, make sure that the contributions of each of the faces are there, and add them up. Only take minus one half of the jump term, since the other half will be taken by the neighboring cell.
* 

* 
* [1.x.206]
* 
*   [1.x.207]  [1.x.208]
* 

* 
*  Next we have the function that is called to estimate the error on a single cell. The function may be called multiple times if the library was configured to use multithreading. Here it goes:
* 

* 
* [1.x.209]
* 
*  Because of WorkStream, estimate_on_one_cell requires a CopyData object even if it is no used. The next line silences a warning about this unused variable.
* 

* 
* [1.x.210]
* 
*  First task on each cell is to compute the cell residual contributions of this cell, and put them into the  [2.x.158]  variable:
* 

* 
* [1.x.211]
* 
*  After computing the cell terms, turn to the face terms. For this, loop over all faces of the present cell, and see whether something needs to be computed on it:
* 

* 
* [1.x.212]
* 
*  First, if this face is part of the boundary, then there is nothing to do. However, to make things easier when summing up the contributions of the faces of cells, we enter this face into the list of faces with a zero contribution to the error.
* 

* 
* [1.x.213]
* 
*  Next, note that since we want to compute the jump terms on each face only once although we access it twice (if it is not at the boundary), we have to define some rules who is responsible for computing on a face:           
*   First, if the neighboring cell is on the same level as this one, i.e. neither further refined not coarser, then the one with the lower index within this level does the work. In other words: if the other one has a lower index, then skip work on this face:
* 

* 
* [1.x.214]
* 
*  Likewise, we always work from the coarser cell if this and its neighbor differ in refinement. Thus, if the neighboring cell is less refined than the present one, then do nothing since we integrate over the subfaces when we visit the coarse cell.
* 

* 
* [1.x.215]
* 
*  Now we know that we are in charge here, so actually compute the face jump terms. If the face is a regular one, i.e.  the other side's cell is neither coarser not finer than this cell, then call one function, and if the cell on the other side is further refined, then use another function. Note that the case that the cell on the other side is coarser cannot happen since we have decided above that we handle this case when we pass over that other cell.
* 

* 
* [1.x.216]
* 
*   [1.x.217]  [1.x.218]
* 

* 
*  As for the actual computation of the error contributions, first turn to the cell terms:
* 

* 
* [1.x.219]
* 
*  The tasks to be done are what appears natural from looking at the error estimation formula: first get the right hand side and Laplacian of the numerical solution at the quadrature points for the cell residual,
* 

* 
* [1.x.220]
* 
*  ...then get the dual weights...
* 

* 
* [1.x.221]
* 
*  ...and finally build the sum over all quadrature points and store it with the present cell:
* 

* 
* [1.x.222]
* 
*   [1.x.223]  [1.x.224]
* 

* 
*  On the other hand, computation of the edge terms for the error estimate is not so simple. First, we have to distinguish between faces with and without hanging nodes. Because it is the simple case, we first consider the case without hanging nodes on a face (let's call this the `regular' case):
* 

* 
* [1.x.225]
* 
*  The first step is to get the values of the gradients at the quadrature points of the finite element field on the present cell. For this, initialize the  [2.x.159]  object corresponding to this side of the face, and extract the gradients using that object.
* 

* 
* [1.x.226]
* 
*  The second step is then to extract the gradients of the finite element solution at the quadrature points on the other side of the face, i.e. from the neighboring cell.       
*   For this, do a sanity check before: make sure that the neighbor actually exists (yes, we should not have come here if the neighbor did not exist, but in complicated software there are bugs, so better check this), and if this is not the case throw an error.
* 

* 
* [1.x.227]
* 
*  If we have that, then we need to find out with which face of the neighboring cell we have to work, i.e. the  [2.x.160]  the neighbor the present cell is of the cell behind the present face. For this, there is a function, and we put the result into a variable with the name  [2.x.161] :
* 

* 
* [1.x.228]
* 
*  Then define an abbreviation for the neighbor cell, initialize the  [2.x.162]  object on that cell, and extract the gradients on that cell:
* 

* 
* [1.x.229]
* 
*  Now that we have the gradients on this and the neighboring cell, compute the jump residual by multiplying the jump in the gradient with the normal vector:
* 

* 
* [1.x.230]
* 
*  Next get the dual weights for this face:
* 

* 
* [1.x.231]
* 
*  Finally, we have to compute the sum over jump residuals, dual weights, and quadrature weights, to get the result for this face:
* 

* 
* [1.x.232]
* 
*  Double check that the element already exists and that it was not already written to...
* 

* 
* [1.x.233]
* 
*  ...then store computed value at assigned location. Note that the stored value does not contain the factor 1/2 that appears in the error representation. The reason is that the term actually does not have this factor if we loop over all faces in the triangulation, but only appears if we write it as a sum over all cells and all faces of each cell; we thus visit the same face twice. We take account of this by using this factor
* 
*  - /2 later, when we sum up the contributions for each cell individually.
* 

* 
* [1.x.234]
* 
*   [1.x.235]  [1.x.236]
* 

* 
*  We are still missing the case of faces with hanging nodes. This is what is covered in this function:
* 

* 
* [1.x.237]
* 
*  First again two abbreviations, and some consistency checks whether the function is called only on faces for which it is supposed to be called:
* 

* 
* [1.x.238]
* 
*  Then find out which neighbor the present cell is of the adjacent cell. Note that we will operate on the children of this adjacent cell, but that their orientation is the same as that of their mother, i.e. the neighbor direction is the same.
* 

* 
* [1.x.239]
* 
*  Then simply do everything we did in the previous function for one face for all the sub-faces now:
* 

* 
* [1.x.240]
* 
*  Start with some checks again: get an iterator pointing to the cell behind the present subface and check whether its face is a subface of the one we are considering. If that were not the case, then there would be either a bug in the  [2.x.163]  function called above, or
* 
*  -  worse
* 

* 
* 
*  -  some function in the library did not keep to some underlying assumptions about cells, their children, and their faces. In any case, even though this assertion should not be triggered, it does not harm to be cautious, and in optimized mode computations the assertion will be removed anyway.
* 

* 
* [1.x.241]
* 
*  Now start the work by again getting the gradient of the solution first at this side of the interface,
* 

* 
* [1.x.242]
* 
*  then at the other side,
* 

* 
* [1.x.243]
* 
*  and finally building the jump residuals. Since we take the normal vector from the other cell this time, revert the sign of the first term compared to the other function:
* 

* 
* [1.x.244]
* 
*  Then get dual weights:
* 

* 
* [1.x.245]
* 
*  At last, sum up the contribution of this sub-face, and set it in the global map:
* 

* 
* [1.x.246]
* 
*  Once the contributions of all sub-faces are computed, loop over all sub-faces to collect and store them with the mother face for simple use when later collecting the error terms of cells. Again make safety checks that the entries for the sub-faces have been computed and do not carry an invalid value.
* 

* 
* [1.x.247]
* 
*  Finally store the value with the parent face.
* 

* 
* [1.x.248]
* 
*   [1.x.249]  [1.x.250]
* 

* 
*  In the previous example program, we have had two functions that were used to drive the process of solving on subsequently finer grids. We extend this here to allow for a number of parameters to be passed to these functions, and put all of that into framework class.   
*   You will have noted that this program is built up of a number of small parts (evaluation functions, solver classes implementing various refinement methods, different dual functionals, different problem and data descriptions), which makes the program relatively simple to extend, but also allows to solve a large number of different problems by replacing one part by another. We reflect this flexibility by declaring a structure in the following framework class that holds a number of parameters that may be set to test various combinations of the parts of this program, and which can be used to test it at various problems and discretizations in a simple way.
* 

* 
* [1.x.251]
* 
*  First, we declare two abbreviations for simple use of the respective data types:
* 

* 
* [1.x.252]
* 
*  Then we have the structure which declares all the parameters that may be set. In the default constructor of the structure, these values are all set to default values, for simple use.
* 

* 
* [1.x.253]
* 
*  First allow for the degrees of the piecewise polynomials by which the primal and dual problems will be discretized. They default to (bi-, tri-)linear ansatz functions for the primal, and (bi-, tri-)quadratic ones for the dual problem. If a refinement criterion is chosen that does not need the solution of a dual problem, the value of the dual finite element degree is of course ignored.
* 

* 
* [1.x.254]
* 
*  Then have an object that describes the problem type, i.e. right hand side, domain, boundary values, etc. The pointer needed here defaults to the Null pointer, i.e. you will have to set it in actual instances of this object to make it useful.
* 

* 
* [1.x.255]
* 
*  Since we allow to use different refinement criteria (global refinement, refinement by the Kelly error indicator, possibly with a weight, and using the dual estimator), define a number of enumeration values, and subsequently a variable of that type. It will default to  [2.x.164] .
* 

* 
* [1.x.256]
* 
*  Next, an object that describes the dual functional. It is only needed if the dual weighted residual refinement is chosen, and also defaults to a Null pointer.
* 

* 
* [1.x.257]
* 
*  Then a list of evaluation objects. Its default value is empty, i.e. no evaluation objects.
* 

* 
* [1.x.258]
* 
*  Next to last, a function that is used as a weight to the  [2.x.165]  class. The default value of this pointer is zero, but you have to set it to some other value if you want to use the  [2.x.166]  refinement criterion.
* 

* 
* [1.x.259]
* 
*  Finally, we have a variable that denotes the maximum number of degrees of freedom we allow for the (primal) discretization. If it is exceeded, we stop the process of solving and intermittent mesh refinement. Its default value is 20,000.
* 

* 
* [1.x.260]
* 
*  Finally the default constructor of this class:
* 

* 
* [1.x.261]
* 
*  The driver framework class only has one method which calls solver and mesh refinement intermittently, and does some other small tasks in between. Since it does not need data besides the parameters given to it, we make it static:
* 

* 
* [1.x.262]
* 
*  As for the implementation, first the constructor of the parameter object, setting all values to their defaults:
* 

* 
* [1.x.263]
* 
*  Then the function which drives the whole process:
* 

* 
* [1.x.264]
* 
*  First create a triangulation from the given data object,
* 

* 
* [1.x.265]
* 
*  then a set of finite elements and appropriate quadrature formula:
* 

* 
* [1.x.266]
* 
*  Next, select one of the classes implementing different refinement criteria.
* 

* 
* [1.x.267]
* 
*  Now that all objects are in place, run the main loop. The stopping criterion is implemented at the bottom of the loop.     
*   In the loop, first set the new cycle number, then solve the problem, output its solution(s), apply the evaluation objects to it, then decide whether we want to refine the mesh further and solve again on this mesh, or jump out of the loop.
* 

* 
* [1.x.268]
* 
*  Clean up the screen after the loop has run:
* 

* 
* [1.x.269]
* 
*   [1.x.270]  [1.x.271]
* 

* 
*  Here finally comes the main function. It drives the whole process by specifying a set of parameters to be used for the simulation (polynomial degrees, evaluation and dual functionals, etc), and passes them packed into a structure to the frame work class above.
* 

* 
* [1.x.272]
* 
*  Describe the problem we want to solve here by passing a descriptor object to the function doing the rest of the work:
* 

* 
* [1.x.273]
* 
*  First set the refinement criterion we wish to use:
* 

* 
* [1.x.274]
* 
*  Here, we could as well have used  [2.x.167]  or  [2.x.168] . Note that the information given about dual finite elements, dual functional, etc is only important for the given choice of refinement criterion, and is ignored otherwise.
* 

* 
*  Then set the polynomial degrees of primal and dual problem. We choose here bi-linear and bi-quadratic ones:
* 

* 
* [1.x.275]
* 
*  Then set the description of the test case, i.e. domain, boundary values, and right hand side. These are prepackaged in classes. We take here the description of  [2.x.169] , but you can also use  [2.x.170] :
* 

* 
* [1.x.276]
* 
*  Next set first a dual functional, then a list of evaluation objects. We choose as default the evaluation of the value at an evaluation point, represented by the classes  [2.x.171]  in the namespaces of evaluation and dual functional classes. You can also set the  [2.x.172]  classes for the x-derivative instead of the value at the evaluation point.       
*   Note that dual functional and evaluation objects should match. However, you can give as many evaluation functionals as you want, so you can have both point value and derivative evaluated after each step.  One such additional evaluation is to output the grid in each step.
* 

* 
* [1.x.277]
* 
*  Set the maximal number of degrees of freedom after which we want the program to stop refining the mesh further:
* 

* 
* [1.x.278]
* 
*  Finally pass the descriptor object to a function that runs the entire solution with it:
* 

* 
* [1.x.279]
* 
*  Catch exceptions to give information about things that failed:
* 

* 
* [1.x.280]
* [1.x.281][1.x.282]
* 

* [1.x.283][1.x.284]
* 

* 
* This program offers a lot of possibilities to play around. We can thusonly show a small part of all possible results that can be obtainedwith the help of this program. However, you are encouraged to just tryit out, by changing the settings in the main program. Here, we startby simply letting it run, unmodified:
* [1.x.285]
* 
* 

* First let's look what the program actually computed. On the seventhgrid, primal and dual numerical solutions look like this (using acolor scheme intended to evoke the snow-capped mountains ofColorado that the original author of this program now callshome): [2.x.173] Apparently, the region at the bottom left is so unimportant for thepoint value evaluation at the top right that the grid is left entirelyunrefined there, even though the solution has singularities at the innercorner of that cell! Dueto the symmetry in right hand side and domain, the solution shouldactually look like at the top right in all four corners, but the meshrefinement criterion involving the dual solution chose to refine themdifferently
* 
*  -  because we said that we really only care about a singlefunction value somewhere at the top right.
* 

* 
* Here are some of the meshes that are produced in refinement cycles 0,2, 4 (top row), and 5, 7, and 8 (bottom row):
*  [2.x.174] 
* Note the subtle interplay between resolving the corner singularities,and resolving around the point of evaluation. It will be ratherdifficult to generate such a mesh by hand, as this would involve tojudge quantitatively how much which of the four corner singularitiesshould be resolved, and to set the weight compared to the vicinity ofthe evaluation point.
* 

* 
* The program prints the point value and the estimated error in thisquantity. From extrapolating it, we can guess that the exact value issomewhere close to 0.0334473, plus or minus 0.0000001 (note that we getalmost 6 valid digits from only 22,000 (primal) degrees offreedom. This number cannot be obtained from the value of thefunctional alone, but I have used the assumption that the errorestimator is mostly exact, and extrapolated the computed value plusthe estimated error, to get an approximation of the truevalue. Computing with more degrees of freedom shows that thisassumption is indeed valid.
* 

* 
* From the computed results, we can generate two graphs: one that showsthe convergence of the error  [2.x.175]  (taking theextrapolated value as correct) in the point value, and the value thatwe get by adding up computed value  [2.x.176]  and estimatederror eta (if the error estimator  [2.x.177]  were exact, then the value [2.x.178]  would equal the exact point value, and the errorin this quantity would always be zero; however, since the errorestimator is only a
* 
*  - good
* 
*  - approximation to the true error, we canby this only reduce the size of the error). In this graph, we alsoindicate the complexity  [2.x.179]  to show that mesh refinementacts optimal in this case. The second chart comparestrue and estimated error, and shows that the two are actually veryclose to each other, even for such a complicated quantity as the pointvalue:
* 

*  [2.x.180] 
* 

* [1.x.286][1.x.287]
* 

* 
* Since we have accepted quite some effort when using the meshrefinement driven by the dual weighted error estimator (for solvingthe dual problem, and for evaluating the error representation), it isworth while asking whether that effort was successful. To this end, wefirst compare the achieved error levels for different mesh refinementcriteria. To generate this data, simply change the value of the meshrefinement criterion variable in the main program. The results arethus (for the weight in the Kelly indicator, we have chosen thefunction  [2.x.181] , where  [2.x.182] is the distance to the evaluation point; it can be shown that this isthe optimal weight if we neglect the effects of boundaries):
*  [2.x.183] 
* 

* 
* Checking these numbers, we see that for global refinement, the erroris proportional to  [2.x.184] , and for the dualestimator  [2.x.185] . Generally speaking, we see that the dualweighted error estimator is better than the other refinementindicators, at least when compared with those that have a similarlyregular behavior. The Kelly indicator produces smaller errors, butjumps about the picture rather irregularly, with the error alsochanging signs sometimes. Therefore, its behavior does not allow toextrapolate the results to larger values of N. Furthermore, if wetrust the error estimates of the dual weighted error estimator, theresults can be improved by adding the estimated error to the computedvalues. In terms of reliability, the weighted estimator is thus betterthan the Kelly indicator, although the latter sometimes producessmaller errors.
* 

* 
* [1.x.288][1.x.289]
* 

* 
* Besides evaluating the values of the solution at a certain point, theprogram also offers the possibility to evaluate the x-derivatives at acertain point, and also to tailor mesh refinement for this. To let theprogram compute these quantities, simply replace the two occurrences of [2.x.186]  in the main function by [2.x.187] , and let the program run:
* [1.x.290]
* 
* 

* 
* The solution looks roughly the same as before (the exact solution ofcourse  [2.x.188] is [2.x.189]  the same, only the grid changed a little), but thedual solution is now different. A close-up around the point ofevaluation shows this: [2.x.190] This time, the grids in refinement cycles 0, 5, 6, 7, 8, and 9 looklike this:
*  [2.x.191] 
* Note the asymmetry of the grids compared with those we obtained forthe point evaluation. This is due to the fact that the domain and the primalsolution may be symmetric about the diagonal, but the  [2.x.192] -derivative isnot, and the latter enters the refinement criterion.
* 

* 
* Then, it is interesting to compare actually computed values of thequantity of interest (i.e. the x-derivative of the solution at onepoint) with a reference value of
* 
*  - .0528223... plus or minus0.0000005. We get this reference value by computing on finer grid aftersome more mesh refinements, with approximately 130,000 cells.Recall that if the error is  [2.x.193]  in the optimal case, thentaking a mesh with ten times more cells gives us one additional digitin the result.
* 

* 
* In the left part of the following chart, you again see the convergenceof the error towards this extrapolated value, while on the right yousee a comparison of true and estimated error:
*  [2.x.194] 
* After an initial phase where the true error changes its sign, theestimated error matches it quite well, again. Also note the dramaticimprovement in the error when using the estimated error to correct thecomputed value of  [2.x.195] .
* 

* 
* [1.x.291][1.x.292]
* 

* 
* If instead of the  [2.x.196]  data set, we choose [2.x.197]  in the main function, and choose  [2.x.198] as the evaluation point, then we can redo thecomputations of the previous example program, to compare whether theresults obtained with the help of the dual weighted error estimatorare better than those we had previously.
* 

* 
* First, the meshes after 9 adaptive refinement cycles obtained withthe point evaluation and derivative evaluation refinementcriteria, respectively, look like this:
*  [2.x.199] 
* The features of the solution can still be seen in the mesh, but since thesolution is smooth, the singularities of the dual solution entirelydominate the mesh refinement criterion, and lead to stronglyconcentrated meshes. The solution after the seventh refinement step lookslike the following:
*  [2.x.200] 
* Obviously, the solution is worse at some places, but the meshrefinement process should have taken care that these places are notimportant for computing the point value.
* 

* 
* 

* The next point is to compare the new (duality based) mesh refinementcriterion with the old ones. These are the results:
*  [2.x.201] 
* 

* 
* The results are, well, somewhat mixed. First, the Kelly indicatordisqualifies itself by its unsteady behavior, changing the sign of theerror several times, and with increasing errors under meshrefinement. The dual weighted error estimator has a monotone decreasein the error, and is better than the weighted Kelly and globalrefinement, but the margin is not as large as expected. This is, here,due to the fact the global refinement can exploit the regularstructure of the meshes around the point of evaluation, which leads toa better order of convergence for the point error. However, if we hada mesh that is not locally rectangular, for example because we had toapproximate curved boundaries, or if the coefficients were notconstant, then this advantage of globally refinement meshes wouldvanish, while the good performance of the duality based estimatorwould remain.
* 

* 
* 

* [1.x.293][1.x.294]
* 

* 
* The results here are not too clearly indicating the superiority of thedual weighted error estimation approach for mesh refinement over othermesh refinement criteria, such as the Kelly indicator. This is due tothe relative simplicity of the shown applications. If you are notconvinced yet that this approach is indeed superior, you are invitedto browse through the literature indicated in the introduction, whereplenty of examples are provided where the dual weighted approach canreduce the necessary numerical work by orders of magnitude, makingthis the only way to compute certain quantities to reasonableaccuracies at all.
* 

* 
* Besides the objections you may raise against its use as a meshrefinement criterion, consider that accurate knowledge of the error inthe quantity one might want to compute is of great use, since we canstop computations when we are satisfied with the accuracy. Using moretraditional approaches, it is very difficult to get accurate estimatesfor arbitrary quantities, except for, maybe, the error in the energynorm, and we will then have no guarantee that the result we computedsatisfies any requirements on its accuracy. Also, as was shown for theevaluation of point values and derivatives, the error estimate can beused to extrapolate the results, yielding much higher accuracy in thequantity we want to know.
* 

* 
* Leaving these mathematical considerations, we tried to write theprogram in a modular way, such that implementing another test case, oranother evaluation and dual functional is simple. You are encouragedto take the program as a basis for your own experiments, and to play alittle.
* 

* [1.x.295][1.x.296] [2.x.202] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-15_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30]
*  [2.x.2] 
* [1.x.31] [2.x.3] 
* 

* [1.x.32][1.x.33][1.x.34]
* 

* [1.x.35][1.x.36]
* 

* This program deals with an example of a non-linear elliptic partialdifferential equation, the[minimal surface equation](https://en.wikipedia.org/wiki/Minimal_surface).You can imagine the solution of this equation to describethe surface spanned by a soap film that is enclosed by aclosed wire loop. We imagine the wire to not just be a planar loop, but infact curved. The surface tension of the soap film will then reduce the surfaceto have minimal surface. The solution of the minimal surface equationdescribes this shape with the wire's vertical displacement as a boundarycondition. For simplicity, we will here assume that the surface can be writtenas a graph  [2.x.4]  although it is clear that it is not very hard toconstruct cases where the wire is bent in such a way that the surface can onlylocally be constructed as a graph but not globally.
* Because the equation is non-linear, we can't solve it directly. Rather, wehave to use Newton's method to compute the solution iteratively.
*  [2.x.5] ( [2.x.6] 
* 

* 
* [1.x.37][1.x.38]
* 

* In a classical sense, the problem is given in the following form:
* 

*  
* [1.x.39]
* 
*  [2.x.7]  is the domain we get by projecting the wire's positions into  [2.x.8] space. In this example, we choose  [2.x.9]  as the unit disk.
* As described above, we solve this equation using Newton's method in which wecompute the  [2.x.10] th approximate solution from the  [2.x.11] th one, and usea damping parameter  [2.x.12]  to get better global convergence behavior: 
* [1.x.40]
* with  [1.x.41]and  [2.x.13]  the derivative of F in direction of  [2.x.14] :[1.x.42]
* Going through the motions to find out what  [2.x.15]  is, we find thatwe have to solve a linear elliptic PDE in every Newton step, with  [2.x.16] as the solution of:
*   [1.x.43]
* In order to solve the minimal surface equation, we have to solve this equationrepeatedly, once per Newton step. To solve this, we have to take a look at theboundary condition of this problem. Assuming that  [2.x.17]  already has theright boundary values, the Newton update  [2.x.18]  should have zeroboundary conditions, in order to have the right boundary condition afteradding both.  In the first Newton step, we are starting with the solution [2.x.19] , the Newton update still has to deliver the right boundarycondition to the solution  [2.x.20] .
* 

* Summing up, we have to solve the PDE above with the boundary condition  [2.x.21]  in the first step and with  [2.x.22]  in all the following steps.
*  [2.x.23]  In some sense, one may argue that if the program already  implements  [2.x.24] , it is duplicative to also have to implement   [2.x.25] . As always, duplication tempts bugs and we would like  to avoid it. While we do not explore this issue in this program, we  will come back to it at the end of the [1.x.44] section below,  and specifically in  [2.x.26] .
* 

* [1.x.45][1.x.46]
* 

* Starting with the strong formulation above, we get the weak formulation by multiplyingboth sides of the PDE with a test function  [2.x.27]  and integrating by parts on both sides:  [1.x.47]Here the solution  [2.x.28]  is a function in  [2.x.29] , subject tothe boundary conditions discussed above.Reducing this space to a finite dimensional space with basis  [2.x.30] , we can write the solution:
* [1.x.48]
* Using the basis functions as test functions and defining  [2.x.31] , we can rewrite the weak formulation:
* [1.x.49]
* where the solution  [2.x.32]  is given by the coefficients  [2.x.33] .This linear system of equations can be rewritten as:
* [1.x.50]
* where the entries of the matrix  [2.x.34]  are given by:
* [1.x.51]
* and the right hand side  [2.x.35]  is given by:
* [1.x.52]
* 

* [1.x.53][1.x.54]
* 

* The matrix that corresponds to the Newton step above can be reformulated toshow its structure a bit better. Rewriting it slightly, we get that it has theform[1.x.55]where the matrix  [2.x.36]  (of size  [2.x.37]  in  [2.x.38]  space dimensions) is givenby the following expression:[1.x.56]From this expression, it is obvious that [2.x.39]  is symmetric, and so  [2.x.40]  is symmetric as well.On the other hand,  [2.x.41]  is also positive definite, which confers the sameproperty onto  [2.x.42] . This can be seen by noting that the vector  [2.x.43]  is an eigenvector of  [2.x.44]  with eigenvalue [2.x.45]  while all vectors  [2.x.46] that are perpendicular to  [2.x.47]  and each other are eigenvectors witheigenvalue  [2.x.48] . Since all eigenvalues are positive,  [2.x.49]  is positive definiteand so is  [2.x.50] . We can thus use the CG method for solving the Newton steps.(The fact that the matrix  [2.x.51]  is symmetric and positive definite should not comeas a surprise. It results from taking the derivative of an operator thatresults from taking the derivative of an energy functional: the minimalsurface equation simply minimizes some non-quadratic energy. Consequently,the Newton matrix, as the matrix of second derivatives of a scalar energy,must be symmetric since the derivative with regard to the  [2.x.52] th and  [2.x.53] thdegree of freedom should clearly commute. Likewise, if the energy functionalis convex, then the matrix of second derivatives must be positive definite,and the direct calculation above simply reaffirms this.)
* It is worth noting, however, that the positive definiteness degenerates forproblems where  [2.x.54]  becomes large. In other words, if we simply multiplyall boundary values by 2, then to first order  [2.x.55]  and  [2.x.56]  will also bemultiplied by two, but as a consequence the smallest eigenvalue of  [2.x.57]  willbecome smaller and the matrix will become more ill-conditioned. (Morespecifically, for  [2.x.58]  we have that [2.x.59]  whereas [2.x.60] ; thus, the condition number of  [2.x.61] ,which is a multiplicative factor in the condition number of  [2.x.62]  growslike  [2.x.63] .) It is simpleto verify with the current program that indeed multiplying the boundary valuesused in the current program by larger and larger values results in a problemthat will ultimately no longer be solvable using the simple preconditioned CGmethod we use here.
* 

* [1.x.57][1.x.58]
* 

* As stated above, Newton's method works by computing a direction [2.x.64]  and then performing the update  [2.x.65]  with a step length  [2.x.66] . It is a commonobservation that for strongly nonlinear models, Newton's method doesnot converge if we always choose  [2.x.67]  unless one starts withan initial guess  [2.x.68]  that is sufficiently close to the solution  [2.x.69] of the nonlinear problem. In practice, we don't always have such aninitial guess, and consequently taking full Newton steps (i.e., using [2.x.70] ) does frequently not work.
* A common strategy therefore is to use a smaller step length for thefirst few steps while the iterate  [2.x.71]  is still far away from thesolution  [2.x.72]  and as we get closer use larger values for  [2.x.73] until we can finally start to use full steps  [2.x.74]  as we areclose enough to the solution. The question is of course how to choose [2.x.75] . There are basically two widely used approaches: linesearch and trust region methods.
* In this program, we simply always choose the step length equal to0.1. This makes sure that for the testcase at hand we do getconvergence although it is clear that by not eventually reverting tofull step lengths we forego the rapid, quadratic convergence thatmakes Newton's method so appealing. Obviously, this is a point oneeventually has to address if the program was made into one that ismeant to solve more realistic problems. We will comment on this issuesome more in the [1.x.59], and use aneven better approach in  [2.x.76] .
* 

* [1.x.60][1.x.61]
* 

* Overall, the program we have here is not unlike  [2.x.77]  in many regards. Thelayout of the main class is essentially the same. On the other hand, thedriving algorithm in the  [2.x.78]  function is different and works asfollows: [2.x.79]  [2.x.80]   Start with the function  [2.x.81]  and modify it in such a way  that the values of  [2.x.82]  along the boundary equal the correct  boundary values  [2.x.83]  (this happens in   [2.x.84] ). Set   [2.x.85] . [2.x.86] 
*  [2.x.87]   Compute the Newton update by solving the system  [2.x.88]   with boundary condition  [2.x.89]  on  [2.x.90] . [2.x.91] 
*  [2.x.92]   Compute a step length  [2.x.93] . In this program, we always set   [2.x.94] . To make things easier to extend later on, this  happens in a function of its own, namely in   [2.x.95] .  (The strategy of always choosing  [2.x.96]  is of course not  optimal
* 
*  -  we should choose a step length that works for a given  search direction
* 
*  -  but it requires a bit of work to do that. In the  end, we leave these sorts of things to external packages:  [2.x.97]   does that.) [2.x.98] 
*  [2.x.99]   The new approximation of the solution is given by   [2.x.100] . [2.x.101] 
*  [2.x.102]   If  [2.x.103]  is a multiple of 5 then refine the mesh, transfer the  solution  [2.x.104]  to the new mesh and set the values of  [2.x.105]   in such a way that along the boundary we have   [2.x.106]  (again in   [2.x.107] ). Note that  this isn't automatically  guaranteed even though by construction we had that before mesh  refinement  [2.x.108]  because mesh refinement  adds new nodes to the mesh where we have to interpolate the old  solution to the new nodes upon bringing the solution from the old to  the new mesh. The values we choose by interpolation may be close to  the exact boundary conditions but are, in general, nonetheless not  the correct values. [2.x.109] 
*  [2.x.110]   Set  [2.x.111]  and go to step 2. [2.x.112]  [2.x.113] 
* The testcase we solve is chosen as follows: We seek to find the solution ofminimal surface over the unit disk  [2.x.114]  where the surface attains the values [2.x.115]  along theboundary.
* 

*  [1.x.62] [1.x.63]
*   [1.x.64]  [1.x.65]
* 

* 
*  The first few files have already been covered in previous examples and will thus not be further commented on.
* 

* 
* [1.x.66]
* 
*  We will use adaptive mesh refinement between Newton iterations. To do so, we need to be able to work with a solution on the new mesh, although it was computed on the old one. The SolutionTransfer class transfers the solution from the old to the new mesh:
* 

* 
*  

* 
* [1.x.67]
* 
*  We then open a namespace for this program and import everything from the dealii namespace into it, as in previous programs:
* 

* 
* [1.x.68]
* 
*   [1.x.69]  [1.x.70]
* 

* 
*  The class template is basically the same as in  [2.x.116] .  Three additions are made:
* 

* 
* 
*  - There are two solution vectors, one for the Newton update  [2.x.117] , and one for the current iterate  [2.x.118] .
* 

* 
* 
*  - The  [2.x.119]  function takes an argument that denotes whether this is the first time it is called or not. The difference is that the first time around we need to distribute the degrees of freedom and set the solution vector for  [2.x.120]  to the correct size. The following times, the function is called after we have already done these steps as part of refining the mesh in  [2.x.121] .
* 

* 
* 
*  - We then also need new functions:  [2.x.122]  takes care of setting the boundary values on the solution vector correctly, as discussed at the end of the introduction.  [2.x.123]  is a function that computes the norm of the nonlinear (discrete) residual. We use this function to monitor convergence of the Newton iteration. The function takes a step length  [2.x.124]  as argument to compute the residual of  [2.x.125] . This is something one typically needs for step length control, although we will not use this feature here. Finally,  [2.x.126]  computes the step length  [2.x.127]  in each Newton iteration. As discussed in the introduction, we here use a fixed step length and leave implementing a better strategy as an exercise. ( [2.x.128]  does this differently: It simply uses an external package for the whole solution process, and a good line search strategy is part of what that package provides.)
* 

* 
*  

* 
* [1.x.71]
* 
*   [1.x.72]  [1.x.73]
* 

* 
*  The boundary condition is implemented just like in  [2.x.129] .  It is chosen as  [2.x.130] :
* 

* 
*  

* 
* [1.x.74]
* 
*   [1.x.75]  [1.x.76]
* 

* 
*   [1.x.77]  [1.x.78]
* 

* 
*  The constructor and destructor of the class are the same as in the first few tutorials.
* 

* 
*  

* 
* [1.x.79]
* 
*   [1.x.80]  [1.x.81]
* 

* 
*  As always in the setup-system function, we setup the variables of the finite element method. There are same differences to  [2.x.131] , because there we start solving the PDE from scratch in every refinement cycle whereas here we need to take the solution from the previous mesh onto the current mesh. Consequently, we can't just reset solution vectors. The argument passed to this function thus indicates whether we can distributed degrees of freedom (plus compute constraints) and set the solution vector to zero or whether this has happened elsewhere already (specifically, in  [2.x.132] ).
* 

* 
*  

* 
* [1.x.82]
* 
*  The remaining parts of the function are the same as in  [2.x.133] .
* 

* 
*  

* 
* [1.x.83]
* 
*   [1.x.84]  [1.x.85]
* 

* 
*  This function does the same as in the previous tutorials except that now, of course, the matrix and right hand side functions depend on the previous iteration's solution. As discussed in the introduction, we need to use zero boundary values for the Newton updates; we compute them at the end of this function.   
*   The top of the function contains the usual boilerplate code, setting up the objects that allow us to evaluate shape functions at quadrature points and temporary storage locations for the local matrices and vectors, as well as for the gradients of the previous solution at the quadrature points. We then start the loop over all cells:
* 

* 
* [1.x.86]
* 
*  For the assembly of the linear system, we have to obtain the values of the previous solution's gradients at the quadrature points. There is a standard way of doing this: the  [2.x.134]  function takes a vector that represents a finite element field defined on a DoFHandler, and evaluates the gradients of this field at the quadrature points of the cell with which the FEValues object has last been reinitialized. The values of the gradients at all quadrature points are then written into the second argument:
* 

* 
* [1.x.87]
* 
*  With this, we can then do the integration loop over all quadrature points and shape functions.  Having just computed the gradients of the old solution in the quadrature points, we are able to compute the coefficients  [2.x.135]  in these points.  The assembly of the system itself then looks similar to what we always do with the exception of the nonlinear terms, as does copying the results from the local objects into the global ones:
* 

* 
* [1.x.88]
* 
*  Finally, we remove hanging nodes from the system and apply zero boundary values to the linear system that defines the Newton updates  [2.x.136] :
* 

* 
* [1.x.89]
* 
*   [1.x.90]  [1.x.91]
* 

* 
*  The solve function is the same as always. At the end of the solution process we update the current solution by setting  [2.x.137] .
* 

* 
* [1.x.92]
* 
*   [1.x.93]  [1.x.94]
* 

* 
*  The first part of this function is the same as in  [2.x.138] ... However, after refining the mesh we have to transfer the old solution to the new one which we do with the help of the SolutionTransfer class. The process is slightly convoluted, so let us describe it in detail:
* 

* 
* [1.x.95]
* 
*  Then we need an additional step: if, for example, you flag a cell that is once more refined than its neighbor, and that neighbor is not flagged for refinement, we would end up with a jump of two refinement levels across a cell interface.  To avoid these situations, the library will silently also have to refine the neighbor cell once. It does so by calling the  [2.x.139]  function before actually doing the refinement and coarsening.  This function flags a set of additional cells for refinement or coarsening, to enforce rules like the one-hanging-node rule.  The cells that are flagged for refinement and coarsening after calling this function are exactly the ones that will actually be refined or coarsened. Usually, you don't have to do this by hand  [2.x.140]  does this for you). However, we need to initialize the SolutionTransfer class and it needs to know the final set of cells that will be coarsened or refined in order to store the data from the old mesh and transfer to the new one. Thus, we call the function by hand:
* 

* 
* [1.x.96]
* 
*  With this out of the way, we initialize a SolutionTransfer object with the present DoFHandler and attach the solution vector to it, followed by doing the actual refinement and distribution of degrees of freedom on the new mesh
* 

* 
* [1.x.97]
* 
*  Finally, we retrieve the old solution interpolated to the new mesh. Since the SolutionTransfer function does not actually store the values of the old solution, but rather indices, we need to preserve the old solution vector until we have gotten the new interpolated values. Thus, we have the new values written into a temporary vector, and only afterwards write them into the solution vector object:
* 

* 
* [1.x.98]
* 
*  On the new mesh, there are different hanging nodes, for which we have to compute constraints again, after throwing away previous content of the object. To be on the safe side, we should then also make sure that the current solution's vector entries satisfy the hanging node constraints (see the discussion in the documentation of the SolutionTransfer class for why this is necessary). We could do this by calling `hanging_node_constraints.distribute(current_solution)` explicitly; we omit this step because this will happen at the end of the call to `set_boundary_values()` below, and it is not necessary to do it twice.
* 

* 
* [1.x.99]
* 
*  Once we have the interpolated solution and all information about hanging nodes, we have to make sure that the  [2.x.141]  we now have actually has the correct boundary values. As explained at the end of the introduction, this is not automatically the case even if the solution before refinement had the correct boundary values, and so we have to explicitly make sure that it now has:
* 

* 
* [1.x.100]
* 
*  We end the function by updating all the remaining data structures, indicating to  [2.x.142]  that this is not the first go-around and that it needs to preserve the content of the solution vector:
* 

* 
* [1.x.101]
* 
*   [1.x.102]  [1.x.103]
* 

* 
*  The next function ensures that the solution vector's entries respect the boundary values for our problem.  Having refined the mesh (or just started computations), there might be new nodal points on the boundary. These have values that are simply interpolated from the previous mesh in `refine_mesh()`, instead of the correct boundary values. This is fixed up by setting all boundary nodes of the current solution vector explicit to the right value.   
*   There is one issue we have to pay attention to, though: If we have a hanging node right next to a new boundary node, then its value must also be adjusted to make sure that the finite element field remains continuous. This is what the call in the last line of this function does.
* 

* 
* [1.x.104]
* 
*   [1.x.105]  [1.x.106]
* 

* 
*  In order to monitor convergence, we need a way to compute the norm of the (discrete) residual, i.e., the norm of the vector  [2.x.143]  with  [2.x.144]  as discussed in the introduction. It turns out that (although we don't use this feature in the current version of the program) one needs to compute the residual  [2.x.145]  when determining optimal step lengths, and so this is what we implement here: the function takes the step length  [2.x.146]  as an argument. The original functionality is of course obtained by passing a zero as argument.   
*   In the function below, we first set up a vector for the residual, and then a vector for the evaluation point  [2.x.147] . This is followed by the same boilerplate code we use for all integration operations:
* 

* 
* [1.x.107]
* 
*  The actual computation is much as in  [2.x.148] . We first evaluate the gradients of  [2.x.149]  at the quadrature points, then compute the coefficient  [2.x.150] , and then plug it all into the formula for the residual:
* 

* 
* [1.x.108]
* 
*  At the end of this function we also have to deal with the hanging node constraints and with the issue of boundary values. With regard to the latter, we have to set to zero the elements of the residual vector for all entries that correspond to degrees of freedom that sit at the boundary. The reason is that because the value of the solution there is fixed, they are of course no "real" degrees of freedom and so, strictly speaking, we shouldn't have assembled entries in the residual vector for them. However, as we always do, we want to do exactly the same thing on every cell and so we didn't not want to deal with the question of whether a particular degree of freedom sits at the boundary in the integration above. Rather, we will simply set to zero these entries after the fact. To this end, we need to determine which degrees of freedom do in fact belong to the boundary and then loop over all of those and set the residual entry to zero. This happens in the following lines which we have already seen used in  [2.x.151] , using the appropriate function from namespace DoFTools:
* 

* 
* [1.x.109]
* 
*  At the end of the function, we return the norm of the residual:
* 

* 
* [1.x.110]
* 
*   [1.x.111]  [1.x.112]
* 

* 
*  As discussed in the introduction, Newton's method frequently does not converge if we always take full steps, i.e., compute  [2.x.152] . Rather, one needs a damping parameter (step length)  [2.x.153]  and set  [2.x.154] . This function is the one called to compute  [2.x.155] .   
*   Here, we simply always return 0.1. This is of course a sub-optimal choice: ideally, what one wants is that the step size goes to one as we get closer to the solution, so that we get to enjoy the rapid quadratic convergence of Newton's method. We will discuss better strategies below in the results section, and  [2.x.156]  also covers this aspect.
* 

* 
* [1.x.113]
* 
*   [1.x.114]  [1.x.115]
* 

* 
*  This last function to be called from `run()` outputs the current solution (and the Newton update) in graphical form as a VTU file. It is entirely the same as what has been used in previous tutorials.
* 

* 
* [1.x.116]
* 
*   [1.x.117]  [1.x.118]
* 

* 
*  In the run function, we build the first grid and then have the top-level logic for the Newton iteration.   
*   As described in the introduction, the domain is the unit disk around the origin, created in the same way as shown in  [2.x.157] . The mesh is globally refined twice followed later on by several adaptive cycles.   
*   Before starting the Newton loop, we also need to do a bit of setup work: We need to create the basic data structures and ensure that the first Newton iterate already has the correct boundary values, as discussed in the introduction.
* 

* 
* [1.x.119]
* 
*  The Newton iteration starts next. We iterate until the (norm of the) residual computed at the end of the previous iteration is less than  [2.x.158] , as checked at the end of the `do { ... } while` loop that starts here. Because we don't have a reasonable value to initialize the variable, we just use the largest value that can be represented as a `double`.
* 

* 
* [1.x.120]
* 
*  On every mesh we do exactly five Newton steps. We print the initial residual here and then start the iterations on this mesh.         
*   In every Newton step the system matrix and the right hand side have to be computed first, after which we store the norm of the right hand side as the residual to check against when deciding whether to stop the iterations. We then solve the linear system (the function also updates  [2.x.159] ) and output the norm of the residual at the end of this Newton step.         
*   After the end of this loop, we then also output the solution on the current mesh in graphical form and increment the counter for the mesh refinement cycle.
* 

* 
* [1.x.121]
* 
*   [1.x.122]  [1.x.123]
* 

* 
*  Finally the main function. This follows the scheme of all other main functions:
* 

* 
* [1.x.124]
* [1.x.125][1.x.126]
* 

* 
* The output of the program looks as follows:
* [1.x.127]
* 
* Obviously, the scheme converges, if not very fast. We will come back tostrategies for accelerating the method below.
* One can visualize the solution after each set of five Newtoniterations, i.e., on each of the meshes on which we approximate thesolution. This yields the following set of images:
*  [2.x.160] 
* It is clearly visible, that the solution minimizes the surfaceafter each refinement. The solution converges to a picture onewould imagine a soap bubble to be that is located inside a wire loopthat is bent likethe boundary. Also it is visible, how the boundaryis smoothed out after each refinement. On the coarse mesh,the boundary doesn't look like a sine, whereas it does thefiner the mesh gets.
* The mesh is mostly refined near the boundary, where the solutionincreases or decreases strongly, whereas it is coarsened onthe inside of the domain, where nothing interesting happens,because there isn't much change in the solution. The ninthsolution and mesh are shown here:
*  [2.x.161] 
* 

* 
* [1.x.128][1.x.129][1.x.130]
* 

* The program shows the basic structure of a solver for a nonlinear, stationaryproblem. However, it does not converge particularly fast, for good reasons:
* 
*  - The program always takes a step size of 0.1. This precludes the rapid,  quadratic convergence for which Newton's method is typically chosen.
* 
*  - It does not connect the nonlinear iteration with the mesh refinement  iteration.
* Obviously, a better program would have to address these two points.We will discuss them in the following.
* 

* [1.x.131][1.x.132]
* 

* Newton's method has two well known properties:
* 
*  - It may not converge from arbitrarily chosen starting points. Rather, a  starting point has to be close enough to the solution to guarantee  convergence. However, we can enlarge the area from which Newton's method  converges by damping the iteration using a [1.x.133] 0< [2.x.162] .
* 
*  - It exhibits rapid convergence of quadratic order if (i) the step length is  chosen as  [2.x.163] , and (ii) it does in fact converge with this choice  of step length.
* A consequence of these two observations is that a successful strategy is tochoose  [2.x.164]  for the initial iterations until the iterate has comeclose enough to allow for convergence with full step length, at which point wewant to switch to  [2.x.165] . The question is how to choose  [2.x.166]  in anautomatic fashion that satisfies these criteria.
* We do not want to review the literature on this topic here, but only brieflymention that there are two fundamental approaches to the problem: backtrackingline search and trust region methods. The former is more widely used forpartial differential equations and essentially does the following:
* 
*  - Compute a search direction
* 
*  - See if the resulting residual of  [2.x.167]  with   [2.x.168]  is "substantially smaller" than that of  [2.x.169]  alone.
* 
*  - If so, then take  [2.x.170] .
* 
*  - If not, try whether the residual is "substantially smaller" with   [2.x.171] .
* 
*  - If so, then take  [2.x.172] .
* 
*  - If not, try whether the residual is "substantially smaller" with   [2.x.173] .
* 
*  - Etc.One can of course choose other factors  [2.x.174]  than the  [2.x.175]  chosen above, for  [2.x.176] . It is obvious where the term"backtracking" comes from: we try a long step, but if that doesn't work we trya shorter step, and ever shorter step, etc. The function [2.x.177]  is written the way it is to supportexactly this kind of use case.
* Whether we accept a particular step length  [2.x.178]  depends on how we define"substantially smaller". There are a number of ways to do so, but withoutgoing into detail let us just mention that the most common ones are to use theWolfe and Armijo-Goldstein conditions. For these, one can show the following:
* 
*  - There is always a step length  [2.x.179]  for which the conditions are  satisfied, i.e., the iteration never gets stuck as long as the problem is  convex.
* 
*  - If we are close enough to the solution, then the conditions allow for   [2.x.180] , thereby enabling quadratic convergence.
* We will not dwell on this here any further but leave the implementation ofsuch algorithms as an exercise. We note, however, that when implementedcorrectly then it is a common observation that most reasonably nonlinearproblems can be solved in anywhere between 5 and 15 Newton iterations toengineering accuracy &mdash; substantially fewer than we need with the currentversion of the program.
* More details on globalization methods including backtracking can be found,for example, in  [2.x.181]  and  [2.x.182] .
* A separate point, very much worthwhile making, however, is that in practicethe implementation of efficient nonlinear solvers is about as complicated asthe implementation of efficient finite element methods. One should notattempt to reinvent the wheel by implementing all of the necessary stepsoneself. Substantial pieces of the puzzle are already available inthe  [2.x.183]  function and could be used to this end.But, instead, just like building finite element solvers on librariessuch as deal.II, one should be building nonlinear solvers on libraries suchas [SUNDIALS](https://computing.llnl.gov/projects/sundials). In fact,deal.II has interfaces to SUNDIALS and in particular to its nonlinear solversub-package KINSOL through the  [2.x.184]  class. It would not bevery difficult to base the current problem on that interface
* 
*  - indeed, that is what  [2.x.185]  does.
* 

* 
* [1.x.134][1.x.135]
* 

* We currently do exactly 5 iterations on each mesh. But is this optimal? Onecould ask the following questions:
* 
*  - Maybe it is worthwhile doing more iterations on the initial meshes since  there, computations are cheap.
* 
*  - On the other hand, we do not want to do too many iterations on every mesh:  yes, we could drive the residual to zero on every mesh, but that would only  mean that the nonlinear iteration error is far smaller than the  discretization error.
* 
*  - Should we use solve the linear systems in each Newton step with higher or  lower accuracy?
* Ultimately, what this boils down to is that we somehow need to couple thediscretization error on the current mesh with the nonlinear residual we wantto achieve with the Newton iterations on a given mesh, and to the lineariteration we want to achieve with the CG method within each Newtoniterations.
* How to do this is, again, not entirely trivial, and we again leave it as afuture exercise.
* 

* 
* [1.x.136][1.x.137]
* 

* As outlined in the introduction, when solving a nonlinear problem ofthe form  [1.x.138]we use a Newton iteration that requires us to repeatedly solve thelinear partial differential equation 
* [1.x.139]
* so that we can compute the update 
* [1.x.140]
* with the solution  [2.x.186]  of the Newton step. For the problemhere, we could compute the derivative  [2.x.187]  by hand andobtained  [1.x.141]But this is already a sizable expression that is cumbersome both toderive and to implement. It is also, in some sense, duplicative: If weimplement what  [2.x.188]  is somewhere in the code, then  [2.x.189] is not an independent piece of information but is something that, atleast in principle, a computer should be able to infer itself.Wouldn't it be nice if that could actually happen? That is, if wereally only had to implement  [2.x.190] , and  [2.x.191]  was then somehowdone implicitly? That is in fact possible, and runs under the name"automatic differentiation".  [2.x.192]  discusses this veryconcept in general terms, and  [2.x.193]  illustrates how this can beapplied in practice for the very problem we are considering here.
* 

* [1.x.142][1.x.143] [2.x.194] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-16_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19]
*  [2.x.2] 
* [1.x.20]
* [1.x.21][1.x.22][1.x.23]
* 

* 
* This example shows the basic usage of the multilevel functions in deal.II. Itsolves almost the same problem as used in  [2.x.3] , but demonstrating the thingsone has to provide when using multigrid as a preconditioner. In particular, thisrequires that we define a hierarchy of levels, provide transfer operators fromone level to the next and back, and provide representations of the Laplaceoperator on each level.
* In order to allow sufficient flexibility in conjunction with systems ofdifferential equations and block preconditioners, quite a few different objectshave to be created before starting the multilevel method, althoughmost of what needs to be done is provided by deal.II itself. These are
* 

* 
* 

* 
* 
*  - the object handling transfer between grids; we use the MGTransferPrebuilt    class for this that does almost all of the work inside the library,
* 

* 
* 

* 
* 
*  - the solver on the coarsest level; here, we use MGCoarseGridHouseholder,
* 

* 
* 

* 
* 
*  - the smoother on all other levels, which in our case will be the     [2.x.4]  class using SOR as the underlying method,
* 

* 
* 

* 
* 
*  - and  [2.x.5]  a class having a special level multiplication, i.e. we    basically store one matrix per grid level and allow multiplication with it.
* Most of these objects will only be needed inside the function thatactually solves the linear system. There, these objects are combinedin an object of type Multigrid, containing the implementation of theV-cycle, which is in turn used by the preconditioner PreconditionMG,ready for plug-in into a linear solver of the LAC library.
* The multigrid method implemented here for adaptively refined meshes follows theoutline in the  [2.x.6]  "Multigrid paper", which describes the underlyingimplementation in deal.II and also introduces a lot of the nomenclature. First,we have to distinguish between level meshes, namely cells that have the samerefinement distance from the coarse mesh, and the leaf mesh consisting of activecells of the hierarchy (in older work we refer to this as the global mesh, butthis term is overused). Most importantly, the leaf mesh is not identical withthe level mesh on the finest level. The following image shows what we considerto be a "level mesh":
*  [2.x.7] 
* The fine level in this mesh consists only of the degrees of freedom that aredefined on the refined cells, but does not extend to that part of the domainthat is not refined. While this guarantees that the overall effort grows as [2.x.8]  as necessary for optimal multigrid complexity, it leads toproblems when defining where to smooth and what boundary conditions to pose forthe operators defined on individual levels if the level boundary is not anexternal boundary. These questions are discussed in detail in the article citedabove.
* [1.x.24][1.x.25]
* 

* The problem we solve here is similar to  [2.x.9] , with two maindifferences: first, the multigrid preconditioner, obviously. We alsochange the discontinuity of the coefficients such that the localassembler does not look more complicated than necessary.
* 

*  [1.x.26] [1.x.27]
*   [1.x.28]  [1.x.29]
* 

* 
*  Again, the first few include files are already known, so we won't comment on them:
* 

* 
* [1.x.30]
* 
*  These, now, are the include necessary for the multilevel methods. The first one declares how to handle Dirichlet boundary conditions on each of the levels of the multigrid method. For the actual description of the degrees of freedom, we do not need any new include file because DoFHandler already has all necessary methods implemented. We will only need to distribute the DoFs for the levels further down.
* 

* 
*  The rest of the include files deals with the mechanics of multigrid as a linear operator (solver or preconditioner).
* 

* 
* [1.x.31]
* 
*  We will be using  [2.x.10]  to loop over the cells, so include it here:
* 

* 
* [1.x.32]
* 
*  This is C++:
* 

* 
* [1.x.33]
* 
*   [1.x.34]  [1.x.35]   
*   We use  [2.x.11]  to assemble our matrices. For this, we need a ScratchData object to store temporary data on each cell (this is just the FEValues object) and a CopyData object that will contain the output of each cell assembly. For more details about the usage of scratch and copy objects, see the WorkStream namespace.
* 

* 
* [1.x.36]
* 
*   [1.x.37]  [1.x.38]
* 

* 
*  This main class is similar to the same class in  [2.x.12] . As far as member functions is concerned, the only additions are:
* 

* 
* 
*  - The  [2.x.13]  function that assembles the matrices that correspond to the discrete operators on intermediate levels.
* 

* 
* 
*  - The  [2.x.14]  function that assembles our PDE on a single cell.
* 

* 
* [1.x.39]
* 
*  The following members are the essential data structures for the multigrid method. The first four represent the sparsity patterns and the matrices on individual levels of the multilevel hierarchy, very much like the objects for the global mesh above.     
*   Then we have two new matrices only needed for multigrid methods with local smoothing on adaptive meshes. They convey data between the interior part of the refined region and the refinement edge, as outlined in detail in the  [2.x.15]  "multigrid paper".     
*   The last object stores information about the boundary indices on each level and information about indices lying on a refinement edge between two different refinement levels. It thus serves a similar purpose as AffineConstraints, but on each level.
* 

* 
* [1.x.40]
* 
*   [1.x.41]  [1.x.42]
* 

* 
*  Just one short remark about the constructor of the Triangulation: by convention, all adaptively refined triangulations in deal.II never change by more than one level across a face between cells. For our multigrid algorithms, however, we need a slightly stricter guarantee, namely that the mesh also does not change by more than refinement level across vertices that might connect two cells. In other words, we must prevent the following situation:   
*    [2.x.16]    
*   This is achieved by passing the  [2.x.17]  flag to the constructor of the triangulation class.
* 

* 
* [1.x.43]
* 
*   [1.x.44]  [1.x.45]
* 

* 
*  In addition to just distributing the degrees of freedom in the DoFHandler, we do the same on each level. Then, we follow the same procedure as before to set up the system on the leaf mesh.
* 

* 
* [1.x.46]
* 
*  The multigrid constraints have to be initialized. They need to know where Dirichlet boundary conditions are prescribed.
* 

* 
* [1.x.47]
* 
*  Now for the things that concern the multigrid data structures. First, we resize the multilevel objects to hold matrices and sparsity patterns for every level. The coarse level is zero (this is mandatory right now but may change in a future revision). Note that these functions take a complete, inclusive range here (not a starting index and size), so the finest level is  [2.x.18] . We first have to resize the container holding the SparseMatrix classes, since they have to release their SparsityPattern before the can be destroyed upon resizing.
* 

* 
* [1.x.48]
* 
*  Now, we have to provide a matrix on each level. To this end, we first use the  [2.x.19]  function to generate a preliminary compressed sparsity pattern on each level (see the  [2.x.20]  module for more information on this topic) and then copy it over to the one we really want. The next step is to initialize the interface matrices with the fitting sparsity pattern.     
*   It may be worth pointing out that the interface matrices only have entries for degrees of freedom that sit at or next to the interface between coarser and finer levels of the mesh. They are therefore even sparser than the matrices on the individual levels of our multigrid hierarchy. Therefore, we use a function specifically build for this purpose to generate it.
* 

* 
* [1.x.49]
* 
*   [1.x.50]  [1.x.51]
* 

* 
*  The cell_worker function is used to assemble the matrix and right-hand side on the given cell. This function is used for the active cells to generate the system_matrix and on each level to build the level matrices.   
*   Note that we also assemble a right-hand side when called from assemble_multigrid() even though it is not used.
* 

* 
* [1.x.52]
* 
*   [1.x.53]  [1.x.54]
* 

* 
*  The following function assembles the linear system on the active cells of the mesh. For this, we pass two lambda functions to the mesh_loop() function. The cell_worker function redirects to the class member function of the same name, while the copier is specific to this function and copies local matrix and vector to the corresponding global ones using the constraints.
* 

* 
* [1.x.55]
* 
*   [1.x.56]  [1.x.57]
* 

* 
*  The next function is the one that builds the matrices that define the multigrid method on each level of the mesh. The integration core is the same as above, but the loop below will go over all existing cells instead of just the active ones, and the results must be entered into the correct level matrices. Fortunately, MeshWorker hides most of that from us, and thus the difference between this function and the previous lies only in the setup of the assembler and the different iterators in the loop.   
*   We generate an AffineConstraints object for each level containing the boundary and interface dofs as constrained entries. The corresponding object is then used to generate the level matrices.
* 

* 
* [1.x.58]
* 
*  Interface entries are ignored by the boundary_constraints object above when filling the mg_matrices[cd.level]. Instead, we copy these entries into the interface matrix of the current level manually:
* 

* 
* [1.x.59]
* 
*   [1.x.60]  [1.x.61]
* 

* 
*  This is the other function that is significantly different in support of the multigrid solver (or, in fact, the preconditioner for which we use the multigrid method).   
*   Let us start out by setting up two of the components of multilevel methods: transfer operators between levels, and a solver on the coarsest level. In finite element methods, the transfer operators are derived from the finite element function spaces involved and can often be computed in a generic way independent of the problem under consideration. In that case, we can use the MGTransferPrebuilt class that, given the constraints of the final linear system and the MGConstrainedDoFs object that knows about the boundary conditions on the each level and the degrees of freedom on interfaces between different refinement level can build the matrices for those transfer operations from a DoFHandler object with level degrees of freedom.   
*   The second part of the following lines deals with the coarse grid solver. Since our coarse grid is very coarse indeed, we decide for a direct solver (a Householder decomposition of the coarsest level matrix), even if its implementation is not particularly sophisticated. If our coarse mesh had many more cells than the five we have here, something better suited would obviously be necessary here.
* 

* 
* [1.x.62]
* 
*  The next component of a multilevel solver or preconditioner is that we need a smoother on each level. A common choice for this is to use the application of a relaxation method (such as the SOR, Jacobi or Richardson method) or a small number of iterations of a solver method (such as CG or GMRES). The  [2.x.21]  and MGSmootherPrecondition classes provide support for these two kinds of smoothers. Here, we opt for the application of a single SOR iteration. To this end, we define an appropriate alias and then setup a smoother object.     
*   The last step is to initialize the smoother object with our level matrices and to set some smoothing parameters. The  [2.x.22]  function can optionally take additional arguments that will be passed to the smoother object on each level. In the current case for the SOR smoother, this could, for example, include a relaxation parameter. However, we here leave these at their default values. The call to  [2.x.23]  indicates that we will use two pre- and two post-smoothing steps on each level; to use a variable number of smoother steps on different levels, more options can be set in the constructor call to the  [2.x.24]  object.     
*   The last step results from the fact that we use the SOR method as a smoother
* 
*  - which is not symmetric
* 
*  - but we use the conjugate gradient iteration (which requires a symmetric preconditioner) below, we need to let the multilevel preconditioner make sure that we get a symmetric operator even for nonsymmetric smoothers:
* 

* 
* [1.x.63]
* 
*  The next preparatory step is that we must wrap our level and interface matrices in an object having the required multiplication functions. We will create two objects for the interface objects going from coarse to fine and the other way around; the multigrid algorithm will later use the transpose operator for the latter operation, allowing us to initialize both up and down versions of the operator with the matrices we already built:
* 

* 
* [1.x.64]
* 
*  Now, we are ready to set up the V-cycle operator and the multilevel preconditioner.
* 

* 
* [1.x.65]
* 
*  With all this together, we can finally get about solving the linear system in the usual way:
* 

* 
* [1.x.66]
* 
*   [1.x.67]  [1.x.68]
* 

* 
*  The following two functions postprocess a solution once it is computed. In particular, the first one refines the mesh at the beginning of each cycle while the second one outputs results at the end of each such cycle. The functions are almost unchanged from those in  [2.x.25] .
* 

* 
* [1.x.69]
* 
*   [1.x.70]  [1.x.71]
* 

* 
*  Like several of the functions above, this is almost exactly a copy of the corresponding function in  [2.x.26] . The only difference is the call to  [2.x.27]  that takes care of forming the matrices on every level that we need in the multigrid method.
* 

* 
* [1.x.72]
* 
*   [1.x.73]  [1.x.74]
* 

* 
*  This is again the same function as in  [2.x.28] :
* 

* 
* [1.x.75]
* [1.x.76][1.x.77]
* 

* On the finest mesh, the solution looks like this:
*  [2.x.29] 
* More importantly, we would like to see if the multigrid method really improvedthe solver performance. Therefore, here is the textual output:
* <pre>Cycle 0   Number of active cells:       80   Number of degrees of freedom: 89 (by level: 8, 25, 89)   Number of CG iterations: 8
* Cycle 1   Number of active cells:       158   Number of degrees of freedom: 183 (by level: 8, 25, 89, 138)   Number of CG iterations: 9
* Cycle 2   Number of active cells:       302   Number of degrees of freedom: 352 (by level: 8, 25, 89, 223, 160)   Number of CG iterations: 10
* Cycle 3   Number of active cells:       578   Number of degrees of freedom: 649 (by level: 8, 25, 89, 231, 494, 66)   Number of CG iterations: 10
* Cycle 4   Number of active cells:       1100   Number of degrees of freedom: 1218 (by level: 8, 25, 89, 274, 764, 417, 126)   Number of CG iterations: 10
* Cycle 5   Number of active cells:       2096   Number of degrees of freedom: 2317 (by level: 8, 25, 89, 304, 779, 1214, 817)   Number of CG iterations: 11
* Cycle 6   Number of active cells:       3986   Number of degrees of freedom: 4366 (by level: 8, 25, 89, 337, 836, 2270, 897, 1617)   Number of CG iterations: 10
* Cycle 7   Number of active cells:       7574   Number of degrees of freedom: 8350 (by level: 8, 25, 89, 337, 1086, 2835, 2268, 1789, 3217)   Number of CG iterations: 11</pre>
* That's almost perfect multigrid performance: the linear residual gets reduced by 12 orders ofmagnitude in 10 iteration steps, and the results are almost independent of the mesh size. That'sobviously in part due to the simple nature of the problem solved, butit shows the power of multigrid methods.
* 

* [1.x.78][1.x.79]
* 

* 
* We encourage you to generate timings for the solve() call and compare to [2.x.30] . You will see that the multigrid method has quite an overheadon coarse meshes, but that it always beats other methods on finemeshes because of its optimal complexity.
* A close inspection of this program's performance shows that it is mostlydominated by matrix-vector operations.  [2.x.31]  shows one wayhow this can be avoided by working with matrix-free methods.
* Another avenue would be to use algebraic multigrid methods. The geometricmultigrid method used here can at times be a bit awkward to implement because itneeds all those additional data structures, and it becomes even more difficultif the program is to run in %parallel on machines coupled through MPI, forexample. In that case, it would be simpler if one could use a black-boxpreconditioner that uses some sort of multigrid hierarchy for good performancebut can figure out level matrices and similar things by itself. Algebraicmultigrid methods do exactly this, and we will use them in  [2.x.32]  for thesolution of a Stokes problem and in  [2.x.33]  and  [2.x.34]  for a parallelvariation. That said, a parallel version of this example program with MPI can befound in  [2.x.35] .
* Finally, one may want to think how to use geometric multigrid for other kinds ofproblems, specifically  [2.x.36]  "vector valued problems". This is thetopic of  [2.x.37]  where we use the techniques shown here for the Stokes equation.
* 

* [1.x.80][1.x.81] [2.x.38] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-16b_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17]
*  [2.x.2] 
* [1.x.18][1.x.19][1.x.20]
* 

* This is a variant of  [2.x.3]  with the only change that we are using theMeshWorker framework with the pre-made LocalIntegrator helper classes insteadof manually assembling the matrices.
* The details of this framework on how it is used in practice will be explainedas part of this tutorial program.
* [1.x.21][1.x.22]
* 

* The problem we solve here is the same as the one in  [2.x.4] .
* 

*  [1.x.23] [1.x.24]
*   [1.x.25]  [1.x.26]
* 

* 
*  Again, the first few include files are already known, so we won't comment on them:
* 

* 
* [1.x.27]
* 
*  These, now, are the include necessary for the multilevel methods. The first one declares how to handle Dirichlet boundary conditions on each of the levels of the multigrid method. For the actual description of the degrees of freedom, we do not need any new include file because DoFHandler already has all necessary methods implemented. We will only need to distribute the DoFs for the levels further down.
* 

* 
*  The rest of the include files deals with the mechanics of multigrid as a linear operator (solver or preconditioner).
* 

* 
* [1.x.28]
* 
*  Finally we include the MeshWorker framework. This framework through its function loop() and integration_loop(), automates loops over cells and assembling of data into vectors, matrices, etc. It obeys constraints automatically. Since we have to build several matrices and have to be aware of several sets of constraints, this will save us a lot of headache.
* 

* 
* [1.x.29]
* 
*  In order to save effort, we use the pre-implemented Laplacian found in
* 

* 
* [1.x.30]
* 
*  This is C++:
* 

* 
* [1.x.31]
* 
*   [1.x.32]  [1.x.33]
* 

* 
*  The  [2.x.5]  expects a class that provides functions for integration on cells and boundary and interior faces. This is done by the following class. In the constructor, we tell the loop that cell integrals should be computed (the 'true'), but integrals should not be computed on boundary and interior faces (the two 'false'). Accordingly, we only need a cell function, but none for the faces.
* 

* 
* [1.x.34]
* 
*  Next the actual integrator on each cell. We solve a Poisson problem with a coefficient one in the right half plane and one tenth in the left half plane.
* 

* 
*  The  [2.x.6]  base class of  [2.x.7]  contains objects that can be filled in this local integrator. How many objects are created is determined inside the MeshWorker framework by the assembler class. Here, we test for instance that one matrix is required  [2.x.8]  The matrices are accessed through  [2.x.9]  which takes the number of the matrix as its first argument. The second argument is only used for integrals over faces when there are two matrices for each test function used. Then, a second matrix with indicator 'true' would exist with the same index.
* 

* 
*   [2.x.10]  provides one or several FEValues objects, which below are used by  [2.x.11]  or  [2.x.12]  Since we are assembling only a single PDE, there is also only one of these objects with index zero.
* 

* 
*  In addition, we note that this integrator serves to compute the matrices for the multilevel preconditioner as well as the matrix and the right hand side for the global system. Since the assembler for a system requires an additional vector,  [2.x.13]  is returning a nonzero value. Accordingly, we fill a right hand side vector at the end of this function. Since LocalResults can deal with several BlockVector objects, but we are again in the simplest case here, we enter the information into block zero of vector zero.
* 

* 
* [1.x.35]
* 
*   [1.x.36]  [1.x.37]
* 

* 
*  This main class is basically the same class as in  [2.x.14] . As far as member functions is concerned, the only addition is the  [2.x.15]  function that assembles the matrices that correspond to the discrete operators on intermediate levels:
* 

* 
* [1.x.38]
* 
*  The following members are the essential data structures for the multigrid method. The first two represent the sparsity patterns and the matrices on individual levels of the multilevel hierarchy, very much like the objects for the global mesh above.     
*   Then we have two new matrices only needed for multigrid methods with local smoothing on adaptive meshes. They convey data between the interior part of the refined region and the refinement edge, as outlined in detail in the  [2.x.16]  "multigrid paper".     
*   The last object stores information about the boundary indices on each level and information about indices lying on a refinement edge between two different refinement levels. It thus serves a similar purpose as AffineConstraints, but on each level.
* 

* 
* [1.x.39]
* 
*   [1.x.40]  [1.x.41]
* 

* 
*  Just one short remark about the constructor of the Triangulation: by convention, all adaptively refined triangulations in deal.II never change by more than one level across a face between cells. For our multigrid algorithms, however, we need a slightly stricter guarantee, namely that the mesh also does not change by more than refinement level across vertices that might connect two cells. In other words, we must prevent the following situation:   
*    [2.x.17]    
*   This is achieved by passing the  [2.x.18]  flag to the constructor of the triangulation class.
* 

* 
* [1.x.42]
* 
*   [1.x.43]  [1.x.44]
* 

* 
*  In addition to just distributing the degrees of freedom in the DoFHandler, we do the same on each level. Then, we follow the same procedure as before to set up the system on the leaf mesh.
* 

* 
* [1.x.45]
* 
*  The multigrid constraints have to be initialized. They need to know about the boundary values as well, so we pass the  [2.x.19]  here as well.
* 

* 
* [1.x.46]
* 
*  Now for the things that concern the multigrid data structures. First, we resize the multilevel objects to hold matrices and sparsity patterns for every level. The coarse level is zero (this is mandatory right now but may change in a future revision). Note that these functions take a complete, inclusive range here (not a starting index and size), so the finest level is  [2.x.20] . We first have to resize the container holding the SparseMatrix classes, since they have to release their SparsityPattern before the can be destroyed upon resizing.
* 

* 
* [1.x.47]
* 
*  Now, we have to provide a matrix on each level. To this end, we first use the  [2.x.21]  function to generate a preliminary compressed sparsity pattern on each level (see the  [2.x.22]  module for more information on this topic) and then copy it over to the one we really want. The next step is to initialize both kinds of level matrices with these sparsity patterns.     
*   It may be worth pointing out that the interface matrices only have entries for degrees of freedom that sit at or next to the interface between coarser and finer levels of the mesh. They are therefore even sparser than the matrices on the individual levels of our multigrid hierarchy. If we were more concerned about memory usage (and possibly the speed with which we can multiply with these matrices), we should use separate and different sparsity patterns for these two kinds of matrices.
* 

* 
* [1.x.48]
* 
*   [1.x.49]  [1.x.50]
* 

* 
*  The following function assembles the linear system on the finest level of the mesh. Since we want to reuse the code here for the level assembling below, we use the local integrator class LaplaceIntegrator and leave the loops to the MeshWorker framework. Thus, this function first sets up the objects necessary for this framework, namely
* 

* 
* 
*  - a  [2.x.23]  object, which will provide all the required data in quadrature points on the cell. This object can be seen as an extension of FEValues, providing a lot more useful information,
* 

* 
* 
*  - a  [2.x.24]  object, which on the one hand side extends the functionality of cell iterators, but also provides space for return values in its base class LocalResults,
* 

* 
* 
*  - an assembler, in this case for the whole system. The term 'simple' here refers to the fact that the global system does not have a block structure,
* 

* 
* 
*  - the local integrator, which implements the actual forms.   
*   After the loop has combined all of these into a matrix and a right hand side, there is one thing left to do: the assemblers leave matrix rows and columns of constrained degrees of freedom untouched. Therefore, we put a one on the diagonal to make the whole system well posed. The value one, or any fixed value has the advantage, that its effect on the spectrum of the matrix is easily understood. Since the corresponding eigenvectors form an invariant subspace, the value chosen does not affect the convergence of Krylov space solvers.
* 

* 
* [1.x.51]
* 
*   [1.x.52]  [1.x.53]
* 

* 
*  The next function is the one that builds the linear operators (matrices) that define the multigrid method on each level of the mesh. The integration core is the same as above, but the loop below will go over all existing cells instead of just the active ones, and the results must be entered into the correct level matrices. Fortunately, MeshWorker hides most of that from us, and thus the difference between this function and the previous lies only in the setup of the assembler and the different iterators in the loop. Also, fixing up the matrices in the end is a little more complicated.
* 

* 
* [1.x.54]
* 
*   [1.x.55]  [1.x.56]
* 

* 
*  This is the other function that is significantly different in support of the multigrid solver (or, in fact, the preconditioner for which we use the multigrid method).   
*   Let us start out by setting up two of the components of multilevel methods: transfer operators between levels, and a solver on the coarsest level. In finite element methods, the transfer operators are derived from the finite element function spaces involved and can often be computed in a generic way independent of the problem under consideration. In that case, we can use the MGTransferPrebuilt class that, given the constraints of the final linear system and the MGConstrainedDoFs object that knows about the boundary conditions on the each level and the degrees of freedom on interfaces between different refinement level can build the matrices for those transfer operations from a DoFHandler object with level degrees of freedom.   
*   The second part of the following lines deals with the coarse grid solver. Since our coarse grid is very coarse indeed, we decide for a direct solver (a Householder decomposition of the coarsest level matrix), even if its implementation is not particularly sophisticated. If our coarse mesh had many more cells than the five we have here, something better suited would obviously be necessary here.
* 

* 
* [1.x.57]
* 
*  The next component of a multilevel solver or preconditioner is that we need a smoother on each level. A common choice for this is to use the application of a relaxation method (such as the SOR, Jacobi or Richardson method) or a small number of iterations of a solver method (such as CG or GMRES). The  [2.x.25]  and MGSmootherPrecondition classes provide support for these two kinds of smoothers. Here, we opt for the application of a single SOR iteration. To this end, we define an appropriate alias and then setup a smoother object.     
*   The last step is to initialize the smoother object with our level matrices and to set some smoothing parameters. The  [2.x.26]  function can optionally take additional arguments that will be passed to the smoother object on each level. In the current case for the SOR smoother, this could, for example, include a relaxation parameter. However, we here leave these at their default values. The call to  [2.x.27]  indicates that we will use two pre- and two post-smoothing steps on each level; to use a variable number of smoother steps on different levels, more options can be set in the constructor call to the  [2.x.28]  object.     
*   The last step results from the fact that we use the SOR method as a smoother
* 
*  - which is not symmetric
* 
*  - but we use the conjugate gradient iteration (which requires a symmetric preconditioner) below, we need to let the multilevel preconditioner make sure that we get a symmetric operator even for nonsymmetric smoothers:
* 

* 
* [1.x.58]
* 
*  The next preparatory step is that we must wrap our level and interface matrices in an object having the required multiplication functions. We will create two objects for the interface objects going from coarse to fine and the other way around; the multigrid algorithm will later use the transpose operator for the latter operation, allowing us to initialize both up and down versions of the operator with the matrices we already built:
* 

* 
* [1.x.59]
* 
*  Now, we are ready to set up the V-cycle operator and the multilevel preconditioner.
* 

* 
* [1.x.60]
* 
*  With all this together, we can finally get about solving the linear system in the usual way:
* 

* 
* [1.x.61]
* 
*   [1.x.62]  [1.x.63]
* 

* 
*  The following two functions postprocess a solution once it is computed. In particular, the first one refines the mesh at the beginning of each cycle while the second one outputs results at the end of each such cycle. The functions are almost unchanged from those in  [2.x.29] , with the exception of one minor difference: we generate output in VTK format, to use the more modern visualization programs available today compared to those that were available when  [2.x.30]  was written.
* 

* 
* [1.x.64]
* 
*   [1.x.65]  [1.x.66]
* 

* 
*  Like several of the functions above, this is almost exactly a copy of the corresponding function in  [2.x.31] . The only difference is the call to  [2.x.32]  that takes care of forming the matrices on every level that we need in the multigrid method.
* 

* 
* [1.x.67]
* 
*   [1.x.68]  [1.x.69]
* 

* 
*  This is again the same function as in  [2.x.33] :
* 

* 
* [1.x.70]
* [1.x.71][1.x.72]
* 

* As in  [2.x.34] , the solution looks like this on the finest mesh:
*  [2.x.35] 
* The output is formatted in a slightly different way compared to  [2.x.36]  but isfunctionally the same and shows the same convergence properties:<pre> [2.x.37]  0DEAL::   Number of active cells:       20DEAL::   Number of degrees of freedom: 25 (by level: 8, 25) [2.x.38]  value 0.510691 [2.x.39]  step 6 value 4.59193e-14 [2.x.40]  1DEAL::   Number of active cells:       44DEAL::   Number of degrees of freedom: 55 (by level: 8, 25, 45) [2.x.41]  value 0.440678 [2.x.42]  step 8 value 1.99419e-13 [2.x.43]  2DEAL::   Number of active cells:       86DEAL::   Number of degrees of freedom: 105 (by level: 8, 25, 69, 49) [2.x.44]  value 0.371855 [2.x.45]  step 9 value 1.13984e-13 [2.x.46]  3DEAL::   Number of active cells:       170DEAL::   Number of degrees of freedom: 200 (by level: 8, 25, 77, 174) [2.x.47]  value 0.318967 [2.x.48]  step 9 value 2.62112e-13 [2.x.49]  4DEAL::   Number of active cells:       332DEAL::   Number of degrees of freedom: 388 (by level: 8, 25, 86, 231, 204) [2.x.50]  value 0.276534 [2.x.51]  step 10 value 1.69562e-13 [2.x.52]  5DEAL::   Number of active cells:       632DEAL::   Number of degrees of freedom: 714 (by level: 8, 25, 89, 231, 514, 141) [2.x.53]  value 0.215300 [2.x.54]  step 10 value 6.47463e-13 [2.x.55]  6DEAL::   Number of active cells:       1202DEAL::   Number of degrees of freedom: 1332 (by level: 8, 25, 89, 282, 771, 435, 257) [2.x.56]  value 0.175848 [2.x.57]  step 10 value 1.80664e-13 [2.x.58]  7DEAL::   Number of active cells:       2288DEAL::   Number of degrees of freedom: 2511 (by level: 8, 25, 89, 318, 779, 1420, 829, 30) [2.x.59]  value 0.136724 [2.x.60]  step 11 value 9.73331e-14</pre>
* 

* [1.x.73][1.x.74] [2.x.61] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-17_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21]
* [1.x.22][1.x.23][1.x.24]
* 

* [1.x.25][1.x.26]
* 

* This program does not introduce any new mathematical ideas; in fact, all itdoes is to do the exact same computations that  [2.x.2] already does, but it does so in a different manner: instead of using deal.II'sown linear algebra classes, we build everything on top of classes deal.IIprovides that wrap around the linear algebra implementation of the [1.x.27] library. Andsince PETSc allows to distribute matrices and vectors across several computerswithin an MPI network, the resulting code will even be able to solve theproblem in %parallel. If you don't know what PETSc is, then this would be agood time to take a quick glimpse at their homepage.
* As a prerequisite of this program, you need to have PETSc installed, and ifyou want to run in %parallel on a cluster, you also need [1.x.28] to partition meshes. The installation of deal.IItogether with these two additional libraries is described in the [1.x.29] file.
* Now, for the details: as mentioned, the program does not compute anything new,so the use of finite element classes, etc., is exactly the same as before. Thedifference to previous programs is that we have replaced almost all uses ofclasses  [2.x.3]  by theirnear-equivalents  [2.x.4]  and [2.x.5]  that store data in a way so thatevery processor in the MPI network only storesa part of the matrix or vector. More specifically, each processor willonly store those rows of the matrix that correspond to a degree offreedom it "owns". For vectors, they either store only elements thatcorrespond to degrees of freedom the processor owns (this is what isnecessary for the right hand side), or also some additional elementsthat make sure that every processor has access the solution componentsthat live on the cells the processor owns (so-called [2.x.6]  "locally active DoFs") or also on neighboring cells(so-called  [2.x.7]  "locally relevant DoFs").
* The interface the classes from the PETScWrapper namespace provide is very similar to thatof the deal.II linear algebra classes, but instead of implementing thisfunctionality themselves, they simply pass on to their corresponding PETScfunctions. The wrappers are therefore only used to give PETSc a more modern,object oriented interface, and to make the use of PETSc and deal.II objects asinterchangeable as possible. The main point of using PETSc is that it can runin %parallel. We will make use of this by partitioning the domain into as manyblocks ("subdomains") as there are processes in the MPI network. At the sametime, PETSc also provides dummy MPI stubs, so you can run this program on asingle machine if PETSc was configured without MPI.
* 

* [1.x.30][1.x.31]
* 

* Developing software to run in %parallel via MPI requires a bit of a change inmindset because one typically has to split up all data structures so thatevery processor only stores a piece of the entire problem. As a consequence,you can't typically access all components of a solution vector on eachprocessor
* 
*  -  each processor may simply not have enough memory to hold theentire solution vector. Because data is split up or "distributed" acrossprocessors, we call the programming model used by MPI "distributed memorycomputing" (as opposed to "shared memory computing", which would meanthat multiple processors can all access all data within one memoryspace, for example whenever multiple cores in a single machine workon a common task). Some of the fundamentals of distributed memorycomputing are discussed in the [2.x.8]  "Parallel computing with multiple processors using distributed memory"documentation module, which is itself a sub-module of the [2.x.9]  "Parallel computing" module.
* In general, to be truly able to scale to large numbers of processors, oneneeds to split between the available processors [1.x.32] data structurewhose size scales with the size of the overall problem. (For a definitionof what it means for a program to "scale", see [2.x.10]  "this glossary entry".) This includes, forexample, the triangulation, the matrix, and all global vectors (solution, righthand side). If one doesn't split all of these objects, one of those will bereplicated on all processors and will eventually simply become too largeif the problem size (and the number of available processors) becomes large.(On the other hand, it is completely fine to keep objects with a size thatis independent of the overall problem size on every processor. For example,each copy of the executable will create its own finite element object, or thelocal matrix we use in the assembly.)
* In the current program (as well as in the related  [2.x.11] ), we will not goquite this far but present a gentler introduction to using MPI. Morespecifically, the only data structures we will parallelize are matrices andvectors. We do, however, not split up the Triangulation andDoFHandler classes: each process still has a complete copy ofthese objects, and all processes have exact copies of what the other processeshave. We will then simply have to mark, in each copy of the triangulationon each of the processors, which processor owns which cells. Thisprocess is called "partitioning" a mesh into  [2.x.12]  "subdomains".
* For larger problems, having to store the [1.x.33] mesh on every processorwill clearly yield a bottleneck. Splitting up the mesh is slightly, though notmuch more complicated (from a user perspective, though it is [1.x.34] morecomplicated under the hood) to achieve andwe will show how to do this in  [2.x.13]  and some other programs. There arenumerous occasions where, in the course of discussing how a function of thisprogram works, we will comment on the fact that it will not scale to largeproblems and why not. All of these issues will be addressed in  [2.x.14]  andin particular  [2.x.15] , which scales to very large numbers of processes.
* Philosophically, the way MPI operates is as follows. You typically run aprogram via
* [1.x.35]
* which means to run it on (say) 32 processors. (If you are on a cluster system,you typically need to [1.x.36] the program to run whenever 32 processorsbecome available; this will be described in the documentation of yourcluster. But under the hood, whenever those processors become available,the same call as above will generally be executed.) What this does is thatthe MPI system will start 32 [1.x.37] of the  [2.x.16] executable. (The MPI term for each of these running executables is that youhave 32  [2.x.17]  "MPI processes".)This may happen on different machines that can't even readfrom each others' memory spaces, or it may happen on the same machine, butthe end result is the same: each of these 32 copies will run with somememory allocated to it by the operating system, and it will not directlybe able to read the memory of the other 31 copies. In order to collaboratein a common task, these 32 copies then have to [1.x.38] witheach other. MPI, short for [1.x.39], makes thispossible by allowing programs to [1.x.40]. You can thinkof this as the mail service: you can put a letter to a specific addressinto the mail and it will be delivered. But that's the extent to whichyou can control things. If you want the receiver to do somethingwith the content of the letter, for example return to you data you wantfrom over there, then two things need to happen: (i) the receiver needsto actually go check whether there is anything in their mailbox, and (ii) ifthere is, react appropriately, for example by sending data back. If youwait for this return message but the original receiver was distractedand not paying attention, then you're out of luck: you'll simply have towait until your requested over there will be worked on. In some cases,bugs will lead the original receiver to never check your mail, and in thatcase you will wait forever
* 
*  -  this is called a [1.x.41].( [2.x.18] 
* In practice, one does not usually program at the level of sending andreceiving individual messages, but uses higher level operations. Forexample, in the program we will use function calls that take a numberfrom each processor, add them all up, and return the sum to allprocessors. Internally, this is implemented using individual messages,but to the user this is transparent. We call such operations [1.x.42]because [1.x.43] processors participate in them. Collectives allow usto write programs where not every copy of the executable is doing somethingcompletely different (this would be incredibly difficult to program) butwhere in essence all copies are doing the same thing (though on differentdata) for themselves, running through the same blocks of code; then theycommunicate data through collectives; and then go back to doing somethingfor themselves again running through the same blocks of data. This is thekey piece to being able to write programs, and it is the key componentto making sure that programs can run on any number of processors,since we do not have to write different code for each of the participatingprocessors.
* (This is not to say that programs are never written in ways wheredifferent processors run through different blocks of code in theircopy of the executable. Programs internally also often communicatein other ways than through collectives. But in practice, %parallel finiteelement codes almost always follow the scheme where every copyof the program runs through the same blocks of code at the same time,interspersed by phases where all processors communicate with each other.)
* In reality, even the level of calling MPI collective functions is toolow. Rather, the program below will not contain any directcalls to MPI at all, but only deal.II functions that hide thiscommunication from users of the deal.II. This has the advantage thatyou don't have to learn the details of MPI and its rather intricatefunction calls. That said, you do have to understand the generalphilosophy behind MPI as outlined above.
* 

* [1.x.44][1.x.45]
* 

* The techniques this program then demonstrates are:
* 
*  - How to use the PETSc wrapper classes; this will already be visible in the  declaration of the principal class of this program,  [2.x.19] .
* 
*  - How to partition the mesh into subdomains; this happens in the   [2.x.20]  function.
* 
*  - How to parallelize operations for jobs running on an MPI network; here, this  is something one has to pay attention to in a number of places, most  notably in the   [2.x.21]  function.
* 
*  - How to deal with vectors that store only a subset of vector entries  and for which we have to ensure that they store what we need on the  current processors. See for example the   [2.x.22]   functions.
* 
*  - How to deal with status output from programs that run on multiple  processors at the same time. This is done via the  [2.x.23]   variable in the program, initialized in the constructor.
* Since all this can only be demonstrated using actual code, let us go straight to thecode without much further ado.
* 

*  [1.x.46] [1.x.47]
*   [1.x.48]  [1.x.49]
* 

* 
*  First the usual assortment of header files we have already used in previous example programs:
* 

* 
* [1.x.50]
* 
*  And here come the things that we need particularly for this example program and that weren't in  [2.x.24] . First, we replace the standard output  [2.x.25]  which is used in parallel computations for generating output only on one of the MPI processes.
* 

* 
* [1.x.51]
* 
*  We are going to query the number of processes and the number of the present process by calling the respective functions in the  [2.x.26]  namespace.
* 

* 
* [1.x.52]
* 
*  Then, we are going to replace all linear algebra components that involve the (global) linear system by classes that wrap interfaces similar to our own linear algebra classes around what PETSc offers (PETSc is a library written in C, and deal.II comes with wrapper classes that provide the PETSc functionality with an interface that is similar to the interface we already had for our own linear algebra classes). In particular, we need vectors and matrices that are distributed across several  [2.x.27]  "processes" in MPI programs (and simply map to sequential, local vectors and matrices if there is only a single process, i.e., if you are running on only one machine, and without MPI support):
* 

* 
* [1.x.53]
* 
*  Then we also need interfaces for solvers and preconditioners that PETSc provides:
* 

* 
* [1.x.54]
* 
*  And in addition, we need some algorithms for partitioning our meshes so that they can be efficiently distributed across an MPI network. The partitioning algorithm is implemented in the  [2.x.28]  namespace, and we need an additional include file for a function in  [2.x.29]  that allows to sort the indices associated with degrees of freedom so that they are numbered according to the subdomain they are associated with:
* 

* 
* [1.x.55]
* 
*  And this is simply C++ again:
* 

* 
* [1.x.56]
* 
*  The last step is as in all previous programs:
* 

* 
* [1.x.57]
* 
*   [1.x.58]  [1.x.59]
* 

* 
*  The first real part of the program is the declaration of the main class.  As mentioned in the introduction, almost all of this has been copied verbatim from  [2.x.30] , so we only comment on the few differences between the two tutorials.  There is one (cosmetic) change in that we let  [2.x.31]  return a value, namely the number of iterations it took to converge, so that we can output this to the screen at the appropriate place.
* 

* 
* [1.x.60]
* 
*  The first change is that we have to declare a variable that indicates the  [2.x.32]  "MPI communicator" over which we are supposed to distribute our computations.
* 

* 
* [1.x.61]
* 
*  Then we have two variables that tell us where in the parallel world we are. The first of the following variables,  [2.x.33] , tells us how many MPI processes there exist in total, while the second one,  [2.x.34] , indicates which is the number of the present process within this space of processes (in MPI language, this corresponds to the  [2.x.35]  "rank" of the process). The latter will have a unique value for each process between zero and (less than)  [2.x.36] . If this program is run on a single machine without MPI support, then their values are  [2.x.37] , respectively.
* 

* 
* [1.x.62]
* 
*  Next up is a stream-like variable  [2.x.38] . It is, in essence, just something we use for convenience: in a parallel program, if each process outputs status information, then there quickly is a lot of clutter. Rather, we would want to only have one  [2.x.39]  "process" output everything once, for example the one with  [2.x.40]  "rank" zero. At the same time, it seems silly to prefix [1.x.63] place where we create output with an  [2.x.41]  condition.     
*   To make this simpler, the ConditionalOStream class does exactly this under the hood: it acts as if it were a stream, but only forwards to a real, underlying stream if a flag is set. By setting this condition to  [2.x.42]  (where  [2.x.43]  corresponds to the rank of an MPI process), we make sure that output is only generated from the first process and that we don't get the same lines of output over and over again, once per process. Thus, we can use  [2.x.44]  everywhere and in every process, but on all but one process nothing will ever happen to the information that is piped into the object via  [2.x.45] .
* 

* 
* [1.x.64]
* 
*  The remainder of the list of member variables is fundamentally the same as in  [2.x.46] . However, we change the declarations of matrix and vector types to use parallel PETSc objects instead. Note that we do not use a separate sparsity pattern, since PETSc manages this internally as part of its matrix data structures.
* 

* 
* [1.x.65]
* 
*   [1.x.66]  [1.x.67]
* 

* 
*  The following is taken from  [2.x.47]  without change:
* 

* 
* [1.x.68]
* 
*   [1.x.69]  [1.x.70]
* 

* 
*   [1.x.71]  [1.x.72]
* 

* 
*  The first step in the actual implementation is the constructor of the main class. Apart from initializing the same member variables that we already had in  [2.x.48] , we here initialize the MPI communicator variable we shall use with the global MPI communicator linking all processes together (in more complex applications, one could here use a communicator object that only links a subset of all processes), and call the  [2.x.49]  helper functions to determine the number of processes and where the present one fits into this picture. In addition, we make sure that output is only generated by the (globally) first process. We do so by passing the stream we want to output to  [2.x.50]  and a true/false flag as arguments where the latter is determined by testing whether the process currently executing the constructor call is the first in the MPI universe.
* 

* 
* [1.x.73]
* 
*   [1.x.74]  [1.x.75]
* 

* 
*  Next, the function in which we set up the various variables for the global linear system to be solved needs to be implemented.   
*   However, before we proceed with this, there is one thing to do for a parallel program: we need to determine which MPI process is responsible for each of the cells. Splitting cells among processes, commonly called "partitioning the mesh", is done by assigning a  [2.x.51]  "subdomain id" to each cell. We do so by calling into the METIS library that does this in a very efficient way, trying to minimize the number of nodes on the interfaces between subdomains. Rather than trying to call METIS directly, we do this by calling the  [2.x.52]  function that does this at a much higher level of programming.   
*  

* 
*  [2.x.53]  As mentioned in the introduction, we could avoid this manual partitioning step if we used the  [2.x.54]  class for the triangulation object instead (as we do in  [2.x.55] ). That class does, in essence, everything a regular triangulation does, but it then also automatically partitions the mesh after every mesh creation or refinement operation.   
*   Following partitioning, we need to enumerate all degrees of freedom as usual.  However, we would like to enumerate the degrees of freedom in a way so that all degrees of freedom associated with cells in subdomain zero (which resides on process zero) come before all DoFs associated with cells on subdomain one, before those on cells on process two, and so on. We need this since we have to split the global vectors for right hand side and solution, as well as the matrix into contiguous chunks of rows that live on each of the processors, and we will want to do this in a way that requires minimal communication. This particular enumeration can be obtained by re-ordering degrees of freedom indices using  [2.x.56]    
*   The final step of this initial setup is that we get ourselves an IndexSet that indicates the subset of the global number of unknowns this process is responsible for. (Note that a degree of freedom is not necessarily owned by the process that owns a cell just because the degree of freedom lives on this cell: some degrees of freedom live on interfaces between subdomains, and are consequently only owned by one of the processes adjacent to this interface.)   
*   Before we move on, let us recall a fact already discussed in the introduction: The triangulation we use here is replicated across all processes, and each process has a complete copy of the entire triangulation, with all cells. Partitioning only provides a way to identify which cells out of all each process "owns", but it knows everything about all of them. Likewise, the DoFHandler object knows everything about every cell, in particular the degrees of freedom that live on each cell, whether it is one that the current process owns or not. This can not scale to large problems because eventually just storing the entire mesh, and everything that is associated with it, on every process will become infeasible if the problem is large enough. On the other hand, if we split the triangulation into parts so that every process stores only those cells it "owns" but nothing else (or, at least a sufficiently small fraction of everything else), then we can solve large problems if only we throw a large enough number of MPI processes at them. This is what we are going to in  [2.x.57] , for example, using the  [2.x.58]  class.  On the other hand, most of the rest of what we demonstrate in the current program will actually continue to work whether we have the entire triangulation available, or only a piece of it.
* 

* 
* [1.x.76]
* 
*  We need to initialize the objects denoting hanging node constraints for the present grid. As with the triangulation and DoFHandler objects, we will simply store [1.x.77] constraints on each process; again, this will not scale, but we show in  [2.x.59]  how one can work around this by only storing on each MPI process the constraints for degrees of freedom that actually matter on this particular process.
* 

* 
* [1.x.78]
* 
*  Now we create the sparsity pattern for the system matrix. Note that we again compute and store all entries and not only the ones relevant to this process (see  [2.x.60]  or  [2.x.61]  for a more efficient way to handle this).
* 

* 
* [1.x.79]
* 
*  Now we determine the set of locally owned DoFs and use that to initialize parallel vectors and matrix. Since the matrix and vectors need to work in parallel, we have to pass them an MPI communication object, as well as information about the partitioning contained in the IndexSet  [2.x.62]   The IndexSet contains information about the global size (the [1.x.80] number of degrees of freedom) and also what subset of rows is to be stored locally.  Note that the system matrix needs that partitioning information for the rows and columns. For square matrices, as it is the case here, the columns should be partitioned in the same way as the rows, but in the case of rectangular matrices one has to partition the columns in the same way as vectors are partitioned with which the matrix is multiplied, while rows have to partitioned in the same way as destination vectors of matrix-vector multiplications:
* 

* 
* [1.x.81]
* 
*   [1.x.82]  [1.x.83]
* 

* 
*  We now assemble the matrix and right hand side of the problem. There are some things worth mentioning before we go into detail. First, we will be assembling the system in parallel, i.e., each process will be responsible for assembling on cells that belong to this particular process. Note that the degrees of freedom are split in a way such that all DoFs in the interior of cells and between cells belonging to the same subdomain belong to the process that  [2.x.63]  the cell. However, even then we sometimes need to assemble on a cell with a neighbor that belongs to a different process, and in these cases when we add up the local contributions into the global matrix or right hand side vector, we have to transfer these entries to the process that owns these elements. Fortunately, we don't have to do this by hand: PETSc does all this for us by caching these elements locally, and sending them to the other processes as necessary when we call the  [2.x.64]  functions on the matrix and vector at the end of this function.   
*   The second point is that once we have handed over matrix and vector contributions to PETSc, it is a) hard, and b) very inefficient to get them back for modifications. This is not only the fault of PETSc, it is also a consequence of the distributed nature of this program: if an entry resides on another processor, then it is necessarily expensive to get it. The consequence of this is that we should not try to first assemble the matrix and right hand side as if there were no hanging node constraints and boundary values, and then eliminate these in a second step (using, for example,  [2.x.65]  Rather, we should try to eliminate hanging node constraints before handing these entries over to PETSc. This is easy: instead of copying elements by hand into the global matrix (as we do in  [2.x.66] ), we use the  [2.x.67]  functions to take care of hanging nodes at the same time. We also already did this in  [2.x.68] . The second step, elimination of boundary nodes, could also be done this way by putting the boundary values into the same AffineConstraints object as hanging nodes (see the way it is done in  [2.x.69] , for example); however, it is not strictly necessary to do this here because eliminating boundary values can be done with only the data stored on each process itself, and consequently we use the approach used before in  [2.x.70] , i.e., via  [2.x.71]    
*   All of this said, here is the actual implementation starting with the general setup of helper variables.  (Note that we still use the deal.II full matrix and vector types for the local systems as these are small and need not be shared across processes.)
* 

* 
* [1.x.84]
* 
*  The next thing is the loop over all elements. Note that we do not have to do [1.x.85] the work on every process: our job here is only to assemble the system on cells that actually belong to this MPI process, all other cells will be taken care of by other processes. This is what the if-clause immediately after the for-loop takes care of: it queries the subdomain identifier of each cell, which is a number associated with each cell that tells us about the owner process. In more generality, the subdomain id is used to split a domain into several parts (we do this above, at the beginning of  [2.x.72] ), and which allows to identify which subdomain a cell is living on. In this application, we have each process handle exactly one subdomain, so we identify the terms  [2.x.73] .     
*   Apart from this, assembling the local system is relatively uneventful if you have understood how this is done in  [2.x.74] . As mentioned above, distributing local contributions into the global matrix and right hand sides also takes care of hanging node constraints in the same way as is done in  [2.x.75] .
* 

* 
* [1.x.86]
* 
*  The next step is to "compress" the vector and the system matrix. This means that each process sends the additions that were made to those entries of the matrix and vector that the process did not own itself to the process that owns them. After receiving these additions from other processes, each process then adds them to the values it already has. These additions are combining the integral contributions of shape functions living on several cells just as in a serial computation, with the difference that the cells are assigned to different processes.
* 

* 
* [1.x.87]
* 
*  The global matrix and right hand side vectors have now been formed. We still have to apply boundary values, in the same way as we did, for example, in  [2.x.76] ,  [2.x.77] , and a number of other programs.     
*   The last argument to the call to  [2.x.78]  below allows for some optimizations. It controls whether we should also delete entries (i.e., set them to zero) in the matrix columns corresponding to boundary nodes, or to keep them (and passing  [2.x.79]  means: yes, do eliminate the columns). If we do eliminate columns, then the resulting matrix will be symmetric again if it was before; if we don't, then it won't. The solution of the resulting system should be the same, though. The only reason why we may want to make the system symmetric again is that we would like to use the CG method, which only works with symmetric matrices. The reason why we may [1.x.88] want to make the matrix symmetric is because this would require us to write into column entries that actually reside on other processes, i.e., it involves communicating data. This is always expensive.     
*   Experience tells us that CG also works (and works almost as well) if we don't remove the columns associated with boundary nodes, which can be explained by the special structure of this particular non-symmetry. To avoid the expense of communication, we therefore do not eliminate the entries in the affected columns.
* 

* 
* [1.x.89]
* 
*   [1.x.90]  [1.x.91]
* 

* 
*  Having assembled the linear system, we next need to solve it. PETSc offers a variety of sequential and parallel solvers, for which we have written wrappers that have almost the same interface as is used for the deal.II solvers used in all previous example programs. The following code should therefore look rather familiar.   
*   At the top of the function, we set up a convergence monitor, and assign it the accuracy to which we would like to solve the linear system. Next, we create an actual solver object using PETSc's CG solver which also works with parallel (distributed) vectors and matrices. And finally a preconditioner; we choose to use a block Jacobi preconditioner which works by computing an incomplete LU decomposition on each diagonal block of the matrix.  (In other words, each MPI process computes an ILU from the rows it stores by throwing away columns that correspond to row indices not stored locally; this yields a square matrix block from which we can compute an ILU. That means that if you run the program with only one process, then you will use an ILU(0) as a preconditioner, while if it is run on many processes, then we will have a number of blocks on the diagonal and the preconditioner is the ILU(0) of each of these blocks. In the extreme case of one degree of freedom per processor, this preconditioner is then simply a Jacobi preconditioner since the diagonal matrix blocks consist of only a single entry. Such a preconditioner is relatively easy to compute because it does not require any kind of communication between processors, but it is in general not very efficient for large numbers of processors.)   
*   Following this kind of setup, we then solve the linear system:
* 

* 
* [1.x.92]
* 
*  The next step is to distribute hanging node constraints. This is a little tricky, since to fill in the value of a constrained node you need access to the values of the nodes to which it is constrained (for example, for a Q1 element in 2d, we need access to the two nodes on the big side of a hanging node face, to compute the value of the constrained node in the middle).     
*   The problem is that we have built our vectors (in  [2.x.80] ) in such a way that every process is responsible for storing only those elements of the solution vector that correspond to the degrees of freedom this process "owns". There are, however, cases where in order to compute the value of the vector entry for a constrained degree of freedom on one process, we need to access vector entries that are stored on other processes.  PETSc (and, for that matter, the MPI model on which it is built) does not allow to simply query vector entries stored on other processes, so what we do here is to get a copy of the "distributed" vector where we store all elements locally. This is simple, since the deal.II wrappers have a conversion constructor for the deal.II Vector class. (This conversion of course requires communication, but in essence every process only needs to send its data to every other process once in bulk, rather than having to respond to queries for individual elements):
* 

* 
* [1.x.93]
* 
*  Of course, as in previous discussions, it is clear that such a step cannot scale very far if you wanted to solve large problems on large numbers of processes, because every process now stores [1.x.94] of the solution vector. (We will show how to do this better in  [2.x.81] .)  On the other hand, distributing hanging node constraints is simple on this local copy, using the usual function  [2.x.82]  In particular, we can compute the values of [1.x.95] constrained degrees of freedom, whether the current process owns them or not:
* 

* 
* [1.x.96]
* 
*  Then transfer everything back into the global vector. The following operation copies those elements of the localized solution that we store locally in the distributed solution, and does not touch the other ones. Since we do the same operation on all processors, we end up with a distributed vector (i.e., a vector that on every process only stores the vector entries corresponding to degrees of freedom that are owned by this process) that has all the constrained nodes fixed.     
*   We end the function by returning the number of iterations it took to converge, to allow for some output.
* 

* 
* [1.x.97]
* 
*   [1.x.98]  [1.x.99]
* 

* 
*  Using some kind of refinement indicator, the mesh can be refined. The problem is basically the same as with distributing hanging node constraints: in order to compute the error indicator (even if we were just interested in the indicator on the cells the current process owns), we need access to more elements of the solution vector than just those the current processor stores. To make this happen, we do essentially what we did in  [2.x.83]  already, namely get a [1.x.100] copy of the solution vector onto every process, and use that to compute. This is in itself expensive as explained above and it is particular unnecessary since we had just created and then destroyed such a vector in  [2.x.84] , but efficiency is not the point of this program and so let us opt for a design in which every function is as self-contained as possible.   
*   Once we have such a "localized" vector that contains [1.x.101] elements of the solution vector, we can compute the indicators for the cells that belong to the present process. In fact, we could of course compute [1.x.102] refinement indicators since our Triangulation and DoFHandler objects store information about all cells, and since we have a complete copy of the solution vector. But in the interest in showing how to operate in %parallel, let us demonstrate how one would operate if one were to only compute [1.x.103] error indicators and then exchange the remaining ones with the other processes. (Ultimately, each process needs a complete set of refinement indicators because every process needs to refine their mesh, and needs to refine it in exactly the same way as all of the other processes.)   
*   So, to do all of this, we need to:
* 

* 
* 
*  - First, get a local copy of the distributed solution vector.
* 

* 
* 
*  - Second, create a vector to store the refinement indicators.
* 

* 
* 
*  - Third, let the KellyErrorEstimator compute refinement indicators for all cells belonging to the present subdomain/process. The last argument of the call indicates which subdomain we are interested in. The three arguments before it are various other default arguments that one usually does not need (and does not state values for, but rather uses the defaults), but which we have to state here explicitly since we want to modify the value of a following argument (i.e., the one indicating the subdomain).
* 

* 
* [1.x.104]
* 
*  Now all processes have computed error indicators for their own cells and stored them in the respective elements of the  [2.x.85]  vector. The elements of this vector for cells not owned by the present process are zero. However, since all processes have a copy of the entire triangulation and need to keep these copies in sync, they need the values of refinement indicators for all cells of the triangulation. Thus, we need to distribute our results. We do this by creating a distributed vector where each process has its share and sets the elements it has computed. Consequently, when you view this vector as one that lives across all processes, then every element of this vector has been set once. We can then assign this parallel vector to a local, non-parallel vector on each process, making [1.x.105] error indicators available on every process.     
*   So in the first step, we need to set up a parallel vector. For simplicity, every process will own a chunk with as many elements as this process owns cells, so that the first chunk of elements is stored with process zero, the next chunk with process one, and so on. It is important to remark, however, that these elements are not necessarily the ones we will write to. This is a consequence of the order in which cells are arranged, i.e., the order in which the elements of the vector correspond to cells is not ordered according to the subdomain these cells belong to. In other words, if on this process we compute indicators for cells of a certain subdomain, we may write the results to more or less random elements of the distributed vector; in particular, they may not necessarily lie within the chunk of vector we own on the present process. They will subsequently have to be copied into another process' memory space, an operation that PETSc does for us when we call the  [2.x.86]  function. This inefficiency could be avoided with some more code, but we refrain from it since it is not a major factor in the program's total runtime.     
*   So here is how we do it: count how many cells belong to this process, set up a distributed vector with that many elements to be stored locally, copy over the elements we computed locally, and finally compress the result. In fact, we really only copy the elements that are nonzero, so we may miss a few that we computed to zero, but this won't hurt since the original values of the vector are zero anyway.
* 

* 
* [1.x.106]
* 
*  So now we have this distributed vector that contains the refinement indicators for all cells. To use it, we need to obtain a local copy and then use it to mark cells for refinement or coarsening, and actually do the refinement and coarsening. It is important to recognize that [1.x.107] process does this to its own copy of the triangulation, and does it in exactly the same way.
* 

* 
* [1.x.108]
* 
*   [1.x.109]  [1.x.110]
* 

* 
*  The final function of significant interest is the one that creates graphical output. This works the same way as in  [2.x.87] , with two small differences. Before discussing these, let us state the general philosophy this function will work: we intend for all of the data to be generated on a single process, and subsequently written to a file. This is, as many other parts of this program already discussed, not something that will scale. Previously, we had argued that we will get into trouble with triangulations, DoFHandlers, and copies of the solution vector where every process has to store all of the data, and that there will come to be a point where each process simply doesn't have enough memory to store that much data. Here, the situation is different: it's not only the memory, but also the run time that's a problem. If one process is responsible for processing [1.x.111] of the data while all of the other processes do nothing, then this one function will eventually come to dominate the overall run time of the program.  In particular, the time this function takes is going to be proportional to the overall size of the problem (counted in the number of cells, or the number of degrees of freedom), independent of the number of processes we throw at it.   
*   Such situations need to be avoided, and we will show in  [2.x.88]  and  [2.x.89]  how to address this issue. For the current problem, the solution is to have each process generate output data only for its own local cells, and write them to separate files, one file per process. This is how  [2.x.90]  operates. Alternatively, one could simply leave everything in a set of independent files and let the visualization software read all of them (possibly also using multiple processors) and create a single visualization out of all of them; this is the path  [2.x.91] ,  [2.x.92] , and all other parallel programs developed later on take.   
*   More specifically for the current function, all processes call this function, but not all of them need to do the work associated with generating output. In fact, they shouldn't, since we would try to write to the same file multiple times at once. So we let only the first process do this, and all the other ones idle around during this time (or start their work for the next iteration, or simply yield their CPUs to other jobs that happen to run at the same time). The second thing is that we not only output the solution vector, but also a vector that indicates which subdomain each cell belongs to. This will make for some nice pictures of partitioned domains.   
*   To implement this, process zero needs a complete set of solution components in a local vector. Just as with the previous function, the efficient way to do this would be to re-use the vector already created in the  [2.x.93]  function, but to keep things more self-contained, we simply re-create one here from the distributed solution vector.   
*   An important thing to realize is that we do this localization operation on all processes, not only the one that actually needs the data. This can't be avoided, however, with the simplified communication model of MPI we use for vectors in this tutorial program: MPI does not have a way to query data on another process, both sides have to initiate a communication at the same time. So even though most of the processes do not need the localized solution, we have to place the statement converting the distributed into a localized vector so that all processes execute it.   
*   (Part of this work could in fact be avoided. What we do is send the local parts of all processes to all other processes. What we would really need to do is to initiate an operation on all processes where each process simply sends its local chunk of data to process zero, since this is the only one that actually needs it, i.e., we need something like a gather operation. PETSc can do this, but for simplicity's sake we don't attempt to make use of this here. We don't, since what we do is not very expensive in the grand scheme of things: it is one vector communication among all processes, which has to be compared to the number of communications we have to do when solving the linear system, setting up the block-ILU for the preconditioner, and other operations.)
* 

* 
* [1.x.112]
* 
*  This being done, process zero goes ahead with setting up the output file as in  [2.x.94] , and attaching the (localized) solution vector to the output object.
* 

* 
* [1.x.113]
* 
*  The only other thing we do here is that we also output one value per cell indicating which subdomain (i.e., MPI process) it belongs to. This requires some conversion work, since the data the library provides us with is not the one the output class expects, but this is not difficult. First, set up a vector of integers, one per cell, that is then filled by the subdomain id of each cell.         
*   The elements of this vector are then converted to a floating point vector in a second step, and this vector is added to the DataOut object, which then goes off creating output in VTK format:
* 

* 
* [1.x.114]
* 
*   [1.x.115]  [1.x.116]
* 

* 
*  Lastly, here is the driver function. It is almost completely unchanged from  [2.x.95] , with the exception that we replace  [2.x.96]  stream. Apart from this, the only other cosmetic change is that we output how many degrees of freedom there are per process, and how many iterations it took for the linear solver to converge:
* 

* 
* [1.x.117]
* 
*   [1.x.118]  [1.x.119]
* 

* 
*  The  [2.x.97]  works the same way as most of the main functions in the other example programs, i.e., it delegates work to the  [2.x.98]  function of a managing object, and only wraps everything into some code to catch exceptions:
* 

* 
* [1.x.120]
* 
*  Here is the only real difference: MPI and PETSc both require that we initialize these libraries at the beginning of the program, and un-initialize them at the end. The class MPI_InitFinalize takes care of all of that. The trailing argument `1` means that we do want to run each MPI process with a single thread, a prerequisite with the PETSc parallel linear algebra.
* 

* 
* [1.x.121]
* [1.x.122][1.x.123]
* 

* 
* If the program above is compiled and run on a single processormachine, it should generate results that are very similar to thosethat we already got with  [2.x.99] . However, it becomes more interestingif we run it on a multicore machine or a cluster of computers. Themost basic way to run MPI programs is using a command line like
* [1.x.124]
* to run the  [2.x.100]  executable with 32 processors.
* (If you work on a cluster, then there is typically a step in between where youneed to set up a job script and submit the script to a scheduler. The schedulerwill execute the script whenever it can allocate 32 unused processors for yourjob. How to write such jobscripts differs from cluster to cluster, and you should find the documentationof your cluster to see how to do this. On my system, I have to use the command [2.x.101]  with a whole host of options to run a job in parallel.)
* Whether directly or through a scheduler, if you run this program on 8processors, you should get output like the following:
* [1.x.125]
* (This run uses a few more refinement cycles than the code available inthe examples/ directory. The run also used a version of METIS from2004 that generated different partitionings; consequently,the numbers you get today are slightly different.)
* As can be seen, we can easily get to almost four million unknowns. In fact, thecode's runtime with 8 processes was less than 7 minutes up to (and including)cycle 14, and 14 minutes including the second to last step. (These are numbersrelevant to when the code was initially written, in 2004.) I lost the timinginformation for the last step, though, but you get the idea. All this is afterrelease mode has been enabled by running  [2.x.102] , andwith the generation of graphical output switched off for the reasons stated inthe program comments above.( [2.x.103] The biggest 2d computations I did had roughly 7.1million unknowns, and were done on 32 processes. It took about 40 minutes.Not surprisingly, the limiting factor for how far one can go is how much memoryone has, since every process has to hold the entire mesh and DoFHandler objects,although matrices and vectors are split up. For the 7.1M computation, the memoryconsumption was about 600 bytes per unknown, which is not bad, but one has toconsider that this is for every unknown, whether we store the matrix and vectorentries locally or not.
* 

* 
* Here is some output generated in the 12th cycle of the program, i.e. with roughly300,000 unknowns:
*  [2.x.104] 
* As one would hope for, the x- (left) and y-displacements (right) shown hereclosely match what we already saw in  [2.x.105] . As shownthere and in  [2.x.106] , we could as well have produced avector plot of the displacement field, rather than plotting it as twoseparate scalar fields. What may be more interesting,though, is to look at the mesh and partition at this step:
*  [2.x.107] 
* Again, the mesh (left) shows the same refinement pattern as seenpreviously. The right panel shows the partitioning of the domain across the 8processes, each indicated by a different color. The picture shows that thesubdomains are smaller where mesh cells are small, a fact that needs to beexpected given that the partitioning algorithm tries to equilibrate the numberof cells in each subdomain; this equilibration is also easily identified inthe output shown above, where the number of degrees per subdomain is roughlythe same.
* 

* 
* It is worth noting that if we ran the same program with a different number ofprocesses, that we would likely get slightly different output: a differentmesh, different number of unknowns and iterations to convergence. The reasonfor this is that while the matrix and right hand side are the same independentof the number of processes used, the preconditioner is not: it performs anILU(0) on the chunk of the matrix of  [2.x.108] each processor separately [2.x.109] . Thus,it's effectiveness as a preconditioner diminishes as the number of processesincreases, which makes the number of iterations increase. Since a differentpreconditioner leads to slight changes in the computed solution, this willthen lead to slightly different mesh cells tagged for refinement, and largerdifferences in subsequent steps. The solution will always look very similar,though.
* 

* 
* Finally, here are some results for a 3d simulation. You can repeat these bychanging
* [1.x.126]
* to
* [1.x.127]
* in the main function. If you then run the program in parallel,you get something similar to this (this is for a job with 16 processes):
* [1.x.128]
* 
* 

* 
* The last step, going up to 1.5 million unknowns, takes about 55 minutes with16 processes on 8 dual-processor machines (of the kind available in 2003). Thegraphical output generated bythis job is rather large (cycle 5 already prints around 82 MB of data), sowe contend ourselves with showing output from cycle 4:
*  [2.x.110] 
* 

* 
* The left picture shows the partitioning of the cube into 16 processes, whereasthe right one shows the x-displacement along two cutplanes through the cube.
* 

* 
* [1.x.129][1.x.130][1.x.131]
* 

* The program keeps a complete copy of the Triangulation and DoFHandler objectson every processor. It also creates complete copies of the solution vector,and it creates output on only one processor. All of this is obviouslythe bottleneck as far as parallelization is concerned.
* Internally, within deal.II, parallelizing the datastructures used in hierarchic and unstructured triangulations is a hardproblem, and it took us a few more years to make this happen. The  [2.x.111] tutorial program and the  [2.x.112]  documentation module talk about howto do these steps and what it takes from an application perspective. Anobvious extension of the current program would be to use this functionality tocompletely distribute computations to many more processors than used here.
* 

* [1.x.132][1.x.133] [2.x.113] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-18_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34][1.x.35][1.x.36][1.x.37]
* [1.x.38][1.x.39][1.x.40]
* 

* 
* This tutorial program is another one in the series on the elasticity problemthat we have already started with  [2.x.2]  and  [2.x.3] . It extends it into twodifferent directions: first, it solves the quasistatic but time dependentelasticity problem for large deformations with a Lagrangian mesh movementapproach. Secondly, it shows some more techniques for solving such problemsusing %parallel processing with PETSc's linear algebra. In addition to this,we show how to work around one of the two major bottlenecks of  [2.x.4] , namelythat we generated graphical output from only one process, and that this scaledvery badly with larger numbers of processes and on large problems. (The otherbottleneck, namely that every processor has to hold the entire mesh andDoFHandler, is addressed in  [2.x.5] .) Finally, agood number of assorted improvements and techniques are demonstrated that havenot been shown yet in previous programs.
* As before in  [2.x.6] , the program runs just as fine on a single sequentialmachine as long as you have PETSc installed. Information on how to telldeal.II about a PETSc installation on your system can be found in the deal.IIREADME file, which is linked to from the [1.x.41]in your installation of deal.II, or on [1.x.42].
* 

* [1.x.43][1.x.44]
* 

* [1.x.45][1.x.46]
* 

* In general, time-dependent small elastic deformations are described by theelastic wave equation[1.x.47]where  [2.x.7]  is the deformation of the body,  [2.x.8] and  [2.x.9]  the density and attenuation coefficient, and  [2.x.10]  external forces.In addition, initial conditions[1.x.48]and Dirichlet (displacement) or Neumann (traction) boundary conditions needto be specified for a unique solution:[1.x.49]
* In above formulation,  [2.x.11]  is the symmetric gradient of the displacement, also called the [2.x.12] strain [2.x.13] .  [2.x.14]  is a tensor of rank 4, called the  [2.x.15] stress-strain  tensor [2.x.16]  (the inverse of the [1.x.50])that contains knowledge of the elastic strength of the material; itssymmetry properties make sure that it maps symmetric tensors of rank 2(&ldquo;matrices&rdquo; of dimension  [2.x.17] , where  [2.x.18]  is the spatial dimensionality) ontosymmetric tensors of the same rank. We will comment on the roles of the strainand stress tensors more below. For the moment it suffices to say that weinterpret the term  [2.x.19]  as the vector withcomponents  [2.x.20] ,where summation over indices  [2.x.21]  is implied.
* The quasistatic limit of this equation is motivated as follows: each smallperturbation of the body, for example by changes in boundary condition or theforcing function, will result in a corresponding change in the configurationof the body. In general, this will be in the form of waves radiating away fromthe location of the disturbance. Due to the presence of the damping term,these waves will be attenuated on a time scale of, say,  [2.x.22] . Now, assumethat all changes in external forcing happen on times scales that aremuch larger than  [2.x.23] . In that case, the dynamic nature of the change isunimportant: we can consider the body to always be in static equilibrium,i.e. we can assume that at all times the body satisfies[1.x.51]
* Note that the differential equation does not contain any time derivatives anymore
* 
*  -  all time dependence is introduced through boundary conditions and apossibly time-varying force function  [2.x.24] . The changes inconfiguration can therefore be considered as being stationaryinstantaneously. An alternative view of this is that  [2.x.25]  is not really a timevariable, but only a time-like parameter that governs the evolution of theproblem.
* While these equations are sufficient to describe small deformations, computinglarge deformations is a little more complicated and, in general, leadsto nonlinear equations such as those treated in  [2.x.26] . In thefollowing, let us consider some of the tools one would employ whensimulating problems in which the deformation becomes [1.x.52].
*  [2.x.27]  The model we will consider below is not founded on anything thatwould be mathematically sound: we will consider a model in which weproduce a small deformation, deform the physical coordinates of thebody by this deformation, and then consider the next loading stepagain as a linear problem. This isn't consistent, since the assumptionof linearity implies that deformations are infinitesimal and so movingaround the vertices of our mesh by a finite amount before solving thenext linear problem is an inconsistent approach. We should thereforenote that it is not surprising that the equations discussed belowcan't be found in the literature: [1.x.53] On the other hand, the implementationtechniques we consider are very much what one would need to use whenimplementing a [1.x.54] model, as we will see in  [2.x.28] .
* 

* To come back to defining our "artificial" model, let us firstintroduce a tensorial stress variable  [2.x.29] , and write the differentialequations in terms of the stress:[1.x.55]
* Note that these equations are posed on a domain  [2.x.30]  thatchanges with time, with the boundary moving according to thedisplacements  [2.x.31]  of the points on the boundary. Tocomplete this system, we have to specify the incremental relationship betweenthe stress and the strain, as follows:[1.x.56][1.x.57]where a dot indicates a time derivative. Both the stress  [2.x.32]  and thestrain  [2.x.33]  are symmetric tensors of rank 2.
* 

* [1.x.58][1.x.59]
* 

* Numerically, this system is solved as follows: first, we discretizethe time component using a backward Euler scheme. This leads to adiscrete equilibrium of force at time step  [2.x.34] :[1.x.60]where[1.x.61]and  [2.x.35]  the incremental displacement for time step [2.x.36] . In addition, we have to specify initial data  [2.x.37] .This way, if we want to solve for the displacement increment, wehave to solve the following system:
* [1.x.62]
* The weak form of this set of equations, which as usual is the basis for thefinite element formulation, reads as follows: find  [2.x.38] such that[1.x.63]
* [1.x.64]
* Using that  [2.x.39] ,these equations can be simplified to
* [1.x.65]
* 
* We note that, for simplicity, in the program we will always assume that thereare no boundary forces, i.e.  [2.x.40] , and that the deformation of thebody is driven by body forces  [2.x.41]  and prescribed boundary displacements [2.x.42]  alone. It is also worth noting that when integrating by parts, wewould get terms of the form  [2.x.43] , but that we replace them with the term involving thesymmetric gradient  [2.x.44]  instead of  [2.x.45] . Due tothe symmetry of  [2.x.46] , the two terms are mathematically equivalent, butthe symmetric version avoids the potential for round-off errors makingthe resulting matrix slightly non-symmetric.
* The system at time step  [2.x.47] , to be solved on the old domain [2.x.48] , has exactly the form of a stationary elasticproblem, and is therefore similar to what we have already implementedin previous example programs. We will therefore not comment on thespace discretization beyond saying that we again use lowest ordercontinuous finite elements.
* There are differences, however: [2.x.49]    [2.x.50]  We have to move (update) the mesh after each time step, in order to be  able to solve the next time step on a new domain;
*    [2.x.51]  We need to know  [2.x.52]  to compute the next incremental  displacement, i.e. we need to compute it at the end of the time step  to make sure it is available for the next time step. Essentially,  the stress variable is our window to the history of deformation of  the body. [2.x.53] These two operations are done in the functions  [2.x.54]  and [2.x.55]  in the program. While movingthe mesh is only a technicality, updating the stress is a little morecomplicated and will be discussed in the next section.
* 

* [1.x.66][1.x.67]
* 

* As indicated above, we need to have the stress variable  [2.x.56]  availablewhen computing time step  [2.x.57] , and we can compute it using[1.x.68][1.x.69]There are, despite the apparent simplicity of this equation, two questionsthat we need to discuss. The first concerns the way we store  [2.x.58] : evenif we compute the incremental updates  [2.x.59]  using lowest-orderfinite elements, then its symmetric gradient  [2.x.60]  isin general still a function that is not easy to describe. In particular, it isnot a piecewise constant function, and on general meshes (with cells that arenot rectangles %parallel to the coordinate axes) or with non-constantstress-strain tensors  [2.x.61]  it is not even a bi- or trilinear function. Thus, itis a priori not clear how to store  [2.x.62]  in a computer program.
* To decide this, we have to see where it is used. The only place where werequire the stress is in the term [2.x.63] . In practice, we ofcourse replace this term by numerical quadrature:[1.x.70]where  [2.x.64]  are the quadrature weights and  [2.x.65]  the quadrature points oncell  [2.x.66] . This should make clear that what we really need is not the stress [2.x.67]  in itself, but only the values of the stress in the quadraturepoints on all cells. This, however, is a simpler task: we only have to providea data structure that is able to hold one symmetric tensor of rank 2 for eachquadrature point on all cells (or, since we compute in parallel, allquadrature points of all cells that the present MPI process &ldquo;owns&rdquo;). At theend of each time step we then only have to evaluate  [2.x.68] , multiply it by the stress-strain tensor  [2.x.69] , and use theresult to update the stress  [2.x.70]  at quadrature point  [2.x.71] .
* The second complication is not visible in our notation as chosen above. It isdue to the fact that we compute  [2.x.72]  on the domain  [2.x.73] ,and then use this displacement increment to both update the stress as well asmove the mesh nodes around to get to  [2.x.74]  on which the next incrementis computed. What we have to make sure, in this context, is that moving themesh does not only involve moving around the nodes, but also makingcorresponding changes to the stress variable: the updated stress is a variablethat is defined with respect to the coordinate system of the material in theold domain, and has to be transferred to the new domain. The reason for thiscan be understood as follows: locally, the incremental deformation  [2.x.75]  can be decomposed into three parts, a linear translation (the constant partof the displacement increment field in the neighborhood of a point), adilationalcomponent (that part of the gradient of the displacement field that has anonzero divergence), and a rotation. A linear translation of the material doesnot affect the stresses that are frozen into it
* 
*  -  the stress values aresimply translated along. The dilational or compressional change produces acorresponding stress update. However, the rotational component does notnecessarily induce a nonzero stress update (think, in 2d, for example of thesituation where  [2.x.76] , with which  [2.x.77] ). Nevertheless, if the material was prestressed in a certaindirection, then this direction will be rotated along with the material.  Tothis end, we have to define a rotation matrix  [2.x.78]  thatdescribes, in each point the rotation due to the displacement increments. Itis not hard to see that the actual dependence of  [2.x.79]  on  [2.x.80]  canonly be through the curl of the displacement, rather than the displacementitself or its full gradient (as mentioned above, the constant components ofthe increment describe translations, its divergence the dilational modes, andthe curl the rotational modes). Since the exact form of  [2.x.81]  is cumbersome, weonly state it in the program code, and note that the correct updating formulafor the stress variable is then[1.x.71][1.x.72]
* Both stress update and rotation are implemented in the function [2.x.82]  of the example program.
* 

* [1.x.73][1.x.74]
* 

* In  [2.x.83] , the main bottleneck for %parallel computations as far as run timeis concernedwas that only the first processor generated output for the entire domain.Since generating graphical output is expensive, this did not scale well whenlarger numbers of processors were involved. We will address this here. (For adefinition of what it means for a program to "scale", see [2.x.84]  "this glossary entry".)
* Basically, what we need to do is let every processgenerate graphical output for that subset of cells that it owns, write theminto separate files and have a way to display all files for a certain timestepat the same time. This way the code produces one  [2.x.85]  file per process pertime step. The two common VTK file viewers ParaView and VisIt both supportopening more than one  [2.x.86]  file at once. To simplify the process of pickingthe correct files and allow moving around in time, both support record filesthat reference all files for a given timestep. Sadly, the record files have adifferent format between VisIt and Paraview, so we write out both formats.
* The code will generate the files  [2.x.87] ,where  [2.x.88]  is the timestep number (starting from 1) and [2.x.89]  is the process rank (starting from0). These files contain the locally owned cells for the timestep andprocessor. The files  [2.x.90]  is the visit recordfor timestep  [2.x.91]  isthe same for ParaView. (More recent versions of VisIt can actually read [2.x.92]  files as well, but it doesn't hurt to output bothkinds of record files.) Finally, the file [2.x.93]  is a special record only supported by ParaView that referencesall time steps. So in ParaView, only solution.pvd needs to be opened, whileone needs to select the group of all .visit files in VisIt for the sameeffect.
* 

* [1.x.75][1.x.76]
* 

* In  [2.x.94] , we used a regular triangulation that was simply replicated onevery processor, and a corresponding DoFHandler. Both had no idea that theywere used in a %parallel context
* 
*  -  they just existed in their entiretyon every processor, and we argued that this was eventually going to be amajor memory bottleneck.
* We do not address this issue here (we will do so in  [2.x.95] ) but makethe situation slightly more automated. In  [2.x.96] , we created the triangulationand then manually "partitioned" it, i.e., we assigned [2.x.97]  "subdomain ids" to every cell that indicated which [2.x.98]  "MPI process" "owned" the cell. Here, we use a class [2.x.99]  that at least does this part automatically:whenever you create or refine such a triangulation, it automaticallypartitions itself among all involved processes (which it knows about becauseyou have to tell it about the  [2.x.100]  "MPI communicator"that connects these processes upon construction of the triangulation).Otherwise, the  [2.x.101]  looks, for all practicalpurposes, like a regular Triangulation object.
* The convenience of using this class does not only result from being ableto avoid the manual call to  [2.x.102]  Rather, the DoFHandlerclass now also knows that you want to use it in a parallel context, andby default automatically enumerates degrees of freedom in such a waythat all DoFs owned by process zero come before all DoFs owned by process 1,etc. In other words, you can also avoid the call to [2.x.103] 
* There are other benefits. For example, because the triangulation knows thatit lives in a %parallel universe, it also knows that it "owns" certaincells (namely, those whose subdomain id equals its MPI rank; previously,the triangulation only stored these subdomain ids, but had no way tomake sense of them). Consequently, in the assembly function, you cantest whether a cell is "locally owned" (i.e., owned by the currentprocess, see  [2.x.104] ) when you loop over all cellsusing the syntax
* [1.x.77]
* This knowledge extends to the DoFHandler object built on such triangulations,which can then identify which degrees of freedom are locally owned(see  [2.x.105] ) via calls such as [2.x.106]  and [2.x.107]  Finally, the DataOut classalso knows how to deal with such triangulations and will simply skipgenerating graphical output on cells not locally owned.
* Of course, as has been noted numerous times in the discussion in  [2.x.108] ,keeping the entire triangulation on every process will not scale: largeproblems may simply not fit into each process's memory any more, even ifwe have sufficiently many processes around to solve them in a reasonabletime. In such cases, the  [2.x.109]  is no longera reasonable basis for computations and we will show in  [2.x.110]  how the [2.x.111]  class can be used to work aroundthis, namely by letting each process store only a [1.x.78] of thetriangulation.
* 

* [1.x.79][1.x.80]
* 

* The overall structure of the program can be inferred from the  [2.x.112] function that first calls  [2.x.113]  for the first timestep, and then  [2.x.114]  on all subsequent time steps. Thedifference between these functions is only that in the first time step westart on a coarse mesh, solve on it, refine the mesh adaptively, and thenstart again with a clean state on that new mesh. This procedure gives us abetter starting mesh, although we should of course keep adapting the mesh asiterations proceed
* 
*  -  this isn't done in this program, but commented on below.
* The common part of the two functions treating time steps is the followingsequence of operations on the present mesh: [2.x.115]  [2.x.116]   [2.x.117] ]:  This first function is also the most interesting one. It assembles the  linear system corresponding to the discretized version of equation  [1.x.81]. This leads to a system matrix  [2.x.118]  built up of local contributions on each cell  [2.x.119]  with entries  [1.x.82]  In practice,  [2.x.120]  is computed using numerical quadrature according to the  formula  [1.x.83]  with quadrature points  [2.x.121]  and weights  [2.x.122] . We have built these  contributions before, in  [2.x.123]  and  [2.x.124] , but in both of these cases we  have done so rather clumsily by using knowledge of how the rank-4 tensor  [2.x.125]   is composed, and considering individual elements of the strain tensors   [2.x.126] . This is not really  convenient, in particular if we want to consider more complicated elasticity  models than the isotropic case for which  [2.x.127]  had the convenient form   [2.x.128] . While we in fact do not use a more complicated  form than this in the present program, we nevertheless want to write it in a  way that would easily allow for this. It is then natural to introduce  classes that represent symmetric tensors of rank 2 (for the strains and  stresses) and 4 (for the stress-strain tensor  [2.x.129] ). Fortunately, deal.II  provides these: the  [2.x.130]  class template  provides a full-fledged implementation of such tensors of rank  [2.x.131]   (which needs to be an even number) and dimension  [2.x.132] .
*   What we then need is two things: a way to create the stress-strain rank-4  tensor  [2.x.133]  as well as to create a symmetric tensor of rank 2 (the strain  tensor) from the gradients of a shape function  [2.x.134]  at a quadrature  point  [2.x.135]  on a given cell. At the top of the implementation of this  example program, you will find such functions. The first one,   [2.x.136] , takes two arguments corresponding to  the Lam&eacute; constants  [2.x.137]  and  [2.x.138]  and returns the stress-strain tensor  for the isotropic case corresponding to these constants (in the program, we  will choose constants corresponding to steel); it would be simple to replace  this function by one that computes this tensor for the anisotropic case, or  taking into account crystal symmetries, for example. The second one,   [2.x.139]  and indices   [2.x.140]  and  [2.x.141]  and returns the symmetric gradient, i.e. the strain,  corresponding to shape function  [2.x.142] , evaluated on the cell  on which the  [2.x.143]  object was last reinitialized.
*   Given this, the innermost loop of  [2.x.144]  computes the  local contributions to the matrix in the following elegant way (the variable   [2.x.145] , corresponding to the tensor  [2.x.146] , has  previously been initialized with the result of the first function above): 
* [1.x.84]
*   It is worth noting the expressive power of this piece of code, and to  compare it with the complications we had to go through in previous examples  for the elasticity problem. (To be fair, the SymmetricTensor class  template did not exist when these previous examples were written.) For  simplicity,  [2.x.147]  provides for the (double summation) product  between symmetric tensors of even rank here.
*   Assembling the local contributions  [1.x.85]
*   to the right hand side of [1.x.86] is equally  straightforward (note that we do not consider any boundary tractions  [2.x.148]  here). Remember that we only had to store the old stress in the  quadrature points of cells. In the program, we will provide a variable   [2.x.149]  that allows to access the stress   [2.x.150]  in each quadrature point. With this the code for the right  hand side looks as this, again rather elegant: 
* [1.x.87]
*   Note that in the multiplication  [2.x.151] , we have made use of the fact that for the chosen finite element, only  one vector component (namely  [2.x.152] ) of  [2.x.153]  is  nonzero, and that we therefore also have to consider only one component of   [2.x.154] .
*   This essentially concludes the new material we present in this function. It  later has to deal with boundary conditions as well as hanging node  constraints, but this parallels what we had to do previously in other  programs already.
*  [2.x.155]   [2.x.156] ]:  Unlike the previous one, this function is not really interesting, since it  does what similar functions have done in all previous tutorial programs
* 
*  -   solving the linear system using the CG method, using an incomplete LU  decomposition as a preconditioner (in the %parallel case, it uses an ILU of  each processor's block separately). It is virtually unchanged  from  [2.x.157] .
*  [2.x.158]   [2.x.159]  [via   [2.x.160] ]: Based on the displacement field  [2.x.161]  computed before, we update the stress values in all quadrature points  according to [1.x.88] and [1.x.89],  including the rotation of the coordinate system.
*  [2.x.162]   [2.x.163] : Given the solution computed before, in this  function we deform the mesh by moving each vertex by the displacement vector  field evaluated at this particular vertex.
*  [2.x.164]   [2.x.165] : This function simply outputs the solution  based on what we have said above, i.e. every processor computes output only  for its own portion of the domain. In addition to the solution, we also compute the norm of  the stress averaged over all the quadrature points on each cell. [2.x.166] 
* With this general structure of the code, we only have to define what case wewant to solve. For the present program, we have chosen to simulate thequasistatic deformation of a vertical cylinder for which the bottom boundaryis fixed and the top boundary is pushed down at a prescribed verticalvelocity. However, the horizontal velocity of the top boundary is leftunspecified
* 
*  -  one can imagine this situation as a well-greased plate pushingfrom the top onto the cylinder, the points on the top boundary of the cylinderbeing allowed to slide horizontally along the surface of the plate, but forcedto move downward by the plate. The inner and outer boundaries of the cylinderare free and not subject to any prescribed deflection or traction. Inaddition, gravity acts on the body.
* The program text will reveal more about how to implement this situation, andthe results section will show what displacement pattern comes out of thissimulation.
* 

*  [1.x.90] [1.x.91]
*  First the usual list of header files that have already been used in previous example programs:
* 

* 
* [1.x.92]
* 
*  And here the only three new things among the header files: an include file in which symmetric tensors of rank 2 and 4 are implemented, as introduced in the introduction:
* 

* 
* [1.x.93]
* 
*  And lastly a header that contains some functions that will help us compute rotaton matrices of the local coordinate systems at specific points in the domain.
* 

* 
* [1.x.94]
* 
*  This is then simply C++ again:
* 

* 
* [1.x.95]
* 
*  The last step is as in all previous programs:
* 

* 
* [1.x.96]
* 
*   [1.x.97]  [1.x.98]
* 

* 
*  As was mentioned in the introduction, we have to store the old stress in quadrature point so that we can compute the residual forces at this point during the next time step. This alone would not warrant a structure with only one member, but in more complicated applications, we would have to store more information in quadrature points as well, such as the history variables of plasticity, etc. In essence, we have to store everything that affects the present state of the material here, which in plasticity is determined by the deformation history variables.   
*   We will not give this class any meaningful functionality beyond being able to store data, i.e. there are no constructors, destructors, or other member functions. In such cases of `dumb' classes, we usually opt to declare them as  [2.x.167] , to indicate that they are closer to C-style structures than C++-style classes.
* 

* 
* [1.x.99]
* 
*   [1.x.100]  [1.x.101]
* 

* 
*  Next, we define the linear relationship between the stress and the strain in elasticity. It is given by a tensor of rank 4 that is usually written in the form  [2.x.168] . This tensor maps symmetric tensor of rank 2 to symmetric tensors of rank 2. A function implementing its creation for given values of the Lam&eacute; constants  [2.x.169]  and  [2.x.170]  is straightforward:
* 

* 
* [1.x.102]
* 
*  With this function, we will define a static member variable of the main class below that will be used throughout the program as the stress-strain tensor. Note that in more elaborate programs, this will probably be a member variable of some class instead, or a function that returns the stress-strain relationship depending on other input. For example in damage theory models, the Lam&eacute; constants are considered a function of the prior stress/strain history of a point. Conversely, in plasticity the form of the stress-strain tensor is modified if the material has reached the yield stress in a certain point, and possibly also depending on its prior history.   
*   In the present program, however, we assume that the material is completely elastic and linear, and a constant stress-strain tensor is sufficient for our present purposes.
* 

* 
*  
*  
* 

* 
*   [1.x.103]  [1.x.104]
* 

* 
*  Before the rest of the program, here are a few functions that we need as tools. These are small functions that are called in inner loops, so we mark them as  [2.x.171] .   
*   The first one computes the symmetric strain tensor for shape function  [2.x.172]  by forming the symmetric gradient of this shape function. We need that when we want to form the matrix, for example.   
*   We should note that in previous examples where we have treated vector-valued problems, we have always asked the finite element object in which of the vector component the shape function is actually non-zero, and thereby avoided to compute any terms that we could prove were zero anyway. For this, we used the  [2.x.173]  function that returns in which component a shape function was zero, and also that the  [2.x.174]  and  [2.x.175]  functions only returned the value and gradient of the single non-zero component of a shape function if this is a vector-valued element.   
*   This was an optimization, and if it isn't terribly time critical, we can get away with a simpler technique: just ask the  [2.x.176]  for the value or gradient of a given component of a given shape function at a given quadrature point. This is what the  [2.x.177]  call does: return the full gradient of the  [2.x.178] th component of shape function  [2.x.179]  at quadrature point  [2.x.180] . If a certain component of a certain shape function is always zero, then this will simply always return zero.   
*   As mentioned, using  [2.x.181]  instead of the combination of  [2.x.182]  and  [2.x.183]  may be less efficient, but its implementation is optimized for such cases and shouldn't be a big slowdown. We demonstrate the technique here since it is so much simpler and straightforward.
* 

* 
* [1.x.105]
* 
*  Declare a temporary that will hold the return value:
* 

* 
* [1.x.106]
* 
*  First, fill diagonal terms which are simply the derivatives in direction  [2.x.184]  component of the vector-valued shape function:
* 

* 
* [1.x.107]
* 
*  Then fill the rest of the strain tensor. Note that since the tensor is symmetric, we only have to compute one half (here: the upper right corner) of the off-diagonal elements, and the implementation of the  [2.x.185]  class makes sure that at least to the outside the symmetric entries are also filled (in practice, the class of course stores only one copy). Here, we have picked the upper right half of the tensor, but the lower left one would have been just as good:
* 

* 
* [1.x.108]
* 
*  The second function does something very similar (and therefore is given the same name): compute the symmetric strain tensor from the gradient of a vector-valued field. If you already have a solution field, the  [2.x.186]  function allows you to extract the gradients of each component of your solution field at a quadrature point. It returns this as a vector of rank-1 tensors: one rank-1 tensor (gradient) per vector component of the solution. From this we have to reconstruct the (symmetric) strain tensor by transforming the data storage format and symmetrization. We do this in the same way as above, i.e. we avoid a few computations by filling first the diagonal and then only one half of the symmetric tensor (the  [2.x.187]  class makes sure that it is sufficient to write only one of the two symmetric components).   
*   Before we do this, though, we make sure that the input has the kind of structure we expect: that is that there are  [2.x.188]  vector components, i.e. one displacement component for each coordinate direction. We test this with the  [2.x.189]  macro that will simply abort our program if the condition is not met.
* 

* 
* [1.x.109]
* 
*  Finally, below we will need a function that computes the rotation matrix induced by a displacement at a given point. In fact, of course, the displacement at a single point only has a direction and a magnitude, it is the change in direction and magnitude that induces rotations. In effect, the rotation matrix can be computed from the gradients of a displacement, or, more specifically, from the curl.   
*   The formulas by which the rotation matrices are determined are a little awkward, especially in 3d. For 2d, there is a simpler way, so we implement this function twice, once for 2d and once for 3d, so that we can compile and use the program in both space dimensions if so desired
* 
*  -  after all, deal.II is all about dimension independent programming and reuse of algorithm thoroughly tested with cheap computations in 2d, for the more expensive computations in 3d. Here is one case, where we have to implement different algorithms for 2d and 3d, but then can write the rest of the program in a way that is independent of the space dimension.   
*   So, without further ado to the 2d implementation:
* 

* 
* [1.x.110]
* 
*  First, compute the curl of the velocity field from the gradients. Note that we are in 2d, so the rotation is a scalar:
* 

* 
* [1.x.111]
* 
*  From this, compute the angle of rotation:
* 

* 
* [1.x.112]
* 
*  And from this, build the antisymmetric rotation matrix. We want this rotation matrix to represent the rotation of the local coordinate system with respect to the global Cartesian basis, to we construct it with a negative angle. The rotation matrix therefore represents the rotation required to move from the local to the global coordinate system.
* 

* 
* [1.x.113]
* 
*  The 3d case is a little more contrived:
* 

* 
* [1.x.114]
* 
*  Again first compute the curl of the velocity field. This time, it is a real vector:
* 

* 
* [1.x.115]
* 
*  From this vector, using its magnitude, compute the tangent of the angle of rotation, and from it the actual angle of rotation with respect to the Cartesian basis:
* 

* 
* [1.x.116]
* 
*  Now, here's one problem: if the angle of rotation is too small, that means that there is no rotation going on (for example a translational motion). In that case, the rotation matrix is the identity matrix.     
*   The reason why we stress that is that in this case we have that  [2.x.190] . Further down, we need to divide by that number in the computation of the axis of rotation, and we would get into trouble when dividing doing so. Therefore, let's shortcut this and simply return the identity matrix if the angle of rotation is really small:
* 

* 
* [1.x.117]
* 
*  Otherwise compute the real rotation matrix. For this, again we rely on a predefined function to compute the rotation matrix of the local coordinate system.
* 

* 
* [1.x.118]
* 
*   [1.x.119]  [1.x.120]
* 

* 
*  This is the main class of the program. Since the namespace already indicates what problem we are solving, let's call it by what it does: it directs the flow of the program, i.e. it is the toplevel driver.   
*   The member variables of this class are essentially as before, i.e. it has to have a triangulation, a DoF handler and associated objects such as constraints, variables that describe the linear system, etc. There are a good number of more member functions now, which we will explain below.   
*   The external interface of the class, however, is unchanged: it has a public constructor and destructor, and it has a  [2.x.191]  function that initiated all the work.
* 

* 
* [1.x.121]
* 
*  The private interface is more extensive than in  [2.x.192] . First, we obviously need functions that create the initial mesh, set up the variables that describe the linear system on the present mesh (i.e. matrices and vectors), and then functions that actually assemble the system, direct what has to be solved in each time step, a function that solves the linear system that arises in each timestep (and returns the number of iterations it took), and finally output the solution vector on the correct mesh:
* 

* 
* [1.x.122]
* 
*  All, except for the first two, of these functions are called in each timestep. Since the first time step is a little special, we have separate functions that describe what has to happen in a timestep: one for the first, and one for all following timesteps:
* 

* 
* [1.x.123]
* 
*  Then we need a whole bunch of functions that do various things. The first one refines the initial grid: we start on the coarse grid with a pristine state, solve the problem, then look at it and refine the mesh accordingly, and start the same process over again, again with a pristine state. Thus, refining the initial mesh is somewhat simpler than refining a grid between two successive time steps, since it does not involve transferring data from the old to the new triangulation, in particular the history data that is stored in each quadrature point.
* 

* 
* [1.x.124]
* 
*  At the end of each time step, we want to move the mesh vertices around according to the incremental displacement computed in this time step. This is the function in which this is done:
* 

* 
* [1.x.125]
* 
*  Next are two functions that handle the history variables stored in each quadrature point. The first one is called before the first timestep to set up a pristine state for the history variables. It only works on those quadrature points on cells that belong to the present processor:
* 

* 
* [1.x.126]
* 
*  The second one updates the history variables at the end of each timestep:
* 

* 
* [1.x.127]
* 
*  This is the new shared Triangulation:
* 

* 
* [1.x.128]
* 
*  One difference of this program is that we declare the quadrature formula in the class declaration. The reason is that in all the other programs, it didn't do much harm if we had used different quadrature formulas when computing the matrix and the right hand side, for example. However, in the present case it does: we store information in the quadrature points, so we have to make sure all parts of the program agree on where they are and how many there are on each cell. Thus, let us first declare the quadrature formula that will be used throughout...
* 

* 
* [1.x.129]
* 
*  ... and then also have a vector of history objects, one per quadrature point on those cells for which we are responsible (i.e. we don't store history data for quadrature points on cells that are owned by other processors). Note that, instead of storing and managing this data ourself, we could use the CellDataStorage class like is done in  [2.x.193] . However, for the purpose of demonstration, in this case we manage the storage manually.
* 

* 
* [1.x.130]
* 
*  The way this object is accessed is through a  [2.x.194]  that each cell, face, or edge holds: it is a  [2.x.195]  pointer that can be used by application programs to associate arbitrary data to cells, faces, or edges. What the program actually does with this data is within its own responsibility, the library just allocates some space for these pointers, and application programs can set and read the pointers for each of these objects.
* 

* 
*  
*   Further: we need the objects of linear systems to be solved, i.e. matrix, right hand side vector, and the solution vector. Since we anticipate solving big problems, we use the same types as in  [2.x.196] , i.e. distributed %parallel matrices and vectors built on top of the PETSc library. Conveniently, they can also be used when running on only a single machine, in which case this machine happens to be the only one in our %parallel universe.     
*   However, as a difference to  [2.x.197] , we do not store the solution vector
* 
*  -  which here is the incremental displacements computed in each time step
* 
*  -  in a distributed fashion. I.e., of course it must be a distributed vector when computing it, but immediately after that we make sure each processor has a complete copy. The reason is that we had already seen in  [2.x.198]  that many functions needed a complete copy. While it is not hard to get it, this requires communication on the network, and is thus slow. In addition, these were repeatedly the same operations, which is certainly undesirable unless the gains of not always having to store the entire vector outweighs it. When writing this program, it turned out that we need a complete copy of the solution in so many places that it did not seem worthwhile to only get it when necessary. Instead, we opted to obtain the complete copy once and for all, and instead get rid of the distributed copy immediately. Thus, note that the declaration of  [2.x.199]  does not denote a distribute vector as would be indicated by the middle namespace  [2.x.200] :
* 

* 
* [1.x.131]
* 
*  The next block of variables is then related to the time dependent nature of the problem: they denote the length of the time interval which we want to simulate, the present time and number of time step, and length of present timestep:
* 

* 
* [1.x.132]
* 
*  Then a few variables that have to do with %parallel processing: first, a variable denoting the MPI communicator we use, and then two numbers telling us how many participating processors there are, and where in this world we are. Finally, a stream object that makes sure only one processor is actually generating output to the console. This is all the same as in  [2.x.201] :
* 

* 
* [1.x.133]
* 
*  We are storing the locally owned and the locally relevant indices:
* 

* 
* [1.x.134]
* 
*  Finally, we have a static variable that denotes the linear relationship between the stress and strain. Since it is a constant object that does not depend on any input (at least not in this program), we make it a static variable and will initialize it in the same place where we define the constructor of this class:
* 

* 
* [1.x.135]
* 
*   [1.x.136]  [1.x.137]
* 

* 
*  Before we go on to the main functionality of this program, we have to define what forces will act on the body whose deformation we want to study. These may either be body forces or boundary forces. Body forces are generally mediated by one of the four basic physical types of forces: gravity, strong and weak interaction, and electromagnetism. Unless one wants to consider subatomic objects (for which quasistatic deformation is irrelevant and an inappropriate description anyway), only gravity and electromagnetic forces need to be considered. Let us, for simplicity assume that our body has a certain mass density, but is either non-magnetic and not electrically conducting or that there are no significant electromagnetic fields around. In that case, the body forces are simply  [2.x.202]  is the material density and  [2.x.203]  is a vector in negative z-direction with magnitude 9.81 m/s^2.  Both the density and  [2.x.204]  are defined in the function, and we take as the density 7700 kg/m^3, a value commonly assumed for steel.   
*   To be a little more general and to be able to do computations in 2d as well, we realize that the body force is always a function returning a  [2.x.205]  dimensional vector. We assume that gravity acts along the negative direction of the last, i.e.  [2.x.206] th coordinate. The rest of the implementation of this function should be mostly self-explanatory given similar definitions in previous example programs. Note that the body force is independent of the location; to avoid compiler warnings about unused function arguments, we therefore comment out the name of the first argument of the  [2.x.207]  function:
* 

* 
* [1.x.138]
* 
*   [1.x.139]  [1.x.140]
* 

* 
*  In addition to body forces, movement can be induced by boundary forces and forced boundary displacement. The latter case is equivalent to forces being chosen in such a way that they induce certain displacement.   
*   For quasistatic displacement, typical boundary forces would be pressure on a body, or tangential friction against another body. We chose a somewhat simpler case here: we prescribe a certain movement of (parts of) the boundary, or at least of certain components of the displacement vector. We describe this by another vector-valued function that, for a given point on the boundary, returns the prescribed displacement.   
*   Since we have a time-dependent problem, the displacement increment of the boundary equals the displacement accumulated during the length of the timestep. The class therefore has to know both the present time and the length of the present time step, and can then approximate the incremental displacement as the present velocity times the present timestep.   
*   For the purposes of this program, we choose a simple form of boundary displacement: we displace the top boundary with constant velocity downwards. The rest of the boundary is either going to be fixed (and is then described using an object of type  [2.x.208] ) or free (Neumann-type, in which case nothing special has to be done).  The implementation of the class describing the constant downward motion should then be obvious using the knowledge we gained through all the previous example programs:
* 

* 
* [1.x.141]
* 
*   [1.x.142]  [1.x.143]
* 

* 
*  Now for the implementation of the main class. First, we initialize the stress-strain tensor, which we have declared as a static const variable. We chose Lam&eacute; constants that are appropriate for steel:
* 

* 
* [1.x.144]
* 
*   [1.x.145]  [1.x.146]
* 

* 
*  The next step is the definition of constructors and destructors. There are no surprises here: we choose linear and continuous finite elements for each of the  [2.x.209]  vector components of the solution, and a Gaussian quadrature formula with 2 points in each coordinate direction. The destructor should be obvious:
* 

* 
* [1.x.147]
* 
*  The last of the public functions is the one that directs all the work,  [2.x.210] . It initializes the variables that describe where in time we presently are, then runs the first time step, then loops over all the other time steps. Note that for simplicity we use a fixed time step, whereas a more sophisticated program would of course have to choose it in some more reasonable way adaptively:
* 

* 
* [1.x.148]
* 
*   [1.x.149]  [1.x.150]
* 

* 
*  The next function in the order in which they were declared above is the one that creates the coarse grid from which we start. For this example program, we want to compute the deformation of a cylinder under axial compression. The first step therefore is to generate a mesh for a cylinder of length 3 and with inner and outer radii of 0.8 and 1, respectively. Fortunately, there is a library function for such a mesh.   
*   In a second step, we have to associated boundary conditions with the upper and lower faces of the cylinder. We choose a boundary indicator of 0 for the boundary faces that are characterized by their midpoints having z-coordinates of either 0 (bottom face), an indicator of 1 for z=3 (top face); finally, we use boundary indicator 2 for all faces on the inside of the cylinder shell, and 3 for the outside.
* 

* 
* [1.x.151]
* 
*  Once all this is done, we can refine the mesh once globally:
* 

* 
* [1.x.152]
* 
*  As the final step, we need to set up a clean state of the data that we store in the quadrature points on all cells that are treated on the present processor.
* 

* 
* [1.x.153]
* 
*   [1.x.154]  [1.x.155]
* 

* 
*  The next function is the one that sets up the data structures for a given mesh. This is done in most the same way as in  [2.x.211] : distribute the degrees of freedom, then sort these degrees of freedom in such a way that each processor gets a contiguous chunk of them. Note that subdivisions into chunks for each processor is handled in the functions that create or refine grids, unlike in the previous example program (the point where this happens is mostly a matter of taste; here, we chose to do it when grids are created since in the  [2.x.212]  and  [2.x.213]  functions we want to output the number of cells on each processor at a point where we haven't called the present function yet).
* 

* 
* [1.x.156]
* 
*  The next step is to set up constraints due to hanging nodes. This has been handled many times before:
* 

* 
* [1.x.157]
* 
*  And then we have to set up the matrix. Here we deviate from  [2.x.214] , in which we simply used PETSc's ability to just know about the size of the matrix and later allocate those nonzero elements that are being written to. While this works just fine from a correctness viewpoint, it is not at all efficient: if we don't give PETSc a clue as to which elements are written to, it is (at least at the time of this writing) unbearably slow when we set the elements in the matrix for the first time (i.e. in the first timestep). Later on, when the elements have been allocated, everything is much faster. In experiments we made, the first timestep can be accelerated by almost two orders of magnitude if we instruct PETSc which elements will be used and which are not.     
*   To do so, we first generate the sparsity pattern of the matrix we are going to work with, and make sure that the condensation of hanging node constraints add the necessary additional entries in the sparsity pattern:
* 

* 
* [1.x.158]
* 
*  Note that we have used the  [2.x.215]  class here that was already introduced in  [2.x.216] , rather than the  [2.x.217]  class that we have used in all other cases. The reason for this is that for the latter class to work we have to give an initial upper bound for the number of entries in each row, a task that is traditionally done by  [2.x.218] . However, this function suffers from a serious problem: it has to compute an upper bound to the number of nonzero entries in each row, and this is a rather complicated task, in particular in 3d. In effect, while it is quite accurate in 2d, it often comes up with much too large a number in 3d, and in that case the  [2.x.219]  allocates much too much memory at first, often several 100 MBs. This is later corrected when  [2.x.220]  is called and we realize that we don't need all that much memory, but at time it is already too late: for large problems, the temporary allocation of too much memory can lead to out-of-memory situations.     
*   In order to avoid this, we resort to the  [2.x.221]  class that is slower but does not require any up-front estimate on the number of nonzero entries per row. It therefore only ever allocates as much memory as it needs at any given time, and we can build it even for large 3d problems.     
*   It is also worth noting that due to the specifics of  [2.x.222]  the sparsity pattern we construct is global, i.e. comprises all degrees of freedom whether they will be owned by the processor we are on or another one (in case this program is run in %parallel via MPI). This of course is not optimal
* 
*  -  it limits the size of the problems we can solve, since storing the entire sparsity pattern (even if only for a short time) on each processor does not scale well. However, there are several more places in the program in which we do this, for example we always keep the global triangulation and DoF handler objects around, even if we only work on part of them. At present, deal.II does not have the necessary facilities to completely distribute these objects (a task that, indeed, is very hard to achieve with adaptive meshes, since well-balanced subdivisions of a domain tend to become unbalanced as the mesh is adaptively refined).     
*   With this data structure, we can then go to the PETSc sparse matrix and tell it to preallocate all the entries we will later want to write to:
* 

* 
* [1.x.159]
* 
*  After this point, no further explicit knowledge of the sparsity pattern is required any more and we can let the  [2.x.223]  variable go out of scope without any problem.
* 

* 
*  The last task in this function is then only to reset the right hand side vector as well as the solution vector to its correct size; remember that the solution vector is a local one, unlike the right hand side that is a distributed %parallel one and therefore needs to know the MPI communicator over which it is supposed to transmit messages:
* 

* 
* [1.x.160]
* 
*   [1.x.161]  [1.x.162]
* 

* 
*  Again, assembling the system matrix and right hand side follows the same structure as in many example programs before. In particular, it is mostly equivalent to  [2.x.224] , except for the different right hand side that now only has to take into account internal stresses. In addition, assembling the matrix is made significantly more transparent by using the  [2.x.225]  class: note the elegance of forming the scalar products of symmetric tensors of rank 2 and 4. The implementation is also more general since it is independent of the fact that we may or may not be using an isotropic elasticity tensor.   
*   The first part of the assembly routine is as always:
* 

* 
* [1.x.163]
* 
*  As in  [2.x.226] , we only need to loop over all cells that belong to the present processor:
* 

* 
* [1.x.164]
* 
*  Then loop over all indices i,j and quadrature points and assemble the system matrix contributions from this cell.  Note how we extract the symmetric gradients (strains) of the shape functions at a given quadrature point from the  [2.x.227]  object, and the elegance with which we form the triple contraction  [2.x.228] ; the latter needs to be compared to the clumsy computations needed in  [2.x.229] , both in the introduction as well as in the respective place in the program:
* 

* 
* [1.x.165]
* 
*  Then also assemble the local right hand side contributions. For this, we need to access the prior stress value in this quadrature point. To get it, we use the user pointer of this cell that points into the global array to the quadrature point data corresponding to the first quadrature point of the present cell, and then add an offset corresponding to the index of the quadrature point we presently consider:
* 

* 
* [1.x.166]
* 
*  In addition, we need the values of the external body forces at the quadrature points on this cell:
* 

* 
* [1.x.167]
* 
*  Then we can loop over all degrees of freedom on this cell and compute local contributions to the right hand side:
* 

* 
* [1.x.168]
* 
*  Now that we have the local contributions to the linear system, we need to transfer it into the global objects. This is done exactly as in  [2.x.230] :
* 

* 
* [1.x.169]
* 
*  Now compress the vector and the system matrix:
* 

* 
* [1.x.170]
* 
*  The last step is to again fix up boundary values, just as we already did in previous programs. A slight complication is that the  [2.x.231]  function wants to have a solution vector compatible with the matrix and right hand side (i.e. here a distributed %parallel vector, rather than the sequential vector we use in this program) in order to preset the entries of the solution vector with the correct boundary values. We provide such a compatible vector in the form of a temporary vector which we then copy into the sequential one.
* 

* 
*  We make up for this complication by showing how boundary values can be used flexibly: following the way we create the triangulation, there are three distinct boundary indicators used to describe the domain, corresponding to the bottom and top faces, as well as the inner/outer surfaces. We would like to impose boundary conditions of the following type: The inner and outer cylinder surfaces are free of external forces, a fact that corresponds to natural (Neumann-type) boundary conditions for which we don't have to do anything. At the bottom, we want no movement at all, corresponding to the cylinder being clamped or cemented in at this part of the boundary. At the top, however, we want a prescribed vertical downward motion compressing the cylinder; in addition, we only want to restrict the vertical movement, but not the horizontal ones
* 
*  -  one can think of this situation as a well-greased plate sitting on top of the cylinder pushing it downwards: the atoms of the cylinder are forced to move downward, but they are free to slide horizontally along the plate.
* 

* 
*  The way to describe this is as follows: for boundary indicator zero (bottom face) we use a dim-dimensional zero function representing no motion in any coordinate direction. For the boundary with indicator 1 (top surface), we use the  [2.x.232]  class, but we specify an additional argument to the  [2.x.233]  function denoting which vector components it should apply to; this is a vector of bools for each vector component and because we only want to restrict vertical motion, it has only its last component set:
* 

* 
* [1.x.171]
* 
*   [1.x.172]  [1.x.173]
* 

* 
*  The next function is the one that controls what all has to happen within a timestep. The order of things should be relatively self-explanatory from the function names:
* 

* 
* [1.x.174]
* 
*   [1.x.175]  [1.x.176]
* 

* 
*  Solving the linear system again works mostly as before. The only difference is that we want to only keep a complete local copy of the solution vector instead of the distributed one that we get as output from PETSc's solver routines. To this end, we declare a local temporary variable for the distributed vector and initialize it with the contents of the local variable (remember that the  [2.x.234]  function called in  [2.x.235]  preset the values of boundary nodes in this vector), solve with it, and at the end of the function copy it again into the complete local vector that we declared as a member variable. Hanging node constraints are then distributed only on the local copy, i.e. independently of each other on each of the processors:
* 

* 
* [1.x.177]
* 
*   [1.x.178]  [1.x.179]
* 

* 
*  This function generates the graphical output in .vtu format as explained in the introduction. Each process will only work on the cells it owns, and then write the result into a file of its own. Additionally, processor 0 will write the record files the reference all the .vtu files.   
*   The crucial part of this function is to give the  [2.x.236]  class a way to only work on the cells that the present process owns.
* 

* 
*  

* 
* [1.x.180]
* 
*  Then, just as in  [2.x.237] , define the names of solution variables (which here are the displacement increments) and queue the solution vector for output. Note in the following switch how we make sure that if the space dimension should be unhandled that we throw an exception saying that we haven't implemented this case yet (another case of defensive programming):
* 

* 
* [1.x.181]
* 
*  The next thing is that we wanted to output something like the average norm of the stresses that we have stored in each cell. This may seem complicated, since on the present processor we only store the stresses in quadrature points on those cells that actually belong to the present process. In other words, it seems as if we can't compute the average stresses for all cells. However, remember that our class derived from  [2.x.238]  only iterates over those cells that actually do belong to the present processor, i.e. we don't have to compute anything for all the other cells as this information would not be touched. The following little loop does this. We enclose the entire block into a pair of braces to make sure that the iterator variables do not remain accidentally visible beyond the end of the block in which they are used:
* 

* 
* [1.x.182]
* 
*  Loop over all the cells...
* 

* 
* [1.x.183]
* 
*  On these cells, add up the stresses over all quadrature points...
* 

* 
* [1.x.184]
* 
*  ...then write the norm of the average to their destination:
* 

* 
* [1.x.185]
* 
*  And on the cells that we are not interested in, set the respective value in the vector to a bogus value (norms must be positive, and a large negative value should catch your eye) in order to make sure that if we were somehow wrong about our assumption that these elements would not appear in the output file, that we would find out by looking at the graphical output:
* 

* 
* [1.x.186]
* 
*  Finally attach this vector as well to be treated for output:
* 

* 
* [1.x.187]
* 
*  As a last piece of data, let us also add the partitioning of the domain into subdomains associated with the processors if this is a parallel job. This works in the exact same way as in the  [2.x.239]  program:
* 

* 
* [1.x.188]
* 
*  Finally, with all this data, we can instruct deal.II to munge the information and produce some intermediate data structures that contain all these solution and other data vectors:
* 

* 
* [1.x.189]
* 
*  Let us call a function that opens the necessary output files and writes the data we have generated into them. The function automatically constructs the file names from the given directory name (the first argument) and file name base (second argument). It augments the resulting string by pieces that result from the time step number and a "piece number" that corresponds to a part of the overall domain that can consist of one or more subdomains.     
*   The function also writes a record files (with suffix `.pvd`) for Paraview that describes how all of these output files combine into the data for this single time step:
* 

* 
* [1.x.190]
* 
*  The record files must be written only once and not by each processor, so we do this on processor 0:
* 

* 
* [1.x.191]
* 
*  Finally, we write the paraview record, that references all .pvtu files and their respective time. Note that the variable times_and_names is declared static, so it will retain the entries from the previous timesteps.
* 

* 
* [1.x.192]
* 
*   [1.x.193]  [1.x.194]
* 

* 
*  This and the next function handle the overall structure of the first and following timesteps, respectively. The first timestep is slightly more involved because we want to compute it multiple times on successively refined meshes, each time starting from a clean state. At the end of these computations, in which we compute the incremental displacements each time, we use the last results obtained for the incremental displacements to compute the resulting stress updates and move the mesh accordingly. On this new mesh, we then output the solution and any additional data we consider important.   
*   All this is interspersed by generating output to the console to update the person watching the screen on what is going on. As in  [2.x.240] , the use of  [2.x.241]  makes sure that only one of the parallel processes is actually writing to the console, without having to explicitly code an if-statement in each place where we generate output:
* 

* 
* [1.x.195]
* 
*   [1.x.196]  [1.x.197]
* 

* 
*  Subsequent timesteps are simpler, and probably do not require any more documentation given the explanations for the previous function above:
* 

* 
* [1.x.198]
* 
*   [1.x.199]  [1.x.200]
* 

* 
*  The following function is called when solving the first time step on successively refined meshes. After each iteration, it computes a refinement criterion, refines the mesh, and sets up the history variables in each quadrature point again to a clean state.
* 

* 
* [1.x.201]
* 
*  First, let each process compute error indicators for the cells it owns:
* 

* 
* [1.x.202]
* 
*  Then set up a global vector into which we merge the local indicators from each of the %parallel processes:
* 

* 
* [1.x.203]
* 
*  Once we have that, copy it back into local copies on all processors and refine the mesh accordingly:
* 

* 
* [1.x.204]
* 
*  Finally, set up quadrature point data again on the new mesh, and only on those cells that we have determined to be ours:
* 

* 
* [1.x.205]
* 
*   [1.x.206]  [1.x.207]
* 

* 
*  At the end of each time step, we move the nodes of the mesh according to the incremental displacements computed in this time step. To do this, we keep a vector of flags that indicate for each vertex whether we have already moved it around, and then loop over all cells and move those vertices of the cell that have not been moved yet. It is worth noting that it does not matter from which of the cells adjacent to a vertex we move this vertex: since we compute the displacement using a continuous finite element, the displacement field is continuous as well and we can compute the displacement of a given vertex from each of the adjacent cells. We only have to make sure that we move each node exactly once, which is why we keep the vector of flags.   
*   There are two noteworthy things in this function. First, how we get the displacement field at a given vertex using the  [2.x.242]  function that returns the index of the  [2.x.243]  of the given cell. In the present case, displacement in the k-th coordinate direction corresponds to the k-th component of the finite element. Using a function like this bears a certain risk, because it uses knowledge of the order of elements that we have taken together for this program in the  [2.x.244]  element. If we decided to add an additional variable, for example a pressure variable for stabilization, and happened to insert it as the first variable of the element, then the computation below will start to produce nonsensical results. In addition, this computation rests on other assumptions: first, that the element we use has, indeed, degrees of freedom that are associated with vertices. This is indeed the case for the present Q1 element, as would be for all Qp elements of polynomial order  [2.x.245] . However, it would not hold for discontinuous elements, or elements for mixed formulations. Secondly, it also rests on the assumption that the displacement at a vertex is determined solely by the value of the degree of freedom associated with this vertex; in other words, all shape functions corresponding to other degrees of freedom are zero at this particular vertex. Again, this is the case for the present element, but is not so for all elements that are presently available in deal.II. Despite its risks, we choose to use this way in order to present a way to query individual degrees of freedom associated with vertices.   
*   In this context, it is instructive to point out what a more general way would be. For general finite elements, the way to go would be to take a quadrature formula with the quadrature points in the vertices of a cell. The  [2.x.246]  formula for the trapezoidal rule does exactly this. With this quadrature formula, we would then initialize an  [2.x.247]  object in each cell, and use the  [2.x.248]  function to obtain the values of the solution function in the quadrature points, i.e. the vertices of the cell. These are the only values that we really need, i.e. we are not at all interested in the weights (or the  [2.x.249]  values) associated with this particular quadrature formula, and this can be specified as the last argument in the constructor to  [2.x.250] . The only point of minor inconvenience in this scheme is that we have to figure out which quadrature point corresponds to the vertex we consider at present, as they may or may not be ordered in the same order.   
*   This inconvenience could be avoided if finite elements have support points on vertices (which the one here has; for the concept of support points, see  [2.x.251]  "support points"). For such a case, one could construct a custom quadrature rule using  [2.x.252]  The first  [2.x.253]  quadrature points will then correspond to the vertices of the cell and are ordered consistent with  [2.x.254] , taking into account that support points for vector elements will be duplicated  [2.x.255]  times.   
*   Another point worth explaining about this short function is the way in which the triangulation class exports information about its vertices: through the  [2.x.256]  function, it advertises how many vertices there are in the triangulation. Not all of them are actually in use all the time
* 
*  -  some are left-overs from cells that have been coarsened previously and remain in existence since deal.II never changes the number of a vertex once it has come into existence, even if vertices with lower number go away. Secondly, the location returned by  [2.x.257]  is not only a read-only object of type  [2.x.258] , but in fact a reference that can be written to. This allows to move around the nodes of a mesh with relative ease, but it is worth pointing out that it is the responsibility of an application program using this feature to make sure that the resulting cells are still useful, i.e. are not distorted so much that the cell is degenerated (indicated, for example, by negative Jacobians). Note that we do not have any provisions in this function to actually ensure this, we just have faith.   
*   After this lengthy introduction, here are the full 20 or so lines of code:
* 

* 
* [1.x.208]
* 
*   [1.x.209]  [1.x.210]
* 

* 
*  At the beginning of our computations, we needed to set up initial values of the history variables, such as the existing stresses in the material, that we store in each quadrature point. As mentioned above, we use the  [2.x.259]  for this that is available in each cell.   
*   To put this into larger perspective, we note that if we had previously available stresses in our model (which we assume do not exist for the purpose of this program), then we would need to interpolate the field of preexisting stresses to the quadrature points. Likewise, if we were to simulate elasto-plastic materials with hardening/softening, then we would have to store additional history variables like the present yield stress of the accumulated plastic strains in each quadrature points. Pre-existing hardening or weakening would then be implemented by interpolating these variables in the present function as well.
* 

* 
* [1.x.211]
* 
*  For good measure, we set all user pointers of all cells, whether ours of not, to the null pointer. This way, if we ever access the user pointer of a cell which we should not have accessed, a segmentation fault will let us know that this should not have happened:
* 

* 
*  

* 
* [1.x.212]
* 
*  Next, allocate the quadrature objects that are within the responsibility of this processor. This, of course, equals the number of cells that belong to this processor times the number of quadrature points our quadrature formula has on each cell. Since the `resize()` function does not actually shrink the amount of allocated memory if the requested new size is smaller than the old size, we resort to a trick to first free all memory, and then reallocate it: we declare an empty vector as a temporary variable and then swap the contents of the old vector and this temporary variable. This makes sure that the `quadrature_point_history` is now really empty, and we can let the temporary variable that now holds the previous contents of the vector go out of scope and be destroyed. In the next step we can then re-allocate as many elements as we need, with the vector default-initializing the `PointHistory` objects, which includes setting the stress variables to zero.
* 

* 
* [1.x.213]
* 
*  Finally loop over all cells again and set the user pointers from the cells that belong to the present processor to point to the first quadrature point objects corresponding to this cell in the vector of such objects:
* 

* 
* [1.x.214]
* 
*  At the end, for good measure make sure that our count of elements was correct and that we have both used up all objects we allocated previously, and not point to any objects beyond the end of the vector. Such defensive programming strategies are always good checks to avoid accidental errors and to guard against future changes to this function that forget to update all uses of a variable at the same time. Recall that constructs using the  [2.x.260]  macro are optimized away in optimized mode, so do not affect the run time of optimized runs:
* 

* 
* [1.x.215]
* 
*   [1.x.216]  [1.x.217]
* 

* 
*  At the end of each time step, we should have computed an incremental displacement update so that the material in its new configuration accommodates for the difference between the external body and boundary forces applied during this time step minus the forces exerted through preexisting internal stresses. In order to have the preexisting stresses available at the next time step, we therefore have to update the preexisting stresses with the stresses due to the incremental displacement computed during the present time step. Ideally, the resulting sum of internal stresses would exactly counter all external forces. Indeed, a simple experiment can make sure that this is so: if we choose boundary conditions and body forces to be time independent, then the forcing terms (the sum of external forces and internal stresses) should be exactly zero. If you make this experiment, you will realize from the output of the norm of the right hand side in each time step that this is almost the case: it is not exactly zero, since in the first time step the incremental displacement and stress updates were computed relative to the undeformed mesh, which was then deformed. In the second time step, we again compute displacement and stress updates, but this time in the deformed mesh
* 
*  -  there, the resulting updates are very small but not quite zero. This can be iterated, and in each such iteration the residual, i.e. the norm of the right hand side vector, is reduced; if one makes this little experiment, one realizes that the norm of this residual decays exponentially with the number of iterations, and after an initial very rapid decline is reduced by roughly a factor of about 3.5 in each iteration (for one testcase I looked at, other testcases, and other numbers of unknowns change the factor, but not the exponential decay).
* 

* 
*  In a sense, this can then be considered as a quasi-timestepping scheme to resolve the nonlinear problem of solving large-deformation elasticity on a mesh that is moved along in a Lagrangian manner.   
*   Another complication is that the existing (old) stresses are defined on the old mesh, which we will move around after updating the stresses. If this mesh update involves rotations of the cell, then we need to also rotate the updated stress, since it was computed relative to the coordinate system of the old cell.   
*   Thus, what we need is the following: on each cell which the present processor owns, we need to extract the old stress from the data stored with each quadrature point, compute the stress update, add the two together, and then rotate the result together with the incremental rotation computed from the incremental displacement at the present quadrature point. We will detail these steps below:
* 

* 
* [1.x.218]
* 
*  First, set up an  [2.x.261]  object by which we will evaluate the incremental displacements and the gradients thereof at the quadrature points, together with a vector that will hold this information:
* 

* 
* [1.x.219]
* 
*  Then loop over all cells and do the job in the cells that belong to our subdomain:
* 

* 
* [1.x.220]
* 
*  Next, get a pointer to the quadrature point history data local to the present cell, and, as a defensive measure, make sure that this pointer is within the bounds of the global array:
* 

* 
* [1.x.221]
* 
*  Then initialize the  [2.x.262]  object on the present cell, and extract the gradients of the displacement at the quadrature points for later computation of the strains
* 

* 
* [1.x.222]
* 
*  Then loop over the quadrature points of this cell:
* 

* 
* [1.x.223]
* 
*  On each quadrature point, compute the strain increment from the gradients, and multiply it by the stress-strain tensor to get the stress update. Then add this update to the already existing strain at this point:
* 

* 
* [1.x.224]
* 
*  Finally, we have to rotate the result. For this, we first have to compute a rotation matrix at the present quadrature point from the incremental displacements. In fact, it can be computed from the gradients, and we already have a function for that purpose:
* 

* 
* [1.x.225]
* 
*  Note that the result, a rotation matrix, is in general an antisymmetric tensor of rank 2, so we must store it as a full tensor.
* 

* 
*  With this rotation matrix, we can compute the rotated tensor by contraction from the left and right, after we expand the symmetric tensor  [2.x.263]  into a full tensor:
* 

* 
* [1.x.226]
* 
*  Note that while the result of the multiplication of these three matrices should be symmetric, it is not due to floating point round off: we get an asymmetry on the order of 1e-16 of the off-diagonal elements of the result. When assigning the result to a  [2.x.264] , the constructor of that class checks the symmetry and realizes that it isn't exactly symmetric; it will then raise an exception. To avoid that, we explicitly symmetrize the result to make it exactly symmetric.
* 

* 
*  The result of all these operations is then written back into the original place:
* 

* 
* [1.x.227]
* 
*  This ends the project specific namespace  [2.x.265] . The rest is as usual and as already shown in  [2.x.266] : A  [2.x.267]  function that initializes and terminates PETSc, calls the classes that do the actual work, and makes sure that we catch all exceptions that propagate up to this point:
* 

* 
* [1.x.228]
* [1.x.229][1.x.230]
* 

* 
* Running the program takes a good while if one uses debug mode; it takes abouteleven minutes on my i7 desktop. Fortunately, the version compiled withoptimizations is much faster; the program only takes about a minute and a halfafter recompiling with the command <tt>make release</tt> on the same machine, amuch more reasonable time.
* 

* If run, the program prints the following output, explaining what it isdoing during all that time:
* [1.x.231]
* In other words, it is computing on 12,000 cells and with some 52,000unknowns. Not a whole lot, but enough for a coupled three-dimensionalproblem to keep a computer busy for a while. At the end of the day,this is what we have for output:
* [1.x.232]
* 
* 

* If we visualize these files with VisIt or Paraview, we get to see the full pictureof the disaster our forced compression wreaks on the cylinder (colors in theimages encode the norm of the stress in the material):
* 

*  [2.x.268] 
* 

*  [2.x.269] 
* 

* As is clearly visible, as we keep compressing the cylinder, it startsto bow out near the fully constrained bottom surface and, after about eighttime units, buckle in an azimuthally symmetric manner.
* 

* Although the result appears plausible for the symmetric geometry and loading,it is yet to be established whether or not the computation is fully converged.In order to see whether it is, we ran the program again with one more globalrefinement at the beginning and with the time step halved. This would havetaken a very long time on a single machine, so we used a proper workstation andran it on 16 processors in parallel. The beginning of the output now looks likethis:
* [1.x.233]
* That's quite a good number of unknowns, given that we are in 3d. The output ofthis program are 16 files for each time step:
* [1.x.234]
* 
* 

* Here are first the mesh on which we compute as well as the partitioningfor the 16 processors:
* 

*  [2.x.270] 
* 

* Finally, here is the same output as we have shown before for the much smallersequential case:
*  [2.x.271] 
* 

*  [2.x.272] 
* 

* As before, we observe that at high axial compression the cylinder beginsto buckle, but this time ultimately collapses on itself. In contrast to ourfirst run, towards the end of the simulation the deflection pattern becomesnonsymmetric (the central bulge deflects laterally). The model clearly does notprovide for this (all our forces and boundary deflections are symmetric) but theeffect is probably physically correct anyway: in reality, small inhomogeneitiesin the body's material properties would lead it to buckle to one sideto evade the forcing; in numerical simulations, small perturbationssuch as numerical round-off or an inexact solution of a linear systemby an iterative solver could have the same effect. Another typical source forasymmetries in adaptive computations is that only a certain fraction of cellsis refined in each step, which may lead to asymmetric meshes even if theoriginal coarse mesh was symmetric.
* 

* If one compares this with the previous run, the results both qualitativelyand quantitatively different. The previous computation wastherefore certainly not converged, though we can't say for sure anything aboutthe present one. One would need an even finer computation to find out. However,the point may be moot: looking at the last picture in detail, it is prettyobvious that not only is the linear small deformation model we chose completelyinadequate, but for a realistic simulation we would also need to make sure thatthe body does not intersect itself during deformation (if we continuedcompressing the cylinder we would observe some self-intersection).Without such a formulation we cannot expect anything to make physical sense,even if it produces nice pictures!
* 

* [1.x.235][1.x.236]
* 

* The program as is does not really solve an equation that has many applicationsin practice: quasi-static material deformation based on a purely elastic lawis almost boring. However, the program may serve as the starting point formore interesting experiments, and that indeed was the initial motivation forwriting it. Here are some suggestions of what the program is missing and inwhat direction it may be extended:
* [1.x.237][1.x.238]
* 

*  The most obvious extension is to use a morerealistic material model for large-scale quasistatic deformation. The naturalchoice for this would be plasticity, in which a nonlinear relationship betweenstress and strain replaces equation [1.x.239]. Plasticitymodels are usually rather complicated to program since the stress-straindependence is generally non-smooth. The material can be thought of being ableto withstand only a maximal stress (the yield stress) after which it starts to&ldquo;flow&rdquo;. A mathematical description to this can be given in the form of avariational inequality, which alternatively can be treated as minimizing theelastic energy[1.x.240]subject to the constraint[1.x.241]on the stress. This extension makes the problem to be solved in each time stepnonlinear, so we need another loop within each time step.
* Without going into further details of this model, we refer to the excellentbook by Simo and Hughes on &ldquo;Computational Inelasticity&rdquo; for acomprehensive overview of computational strategies for solving plasticmodels. Alternatively, a brief but concise description of an algorithm forplasticity is given in an article by S. Commend, A. Truty, and Th. Zimmermann; [2.x.273] .
* 

* [1.x.242][1.x.243]
* 

* The formulation we have chosen, i.e. usingpiecewise (bi-, tri-)linear elements for all components of the displacementvector, and treating the stress as a variable dependent on the displacement isappropriate for most materials. However, this so-called displacement-basedformulation becomes unstable and exhibits spurious modes for incompressible ornearly-incompressible materials. While fluids are usually not elastic (in mostcases, the stress depends on velocity gradients, not displacement gradients,although there are exceptions such as electro-rheologic fluids), there are afew solids that are nearly incompressible, for example rubber. Another case isthat many plasticity models ultimately let the material become incompressible,although this is outside the scope of the present program.
* Incompressibility is characterized by Poisson's ratio[1.x.244]where  [2.x.274]  are the Lam&eacute; constants of the material.Physical constraints indicate that  [2.x.275]  (the conditionalso follows from mathematical stability considerations). If  [2.x.276] approaches  [2.x.277] , then the material becomes incompressible. In thatcase, pure displacement-based formulations are no longer appropriate for thesolution of such problems, and stabilization techniques have to be employedfor a stable and accurate solution. The book and paper cited above giveindications as to how to do this, but there is also a large volume ofliterature on this subject; a good start to get an overview of the topic canbe found in the references of the paper by H.-Y. Duan and Q. Lin;  [2.x.278] .
* 

* [1.x.245][1.x.246]
* 

* In the present form, the programonly refines the initial mesh a number of times, but then never again. For anykind of realistic simulation, one would want to extend this so that the meshis refined and coarsened every few time steps instead. This is not hard to do,in fact, but has been left for future tutorial programs or as an exercise, ifyou wish.
* The main complication one has to overcome is that one has totransfer the data that is stored in the quadrature points of the cells of theold mesh to the new mesh, preferably by some sort of projection scheme. Thegeneral approach to this would go like this:
* 
*  - At the beginning, the data is only available in the quadrature points of  individual cells, not as a finite element field that is defined everywhere.
* 
*  - So let us find a finite element field that [1.x.247] defined everywhere so  that we can later interpolate it to the quadrature points of the new  mesh. In general, it will be difficult to find a continuous finite element  field that matches the values in the quadrature points exactly because the  number of degrees of freedom of these fields does not match the number of  quadrature points there are, and the nodal values of this global field will  either be over- or underdetermined. But it is usually not very difficult to  find a discontinuous field that matches the values in the quadrature points;  for example, if you have a QGauss(2) quadrature formula (i.e. 4 points per  cell in 2d, 8 points in 3d), then one would use a finite element of kind  FE_DGQ(1), i.e. bi-/tri-linear functions as these have 4 degrees of freedom  per cell in 2d and 8 in 3d.
* 
*  - There are functions that can make this conversion from individual points to  a global field simpler. The following piece of pseudo-code should help if  you use a QGauss(2) quadrature formula. Note that the multiplication by the  projection matrix below takes a vector of scalar components, i.e., we can only  convert one set of scalars at a time from the quadrature points to the degrees  of freedom and vice versa. So we need to store each component of stress separately,  which requires  [2.x.279]  vectors. We'll store this set of vectors in a 2D array to  make it easier to read off components in the same way you would the stress tensor.  Thus, we'll loop over the components of stress on each cell and store  these values in the global history field. (The prefix  [2.x.280]   indicates that we work with quantities related to the history variables defined  in the quadrature points.) 
* [1.x.248]
* 
* 
*  - Now that we have a global field, we can refine the mesh and transfer the  history_field vector as usual using the SolutionTransfer class. This will  interpolate everything from the old to the new mesh.
* 
*  - In a final step, we have to get the data back from the now interpolated  global field to the quadrature points on the new mesh. The following code  will do that: 
* [1.x.249]
* 
* It becomes a bit more complicated once we run the program in parallel, sincethen each process only stores this data for the cells it owned on the oldmesh. That said, using a parallel vector for  [2.x.281]  willdo the trick if you put a call to  [2.x.282]  after the transferfrom quadrature points into the global vector.
* 

* [1.x.250][1.x.251]
* 

* At present, the program makes no attemptto make sure that a cell, after moving its vertices at the end of the timestep, still has a valid geometry (i.e. that its Jacobian determinant ispositive and bounded away from zero everywhere). It is, in fact, not very hardto set boundary values and forcing terms in such a way that one gets distortedand inverted cells rather quickly. Certainly, in some cases of largedeformation, this is unavoidable with a mesh of finite mesh size, but in someother cases this should be preventable by appropriate mesh refinement and/or areduction of the time step size. The program does not do that, but a moresophisticated version definitely should employ some sort of heuristic definingwhat amount of deformation of cells is acceptable, and what isn't.
* 

* [1.x.252][1.x.253] [2.x.283] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-19_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32]
* 
*  [2.x.2] 
* [1.x.33]
*  [2.x.3]  Support for particles exists in deal.II primarily due to the initial  efforts of Rene Gassmoeller. Please acknowledge this work by citing  the publication  [2.x.4]  if you use particle functionality in your  own work.
* [1.x.34][1.x.35][1.x.36]
* 

* The finite element method in general, and deal.II in particular, were inventedto solve partial differential equations
* 
*  -  in other words, to solve[continuum mechanics](https://en.wikipedia.org/wiki/Continuum_mechanics) problems.On the other hand, sometimes one wants to solve problems in which it is usefulto track individual objects ("particles") and how their positions evolve. Ifthis simply leads to a set of ordinary differential equations, for exampleif you want to track the positions of the planets in the solar system overtime, then deal.II is clearly not your right tool. On the other hand, ifthis evolution is due to the interaction with the solution of partial differentialequation, or if having a mesh to determine which particles interactwith others (such as in the[smoothed particle hydrodynamics (SPH)](https://en.wikipedia.org/wiki/Smoothed-particle_hydrodynamics)method), then deal.II has support for you.
* The case we will consider here is how electrically charged particles move throughan electric field. As motivation, we will consider[cathode rays](https://en.wikipedia.org/wiki/Cathode_ray): Electrons emitted by aheated piece of metal that is negatively charged (the "cathode"), and that arethen accelerated by an electric field towards the positively charged electrode(the "anode"). The anode is typically ring-shaped so that the majority ofelectrons can fly through the hole in the form of an electron beam. In the oldentimes, they might then have illuminated the screen of a TV built from a[cathode ray tube](https://en.wikipedia.org/wiki/Cathode-ray_tube).Today, instead, electron beams are useful in[X-ray machines](https://en.wikipedia.org/wiki/X-ray_tube),[electron beam lithography](https://en.wikipedia.org/wiki/Electron-beam_lithography),[electron beam welding](https://en.wikipedia.org/wiki/Electron-beam_welding), anda number of other areas.
* The equations we will then consider are as follows: First, we need to describethe electric field. This is most easily accomplished by noting that the electricpotential  [2.x.5]  satisfied the equation[1.x.37]where  [2.x.6]  is the dielectric constant of vacuum, and  [2.x.7]  is the chargedensity. This is augmented by boundary conditions that we will choose as follows:
* [1.x.38]
* In other words, we prescribe voltages  [2.x.8]  and  [2.x.9]  at the two electrodesand insulating (Neumann) boundary conditions elsewhere. Since the dynamics of theparticles are purely due to the electric field  [2.x.10] , we couldas well have prescribed  [2.x.11]  and  [2.x.12]  at the two electrodes
* 
*  -  all that mattersis the voltage difference at the two electrodes.
* Given this electric potential  [2.x.13]  and the electric field  [2.x.14] ,we can describe the trajectory of the  [2.x.15] th particle using the differentialequation[1.x.39]where  [2.x.16]  are the mass and electric charge of each particle. In practice, itis convenient to write this as a system of first-order differential equationsin the position  [2.x.17]  and velocity  [2.x.18] :
* [1.x.40]
* The deal.II class we will use to deal with particles,  [2.x.19] stores particles in a way so that the position  [2.x.20]  is part of the [2.x.21]  data structures. (It stores particles sortedby cell they are in, and consequently needs to know where each particle is.)The velocity  [2.x.22] , on the other hand, is of no concern to [2.x.23]  and consequently we will store it as a"property" of each particle that we will update in each time step. Propertiescan also be used to store any other quantity we might care about each particle:its charge, or if they were larger than just an electron, its color, mass,attitude in space, chemical composition, etc.
* There remain two things to discuss to complete the model:Where particles start and what the charge density  [2.x.24]  is.
* First, historically, cathode rays used very large electric fields to pullelectrons out of the metal. This produces only a relatively small current. Onecan do better by heating the cathode: a statistical fraction of electrons in thatcase have enough thermal energy to leave the metal; the electric field then justhas to be strong enough to pull them away from the attraction of their hostbody. We will model this in the following way: We will create a new particle if(i) the electric field points away from the electrode, i.e., if [2.x.25]  where  [2.x.26]  is the normal vector at aface pointing out of the domain (into the electrode), and (ii) the electricfield exceeds a threshold value  [2.x.27] . This issurely not a sufficiently accurate model for what really happens, but is goodenough for our current tutorial program.
* Second, in principle we would have to model the charge density via[1.x.41]
*  [2.x.28] The issue now is that in reality, a cathode ray tube in an old televisionyields a current of somewhere around a few milli-Amperes. In the much higherenergy beams of particle accelerators, the current may only be a fewnano-Ampere. But an Ampere is  [2.x.29]  electrons flowing persecond. Now, as you will see in the results section, we really only simulatea few microseconds ( [2.x.30]  seconds), but that still results in very verylarge numbers of electrons
* 
*  -  far more than we can hope to simulatewith a program as small as the current one. As a consequence, let uspresume that each particle represents  [2.x.31]  electrons. Then the particlemass and charge are also  [2.x.32]  and  [2.x.33]  and the equations we have tosolve are[1.x.42]which is of course exactly the same as above. On the other hand, the chargedensity for these "clumps" of electrons is given by[1.x.43]It is this form that we will implement in the program, where  [2.x.34]  is chosenrather large in the program to ensure that the particles actually affectthe electric field. (This may not be realistic in practice: In most cases,there are just not enough electrons to actually affect the overallelectric field. But realism is not our goal here.)
* 

* 
*  [2.x.35]  One may wonder why the equation for the electric field (or, rather,the electric potential) has no time derivative whereas the equations forthe electron positions do. In essence, this is a modeling assumption: Weassume that the particles move so slowly that at any given time theelectric field is in equilibrium. This is saying, in other words, thatthe velocity of the electrons is much less than the speed of light. Inyet other words, we can rephrase this in terms of the electrode voltage [2.x.36] : Since every volt of electric potential accelerates electrons byapproximately 600 km/s (neglecting relativistic effects), requiring [2.x.37]  is equivalent to saying that  [2.x.38] .Under this assumption (and the assumption that the total numberof electrons is small), one can also neglect the creation ofmagnetic fields by the moving charges, which would otherwise also affectthe movement of the electrons.
* 

* [1.x.44][1.x.45]
* 

* The equations outlined above form a set of coupled differential equations.Let us bring them all together in one place again to make that clear:
* [1.x.46]
* Because of the awkward dependence of the electric potential on theparticle locations, we don't want to solve this as a coupled systembut instead use a decoupled approach where we first solve for thepotential in each time step and then the particle locations. (Onecould also do it the other way around, of course.) This is verymuch in the same spirit as we do in  [2.x.39] ,  [2.x.40] , and  [2.x.41] ,to name just a few, and can all be understood in the context ofthe operator splitting methods discussed in  [2.x.42] .
* So, if we denote by an upper index  [2.x.43]  the time step, and if weuse a simple time discretization for the ODE, then this meansthat we have to solve the following set of equations in each timestep:
* [1.x.47]
* There are of course many better ways to do a time discretization (forexample the simple [leapfrog scheme](https://en.wikipedia.org/wiki/Leapfrog_integration))but this isn't the point of the tutorial program, and so we will be contentwith what we have here. (We will comment on a piece of this puzzle in the[1.x.48] section of this program,however.)
* There remains the question of how we should choose the time step size  [2.x.44] .The limitation here is that the  [2.x.45]  class needs tokeep track of which cell each particle is in. This is particularly an issue ifwe are running computations in parallel (say, in  [2.x.46] ) because in that caseevery process only stores those cells it owns plus one layer of "ghost cells".That's not relevant here, but in general we should make sure that over thecourse of each time step, a particle moves only from one cell to anyof its immediate neighbors (face, edge, or vertex neighbors). If we can ensurethat, then  [2.x.47]  is guaranteed to be able to figure outwhich cell a particle ends up in. To do this, a useful rule of thumbis that we should choose the time step so that for all particles the expecteddistance the particle moves by is less than one cell diameter:[1.x.49]or equivalently[1.x.50]Here,  [2.x.48]  is the length of the shortest edge of the cell on which particle [2.x.49]  is located
* 
*  -  in essence, a measure of the size of a cell.
* On the other hand, a particle might already be at the boundary of one celland the neighboring cell might be once further refined. So then the time tocross thatneighboring* cell would actually be half the amount above,suggesting[1.x.51]
* But even that is not good enough: The formula above updates the particlepositions in each time using the formula[1.x.52]that is, using thecurrent* velocity  [2.x.50] . But we don't havethe current velocity yet at the time when we need to choose  [2.x.51] 
* 
*  -  whichis after we have updated the potential  [2.x.52]  but before we update thevelocity from  [2.x.53]  to  [2.x.54] . All we have is [2.x.55] . So we need an additional safety factor for our finalchoice:[1.x.53]How large should  [2.x.56]  be? That depends on how much of underestimate [2.x.57]  might be compared to  [2.x.58] , and thatis actually quite easy to assess: A particle created in one time step withzero velocity will roughly pick up equal velocity increments in each successivetime step if the electric field it encounters along the way were roughlyconstant. So the maximal difference between  [2.x.59]  and [2.x.60]  would be a factor of two. As a consequence,we will choose  [2.x.61] .
* There is only one other case we ought to consider: What happens inthe very first time step? There, any particles to be moved along have justbeen created, but they have a zero velocity. So we don't know whatvelocity we should choose for them. Of course, in all other time stepsthere are also particles that have just been created, but in general,the particles with the highest velocity limit the time step size and so thenewly created particles with their zero velocity don't matter. But if weonly*
have such particles?
* In that case, we can use the following approximation: If a particlestarts at  [2.x.62] , then the update formula tells us that[1.x.54]and consequently[1.x.55]which we can write as[1.x.56]Not wanting to move a particle by more than  [2.x.63]  then implies that we shouldchoose the time step as[1.x.57]Using the same argument about neighboring cells possibly being smaller bya factor of two then leads to the final formula for time step zero:[1.x.58]
* Strictly speaking, we would have to evaluate the electric potential  [2.x.64]  atthe location of each particle, but a good enough approximation is to use themaximum of the values at the vertices of the respective cell. (Why the verticesand not the midpoint? Because the gradient of the solution of the Laplace equation,i.e., the electric field, is largest in corner singularities which are locatedat the vertices of cells.) This has the advantage that we can make good use of theFEValues functionality which can recycle pre-computed material as long as thequadrature points are the same from one cell to the next.
* We could always run this kind of scheme to estimate the difference between [2.x.65]  and  [2.x.66] , but it relies on evaluating theelectric field  [2.x.67]  on each cell, and that is expensive. As aconsequence, we will limit this approach to the very first time step.
* 

* [1.x.59][1.x.60]
* 

* Having discussed the time discretization, the discussion of the spatialdiscretization is going to be short: We use quadratic finite elements,i.e., the space  [2.x.68] , to approximate the electric potential  [2.x.69] . Themesh is adapted a couple of times during the initial time step. Allof this is entirely standard if you have read  [2.x.70] , and the implementationdoes not provide for any kind of surprise.
* 

* 
* [1.x.61][1.x.62]
* 

* Adding and moving particles is, in practice, not very difficult in deal.II.To add one, the `create_particles()` function of this program simplyuses a code snippet of the following form:
* [1.x.63]
* In other words, it is not all that different from inserting an objectinto a  [2.x.71]  or  [2.x.72]  Create the object, set its properties(here, the current location, its reference cell location, and its id)and call `insert_particle`. The only thing that may be surprising isthe reference location: In order to evaluate things such as [2.x.73] , it is necessary to evaluate finite elementfields at locations  [2.x.74] . But this requires evaluating thefinite element shape functions at points on the reference cell [2.x.75] . To make this efficient, every particle doesn'tjust store its location and the cell it is on, but also what locationthat point corresponds to in the cell's reference coordinate system.
* Updating a particle's position is then no more difficult: One just hasto call
* [1.x.64]
* We do this in the `move_particles()` function. The only differenceis that we then have to tell the  [2.x.76]  classto also find what cell that position corresponds to (and, when computingin parallel, which process owns this cell). For efficiency reason,this is most easily done after updating all particles' locations,and is achieved via the [2.x.77] function.
* There are, of course, times where a particle may leave the domain inquestion. In that case, [2.x.78] can not find a surrounding cell and simply deletes the particle. But, itis often useful to track the number of particles that have been lostthis way, and for this the  [2.x.79]  class offers a"signal" that one can attach to. We show how to do this in theconstructor of the main class to count how many particles were lostin each time step. Specifically, the way this works is thatthe  [2.x.80]  class has a "signal" to which onecan attach a function that will be executed whenever the signalis triggered. Here, this looks as follows:
* [1.x.65]
* That's a bit of a mouthful, but what's happening is this: We declarea lambda function that "captures" the `this` pointer (so that we can accessmember functions of the surrounding object inside the lambda function), andthat takes two arguments:
* 
*  - A reference to the particle that has been "lost".
* 
*  - A reference to the cell it was on last.The lambda function then simply calls the  [2.x.81] function with these arguments. When we attach this lambda function to thesignal, the  [2.x.82] function will trigger the signal for every particle for which it can'tfind a new home. This gives us the chance to record where the particleis, and to record statistics on it.
* 

* 
*  [2.x.83]  In this tutorial program, we insert particles by hand and at  locations we specifically choose based on conditions that include  the solution of the electrostatic problem. But there are other cases  where one primarily wants to use particles as passive objects, for  example to trace and visualize the flow field of a fluid flow  problem. In those cases, there are numerous functions in the   [2.x.84]  namespace that can generate particles  automatically. One of the functions of this namespace is also used  in the  [2.x.85]  tutorial program, for example.
* 

* [1.x.66][1.x.67]
* 

* The test case here is not meant to be a realistic depiction of a cathoderay tube, but it has the right general characteristics and the point is,in any case, only to demonstrate how one would implement deal.II codesthat use particles.
* The following picture shows the geometry that we're going to use:
*  [2.x.86] 
* In this picture, the parts of the boundary marked in red and blue are thecathode, held at an electric potential  [2.x.87] . The part of the cathode shownin red is the part that is heated, leading to electrons leaving the metaland then being accelerated by the electric field (a few electricfield lines are also shown). The green part of the boundary is the anode,held at  [2.x.88] . The rest of the boundary satisfies a Neumann boundarycondition.
* This setup mimics real devices. The re-entrant corner results in anelectric potential  [2.x.89]  whose derivative (the electric field  [2.x.90] )has a singularity
* 
*  -  in other words, it becomes very large in the vicinityof the corner, allowing it to rip electrons away from the metal. Theseelectrons are then accelerated towards the (green) anode which has ahole in the middle through which the electrons can escape the device andfly on to hit the screen, where they excite the "phosphor" to then emitthe light that we see from these old-style TV screens. The non-heatedpart of the cathode is not subjectto the emission of electrons
* 
*  -  in the code, we will mark this as the"focussing element" of the tube, because its negative electric voltagerepels the electrons and makes sure that they do not just flyaway from the heated part of the cathode perpendicular to the boundary,but in fact bend their paths towards the anode on the right.
* The electric field lines also shown in the picture illustratethat the electric field connects the negative and positiveelectrodes, respectively. The accelerating force the electronsexperience is along these field lines. Finally, the picture shows themesh used in the computation, illustrating that there aresingularities at the tip of the re-rentrant corner as wellas at all places where the boundary conditions change; thesesingularities are visible because the mesh is refined in theselocations.
* Of practical interest is to figure out which fraction of theelectrons emitted from the cathode actually make it through thehole in the anode
* 
*  -  electrons that just bounce into the anodeitself are not actually doing anything useful other than convertingelectricity into heat. As a consequence, in the `track_lost_particle()`function (which is called for each particle that leaves the domain,see above), we will estimate where it might have left the domainand report this in the output.
* 

* 
*  [2.x.91]  It is worth repeating that neither the geometry used here,nor in fact any other aspect of this program is intended to representanything even half-way realistic. Tutorial programs are our tools toteach how deal.II works, and we often use situations for which wehave some kind of intuition since this helps us interpret the outputof a program, but that's about the extent to which we intend theprogram to do anything of use besides being a teaching tool.
* 

*  [1.x.68] [1.x.69]
*   [1.x.70]  [1.x.71]
* 

* 
*  The majority of the include files used in this program are well known from  [2.x.92]  and similar programs:
* 

* 
*  

* 
* [1.x.72]
* 
*  The ones that are new are only the following three: The first declares the DiscreteTime class that helps us keep track of time in a time-dependent simulation. The latter two provide all of the particle functionality, namely a way to keep track of particles located on a mesh (the  [2.x.93]  class) and the ability to output these particles' locations and their properties for the purposes of visualization (the  [2.x.94]  class).
* 

* 
* [1.x.73]
* 
*   [1.x.74]  [1.x.75]
* 

* 
*  As is customary, we put everything that corresponds to the details of the program into a namespace of its own. At the top, we define a few constants for which we would rather use symbolic names than hard-coded numbers.
* 

* 
*  Specifically, we define numbers for  [2.x.95]  "boundary indicators" for the various parts of the geometry, as well as the physical properties of electrons and other specifics of the setup we use here.
* 

* 
*  For the boundary indicators, let us start enumerating at some random value 101. The principle here is to use numbers that areuncommon*. If there are pre-defined boundary indicators previously set by the `GridGenerator` functions, they will likely be small integers starting from zero, but not in this rather randomly chosen range. Using numbers such as those below avoids the possibility for conflicts, and also reduces the temptation to just spell these numbers out in the program (because you will probably never remember which is which, whereas you might have been tempted if they had started at 0).
* 

* 
* [1.x.76]
* 
*   [1.x.77]  [1.x.78]
* 

* 
*  The following is then the main class of this program. It has, fundamentally, the same structure as  [2.x.96]  and many other tutorial programs. This includes the majority of the member functions (with the purpose of the rest probably self-explanatory from their names) as well as only a small number of member variables beyond those of  [2.x.97] , all of which are related to dealing with particles.
* 

* 
* [1.x.79]
* 
*   [1.x.80]  [1.x.81]
* 

* 
*   [1.x.82]  [1.x.83]
* 

* 
*  So then let us get started on the implementation. What the constructor does is really only a straight-forward initialization of all of the member variables at the top. The only two worth mentioning are the `particle_handler`, which is handed a reference to the triangulation on which the particles will live (currently of course still empty, but the particle handler stores the reference and will use it once particles are added
* 
*  -  which happens after the triangulation is built). The other piece of information it gets is how many "properties" each particle needs to store. Here, all we need each particle to remember is its current velocity, i.e., a vector with `dim` components. There are, however, other intrinsic properties that each particle has and that the  [2.x.98]  class automatically and always makes sure are available; in particular, these are the current location of a particle, the cell it is on, it's reference location within that cell, and the particle's ID.   
*   The only other variable of interest is `time`, an object of type DiscreteTime. It keeps track of the current time we are in a time-dependent simulation, and is initialized with the start time (zero) and end time ( [2.x.99] ). We will later set the time step size in `update_timestep_size()`.   
*   The body of the constructor consists of a piece of code we have already discussed in the introduction. Namely, we make sure that the `track_lost_particle()` function is called by the `particle_handler` object every time a particle leaves the domain.
* 

* 
* [1.x.84]
* 
*   [1.x.85]  [1.x.86]
* 

* 
*  The next function is then responsible for generating the mesh on which we want to solve. Recall how the domain looks like:  [2.x.100]  We subdivide this geometry into a mesh of  [2.x.101]  cells that looks like this:  [2.x.102]  The way this is done is by first defining where the  [2.x.103]  vertices are located
* 
*  -  here, we say that they are on integer points with the middle one on the left side moved to the right by a value of `delta=0.5`.   
*   In the following, we then have to say which vertices together form the 8 cells. The following code is then entirely equivalent to what we also do in  [2.x.104] :
* 

* 
* [1.x.88]
* 
*  With these arrays out of the way, we can move to slightly higher higher-level data structures. We create a vector of CellData objects that store for each cell to be created the vertices in question as well as the  [2.x.105]  "material id" (which we will here simply set to zero since we don't use it in the program).     
*   This information is then handed to the  [2.x.106]  function, and the mesh is twice globally refined.
* 

* 
* [1.x.89]
* 
*  The remaining part of the function loops over all cells and their faces, and if a face is at the boundary determines which boundary indicator should be applied to it. The various conditions should make sense if you compare the code with the picture of the geometry above.     
*   Once done with this step, we refine the mesh once more globally.
* 

* 
* [1.x.90]
* 
*   [1.x.91]  [1.x.92]
* 

* 
*  The next function in this program deals with setting up the various objects related to solving the partial differential equations. It is in essence a copy of the corresponding function in  [2.x.107]  and requires no further discussion.
* 

* 
* [1.x.93]
* 
*   [1.x.94]  [1.x.95]
* 

* 
*  The function that computes the matrix entries is again in essence a copy of the corresponding function in  [2.x.108] :
* 

* 
* [1.x.96]
* 
*  The only interesting part of this function is how it forms the right hand side of the linear system. Recall that the right hand side of the PDE is [1.x.97] where we have used  [2.x.109]  to index the particles here to avoid confusion with the shape function  [2.x.110] ;  [2.x.111]  is the position of the  [2.x.112] th particle.         
*   When multiplied by a test function  [2.x.113]  and integrated over the domain results in a right hand side vector

* 
* [1.x.98]
*  Note that the final line no longer contains an integral, and consequently also no occurrence of  [2.x.114]  which would require the appearance of the `JxW` symbol in our code.         
*   For a given cell  [2.x.115] , this cell's contribution to the right hand side is then

* 
* [1.x.99]
*  i.e., we only have to worry about those particles that are actually located on the current cell  [2.x.116] .         
*   In practice, what we do here is the following: If there are any particles on the current cell, then we first obtain an iterator range pointing to the first particle of that cell as well as the particle past the last one on this cell (or the end iterator)
* 
*  -  i.e., a half-open range as is common for C++ functions. Knowing now the list of particles, we query their reference locations (with respect to the reference cell), evaluate the shape functions in these reference locations, and compute the force according to the formula above (without any  [2.x.117]          
*  

* 
*  [2.x.118]  It is worth pointing out that calling the  [2.x.119]  and  [2.x.120]  functions is not very efficient on problems with a large number of particles. But it illustrates the easiest way to write this algorithm, and so we are willing to incur this cost for the moment for expository purposes. We discuss the issue in more detail in the [1.x.100] below, and use a better approach in  [2.x.121] , for example.
* 

* 
* [1.x.101]
* 
*  Finally, we can copy the contributions of this cell into the global matrix and right hand side vector:
* 

* 
* [1.x.102]
* 
*   [1.x.103]  [1.x.104]
* 

* 
*  The function that solves the linear system is then again exactly as in  [2.x.122] :
* 

* 
* [1.x.105]
* 
*   [1.x.106]  [1.x.107]
* 

* 
*  The final field-related function is the one that refines the grid. We will call it a number of times in the first time step to obtain a mesh that is well-adapted to the structure of the solution and, in particular, resolves the various singularities in the solution that are due to re-entrant corners and places where the boundary condition type changes. You might want to refer to  [2.x.123]  again for more details:
* 

* 
* [1.x.108]
* 
*   [1.x.109]  [1.x.110]
* 

* 
*  Let us now turn to the functions that deal with particles. The first one is about the creation of particles. As mentioned in the introduction, we want to create a particle at points of the cathode if the the electric field  [2.x.124]  exceeds a certain threshold, i.e., if  [2.x.125] , and if furthermore the electric field points into the domain (i.e., if  [2.x.126] ). As is common in the finite element method, we evaluate fields (and their derivatives) at specific evaluation points; typically, these are "quadrature points", and so we create a "quadrature formula" that we will use to designate the points at which we want to evaluate the solution. Here, we will simply take QMidpoint implying that we will only check the threshold condition at the midpoints of faces. We then use this to initialize an object of type FEFaceValues to evaluate the solution at these points.   
*   All of this will then be used in a loop over all cells, their faces, and specifically those faces that are at the boundary and, moreover, the cathode part of the boundary.
* 

* 
* [1.x.111]
* 
*  So we have found a face on the cathode. Next, we let the FEFaceValues object compute the gradient of the solution at each "quadrature" point, and extract the electric field vector from the gradient in the form of a Tensor variable through the methods discussed in the  [2.x.127]  "vector-valued problems" documentation module.
* 

* 
* [1.x.112]
* 
*  Electrons can only escape the cathode if the electric field strength exceeds a threshold and, crucially, if the electric field pointsinto* the domain. Once we have that checked, we create a new  [2.x.128]  object at this location and insert it into the  [2.x.129]  object with a unique ID.                 
*   The only thing that may be not obvious here is that we also associate with this particle the location in the reference coordinates of the cell we are currently on. This is done because we will in downstream functions compute quantities such as the electric field at the location of the particle (e.g., to compute the forces that act on it when updating its position in each time step). Evaluating a finite element field at arbitrary coordinates is quite an expensive operation because shape functions are really only defined on the reference cell, and so when asking for the electric field at an arbitrary point requires us first to determine what the reference coordinates of that point are. To avoid having to do this over and over, we determine these coordinates once and for all and then store these reference coordinates directly with the particle.
* 

* 
* [1.x.113]
* 
*  At the end of all of these insertions, we let the `particle_handler` update some internal statistics about the particles it stores.
* 

* 
* [1.x.114]
* 
*   [1.x.115]  [1.x.116]
* 

* 
*  The second particle-related function is the one that moves the particles in each time step. To do this, we have to loop over all cells, the particles in each cell, and evaluate the electric field at each of the particles' positions.   
*   The approach used here is conceptually the same used in the `assemble_system()` function: We loop over all cells, find the particles located there (with the same caveat about the inefficiency of the algorithm used here to find these particles), and use FEPointEvaluation object to evaluate the gradient at these positions:
* 

* 
* [1.x.117]
* 
*  Then we can ask the FEPointEvaluation object for the gradients of the solution (i.e., the electric field  [2.x.130] ) at these locations and loop over the individual particles:
* 

* 
* [1.x.118]
* 
*  Having now obtained the electric field at the location of one of the particles, we use this to update first the velocity and then the position. To do so, let us first get the old velocity out of the properties stored with the particle, compute the acceleration, update the velocity, and store this new velocity again in the properties of the particle. Recall that this corresponds to the first of the following set of update equations discussed in the introduction:

* 
* [1.x.119]
* 
* 

* 
* [1.x.120]
* 
*  With the new velocity, we can then also update the location of the particle and tell the particle about it.
* 

* 
* [1.x.121]
* 
*  Having updated the locations and properties (i.e., velocities) of all particles, we need to make sure that the `particle_handler` again knows which cells they are in, and what their locations in the coordinate system of the reference cell are. The following function does that. (It also makes sure that, in parallel computations, particles are moved from one processor to another processor if a particle moves from the subdomain owned by the former to the subdomain owned by the latter.)
* 

* 
* [1.x.122]
* 
*   [1.x.123]  [1.x.124]
* 

* 
*  The final particle-related function is the one that is called whenever a particle is lost from the simulation. This typically happens if it leaves the domain. If that happens, this function is called both the cell (which we can ask for its new location) and the cell it was previously on. The function then keeps track of updating the number of particles lost in this time step, the total number of lost particles, and then estimates whether the particle left through the hole in the middle of the anode. We do so by first checking whether the cell it was in last had an  [2.x.131]  coordinate to the left of the right boundary (located at  [2.x.132] ) and the particle now has a position to the right of the right boundary. If that is so, we compute a direction vector of its motion that is normalized so that the  [2.x.133]  component of the direction vector is equal to  [2.x.134] . With this direction vector, we can compute where it would have intersected the line  [2.x.135] . If this intersect is between  [2.x.136]  and  [2.x.137] , then we claim that the particle left through the hole and increment a counter.
* 

* 
* [1.x.125]
* 
*   [1.x.126]  [1.x.127]
* 

* 
*  As discussed at length in the introduction, we need to respect a time step condition whereby particles can not move further than one cell in one time step. To ensure that this is the case, we again first compute the maximal speed of all particles on each cell, and divide the cell size by that speed. We then compute the next time step size as the minimum of this quantity over all cells, using the safety factor discussed in the introduction, and set this as the desired time step size using the  [2.x.138]  function.
* 

* 
* [1.x.128]
* 
*  As mentioned in the introduction, we have to treat the very first time step differently since there, particles are not available yet or do not yet have the information associated that we need for the computation of a reasonable step length. The formulas below follow the discussion in the introduction.
* 

* 
* [1.x.129]
* 
*   [1.x.130]  [1.x.131]
* 

* 
*  The final function implementing pieces of the overall algorithm is the one that generates graphical output. In the current context, we want to output both the electric potential field as well as the particle locations and velocities. But we also want to output the electric field, i.e., the gradient of the solution.   
*   deal.II has a general way how one can compute derived quantities from the solution and output those as well. Here, this is the electric field, but it could also be some other quantity
* 
*  -  say, the norm of the electric field, or in fact anything else one could want to compute from the solution  [2.x.139]  or its derivatives. This general solution uses the DataPostprocessor class and, in cases like the one here where we want to output a quantity that represents a vector field, the DataPostprocessorVector class.   
*   Rather than try and explain how this class works, let us simply refer to the documentation of the DataPostprocessorVector class that has essentially this case as a well-documented example.
* 

* 
* [1.x.132]
* 
*  With this, the `output_results()` function becomes relatively straightforward: We use the DataOut class as we have in almost every one of the previous tutorial programs to output the solution (the "electric potential") and we use the postprocessor defined above to also output its gradient (the "electric field"). This all is then written into a file in VTU format after also associating the current time and time step number with this file.
* 

* 
* [1.x.133]
* 
*  Output the particle positions and properties is not more complicated. The  [2.x.140]  class plays the role of the DataOut class for particles, and all we have to do is tell that class where to take particles from and how to interpret the `dim` components of the properties
* 
*  -  namely, as a single vector indicating the velocity, rather than as `dim` scalar properties. The rest is then the same as above:
* 

* 
* [1.x.134]
* 
*   [1.x.135]  [1.x.136]
* 

* 
*  The last member function of the principal class of this program is then the driver. At the top, it refines the mesh a number of times by solving the problem (with not particles yet created) on a sequence of finer and finer meshes.
* 

* 
* [1.x.137]
* 
*  do a few refinement cycles up front
* 

* 
* [1.x.138]
* 
*  Now do the loop over time. The sequence of steps follows closely the outline of the algorithm discussed in the introduction. As discussed in great detail in the documentation of the DiscreteTime class, while we move the field and particle information forward by one time step, the time stored in the `time` variable is not consistent with where (some of) these quantities are (in the diction of DiscreteTime, this is the "update stage"). The call to `time.advance_time()` makes everything consistent again by setting the `time` variable to the time at which the field and particles already are, and once we are in this "consistent stage", we can generate graphical output and write information about the current state of the simulation to screen.
* 

* 
* [1.x.139]
* 
*   [1.x.140]  [1.x.141]
* 

* 
*  The final function of the program is then again the `main()` function. It is unchanged in all tutorial programs since  [2.x.141]  and so there is nothing new to discuss:
* 

* 
* [1.x.142]
* [1.x.143][1.x.144]
* 

* When this program is run, it produces output that looks as follows:```Timestep 1  Field degrees of freedom:                                 4989  Total number of particles in simulation:  20  Number of particles lost this time step:  0
*   Now at t=2.12647e-07, dt=2.12647e-07.
* Timestep 2  Field degrees of freedom:                 4989  Total number of particles in simulation:  24  Number of particles lost this time step:  0
*   Now at t=4.14362e-07, dt=2.01715e-07.
* Timestep 3  Field degrees of freedom:                 4989  Total number of particles in simulation:  28  Number of particles lost this time step:  0
*   Now at t=5.96019e-07, dt=1.81657e-07.
* Timestep 4  Field degrees of freedom:                 4989  Total number of particles in simulation:  32  Number of particles lost this time step:  0
*   Now at t=7.42634e-07, dt=1.46614e-07.
* 

* ...
* 

*   Timestep 1000  Field degrees of freedom:                 4989  Total number of particles in simulation:  44  Number of particles lost this time step:  6  Fraction of particles lost through anode: 0.0601266
*   Now at t=4.93276e-05, dt=4.87463e-08.
* Timestep 1001  Field degrees of freedom:                 4989  Total number of particles in simulation:  44  Number of particles lost this time step:  0  Fraction of particles lost through anode: 0.0601266
*   Now at t=4.93759e-05, dt=4.82873e-08.
* 

* ...
* 

* Timestep 2091  Field degrees of freedom:                 4989  Total number of particles in simulation:  44  Number of particles lost this time step:  0  Fraction of particles lost through anode: 0.0503338
*   Now at t=9.99237e-05, dt=4.26254e-08.
* Timestep 2092  Field degrees of freedom:                 4989  Total number of particles in simulation:  44  Number of particles lost this time step:  0  Fraction of particles lost through anode: 0.0503338
*   Now at t=9.99661e-05, dt=4.24442e-08.
* Timestep 2093  Field degrees of freedom:                 4989  Total number of particles in simulation:  44  Number of particles lost this time step:  2  Fraction of particles lost through anode: 0.050308
*   Now at t=0.0001, dt=3.38577e-08.```
* Picking a random few time steps, we can visualize the solution in theform of streamlines for the electric field and dots for the electrons: [2.x.142] 
* That said, a more appropriate way to visualize the results of thisprogram are by creating a video that shows how these electrons move, and howthe electric field changes in response to their motion:
* [1.x.145]
* 
* What you can see here is how the "focus element" of the boundary with its negativevoltage repels the electrons and makes sure that they do not just fly awayperpendicular from the cathode (as they do in the initial part of theirtrajectories). It also shows how the electric field linesmove around over time, in response to the charges flying by
* 
*  -  in other words,the feedback the particles have on the electric field that itself drives themotion of the electrons.
* The movie suggests that electrons move in "bunches" or "bursts". One element ofthis appearance is an artifact of how the movie was created: Every frame of themovie corresponds to one time step, but the time step length varies. More specifically,the fastest particle moving through the smallest cell determines the length of thetime step (see the discussion in the introduction), and consequently time stepsare small whenever a (fast) particle moves through the small cells at the rightedge of the domain; time steps are longer again once the particle has leftthe domain. This slowing-accelerating effect can easily be visualized by plottingthe time step length shown in the screen output.
* The second part of this is real, however: The simulation creates a large groupof particles in the beginning, and fewer after about the 300th time step. Thisis probably because of the negative charge of the particles in the simulation:They reduce the magnitude of the electric field at the (also negatively chargedelectrode) and consequently reduce the number of points on the cathode at whichthe magnitude exceeds the threshold necessary to draw an electron out of theelectrode.
* 

* [1.x.146][1.x.147][1.x.148]
* 

* [1.x.149][1.x.150]
* 

* The `assemble_system()`, `move_particles()`, and `update_timestep_size()`functions all call  [2.x.143]  and [2.x.144]  that query informationabout the particles located on the current cell. While this is convenient,it's also inefficient. To understand why this is so, one needs to knowhow particles are stored in  [2.x.145]  namely, in adata structure in which particles are ordered in some kind of linearfashion sorted by the cell they are on. Consequently, in order to findthe particles associated with a given cell, these functions need tosearch for the first (and possibly last) particle on a given cell
* 
*  - an effort that costs  [2.x.146]  operations where  [2.x.147]  is thenumber of particles. But this is repeated on every cell; assuming thatfor large computations, the number of cells and particles are roughlyproportional, the accumulated cost of these function calls is then [2.x.148]  and consequently larger than the  [2.x.149] cost that we should shoot for with all parts of a program.
* We can make this cheaper, though. First, instead of calling [2.x.150]  we might first call [2.x.151]  and then compute thenumber of particles on a cell by just computing the distance of the lastto the first particle on the current cell:
* [1.x.151]
* The first of these calls is of course still  [2.x.152] ,but at least the second call only takes a compute time proportional tothe number of particles on the current cell and so, when accumulatedover all cells, has a cost of  [2.x.153] .
* But we can even get rid of the first of these calls with some proper algorithmdesign. That's because particles are ordered in the same way as cells, and sowe can just walk them as we move along on the cells. The following outlineof an algorithm does this:
* [1.x.152]
* 
* In this code, we touch every cell exactly once and we never have to searchthe big data structure for the first or last particle on each cell. As aconsequence, the algorithm costs a total of  [2.x.154]  for a completesweep of all particles and all cells.
* It would not be very difficult to implement this scheme for all three of thefunctions in this program that have this issue.
* 

* [1.x.153][1.x.154]
* 

* The program already computes the fraction of the electrons that leave thedomain through the hole in the anode. But there are other quantities one might beinterested in. For example, the average velocity of these particles. It wouldnot be very difficult to obtain each particle's velocity from its properties,in the same way as we do in the `move_particles()` function, and computestatistics from it.
* 

* [1.x.155][1.x.156]
* 

* As discussed above, there is a varying time difference between different framesof the video because we create output for every time step. A better way tocreate movies would be to generate a new output file in fixed time intervals,regardless of how many time steps lie between each such point.
* 

* [1.x.157][1.x.158]
* 

* The problem we are considering in this program is a coupled, multiphysicsproblem. But the way we solve it is by first computing the (electric) potentialfield and then update the particle locations. This is what is called an"operator-splitting method", a concept we will investigate in more detailin  [2.x.155] .
* While it is awkward to think of a way to solve this problem that does not involvesplitting the problem into a PDE piece and a particles piece, one
*can* (and probably should!) think of a better way to update the particlelocations. Specifically, the equations we use to update the particle locationare
* [1.x.159]
* This corresponds to a simple forward-Euler time discretization
* 
*  -  a method offirst order accuracy in the time step size  [2.x.156]  that we know we shouldavoid because we can do better. Rather, one might want to consider a scheme suchas the[leapfrog scheme](https://en.wikipedia.org/wiki/Leapfrog_integration)or more generally[symplectic integrators](https://en.wikipedia.org/wiki/Symplectic_integrator)such as the[Verlet scheme](https://en.wikipedia.org/wiki/Verlet_integration).
* 

* [1.x.160][1.x.161]
* 

* In release mode, the program runs in about 3.5 minutes on one of the author'slaptops at the time of writing this. That's acceptable. But what if we wantedto make the simulation three-dimensional? If we wanted to not use a maximumof around 100 particles at any given time (as happens with the parametersused here) but 100,000? If we needed a substantially finer mesh?
* In those cases, one would want to run the program not just on a single processor,but in fact on as many as one has available. This requires parallelizationboth the PDE solution as well as over particles. In practice, while thereare substantial challenges to making this efficient and scale well, thesechallenges are all addressed in deal.II itself. For example,  [2.x.157]  showshow to parallelize the finite element part, and  [2.x.158]  shows how one canthen also parallelize the particles part.
* 

* [1.x.162][1.x.163] [2.x.159] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-20_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33]
* [1.x.34][1.x.35][1.x.36]
* 

*  [2.x.2] 
* This program is devoted to two aspects: the use of mixed finite elements
* 
*  -  inparticular Raviart-Thomas elements
* 
*  -  and using block matrices to definesolvers, preconditioners, and nested versions of those that use thesubstructure of the system matrix. The equation we are going to solve is againthe Poisson equation, though with a matrix-valued coefficient:[1.x.37]
*  [2.x.3]  is assumed to be uniformly positive definite, i.e., there is [2.x.4]  such that the eigenvalues  [2.x.5]  of  [2.x.6]  satisfy [2.x.7] . The use of the symbol  [2.x.8]  instead of the usual [2.x.9]  for the solution variable will become clear in the next section.
* After discussing the equation and the formulation we are going to use to solveit, this introduction will cover the use of block matrices and vectors, thedefinition of solvers and preconditioners, and finally the actual test case weare going to solve.
* We are going to extend this tutorial program in  [2.x.10]  tosolve not only the mixed Laplace equation, but add another equation thatdescribes the transport of a mixture of two fluids.
* The equations covered here fall into the class of vector-valued problems. Atoplevel overview of this topic can be found in the  [2.x.11]  module.
* 

* [1.x.38][1.x.39]
* 

* In the form above, the Poisson equation (i.e., the Laplace equation with a nonzeroright hand side) is generally considered a good model equationfor fluid flow in porous media. Of course, one typically models fluid flow throughthe [1.x.40] or, if fluid velocities are slow or the viscosity is large, the[1.x.41](which we cover in  [2.x.12] ).In the first of these two models, the forces that act are inertia andviscous friction, whereas in the second it is only viscous friction
* 
*  -  i.e.,forces that one fluid particle exerts on a nearby one. This is appropriateif you have free flow in a large domain, say a pipe, a river, or in the air.On the other hand, if the fluid is confined in pores, then friction forcesexerted by the pore walls on the fluid become more and more important andinternal viscous friction becomes less and less important. Modeling thisthen first leads to the[1.x.42] if both effects are important, and in the limit of very small poresto the [1.x.43].The latter is just a different name for the Poisson or Laplace equation,connotating it with the area to which one wants to apply it: slow flowin a porous medium. In essence it says that the velocity is proportionalto the negative pressure gradient that drives the fluid through theporous medium.
* The Darcy equation models this pressure that drives the flow. (Because thesolution variable is a pressure, we here use the name  [2.x.13]  instead of thename  [2.x.14]  more commonly used for the solution of partial differential equations.)Typical applications of this view of the Laplace equation are then modelinggroundwater flow, or the flow of hydrocarbons in oil reservoirs. In theseapplications,  [2.x.15]  is the permeability tensor, i.e., a measure for how muchresistance the soil or rock matrix asserts on the fluid flow.
* In the applications named above, a desirable feature for a numericalscheme is that it should be locally conservative, i.e., that whateverflows into a cell also flows out of it (or the difference is equal tothe integral over the source terms over each cell, if the sources arenonzero). However, as it turns out, the usual discretizations of theLaplace equation (such as those used in  [2.x.16] ,  [2.x.17] , or  [2.x.18] ) donot satisfy this property. But, one can achieve this by choosing adifferent formulation of the problem and a particular combination offinite element spaces.
* 

* [1.x.44][1.x.45]
* 

* To this end, one first introduces a second variable, called the velocity, [2.x.19] . By its definition, the velocity is a vector in thenegativedirection of the pressure gradient, multiplied by the permeability tensor. Ifthe permeability tensor is proportional to the unit matrix, this equation iseasy to understand and intuitive: the higher the permeability, the higher thevelocity; and the velocity is proportional to the gradient of the pressure, going fromareas of high pressure to areas of low pressure (thus the negative sign).
* With this second variable, one then finds an alternative version of theLaplace equation, called the [1.x.46]:[1.x.47]
* Here, we have multiplied the equation defining the velocity  [2.x.20]  by  [2.x.21]  because this makes the set of equations symmetric: oneof the equations has the gradient, the second the negative divergence,and these two are of course adjoints of each other, resulting in asymmetric bilinear form and a consequently symmetric system matrixunder the common assumption that  [2.x.22]  is a symmetric tensor.
* The weak formulation of this problem is found by multiplying the twoequations with test functions and integrating some terms by parts:[1.x.48]
* where[1.x.49]
* Here,  [2.x.23]  is the outward normal vector at the boundary. Note how in thisformulation, Dirichlet boundary values of the original problem areincorporated in the weak form.
* To be well-posed, we have to look for solutions and test functions in thespace  [2.x.24] for  [2.x.25] , [2.x.26] , and  [2.x.27]  for  [2.x.28] . It is a well-known fact stated inalmost every book on finite element theory that if one chooses discrete finiteelement spaces for the approximation of  [2.x.29]  inappropriately, then theresulting discrete problem is instable and the discrete solutionwill not converge to the exact solution. (Some details on the problemconsidered here
* 
*  -  which falls in the class of "saddle-point problems"
* 
*  -  can be found on the Wikipedia page on the [1.x.50].)
* To overcome this, a number of different finite element pairs for  [2.x.30] have been developed that lead to a stable discrete problem. One such pair isto use the Raviart-Thomas spaces  [2.x.31]  for the velocity  [2.x.32]  anddiscontinuous elements of class  [2.x.33]  for the pressure  [2.x.34] . For detailsabout these spaces, we refer in particular to the book on mixed finite elementmethods by Brezzi and Fortin, but many other books on the theory of finiteelements, for example the classic book by Brenner and Scott, also state therelevant results. In any case, with appropriate choices of functionspaces, the discrete formulation reads as follows: Find  [2.x.35]  so that[1.x.51]
* 
* 

* Before continuing, let us briefly pause and show that the choice offunction spaces above provides us with the desired local conservationproperty. In particular, because the pressure space consists ofdiscontinuous piecewise polynomials, we can choose the test function [2.x.36]  as the function that is equal to one on any given cell  [2.x.37]  andzero everywhere else. If we also choose  [2.x.38]  everywhere(remember that the weak form above has to hold for [1.x.52] discretetest functions  [2.x.39] ), then putting these choices of test functionsinto the weak formulation above implies in particular that[1.x.53]
* which we can of course write in more explicit form as[1.x.54]
* Applying the divergence theorem results in the fact that  [2.x.40]  has to satisfy, for every choice of cell  [2.x.41] , the relationship[1.x.55]
* If you now recall that  [2.x.42]  was the velocity, then theintegral on the left is exactly the (discrete) flux across theboundary of the cell  [2.x.43] . The statement is then that the flux must be equalto the integral over the sources within  [2.x.44] . In particular, if thereare no sources (i.e.,  [2.x.45]  in  [2.x.46] ), then the statement is that[1.x.56] flux is zero, i.e., whatever flows into a cell must flow outof it through some other part of the cell boundary. This is what we call[1.x.57] because it holds for every cell.
* On the other hand, the usual continuous  [2.x.47]  elements would not result inthis kind of property when used for the pressure (as, for example, wedo in  [2.x.48] ) because one can not choose a discrete test function [2.x.49]  that is one on a cell  [2.x.50]  and zero everywhere else: It would bediscontinuous and consequently not in the finite elementspace. (Strictly speaking, all we can say is that the proof abovewould not work for continuous elements. Whether these elements mightstill result in local conservation is a different question as onecould think that a different kind of proof might still work; inreality, however, the property really does not hold.)
* 

* 
* [1.x.58][1.x.59]
* 

* The deal.II library (of course) implements Raviart-Thomas elements  [2.x.51]  ofarbitrary order  [2.x.52] , as well as discontinuous elements  [2.x.53] . If we forgetabout their particular properties for a second, we then have to solve adiscrete problem[1.x.60]
* with the bilinear form and right hand side as stated above, and  [2.x.54] ,  [2.x.55] . Both  [2.x.56]  and  [2.x.57]  are from the space [2.x.58] , where  [2.x.59]  is itself a space of  [2.x.60] -dimensionalfunctions to accommodate for the fact that the flow velocity is vector-valued.The necessary question then is: how do we do this in a program?
* Vector-valued elements have already been discussed in previous tutorialprograms, the first time and in detail in  [2.x.61] . The main difference therewas that the vector-valued space  [2.x.62]  is uniform in all its components: the [2.x.63]  components of the displacement vector are all equal and from the samefunction space. What we could therefore do was to build  [2.x.64]  as the outerproduct of the  [2.x.65]  times the usual  [2.x.66]  finite element space, and by thismake sure that all our shape functions have only a single non-zero vectorcomponent. Instead of dealing with vector-valued shape functions, all we didin  [2.x.67]  was therefore to look at the (scalar) only non-zero component anduse the  [2.x.68]  call to figure outwhich component this actually is.
* This doesn't work with Raviart-Thomas elements: following from theirconstruction to satisfy certain regularity properties of the space [2.x.69] , the shape functions of  [2.x.70]  are usually nonzero in alltheir vector components at once. For this reason, were [2.x.71]  applied to determine the onlynonzero component of shape function  [2.x.72] , an exception would be generated. Whatwe really need to do is to get at  [2.x.73] all [2.x.74]  vector components of a shapefunction. In deal.II diction, we call such finite elements [2.x.75] non-primitive [2.x.76] , whereas finite elements that are either scalar or forwhich every vector-valued shape function is nonzero only in a single vectorcomponent are called  [2.x.77] primitive [2.x.78] .
* So what do we have to do for non-primitive elements? To figure this out, letus go back in the tutorial programs, almost to the very beginnings. There, welearned that we use the  [2.x.79]  class to determine the values andgradients of shape functions at quadrature points. For example, we would call [2.x.80]  to obtain the value of the [2.x.81] th shape function on the quadrature point with number [2.x.82] . Later, in  [2.x.83]  and other tutorial programs, we learnedthat this function call also works for vector-valued shape functions (ofprimitive finite elements), and that it returned the value of the onlynon-zero component of shape function  [2.x.84]  at quadrature point [2.x.85] .
* For non-primitive shape functions, this is clearly not going to work: there isno single non-zero vector component of shape function  [2.x.86] , and the callto  [2.x.87]  would consequently not makemuch sense. However, deal.II offers a second function call, [2.x.88]  that returns thevalue of the  [2.x.89]  atquadrature point  [2.x.90]  is an index betweenzero and the number of vector components of the present finite element; forexample, the element we will use to describe velocities and pressures is goingto have  [2.x.91]  components. It is worth noting that this function call canalso be used for primitive shape functions: it will simply return zero for allcomponents except one; for non-primitive shape functions, it will in generalreturn a non-zero value for more than just one component.
* We could now attempt to rewrite the bilinear form above in terms of vectorcomponents. For example, in 2d, the first term could be rewritten like this(note that  [2.x.92] ):[1.x.61]
* If we implemented this, we would get code like this:
* [1.x.62]
* 
* This is, at best, tedious, error prone, and not dimension independent. Thereare obvious ways to make things dimension independent, but in the end, thecode is simply not pretty. What would be much nicer is if we could simplyextract the  [2.x.93]  and  [2.x.94]  components of a shape function  [2.x.95] . In theprogram we do that in the following way:
* [1.x.63]
* 
* This is, in fact, not only the first term of the bilinear form, but thewhole thing (sans boundary contributions).
* What this piece of code does is, given an  [2.x.96]  object, to extractthe values of the first  [2.x.97]  components of shape function  [2.x.98]  atquadrature points  [2.x.99] , that is the velocity components of that shapefunction. Put differently, if we write shape functions  [2.x.100]  as the tuple [2.x.101] , then the function returns the velocity part of thistuple. Note that the velocity is of course a  [2.x.102] -dimensional tensor, andthat the function returns a corresponding object. Similarly, where wesubscript with the pressure extractor, we extract the scalar pressurecomponent. The whole mechanism is described in more detail in the [2.x.103]  module.
* In practice, it turns out that we can do a bit better if we evaluate the shapefunctions, their gradients and divergences only once per outermost loop, andstore the result, as this saves us a few otherwise repeated computations (it ispossible to save even more repeated operations by calculating all relevantquantities in advance and then only inserting the results in the actual loop,see  [2.x.104]  for a realization of that approach).The final result then looks like this, working in every space dimension:
* [1.x.64]
* 
* This very closely resembles the form in which we have originally written downthe bilinear form and right hand side.
* There is one final term that we have to take care of: the right hand sidecontained the term  [2.x.105] , constituting theweak enforcement of pressure boundary conditions. We have already seen in [2.x.106]  how to deal with face integrals: essentially exactly the same as withdomain integrals, except that we have to use the FEFaceValues classinstead of  [2.x.107] . To compute the boundary term we then simply haveto loop over all boundary faces and integrate there. The mechanism works inthe same way as above, i.e. the extractor classes also work on FEFaceValues objects:
* [1.x.65]
* 
* You will find the exact same code as above in the sources for the presentprogram. We will therefore not comment much on it below.
* 

* [1.x.66][1.x.67]
* 

* After assembling the linear system we are faced with the task of solvingit. The problem here is that the matrix possesses two undesirable properties:
* 
*  - It is [1.x.68],  i.e., it has both positive and negative eigenvalues.  We don't want to prove this property here, but note that this is true  for all matrices of the form   [2.x.108]   such as the one here where  [2.x.109]  is positive definite.
* 
*  - The matrix has a zero block at the bottom right (there is no term in  the bilinear form that couples the pressure  [2.x.110]  with the  pressure test function  [2.x.111] ).
* At least it is symmetric, but the first issue above still means thatthe Conjugate Gradient method is not going to work since it is onlyapplicable to problems in which the matrix is symmetric and positive definite.We would have to resort to other iterative solvers instead, such asMinRes, SymmLQ, or GMRES, that can deal with indefinite systems. However, thenthe next problem immediately surfaces: Due to the zero block, there are zeroson the diagonal and none of the usual, "simple" preconditioners (Jacobi, SSOR)will work as they require division by diagonal elements.
* For the matrix sizes we expect to run with this program, the by far simplestapproach would be to just use a direct solver (in particular, theSparseDirectUMFPACK class that is bundled with deal.II).  [2.x.112]  goes thisroute and shows that solving [1.x.69] linear system can be done in just3 or 4 lines of code.
* But then, this is a tutorial: We teach how to do things. Consequently,in the following, we will introduce some techniques that can be used in caseslike these. Namely, we will consider the linear system as not consisting of onelarge matrix and vectors, but we will want to decompose matricesinto [1.x.70] that correspond to the individual operators that appear inthe system. We note that the resulting solver is not optimal
* 
*  -  there aremuch better ways to efficiently compute the system, for example thoseexplained in the results section of  [2.x.113]  or the one we use in  [2.x.114] for a problem similar to the current one. Here, our goal is simply tointroduce new solution techniques and how they can be implemented indeal.II.
* 

* [1.x.71][1.x.72]
* 

* In view of the difficulties using standard solvers and preconditionersmentioned above, let us take another look at the matrix. If we sort ourdegrees of freedom so that all velocity come before all pressure variables,then we can subdivide the linear system  [2.x.115]  into the following blocks:[1.x.73]
* where  [2.x.116]  are the values of velocity and pressure degrees of freedom,respectively,  [2.x.117]  is the mass matrix on the velocity space,  [2.x.118]  corresponds tothe negative divergence operator, and  [2.x.119]  is its transpose and correspondsto the gradient.
* By block elimination, we can then re-order this system in the following way(multiply the first row of the system by  [2.x.120]  and then subtract thesecond row from it):[1.x.74]
* Here, the matrix  [2.x.121]  (called the[1.x.75]of  [2.x.122] )is obviously symmetric and, owing to the positive definiteness of  [2.x.123]  and thefact that  [2.x.124]  has full column rank,  [2.x.125]  is also positivedefinite.
* Consequently, if we could compute  [2.x.126] , we could apply the Conjugate Gradientmethod to it. However, computing  [2.x.127]  is expensive because it requires usto compute the inverse of the (possibly large) matrix  [2.x.128] ; and  [2.x.129]  is in factalso a full matrix because even though  [2.x.130]  is sparse, its inverse  [2.x.131] will generally be a dense matrix.On the other hand, the CG algorithm doesn't requireus to actually have a representation of  [2.x.132] : It is sufficient to formmatrix-vector products with it. We can do so in steps, using the fact thatmatrix products are associative (i.e., we can set parentheses in such away that the product is more convenient to compute):To compute  [2.x.133] , we [2.x.134]   [2.x.135]  compute  [2.x.136] ;  [2.x.137]  solve  [2.x.138]  for  [2.x.139] , using the CG method applied to the  positive definite and symmetric mass matrix  [2.x.140] ;  [2.x.141]  compute  [2.x.142]  to obtain  [2.x.143] . [2.x.144] Note how we evaluate the expression  [2.x.145]  right to left toavoid matrix-matrix products; this way, all we have to do is evaluatematrix-vector products.
* In the following, we will then have to come up with ways to represent thematrix  [2.x.146]  so that it can be used in a Conjugate Gradient solver,as well as to define ways in which we can precondition the solutionof the linear system involving  [2.x.147] , and deal with solving linear systemswith the matrix  [2.x.148]  (the second step above).
*  [2.x.149]  The key point in this consideration is to recognize that to implementan iterative solver such as CG or GMRES, we never actually need the actual[1.x.76] of a matrix! All that is required is that we can formmatrix-vector products. The same is true for preconditioners. In deal.II weencode this requirement by only requiring that matrices and preconditionersgiven to solver classes have a  [2.x.150]  member function thatdoes the matrix-vector product. How a class chooses to implement thisfunction is not important to the solver. Consequently, classes canimplement it by, for example, doing a sequence of products and linearsolves as discussed above.
* 

* [1.x.77][1.x.78]
* 

* deal.II includes support for describing such linear operations in a verygeneral way. This is done with the LinearOperator class that, like [2.x.151]  "the MatrixType concept",defines a minimal interface for [1.x.79] a linear operation to avector:
* [1.x.80]
* The key difference between a LinearOperator and an ordinary matrix ishowever that a LinearOperator does not allow any further access to theunderlying object. All you can do with a LinearOperator is to apply its"action" to a vector! We take the opportunity to introduce theLinearOperator concept at this point because it is a very useful tool thatallows you to construct complex solvers and preconditioners in a veryintuitive manner.
* As a first example let us construct a LinearOperator object that represents [2.x.152] . This means that whenever the  [2.x.153]  function ofthis operator is called it has to solve a linear system. This requires usto specify a solver (and corresponding) preconditioner. Assuming that [2.x.154]  is a reference to the upper left block of the system matrixwe can write:
* [1.x.81]
* Rather than using a SolverControl we use the ReductionControl class herethat stops iterations when either an absolute tolerance is reached (forwhich we choose  [2.x.155] ) or when the residual is reduced by a certainfactor (here,  [2.x.156] ). In contrast the SolverControl class only checksfor absolute tolerances. We have to use ReductionControl in our case towork around a minor issue: The right hand sides that we  will feed to [2.x.157]  are essentially formed by residuals that naturallydecrease vastly in norm as the outer iterations progress. This makescontrol by an absolute tolerance very error prone.
* We now have a LinearOperator  [2.x.158]  that we can use toconstruct more complicated operators such as the Schur complement  [2.x.159] .Assuming that  [2.x.160]  is a reference to the upper right blockconstructing a LinearOperator  [2.x.161]  is a matter of two lines:
* [1.x.82]
* Here, the multiplication of three LinearOperator objects yields a compositeobject  [2.x.162]  function first applies [2.x.163] , then  [2.x.164]  (i.e. solving an equation with  [2.x.165] ), and finally  [2.x.166] to any given input vector. In that sense  [2.x.167]  issimilar to the following code:
* [1.x.83]
* ( [2.x.168]  are two temporary vectors). Thekey point behind this approach is the fact that we never actually create aninner product of matrices. Instead, whenever we have to perform a matrixvector multiplication with  [2.x.169]  we simply run all individual [2.x.170]  operations in above sequence.
*  [2.x.171]  We could have achieved the same goal of creating a "matrix like"object by implementing a specialized class  [2.x.172] that provides a suitable  [2.x.173]  function. Skipping over somedetails this might have looked like the following:
* [1.x.84]
* Even though both approaches are exactly equivalent, the LinearOperatorclass has a big advantage over this manual approach.It provides so-called[1.x.85][1.x.86]:Mathematically, we think about  [2.x.174]  as being the composite matrix [2.x.175]  and the LinearOperator class allows you to write this outmore or less verbatim,
* [1.x.87]
* The manual approach on the other hand obscures this fact.
* All that is left for us to do now is to form the right hand sides of thetwo equations defining  [2.x.176]  and  [2.x.177] , and then solve them with the Schurcomplement matrix and the mass matrix, respectively. For example the righthand side of the first equation reads  [2.x.178] . This could beimplemented as follows:
* [1.x.88]
* Again, this is a perfectly valid approach, but the fact that deal.IIrequires us to manually resize the final and temporary vector, and thatevery operation takes up a new line makes this hard to read. This is thepoint where a second class in the linear operator framework can will helpus. Similarly in spirit to LinearOperator, a PackagedOperation stores a"computation":
* [1.x.89]
* The class allows[1.x.90]of expressions involving vectors and linear operators. This is done bystoring the computational expression and only performing the computationwhen either the object is converted to a vector object, or [2.x.179]  (or  [2.x.180]  is invokedby hand. Assuming that  [2.x.181]  are the twovectors of the right hand side we can simply write:
* [1.x.91]
* Here,  [2.x.182]  is a PackagedOperation that [1.x.92] thecomputation we specified. It does not create a vector with the actualresult immediately.
* With these prerequisites at hand, solving for  [2.x.183]  and  [2.x.184]  is a matter ofcreating another solver and inverse:
* [1.x.93]
* 
*  [2.x.185]  The functionality that we developed in this example step by hand isalready readily available in the library. Have a look atschur_complement(), condense_schur_rhs(), and postprocess_schur_solution().
* 

* [1.x.94][1.x.95]
* 

* One may ask whether it would help if we had a preconditioner for the Schurcomplement  [2.x.186] . The general answer, as usual, is: of course. Theproblem is only, we don't know anything about this Schur complement matrix. Wedo not know its entries, all we know is its action. On the other hand, we haveto realize that our solver is expensive since in each iteration we have to doone matrix-vector product with the Schur complement, which means that we haveto do invert the mass matrix once in each iteration.
* There are different approaches to preconditioning such a matrix. On the oneextreme is to use something that is cheap to apply and therefore has no realimpact on the work done in each iteration. The other extreme is apreconditioner that is itself very expensive, but in return really brings downthe number of iterations required to solve with  [2.x.187] .
* We will try something along the second approach, as much to improve theperformance of the program as to demonstrate some techniques. To this end, letus recall that the ideal preconditioner is, of course,  [2.x.188] , but that isunattainable. However, how about[1.x.96]
* as a preconditioner? That would mean that every time we have to do onepreconditioning step, we actually have to solve with  [2.x.189] . At first,this looks almost as expensive as solving with  [2.x.190]  right away. However, notethat in the inner iteration, we do not have to calculate  [2.x.191] , but onlythe inverse of its diagonal, which is cheap.
* Thankfully, the LinearOperator framework makes this very easy to write out.We already used a Jacobi preconditioner ( [2.x.192] ) forthe  [2.x.193]  matrix earlier. So all that is left to do is to write out how theapproximate Schur complement should look like:
* [1.x.97]
* Note how this operator differs in simply doing one Jacobi sweep(i.e. multiplying with the inverses of the diagonal) instead of multiplyingwith the full  [2.x.194] . (This is how a single Jacobi preconditioner stepwith  [2.x.195]  is defined: it is the multiplication with the inverse of thediagonal of  [2.x.196] ; in other words, the operation  [2.x.197] on a vector  [2.x.198]  is exactly what PreconditionJacobi does.)
* With all this we almost have the preconditioner completed: it should be theinverse of the approximate Schur complement. We implement this again bycreating a linear operator with inverse_operator() function. This timehowever we would like to choose a relatively modest tolerance for the CGsolver (that inverts  [2.x.199] ). The reasoning is that [2.x.200] , so weactually do not need to invert it exactly. This, however creates a subtleproblem:  [2.x.201]  will be used in the final outer CGiteration to create an orthogonal basis. But for this to work, it must beprecisely the same linear operation for every invocation. We ensure this byusing an IterationNumberControl that allows us to fix the number of CGiterations that are performed to a fixed small number (in our case 30):
* [1.x.98]
* 
* That's all!
* Obviously, applying this inverse of the approximate Schur complement is a veryexpensive preconditioner, almost as expensive as inverting the Schurcomplement itself. We can expect it to significantly reduce the number ofouter iterations required for the Schur complement. In fact it does: in atypical run on 7 times refined meshes using elements of order 0, the number ofouter iterations drops from 592 to 39. On the other hand, we now have to applya very expensive preconditioner 25 times. A better measure is therefore simplythe run-time of the program: on a current laptop (as of January 2019), itdrops from 3.57 to 2.05 seconds for this test case. That doesn't seem tooimpressive, but the savings become more pronounced on finer meshes and withelements of higher order. For example, an seven times refined mesh andusing elements of order 2 (which amounts to about 0.4 million degrees offreedom) yields an improvement of 1134 to 83 outer iterations, at a runtimeof 168 seconds to 40 seconds. Not earth shattering, but significant.
* 

* [1.x.99][1.x.100]
* 

* In this tutorial program, we will solve the Laplace equation in mixedformulation as stated above. Since we want to monitor convergence of thesolution inside the program, we choose right hand side, boundary conditions,and the coefficient so that we recover a solution function known to us. Inparticular, we choose the pressure solution[1.x.101]
* and for the coefficient we choose the unit matrix  [2.x.202]  forsimplicity. Consequently, the exact velocity satisfies[1.x.102]
* This solution was chosen since it is exactly divergence free, making it arealistic test case for incompressible fluid flow. By consequence, the righthand side equals  [2.x.203] , and as boundary values we have to choose [2.x.204] .
* For the computations in this program, we choose  [2.x.205] . You canfind the resulting solution in the [1.x.103], after the commented program.
* 

*  [1.x.104] [1.x.105]
*   [1.x.106]  [1.x.107]
* 

* 
*  Since this program is only an adaptation of  [2.x.206] , there is not much new stuff in terms of header files. In deal.II, we usually list include files in the order base-lac-grid-dofs-fe-numerics, followed by C++ standard include files:
* 

* 
* [1.x.108]
* 
*  The only two new header files that deserve some attention are those for the LinearOperator and PackagedOperation classes:
* 

* 
* [1.x.109]
* 
*  This is the only significant new header, namely the one in which the Raviart-Thomas finite element is declared:
* 

* 
* [1.x.110]
* 
*  Finally, as a bonus in this program, we will use a tensorial coefficient. Since it may have a spatial dependence, we consider it a tensor-valued function. The following include file provides the  [2.x.207]  class that offers such functionality:
* 

* 
* [1.x.111]
* 
*  The last step is as in all previous programs: We put all of the code relevant to this program into a namespace. (This idea was first introduced in  [2.x.208] .)
* 

* 
* [1.x.112]
* 
*   [1.x.113]  [1.x.114]
* 

* 
*  Again, since this is an adaptation of  [2.x.209] , the main class is almost the same as the one in that tutorial program. In terms of member functions, the main differences are that the constructor takes the degree of the Raviart-Thomas element as an argument (and that there is a corresponding member variable to store this value) and the addition of the  [2.x.210]  function in which, no surprise, we will compute the difference between the exact and the numerical solution to determine convergence of our computations:
* 

* 
* [1.x.115]
* 
*  The second difference is that the sparsity pattern, the system matrix, and solution and right hand side vectors are now blocked. What this means and what one can do with such objects is explained in the introduction to this program as well as further down below when we explain the linear solvers and preconditioners for this problem:
* 

* 
* [1.x.116]
* 
*   [1.x.117]  [1.x.118]
* 

* 
*  Our next task is to define the right hand side of our problem (i.e., the scalar right hand side for the pressure in the original Laplace equation), boundary values for the pressure, and a function that describes both the pressure and the velocity of the exact solution for later computations of the error. Note that these functions have one, one, and  [2.x.211]  components, respectively, and that we pass the number of components down to the  [2.x.212]  base class. For the exact solution, we only declare the function that actually returns the entire solution vector (i.e. all components of it) at once. Here are the respective declarations:
* 

* 
* [1.x.119]
* 
*  And then we also have to define these respective functions, of course. Given our discussion in the introduction of how the solution should look, the following computations should be straightforward:
* 

* 
* [1.x.120]
* 
*   [1.x.121]  [1.x.122]
* 

* 
*  In addition to the other equation data, we also want to use a permeability tensor, or better
* 
*  -  because this is all that appears in the weak form
* 
*  -  the inverse of the permeability tensor,  [2.x.213] . For the purpose of verifying the exactness of the solution and determining convergence orders, this tensor is more in the way than helpful. We will therefore simply set it to the identity matrix.     
*   However, a spatially varying permeability tensor is indispensable in real-life porous media flow simulations, and we would like to use the opportunity to demonstrate the technique to use tensor valued functions.     
*   Possibly unsurprisingly, deal.II also has a base class not only for scalar and generally vector-valued functions (the  [2.x.214]  base class) but also for functions that return tensors of fixed dimension and rank, the  [2.x.215]  template. Here, the function under consideration returns a dim-by-dim matrix, i.e. a tensor of rank 2 and dimension  [2.x.216] . We then choose the template arguments of the base class appropriately.     
*   The interface that the  [2.x.217]  class provides is essentially equivalent to the  [2.x.218]  class. In particular, there exists a  [2.x.219]  function that takes a list of points at which to evaluate the function, and returns the values of the function in the second argument, a list of tensors:
* 

* 
* [1.x.123]
* 
*  The implementation is less interesting. As in previous examples, we add a check to the beginning of the class to make sure that the sizes of input and output parameters are the same (see  [2.x.220]  for a discussion of this technique). Then we loop over all evaluation points, and for each one set the output tensor to the identity matrix.     
*   There is an oddity at the top of the function (the `(void)points;` statement) that is worth discussing. The values we put into the output `values` array does not actually depend on the `points` arrays of coordinates at which the function is evaluated. In other words, the `points` argument is in fact unused, and we could have just not given it a name if we had wanted. But we want to use the `points` object for checking that the `values` object has the correct size. The problem is that in release mode, `AssertDimension` is defined as a macro that expands to nothing; the compiler will then complain that the `points` object is unused. The idiomatic approach to silencing this warning is to have a statement that evaluates (reads) variable but doesn't actually do anything: That's what `(void)points;` does: It reads from `points`, and then casts the result of the read to `void`, i.e., nothing. This statement is, in other words, completely pointless and implies no actual action except to explain to the compiler that yes, this variable is in fact used even in release mode. (In debug mode, the `AssertDimension` macro expands to something that reads from the variable, and so the funny statement would not be necessary in debug mode.)
* 

* 
* [1.x.124]
* 
*   [1.x.125]  [1.x.126]
* 

* 
*   [1.x.127]  [1.x.128]
* 

* 
*  In the constructor of this class, we first store the value that was passed in concerning the degree of the finite elements we shall use (a degree of zero, for example, means to use RT(0) and DG(0)), and then construct the vector valued element belonging to the space  [2.x.221]  described in the introduction. The rest of the constructor is as in the early tutorial programs.   
*   The only thing worth describing here is the constructor call of the  [2.x.222]  class to which this variable belongs has a number of different constructors that all refer to binding simpler elements together into one larger element. In the present case, we want to couple a single RT(degree) element with a single DQ(degree) element. The constructor to  [2.x.223]  that does this requires us to specify first the first base element (the  [2.x.224]  object of given degree) and then the number of copies for this base element, and then similarly the kind and number of  [2.x.225]  elements. Note that the Raviart-Thomas element already has  [2.x.226]  vector components, so that the coupled element will have  [2.x.227]  vector components, the first  [2.x.228]  of which correspond to the velocity variable whereas the last one corresponds to the pressure.   
*   It is also worth comparing the way we constructed this element from its base elements, with the way we have done so in  [2.x.229] : there, we have built it as  [2.x.230] , i.e. we have simply used  [2.x.231]  element, one copy for the displacement in each coordinate direction.
* 

* 
* [1.x.129]
* 
*   [1.x.130]  [1.x.131]
* 

* 
*  This next function starts out with well-known functions calls that create and refine a mesh, and then associate degrees of freedom with it:
* 

* 
* [1.x.132]
* 
*  However, then things become different. As mentioned in the introduction, we want to subdivide the matrix into blocks corresponding to the two different kinds of variables, velocity and pressure. To this end, we first have to make sure that the indices corresponding to velocities and pressures are not intermingled: First all velocity degrees of freedom, then all pressure DoFs. This way, the global matrix separates nicely into a  [2.x.232]  system. To achieve this, we have to renumber degrees of freedom based on their vector component, an operation that conveniently is already implemented:
* 

* 
* [1.x.133]
* 
*  The next thing is that we want to figure out the sizes of these blocks so that we can allocate an appropriate amount of space. To this end, we call the  [2.x.233]  function that counts how many shape functions are non-zero for a particular vector component. We have  [2.x.234]  vector components, and  [2.x.235]  will count how many shape functions belong to each of these components.     
*   There is one problem here. As described in the documentation of that function, it [1.x.134] to put the number of  [2.x.236] -velocity shape functions into  [2.x.237] , the number of  [2.x.238] -velocity shape functions into  [2.x.239]  (and similar in 3d), and the number of pressure shape functions into  [2.x.240] . But, the Raviart-Thomas element is special in that it is non- [2.x.241]  "primitive", i.e., for Raviart-Thomas elements all velocity shape functions are nonzero in all components. In other words, the function cannot distinguish between  [2.x.242]  and  [2.x.243]  velocity functions because there [1.x.135] no such distinction. It therefore puts the overall number of velocity into each of  [2.x.244] ,  [2.x.245] . On the other hand, the number of pressure variables equals the number of shape functions that are nonzero in the dim-th component.     
*   Using this knowledge, we can get the number of velocity shape functions from any of the first  [2.x.246]  elements of  [2.x.247] , and then use this below to initialize the vector and matrix block sizes, as well as create output.     
*  

* 
*  [2.x.248]  If you find this concept difficult to understand, you may want to consider using the function  [2.x.249]  instead, as we do in the corresponding piece of code in  [2.x.250] . You might also want to read up on the difference between  [2.x.251]  "blocks" and  [2.x.252]  "components" in the glossary.
* 

* 
* [1.x.136]
* 
*  The next task is to allocate a sparsity pattern for the matrix that we will create. We use a compressed sparsity pattern like in the previous steps, but as  [2.x.253]  is a block matrix we use the class  [2.x.254]  instead of just  [2.x.255] . This block sparsity pattern has four blocks in a  [2.x.256]  pattern. The blocks' sizes depend on  [2.x.257] , which hold the number of velocity and pressure variables. In the second step we have to instruct the block system to update its knowledge about the sizes of the blocks it manages; this happens with the  [2.x.258]  call.
* 

* 
* [1.x.137]
* 
*  We use the compressed block sparsity pattern in the same way as the non-block version to create the sparsity pattern and then the system matrix:
* 

* 
* [1.x.138]
* 
*  Then we have to resize the solution and right hand side vectors in exactly the same way as the block compressed sparsity pattern:
* 

* 
* [1.x.139]
* 
*   [1.x.140]  [1.x.141]
* 

* 
*  Similarly, the function that assembles the linear system has mostly been discussed already in the introduction to this example. At its top, what happens are all the usual steps, with the addition that we do not only allocate quadrature and  [2.x.259]  objects for the cell terms, but also for face terms. After that, we define the usual abbreviations for variables, and the allocate space for the local matrix and right hand side contributions, and the array that holds the global numbers of the degrees of freedom local to the present cell.
* 

* 
* [1.x.142]
* 
*  The next step is to declare objects that represent the source term, pressure boundary value, and coefficient in the equation. In addition to these objects that represent continuous functions, we also need arrays to hold their values at the quadrature points of individual cells (or faces, for the boundary values). Note that in the case of the coefficient, the array has to be one of matrices.
* 

* 
* [1.x.143]
* 
*  Finally, we need a couple of extractors that we will use to get at the velocity and pressure components of vector-valued shape functions. Their function and use is described in detail in the  [2.x.260]  vector_valued report. Essentially, we will use them as subscripts on the FEValues objects below: the FEValues object describes all vector components of shape functions, while after subscription, it will only refer to the velocities (a set of  [2.x.261]  components starting at component zero) or the pressure (a scalar component located at position  [2.x.262] ):
* 

* 
* [1.x.144]
* 
*  With all this in place, we can go on with the loop over all cells. The body of this loop has been discussed in the introduction, and will not be commented any further here:
* 

* 
* [1.x.145]
* 
*  The final step in the loop over all cells is to transfer local contributions into the global matrix and right hand side vector. Note that we use exactly the same interface as in previous examples, although we now use block matrices and vectors instead of the regular ones. In other words, to the outside world, block objects have the same interface as matrices and vectors, but they additionally allow to access individual blocks.
* 

* 
* [1.x.146]
* 
*   [1.x.147]  [1.x.148]
* 

* 
*  The linear solvers and preconditioners we use in this example have been discussed in significant detail already in the introduction. We will therefore not discuss the rationale for our approach here any more, but rather only comment on some remaining implementational aspects.
* 

* 
*   [1.x.149]  [1.x.150]
* 

* 
*  As already outlined in the introduction, the solve function consists essentially of two steps. First, we have to form the first equation involving the Schur complement and solve for the pressure (component 1 of the solution). Then, we can reconstruct the velocities from the second equation (component 0 of the solution).
* 

* 
* [1.x.151]
* 
*  As a first step we declare references to all block components of the matrix, the right hand side and the solution vector that we will need.
* 

* 
* [1.x.152]
* 
*  Then, we will create corresponding LinearOperator objects and create the  [2.x.263]  operator:
* 

* 
* [1.x.153]
* 
*  This allows us to declare the Schur complement  [2.x.264]  and the approximate Schur complement  [2.x.265] :
* 

* 
* [1.x.154]
* 
*  We now create a preconditioner out of  [2.x.266]  that applies a fixed number of 30 (inexpensive) CG iterations:
* 

* 
* [1.x.155]
* 
*  Now on to the first equation. The right hand side of it is  [2.x.267] , which is what we compute in the first few lines. We then solve the first equation with a CG solver and the preconditioner we just declared.
* 

* 
* [1.x.156]
* 
*  After we have the pressure, we can compute the velocity. The equation reads  [2.x.268] , and we solve it by first computing the right hand side, and then multiplying it with the object that represents the inverse of the mass matrix:
* 

* 
* [1.x.157]
* 
*   [1.x.158]  [1.x.159]
* 

* 
*   [1.x.160]  [1.x.161]
* 

* 
*  After we have dealt with the linear solver and preconditioners, we continue with the implementation of our main class. In particular, the next task is to compute the errors in our numerical solution, in both the pressures as well as velocities.   
*   To compute errors in the solution, we have already introduced the  [2.x.269]  function in  [2.x.270]  and  [2.x.271] . However, there we only dealt with scalar solutions, whereas here we have a vector-valued solution with components that even denote different quantities and may have different orders of convergence (this isn't the case here, by choice of the used finite elements, but is frequently the case in mixed finite element applications). What we therefore have to do is to `mask' the components that we are interested in. This is easily done: the  [2.x.272]  function takes as one of its arguments a pointer to a weight function (the parameter defaults to the null pointer, meaning unit weights). What we have to do is to pass a function object that equals one in the components we are interested in, and zero in the other ones. For example, to compute the pressure error, we should pass a function that represents the constant vector with a unit value in component  [2.x.273] , whereas for the velocity the constant vector should be one in the first  [2.x.274]  components, and zero in the location of the pressure.   
*   In deal.II, the  [2.x.275]  does exactly this: it wants to know how many vector components the function it is to represent should have (in our case this would be  [2.x.276] , for the joint velocity-pressure space) and which individual or range of components should be equal to one. We therefore define two such masks at the beginning of the function, following by an object representing the exact solution and a vector in which we will store the cellwise errors as computed by  [2.x.277] :
* 

* 
* [1.x.162]
* 
*  As already discussed in  [2.x.278] , we have to realize that it is impossible to integrate the errors exactly. All we can do is approximate this integral using quadrature. This actually presents a slight twist here: if we naively chose an object of type  [2.x.279]  as one may be inclined to do (this is what we used for integrating the linear system), one realizes that the error is very small and does not follow the expected convergence curves at all. What is happening is that for the mixed finite elements used here, the Gauss points happen to be superconvergence points in which the pointwise error is much smaller (and converges with higher order) than anywhere else. These are therefore not particularly good points for integration. To avoid this problem, we simply use a trapezoidal rule and iterate it  [2.x.280]  times in each coordinate direction (again as explained in  [2.x.281] ):
* 

* 
* [1.x.163]
* 
*  With this, we can then let the library compute the errors and output them to the screen:
* 

* 
* [1.x.164]
* 
*   [1.x.165]  [1.x.166]
* 

* 
*  The last interesting function is the one in which we generate graphical output. Note that all velocity components get the same solution name "u". Together with using  [2.x.282]  this will cause  [2.x.283]  to generate a vector representation of the individual velocity components, see  [2.x.284]  or the  [2.x.285]  "Generating graphical output" section of the  [2.x.286]  module for more information. Finally, it seems inappropriate for higher order elements to only show a single bilinear quadrilateral per cell in the graphical output. We therefore generate patches of size (degree+1)x(degree+1) to capture the full information content of the solution. See the  [2.x.287]  tutorial program for more information on this.
* 

* 
* [1.x.167]
* 
*   [1.x.168]  [1.x.169]
* 

* 
*  This is the final function of our main class. It's only job is to call the other functions in their natural order:
* 

* 
* [1.x.170]
* 
*   [1.x.171]  [1.x.172]
* 

* 
*  The main function we stole from  [2.x.288]  instead of  [2.x.289] . It is almost equal to the one in  [2.x.290]  (apart from the changed class names, of course), the only exception is that we pass the degree of the finite element space to the constructor of the mixed Laplace problem (here, we use zero-th order elements).
* 

* 
* [1.x.173]
* [1.x.174][1.x.175]
* 

* [1.x.176][1.x.177]
* 

* 
* If we run the program as is, we get this output for the  [2.x.291] mesh we use (for a total of 1024 cells with 1024 pressure degrees offreedom since we use piecewise constants, and 2112 velocities becausethe Raviart-Thomas element defines one degree per freedom per face andthere are  [2.x.292]  faces parallel to the  [2.x.293] -axis and the samenumber parallel to the  [2.x.294] -axis):
* [1.x.178]
* 
* The fact that the number of iterations is so small, of course, is due tothe good (but expensive!) preconditioner we have developed. To getconfidence in the solution, let us take a look at it. The following threeimages show (from left to right) the x-velocity, the y-velocity, and thepressure:
*  [2.x.295] 
* 

* 
* Let us start with the pressure: it is highest at the left and lowest at theright, so flow will be from left to right. In addition, though hardly visiblein the graph, we have chosen the pressure field such that the flow left-rightflow first channels towards the center and then outward again. Consequently,the x-velocity has to increase to get the flow through the narrow part,something that can easily be seen in the left image. The middle imagerepresents inward flow in y-direction at the left end of the domain, andoutward flow in y-direction at the right end of the domain.
* 

* 
* As an additional remark, note how the x-velocity in the left image is onlycontinuous in x-direction, whereas the y-velocity is continuous iny-direction. The flow fields are discontinuous in the other directions. Thisvery obviously reflects the continuity properties of the Raviart-Thomaselements, which are, in fact, only in the space H(div) and not in the space [2.x.296] . Finally, the pressure field is completely discontinuous, butthat should not surprise given that we have chosen  [2.x.297]  asthe finite element for that solution component.
* 

* 
* [1.x.179][1.x.180]
* 

* 
* The program offers two obvious places where playing and observing convergenceis in order: the degree of the finite elements used (passed to the constructorof the  [2.x.298] ), andthe refinement level (determined in [2.x.299] ). What one can do is tochange these values and observe the errors computed later on in the course ofthe program run.
* 

* 
* If one does this, one finds the following pattern for the  [2.x.300]  errorin the pressure variable: [2.x.301] 
* The theoretically expected convergence orders are very nicely reflected by theexperimentally observed ones indicated in the last row of the table.
* 

* 
* One can make the same experiment with the  [2.x.302]  errorin the velocity variables: [2.x.303] The result concerning the convergence order is the same here.
* 

* 
* [1.x.181][1.x.182][1.x.183]
* 

* [1.x.184][1.x.185]
* 

* Realistic flow computations for ground water or oil reservoir simulations willnot use a constant permeability. Here's a first, rather simple way to changethis situation: we use a permeability that decays very rapidly away from acentral flowline until it hits a background value of 0.001. This is to mimicthe behavior of fluids in sandstone: in most of the domain, the sandstone ishomogeneous and, while permeable to fluids, not overly so; on the other stone,the stone has cracked, or faulted, along one line, and the fluids flow mucheasier along this large crack. Here is how we could implement something likethis:
* [1.x.186]
* Remember that the function returns the inverse of the permeability tensor.
* 

* 
* With a significantly higher mesh resolution, we can visualize this, here withx- and y-velocity:
*  [2.x.304] 
* It is obvious how fluids flow essentially only along the middle line, and notanywhere else.
* 

* 
* Another possibility would be to use a random permeability field. A simple wayto achieve this would be to scatter a number of centers around the domain andthen use a permeability field that is the sum of (negative) exponentials foreach of these centers. Flow would then try to hop from one center of highpermeability to the next one. This is an entirely unscientific attempt atdescribing a random medium, but one possibility to implement this behaviorwould look like this:
* [1.x.187]
* 
* A piecewise constant interpolation of the diagonal elements of theinverse of this tensor (i.e., of  [2.x.305] )looks as follows:
*  [2.x.306] 
* 

* With a permeability field like this, we would get x-velocities and pressures asfollows:
*  [2.x.307] 
* We will use these permeability fields again in  [2.x.308]  and  [2.x.309] .
* 

* [1.x.188][1.x.189]
* 

* As mentioned in the introduction, the Schur complement solver used here is notthe best one conceivable (nor is it intended to be a particularly goodone). Better ones can be found in the literature and can be built using thesame block matrix techniques that were introduced here. We pick up on thistheme again in  [2.x.310] , where we first build a Schur complement solver for theStokes equation as we did here, and then in the [1.x.190] section discuss betterways based on solving the system as a whole but preconditioning based onindividual blocks. We will also come back to this in  [2.x.311] .
* 

* [1.x.191][1.x.192] [2.x.312] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-2_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12]
* [1.x.13][1.x.14][1.x.15]
* 

*  [2.x.2] 
* After we have created a grid in the previous example, we now show howto define degrees of freedom on this mesh. For this example, wewill use the lowest order ( [2.x.3] ) finite elements, for which the degreesof freedom are associated with the vertices of the mesh. Laterexamples will demonstrate higher order elements where degrees of freedom arenot necessarily associated with vertices any more, but can be associatedwith edges, faces, or cells.
* The term "degree of freedom" is commonly used in the finite element communityto indicate two slightly different, but related things. The first is that we'dlike to represent the finite element solution as a linear combination of shapefunctions, in the form  [2.x.4] . Here,  [2.x.5]  is a vector of expansion coefficients. Because we don't knowtheir values yet (we will compute them as the solution of a linear ornonlinear system), they are called "unknowns" or "degrees of freedom". Thesecond meaning of the term can be explained as follows: A mathematicaldescription of finite element problems is often to say that we are looking fora finite dimensional function  [2.x.6]  that satisfies some set of equations(e.g.  [2.x.7]  for all test functions  [2.x.8] ). In other words, all we say here that the solution needs to lie in somespace  [2.x.9] . However, to actually solve this problem on a computer we need tochoose a basis of this space; this is the set of shape functions [2.x.10]  we have used above in the expansion of  [2.x.11] with coefficients  [2.x.12] . There are of course many bases of the space  [2.x.13] ,but we will specifically choose the one that is described by the finiteelement functions that are traditionally defined locally on the cells of themesh. Describing "degrees of freedom" in this context requires us to simply[1.x.16] the basis functions of the space  [2.x.14] . For  [2.x.15]  elementsthis means simply enumerating the vertices of the mesh in some way, but forhigher order elements, one also has to enumerate the shape functions that areassociated with edges, faces, or cell interiors of the mesh. In other words,the enumeration of degrees of freedom is an entirely separate thing from theindices we use for vertices. The class thatprovides this enumeration of the basis functions of  [2.x.16]  is called DoFHandler.
* Defining degrees of freedom ("DoF"s in short) on a mesh is a rathersimple task, since the library does all the work for you. Essentially,all you have to do is create a finite element object (from one of themany finite element classes deal.II already has, see for example the [2.x.17]  documentation) and give it to a DoFHandler object through the [2.x.18]  function ("distributing DoFs" is the term we useto describe the process of [1.x.17] the basis functions as discussedabove). The DoFHandler is a class thatknows which degrees of freedom live where, i.e., it can answerquestions like "how many degrees of freedom are there globally" and"on this cell, give me the global indices of the shape functions thatlive here". This is the sort of information you need when determininghow big your system matrix should be, and when copying thecontributions of a single cell into the global matrix.
* [1.x.18][1.x.19]
* 

* The next step would then be to compute a matrix and right hand sidecorresponding to a particular differential equation using this finite elementand mesh. We will keep this step for the  [2.x.19]  program and rather talk aboutone practical aspect of a finite element program, namely that finite elementmatrices are always very sparse: almost all entries in thesematrices are zero.
* To be more precise, we say that a matrix is sparseif the number of nonzero entries [1.x.20] in the matrix isbounded by a number that is independent of the overall number of degrees offreedom. For example, the simple 5-point stencil of a finite differenceapproximation of the Laplace equation leads to a sparse matrix since thenumber of nonzero entries per row is five, and therefore independent of thetotal size of the matrix. For more complicated problems
* 
*  -  say, the Stokesproblem of  [2.x.20] 
* 
*  -  and in particular in 3d, the number of entries per rowmay be several hundred. But the important point is that this number isindependent of the overall size of the problem: If you refine the mesh, themaximal number of unknowns per row remains the same.
* Sparsity is one of the distinguishing feature ofthe finite element method compared to, say, approximating the solution of apartial differential equation using a Taylor expansion and matchingcoefficients, or using a Fourier basis.
* In practical terms, it is the sparsity of matrices that enables us to solveproblems with millions or billions of unknowns. To understand this, note thata matrix with  [2.x.21]  rows, each with a fixed upper bound for the number ofnonzero entries, requires  [2.x.22]  memory locations for storage, and amatrix-vector multiplication also requires only  [2.x.23] operations. Consequently, if we had a linear solver that requires only a fixednumber of matrix-vector multiplications to come up with the solution of alinear system with this matrix, then we would have a solver that can find thevalues of all  [2.x.24]  unknowns with optimal complexity, i.e., with a total of [2.x.25]  operations. It is clear that this wouldn't be possible if thematrix were not sparse (because then the number of entries in the matrix wouldhave to be  [2.x.26]  with some  [2.x.27] , and doing a fixed number ofmatrix-vector products would take  [2.x.28]  operations),but it also requires very specialized solvers such asmultigrid methods to satisfy the requirement that the solution requires only afixed number of matrix-vector multiplications. We will frequently look at thequestion of what solver to use in the remaining programs of this tutorial.
* The sparsity is generated by the fact that finite element shapefunctions are defined [1.x.21] on individual cells, rather thanglobally, and that the local differential operators in the bilinearform only couple shape functions whose support overlaps. (The "support" ofa function is the area where it is nonzero. For the finite element method,the support of a shape function is generally the cells adjacent to the vertex,edge, or face it is defined on.) In other words, degrees of freedom  [2.x.29]  and  [2.x.30] that are not defined on the same cell do not overlap, and consequentlythe matrix entry  [2.x.31]  will be zero.  (In some cases suchas the Discontinuous Galerkin method, shape functions may also connectto neighboring cells through face integrals. But finite elementmethods do not generally couple shape functions beyond the immediateneighbors of a cell on which the function is defined.)
* 

* [1.x.22][1.x.23]
* 

* By default, the DoFHandler class enumerates degrees of freedom on a mesh in arather random way; consequently, the sparsity pattern is also notoptimized for any particular purpose. To show this, the code below willdemonstrate a simple way to output the "sparsity pattern" that corresponds toa DoFHandler, i.e., an object that represents all of the potentially nonzeroelements of a matrix one may build when discretizing a partial differentialequation on a mesh and its DoFHandler. This lack of structure in the sparsitypattern will be apparent from the pictures we show below.
* For most applications and algorithms, the exact way in which degrees of freedomare numbered does not matter. For example, the Conjugate Gradient method weuse to solve linear systems does not care. On the other hand,some algorithms do care: in particular, some preconditioners such as SSORwill work better if they can walk through degrees of freedom in a particularorder, and it would be nice if we could just sort them in such a way thatSSOR can iterate through them from zero to  [2.x.32]  in this order. Other examplesinclude computing incomplete LU or Cholesky factorizations, or if we careabout the block structure of matrices (see  [2.x.33]  for an example).deal.II therefore has algorithms that can re-enumerate degrees of freedomin particular ways in namespace DoFRenumbering. Renumbering can be thoughtof as choosing a different, permuted basis of the finite element space. Thesparsity pattern and matrices that result from this renumbering are thereforealso simply a permutation of rows and columns compared to the ones we wouldget without explicit renumbering.
* In the program below, we will use the algorithm of Cuthill and McKee to doso. We will show the sparsity pattern for both the original enumeration ofdegrees of freedom and of the renumbered version below,in the [1.x.24].
* 

*  [1.x.25] [1.x.26]
*  The first few includes are just like in the previous program, so do not require additional comments:
* 

* 
* [1.x.27]
* 
*  However, the next file is new. We need this include file for the association of degrees of freedom ("DoF"s) to vertices, lines, and cells:
* 

* 
* [1.x.28]
* 
*  The following include contains the description of the bilinear finite element, including the facts that it has one degree of freedom on each vertex of the triangulation, but none on faces and none in the interior of the cells.
* 

* 
*  (In fact, the file contains the description of Lagrange elements in general, i.e. also the quadratic, cubic, etc versions, and not only for 2d but also 1d and 3d.)
* 

* 
* [1.x.29]
* 
*  In the following file, several tools for manipulating degrees of freedom can be found:
* 

* 
* [1.x.30]
* 
*  We will use a sparse matrix to visualize the pattern of nonzero entries resulting from the distribution of degrees of freedom on the grid. That class can be found here:
* 

* 
* [1.x.31]
* 
*  We will also need to use an intermediate sparsity pattern structure, which is found in this file:
* 

* 
* [1.x.32]
* 
*  We will want to use a special algorithm to renumber degrees of freedom. It is declared here:
* 

* 
* [1.x.33]
* 
*  And this is again needed for C++ output:
* 

* 
* [1.x.34]
* 
*  Finally, as in  [2.x.34] , we import the deal.II namespace into the global scope:
* 

* 
* [1.x.35]
* 
*   [1.x.36]  [1.x.37]
* 

* 
*  This is the function that produced the circular grid in the previous  [2.x.35]  example program with fewer refinements steps. The sole difference is that it returns the grid it produces via its argument.
* 

* 
* [1.x.38]
* 
*   [1.x.39]  [1.x.40]
* 

* 
*  Up to now, we only have a grid, i.e. some geometrical (the position of the vertices) and some topological information (how vertices are connected to lines, and lines to cells, as well as which cells neighbor which other cells). To use numerical algorithms, one needs some logic information in addition to that: we would like to associate degree of freedom numbers to each vertex (or line, or cell, in case we were using higher order elements) to later generate matrices and vectors which describe a finite element field on the triangulation.
* 

* 
*  This function shows how to do this. The object to consider is the  [2.x.36]  class template.  Before we do so, however, we first need something that describes how many degrees of freedom are to be associated to each of these objects. Since this is one aspect of the definition of a finite element space, the finite element base class stores this information. In the present context, we therefore create an object of the derived class  [2.x.37]  that describes Lagrange elements. Its constructor takes one argument that states the polynomial degree of the element, which here is one (indicating a bi-linear element); this then corresponds to one degree of freedom for each vertex, while there are none on lines and inside the quadrilateral. A value of, say, three given to the constructor would instead give us a bi-cubic element with one degree of freedom per vertex, two per line, and four inside the cell. In general,  [2.x.38]  denotes the family of continuous elements with complete polynomials (i.e. tensor-product polynomials) up to the specified order.
* 

* 
*  We first need to create an object of this class and then pass it on to the  [2.x.39]  object to allocate storage for the degrees of freedom (in deal.II lingo: we [1.x.41]).
* 

* 
* [1.x.42]
* 
*  Now that we have associated a degree of freedom with a global number to each vertex, we wonder how to visualize this?  There is no simple way to directly visualize the DoF number associated with each vertex. However, such information would hardly ever be truly important, since the numbering itself is more or less arbitrary. There are more important factors, of which we will demonstrate one in the following.   
*   Associated with each vertex of the triangulation is a shape function. Assume we want to solve something like Laplace's equation, then the different matrix entries will be the integrals over the gradient of each pair of such shape functions. Obviously, since the shape functions are nonzero only on the cells adjacent to the vertex they are associated with, matrix entries will be nonzero only if the supports of the shape functions associated to that column and row %numbers intersect. This is only the case for adjacent shape functions, and therefore only for adjacent vertices. Now, since the vertices are numbered more or less randomly by the above function  [2.x.40]  the pattern of nonzero entries in the matrix will be somewhat ragged, and we will take a look at it now.   
*   First we have to create a structure which we use to store the places of nonzero elements. This can then later be used by one or more sparse matrix objects that store the values of the entries in the locations stored by this sparsity pattern. The class that stores the locations is the SparsityPattern class. As it turns out, however, this class has some drawbacks when we try to fill it right away: its data structures are set up in such a way that we need to have an estimate for the maximal number of entries we may wish to have in each row. In two space dimensions, reasonable values for this estimate are available through the  [2.x.41]  function, but in three dimensions the function almost always severely overestimates the true number, leading to a lot of wasted memory, sometimes too much for the machine used, even if the unused memory can be released immediately after computing the sparsity pattern. In order to avoid this, we use an intermediate object of type DynamicSparsityPattern that uses a different %internal data structure and that we can later copy into the SparsityPattern object without much overhead. (Some more information on these data structures can be found in the  [2.x.42]  module.) In order to initialize this intermediate data structure, we have to give it the size of the matrix, which in our case will be square with as many rows and columns as there are degrees of freedom on the grid:
* 

* 
* [1.x.43]
* 
*  We then fill this object with the places where nonzero elements will be located given the present numbering of degrees of freedom:
* 

* 
* [1.x.44]
* 
*  Now we are ready to create the actual sparsity pattern that we could later use for our matrix. It will just contain the data already assembled in the DynamicSparsityPattern.
* 

* 
* [1.x.45]
* 
*  With this, we can now write the results to a file:
* 

* 
* [1.x.46]
* 
*  The result is stored in an  [2.x.43]  file, where each nonzero entry in the matrix corresponds with a red square in the image. The output will be shown below.   
*   If you look at it, you will note that the sparsity pattern is symmetric. This should not come as a surprise, since we have not given the  [2.x.44]  any information that would indicate that our bilinear form may couple shape functions in a non-symmetric way. You will also note that it has several distinct region, which stem from the fact that the numbering starts from the coarsest cells and moves on to the finer ones; since they are all distributed symmetrically around the origin, this shows up again in the sparsity pattern.
* 

* 
* [1.x.47]
* 
*   [1.x.48]  [1.x.49]
* 

* 
*  In the sparsity pattern produced above, the nonzero entries extended quite far off from the diagonal. For some algorithms, for example for incomplete LU decompositions or Gauss-Seidel preconditioners, this is unfavorable, and we will show a simple way how to improve this situation.
* 

* 
*  Remember that for an entry  [2.x.45]  in the matrix to be nonzero, the supports of the shape functions i and j needed to intersect (otherwise in the integral, the integrand would be zero everywhere since either the one or the other shape function is zero at some point). However, the supports of shape functions intersected only if they were adjacent to each other, so in order to have the nonzero entries clustered around the diagonal (where  [2.x.46]  equals  [2.x.47] ), we would like to have adjacent shape functions to be numbered with indices (DoF numbers) that differ not too much.
* 

* 
*  This can be accomplished by a simple front marching algorithm, where one starts at a given vertex and gives it the index zero. Then, its neighbors are numbered successively, making their indices close to the original one. Then, their neighbors, if not yet numbered, are numbered, and so on.
* 

* 
*  One algorithm that adds a little bit of sophistication along these lines is the one by Cuthill and McKee. We will use it in the following function to renumber the degrees of freedom such that the resulting sparsity pattern is more localized around the diagonal. The only interesting part of the function is the first call to  [2.x.48] , the rest is essentially as before:
* 

* 
* [1.x.50]
* 
*  Again, the output is shown below. Note that the nonzero entries are clustered far better around the diagonal than before. This effect is even more distinguished for larger matrices (the present one has 1260 rows and columns, but large matrices often have several 100,000s).
* 

* 
*  It is worth noting that the  [2.x.49]  class offers a number of other algorithms as well to renumber degrees of freedom. For example, it would of course be ideal if all couplings were in the lower or upper triangular part of a matrix, since then solving the linear system would amount to only forward or backward substitution. This is of course unachievable for symmetric sparsity patterns, but in some special situations involving transport equations, this is possible by enumerating degrees of freedom from the inflow boundary along streamlines to the outflow boundary. Not surprisingly,  [2.x.50]  also has algorithms for this.
* 

* 
*  
*  
*  [1.x.51]  [1.x.52]
* 

* 
*  Finally, this is the main program. The only thing it does is to allocate and create the triangulation, then create a  [2.x.51]  object and associate it to the triangulation, and finally call above two functions on it:
* 

* 
* [1.x.53]
* [1.x.54][1.x.55]
* 

* The program has, after having been run, produced two sparsitypatterns. We can visualize them by opening the  [2.x.52]  files in a web browser.
* The results then look like this (every point denotes an entry whichmight be nonzero; of course the fact whether the entry actually iszero or not depends on the equation under consideration, but theindicated positions in the matrix tell us which shape functions canand which can't couple when discretizing a local, i.e. differential,equation): [2.x.53] 
* The different regions in the left picture, indicated by kinks in the lines andsingle dots on the left and top, represent the degrees offreedom on the different refinement levels of the triangulation.  Ascan be seen in the right picture, the sparsity pattern is much betterclustered around the main diagonal of the matrix afterrenumbering. Although this might not be apparent, the number ofnonzero entries is the same in both pictures, of course.
* 

* 
* [1.x.56][1.x.57]
* 

* Just as with  [2.x.54] , you may want to play with the program a bit tofamiliarize yourself with deal.II. For example, in the [2.x.55]  function, we use linear finite elements(that's what the argument "1" to the FE_Q object is). Explore how thesparsity pattern changes if you use higher order elements, for examplecubic or quintic ones (by using 3 and 5 as the respective arguments).
* You could also explore how the sparsity pattern changes by refiningthe mesh. You will see that not only the size of the matrixchanges, but also its bandwidth (the distance from the diagonal ofthose nonzero elements of the matrix that are farthest away from thediagonal), though the ratio of bandwidth to size typically shrinks,i.e. the matrix clusters more around the diagonal.
* Another idea of experiments would be to try other renumberingstrategies than Cuthill-McKee from the DoFRenumbering namespace and see howthey affect the sparsity pattern.
* You can also visualize the output using [1.x.58] (one of the simpler visualizationprograms; maybe not the easiest to use since it is command line driven, butalso universally available on all Linux and other Unix-like systems) by changing from  [2.x.56] :
* [1.x.59]
* 
* Another practice based on[1.x.60] is trying toprint out the mesh with locations and numbering of the supportpoints. For that, you need to include header files for GridOut and MappingQ1.The code for this is:
* [1.x.61]
* After we run the code, we get a file called gnuplot.gpl. To view thisfile, we can run the following code in the command line:
* [1.x.62].With that, you will get a picture similar to [2.x.57] depending on the mesh you are looking at. For more information, see  [2.x.58] 
* 

* [1.x.63][1.x.64] [2.x.59] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-21_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34][1.x.35][1.x.36][1.x.37][1.x.38]
* [1.x.39][1.x.40] [1.x.41]
* 

* This program grew out of a student project by Yan Li at Texas A&amp;MUniversity. Most of the work for this program is by her.
* In this project, we propose a numerical simulation for two phaseflow problems in porous media. This problem includes oneelliptic equation and one nonlinear, time dependent transportequation. This is therefore also the first time-dependent tutorialprogram (besides the somewhat strange time-dependence of  [2.x.2] " [2.x.3] ").
* The equations covered here are an extension of the material already covered in [2.x.4] . In particular, they fall into the class ofvector-valued problems. A toplevel overview of this topic can be found in the [2.x.5]  module.
* 

* [1.x.42][1.x.43]
* 

* Modeling of two phase flow in porous media is important for bothenvironmental remediation and the management of petroleum and groundwaterreservoirs. Practical situations involving two phase flow include thedispersal of a nonaqueous phase liquid in an aquifer, or the jointmovement of a mixture of fluids such as oil and water in areservoir. Simulation models, if they are to provide realisticpredictions, must accurately account for these effects.
* To derive the governing equations, consider two phase flow in areservoir  [2.x.6]  under the assumption that the movement of fluids isdominated by viscous effects; i.e. we neglect the effects of gravity,compressibility, and capillary pressure. Porosity will be consideredto be constant. We will denote variables referring to either of the twophases using subscripts  [2.x.7]  and  [2.x.8] , short for water and oil. Thederivation of the equations holds for other pairs of fluids as well,however.
* The velocity with which molecules of each of the two phases move isdetermined by Darcy's law that states that the velocity isproportional to the pressure gradient:[1.x.44]
* where  [2.x.9]  is the velocity of phase  [2.x.10] ,  [2.x.11]  is thepermeability tensor,  [2.x.12]  is the relative permeability of phase [2.x.13] ,  [2.x.14]  is thepressure and  [2.x.15]  is the viscosity of phase  [2.x.16] . Finally,  [2.x.17]  isthe saturation (volume fraction), i.e. a function with values between0 and 1 indicating the composition of the mixture of fluids. Ingeneral, the coefficients  [2.x.18]  may be spatially dependentvariables, and we will always treat them as non-constant functions inthe following.
* We combine Darcy's law with the statement of conservation of mass foreach phase,[1.x.45]with a source term for each phase. By summing over the two phases,we can express the governing equations in terms of theso-called pressure equation:[1.x.46]
* Here,  [2.x.19]  is the sum source term, and[1.x.47]is the total mobility.
* So far, this looks like an ordinary stationary, Poisson-like equation that wecan solve right away with the techniques of the first few tutorial programs(take a look at  [2.x.20] , for example, for something verysimilar). However, we have not said anything yet about the saturation, whichof course is going to change as the fluids move around.
* The second part of the equations is the description of thedynamics of the saturation, i.e., how the relative concentration of thetwo fluids changes with time. The saturation equation for the displacingfluid (water) is given by the following conservation law:[1.x.48]
* which can be rewritten by using the product rule of the divergence operatorin the previous equation:[1.x.49]
* Here,  [2.x.21]  is the total influx introducedabove, and  [2.x.22]  is the flow rate of the displacing fluid (water).These two are related to the fractional flow  [2.x.23]  in the following way:[1.x.50]where the fractional flow is often parameterized via the (heuristic) expression[1.x.51]Putting it all together yields the saturation equation in the following,advected form:[1.x.52]
* where  [2.x.24]  is the total velocity[1.x.53]Note that the advection equation contains the term  [2.x.25]  rather than  [2.x.26]  to indicate that the saturationis not simply transported along; rather, since the two phases move withdifferent velocities, the saturation can actually change even in the advectedcoordinate system. To see this, rewrite  [2.x.27]  to observe that the [1.x.54]velocity with which the phase with saturation  [2.x.28]  is transported is [2.x.29]  whereas the other phase is transported at velocity [2.x.30] .  [2.x.31]  is consequently often referred to as the[1.x.55].
* In summary, what we get are the following two equations:[1.x.56]
* Here,  [2.x.32]  are now time dependentfunctions: while at every time instant the flow field is inequilibrium with the pressure (i.e. we neglect dynamicaccelerations), the saturation is transported along with the flow andtherefore changes over time, in turn affected the flow field againthrough the dependence of the first equation on  [2.x.33] .
* This set of equations has a peculiar character: one of the twoequations has a time derivative, the other one doesn't. Thiscorresponds to the character that the pressure and velocities arecoupled through an instantaneous constraint, whereas the saturationevolves over finite time scales.
* Such systems of equations are called Differential Algebraic Equations(DAEs), since one of the equations is a differential equation, theother is not (at least not with respect to the time variable) and istherefore an "algebraic" equation. (The notation comes from the fieldof ordinary differential equations, where everything that does nothave derivatives with respect to the time variable is necessarily analgebraic equation.) This class of equations contains prettywell-known cases: for example, the time dependent Stokes andNavier-Stokes equations (where the algebraic constraint is that thedivergence of the flow field,  [2.x.34] , must be zero)as well as the time dependent Maxwell equations (here, the algebraicconstraint is that the divergence of the electric displacement fieldequals the charge density,  [2.x.35]  and that thedivergence of the magnetic flux density is zero:  [2.x.36] ); even the quasistatic model of  [2.x.37]  falls into thiscategory. We will see that the different character of the two equationswill inform our discretization strategy for the two equations.
* 

* [1.x.57][1.x.58]
* 

* In the reservoir simulation community, it is common to solve the equationsderived above by going back to the first order, mixed formulation. To thisend, we re-introduce the total velocity  [2.x.38]  and write the equations inthe following form:[1.x.59]
* This formulation has the additional benefit that we do not have to express thetotal velocity  [2.x.39]  appearing in the transport equation as a functionof the pressure, but can rather take the primary variable for it. Given thesaddle point structure of the first two equations and their similarity to themixed Laplace formulation we have introduced in  [2.x.40] , itwill come as no surprise that we will use a mixed discretization again.
* But let's postpone this for a moment. The first business we have with theseequations is to think about the time discretization. In reservoir simulation,there is a rather standard algorithm that we will use here. It first solvesthe pressure using an implicit equation, then the saturation using an explicittime stepping scheme. The algorithm is called IMPES for IMplicit PressureExplicit Saturation and was first proposed a long time ago: by Sheldon etal. in 1959 and Stone and Gardner in 1961 (J. W. Sheldon, B. Zondek andW. T. Cardwell: [1.x.60], Trans. SPE AIME, 216 (1959), pp. 290-296; H.L. Stone and A. O. Gardner Jr: [1.x.61], Trans. SPE AIME, 222 (1961), pp. 92-104).In a slightly modified form, this algorithm can bewritten as follows: for each time step, solve[1.x.62]
* where  [2.x.41]  is the length of a time step. Note how we solve theimplicit pressure-velocity system that only depends on the previously computedsaturation  [2.x.42] , and then do an explicit time step for  [2.x.43]  that onlydepends on the previously known  [2.x.44]  and the just computed [2.x.45] . This way, we never have to iterate for the nonlinearitiesof the system as we would have if we used a fully implicit method. (Ina more modern perspective, this should be seen as an "operatorsplitting" method.  [2.x.46]  has a long description of the idea behind this.)
* We can then state the problem in weak form as follows, by multiplying eachequation with test functions  [2.x.47] ,  [2.x.48] , and  [2.x.49]  and integratingterms by parts:[1.x.63]
* Note that in the first term, we have to prescribe the pressure  [2.x.50]  onthe boundary  [2.x.51]  as boundary values for our problem.  [2.x.52] denotes the unit outward normal vector to  [2.x.53] , as usual.
* For the saturation equation, we obtain after integrating by parts[1.x.64]
* Using the fact that  [2.x.54] , we can rewrite thecell term to get an equation as follows:[1.x.65]
* We introduce an object of type DiscreteTime in order to keep track of thecurrent value of time and time step in the code. This class encapsulates manycomplexities regarding adjusting time step size and stopping at a specifiedfinal time.
* 

* 
* [1.x.66][1.x.67]
* 

* In each time step, we then apply the mixed finite method of  [2.x.55] " [2.x.56] " to the velocity and pressure. To be well-posed, we chooseRaviart-Thomas spaces  [2.x.57]  for  [2.x.58]  and discontinuous elements ofclass  [2.x.59]  for  [2.x.60] . For the saturation, we will also choose  [2.x.61] spaces.
* Since we have discontinuous spaces, we have to think about how to evaluateterms on the interfaces between cells, since discontinuous functions are notreally defined there. In particular, we have to give a meaning to the lastterm on the left hand side of the saturation equation. To this end, let usdefine that we want to evaluate it in the following sense:[1.x.68]
* where  [2.x.62] denotes the inflow boundary and  [2.x.63]  is the outflow part of the boundary.The quantities  [2.x.64]  then correspond to the values of thesevariables on the present cell, whereas  [2.x.65]  (needed on theinflow part of the boundary of  [2.x.66] ) are quantities taken from the neighboringcell. Some more context on discontinuous element techniques and evaluation offluxes can also be found in  [2.x.67]  and  [2.x.68] b.
* 

* [1.x.69][1.x.70]
* 

* The linear solvers used in this program are a straightforward extension of theones used in  [2.x.69]  (but without LinearOperator). Essentially, we simply haveto extend everything fromtwo to three solution components. If we use the discrete spacesmentioned above and put shape functions into the bilinear forms, wearrive at the following linear system to be solved for time step  [2.x.70] :[1.x.71]where the individual matrices and vectors are defined as follows usingshape functions  [2.x.71]  (of type Raviart Thomas  [2.x.72] ) forvelocities and  [2.x.73]  (of type  [2.x.74] ) for both pressures and saturations:[1.x.72]
* 
*  [2.x.75]  Due to historical accidents, the role of matrices  [2.x.76]  and  [2.x.77] has been reverted in this program compared to  [2.x.78] . In other words,here  [2.x.79]  refers to the divergence and  [2.x.80]  to the gradient operatorswhen it was the other way around in  [2.x.81] .
* The system above presents a complication: Since the matrix  [2.x.82] depends on  [2.x.83]  implicitly (the velocities are needed todetermine which parts of the boundaries  [2.x.84]  of cells areinflux or outflux parts), we can only assemble this matrix after wehave solved for the velocities.
* The solution scheme then involves the following steps: [2.x.85]    [2.x.86] Solve for the pressure  [2.x.87]  using the Schur complement  technique introduced in  [2.x.88] .
*    [2.x.89] Solve for the velocity  [2.x.90]  as also discussed in   [2.x.91] .
*    [2.x.92] Compute the term  [2.x.93] , using  the just computed velocities.
*    [2.x.94] Solve for the saturation  [2.x.95] . [2.x.96] 
* In this scheme, we never actually build the matrix  [2.x.97] , but rathergenerate the right hand side of the third equation once we are readyto do so.
* In the program, we use a variable  [2.x.98]  to store thesolution of the present time step. At the end of each step, we copyits content, i.e. all three of its block components, into the variable [2.x.99]  for use in the next time step.
* 

* [1.x.73][1.x.74]
* 

* A general rule of thumb in hyperbolic transport equations like the equation wehave to solve for the saturation equation is that if we use an explicit timestepping scheme, then we should use a time step such that the distance that aparticle can travel within one time step is no larger than the diameter of asingle cell. In other words, here, we should choose[1.x.75]Fortunately, we are in a position where we can do that: we only need thetime step when we want to assemble the right hand side of the saturationequation, which is after we have already solved for  [2.x.100] . All wetherefore have to do after solving for the velocity is to loop over allquadrature points in the domain and determine the maximal magnitude of thevelocity. We can then set the time step for the saturation equation to[1.x.76]
* Why is it important to do this? If we don't, then we will end up with lots ofplaces where our saturation is larger than one or less than zero, as caneasily be verified. (Remember that the saturation corresponds to somethinglike the water fraction in the fluid mixture, and therefore must physically bebetween 0 and 1.) On the other hand, if we choose our time step according tothe criterion listed above, this only happens very very infrequently &mdash;in fact only once for the entire run of the program. However, to be on thesafe side, however, we run a function  [2.x.101]  atthe end of each time step, that simply projects the saturation back onto theinterval  [2.x.102] , should it have gotten out of the physical range. This isuseful since the functions  [2.x.103]  and  [2.x.104]  do not represent anythingphysical outside this range, and we should not expect the program to doanything useful once we have negative saturations or ones larger than one.
* Note that we will have similar restrictions on the time step also in [2.x.105]  and  [2.x.106]  where we solve the time dependentwave equation, another hyperbolic problem. We will also come back to the issueof time step choice below in the section on [1.x.77].
* 

* [1.x.78][1.x.79]
* 

* For simplicity, this program assumes that there is no source,  [2.x.107] , and thatthe heterogeneous porous medium is isotropic  [2.x.108] . The first one of these is a realistic assumption inoil reservoirs: apart from injection and production wells, there are usuallyno mechanisms for fluids to appear or disappear out of the blue. The secondone is harder to justify: on a microscopic level, most rocks are isotropic,because they consist of a network of interconnected pores. However, thismicroscopic scale is out of the range of today's computer simulations, and wehave to be content with simulating things on the scale of meters. On thatscale, however, fluid transport typically happens through a network of cracksin the rock, rather than through pores. However, cracks often result fromexternal stress fields in the rock layer (for example from tectonic faulting)and the cracks are therefore roughly aligned. This leads to a situation wherethe permeability is often orders of magnitude larger in the direction parallelto the cracks than perpendicular to the cracks. A problem typically faces inreservoir simulation, however, is that the modeler doesn't know the directionof cracks because oil reservoirs are not accessible to easy inspection. Theonly solution in that case is to assume an effective, isotropic permeability.
* Whatever the matter, both of these restrictions, no sources and isotropy,would be easy to lift with a few lines of code in the program.
* Next, for simplicity, our numerical simulation will be done on theunit cell  [2.x.109]  for  [2.x.110] . Our initialconditions are  [2.x.111] ; in the oil reservoir picture, where  [2.x.112] would indicate the water saturation, this means that the reservoir containspure oil at the beginning. Note that we do not need any initialconditions for pressure or velocity, since the equations do not contain timederivatives of these variables. Finally, we impose the following pressureboundary conditions:[1.x.80]Since the pressure and velocity solve a mixed form Poisson equation, theimposed pressure leads to a resulting flow field for the velocity. On theother hand, this flow field determines whether a piece of the boundary is ofinflow or outflow type, which is of relevance because we have to imposeboundary conditions for the saturation on the inflow part of the boundary,[1.x.81]On this inflow boundary, we impose the following saturation values:[1.x.82]
* In other words, we have pure water entering the reservoir at the left, whereasthe other parts of the boundary are in contact with undisturbed parts of thereservoir and whenever influx occurs on these boundaries, pure oil will enter.
* In our simulations, we choose the total mobility as[1.x.83]where we use  [2.x.113]  for the viscosity. In addition, the fractional flow ofwater is given by[1.x.84]
*  [2.x.114]  Coming back to this testcase in  [2.x.115]  several years later revealed anoddity in the setup of this testcase. To this end, consider that we canrewrite the advection equation for the saturation as  [2.x.116] . Now, at the initial time, we have  [2.x.117] , and withthe given choice of function  [2.x.118] , we happen to have  [2.x.119] . In otherwords, at  [2.x.120] , the equation reduces to  [2.x.121]  for all  [2.x.122] , so thesaturation is zero everywhere and it is going to stay zero everywhere! This isdespite the fact that  [2.x.123]  is not necessarily zero: the combined fluidis moving, but we've chosen our partial flux  [2.x.124]  in such a way thatinfinitesimal amounts of wetting fluid also only move at infinitesimal speeds(i.e., they stick to the medium more than the non-wetting phase in which theyare embedded). That said, how can we square this with the knowledge thatwetting fluid is invading from the left, leading to the flow patterns seen inthe [1.x.85]? That's where we get intomathematics: Equations like the transport equation we are considering herehave infinitely many solutions, but only one of them is physical: the one thatresults from the so-called viscosity limit, called the [1.x.86]. The thing is that with discontinuous elements we arrive at thisviscosity limit because using a numerical flux introduces a finite amount ofartificial viscosity into the numerical scheme. On the other hand, in  [2.x.125] ,we use an artificial viscosity that is proportional to  [2.x.126] on every cell, which at the initial time is zero. Thus, the saturation there iszero and remains zero; the solution we then get is [1.x.87] solution of theadvection equation, but the method does not converge to the viscosity solutionwithout further changes. We will therefore use a different initial condition inthat program.
* 

* Finally, to come back to the description of the testcase, we will show resultsfor computations with the two permeabilityfunctions introduced at the end of the results section of  [2.x.127] " [2.x.128] ": [2.x.129]    [2.x.130] A function that models a single, winding crack that snakes through the  domain. In analogy to  [2.x.131] , but taking care of the slightly  different geometry we have here, we describe this by the following function:  [1.x.88]  Taking the maximum is necessary to ensure that the ratio between maximal and  minimal permeability remains bounded. If we don't do that, permeabilities  will span many orders of magnitude. On the other hand, the ratio between  maximal and minimal permeability is a factor in the condition number of the  Schur complement matrix, and if too large leads to problems for which our  linear solvers will no longer converge properly.
*    [2.x.132] A function that models a somewhat random medium. Here, we choose  [1.x.89]
*   where the centers  [2.x.133]  are  [2.x.134]  randomly chosen locations inside  the domain. This function models a domain in which there are  [2.x.135]  centers of  higher permeability (for example where rock has cracked) embedded in a  matrix of more pristine, unperturbed background rock. Note that here we have  cut off the permeability function both above and below to ensure a bounded  condition number. [2.x.136] 
* 

*  [1.x.90] [1.x.91]
*  This program is an adaptation of  [2.x.137]  and includes some technique of DG methods from  [2.x.138] . A good part of the program is therefore very similar to  [2.x.139]  and we will not comment again on these parts. Only the new stuff will be discussed in more detail.
* 

* 
*   [1.x.92]  [1.x.93]
* 

* 
*  All of these include files have been used before:
* 

* 
* [1.x.94]
* 
*  In this program, we use a tensor-valued coefficient. Since it may have a spatial dependence, we consider it a tensor-valued function. The following include file provides the  [2.x.140]  class that offers such functionality:
* 

* 
* [1.x.95]
* 
*  Additionally, we use the class  [2.x.141]  to perform operations related to time incrementation.
* 

* 
* [1.x.96]
* 
*  The last step is as in all previous programs:
* 

* 
* [1.x.97]
* 
*   [1.x.98]  [1.x.99]
* 

* 
*  This is the main class of the program. It is close to the one of  [2.x.142] , but with a few additional functions:   
*    [2.x.143]   [2.x.144]  [2.x.145]  assembles the right hand side of the saturation equation. As explained in the introduction, this can't be integrated into  [2.x.146]  since it depends on the velocity that is computed in the first part of the time step.   
*    [2.x.147]  [2.x.148]  does as its name suggests. This function is used in the computation of the time step size.   
*    [2.x.149]  [2.x.150]  resets all saturation degrees of freedom with values less than zero to zero, and all those with saturations greater than one to one.   [2.x.151]    
*   The rest of the class should be pretty much obvious. The  [2.x.152]  variable stores the viscosity  [2.x.153]  that enters several of the formulas in the nonlinear equations. The variable  [2.x.154]  keeps track of the time information within the simulation.
* 

* 
* [1.x.100]
* 
*   [1.x.101]  [1.x.102]
* 

* 
*   [1.x.103]  [1.x.104]
* 

* 
*  At present, the right hand side of the pressure equation is simply the zero function. However, the rest of the program is fully equipped to deal with anything else, if this is desired:
* 

* 
* [1.x.105]
* 
*   [1.x.106]  [1.x.107]
* 

* 
*  The next are pressure boundary values. As mentioned in the introduction, we choose a linear pressure field:
* 

* 
* [1.x.108]
* 
*   [1.x.109]  [1.x.110]
* 

* 
*  Then we also need boundary values on the inflow portions of the boundary. The question whether something is an inflow part is decided when assembling the right hand side, we only have to provide a functional description of the boundary values. This is as explained in the introduction:
* 

* 
* [1.x.111]
* 
*   [1.x.112]  [1.x.113]
* 

* 
*  Finally, we need initial data. In reality, we only need initial data for the saturation, but we are lazy, so we will later, before the first time step, simply interpolate the entire solution for the previous time step from a function that contains all vector components.   
*   We therefore simply create a function that returns zero in all components. We do that by simply forward every function to the  [2.x.155]  class. Why not use that right away in the places of this program where we presently use the  [2.x.156]  class? Because this way it is simpler to later go back and choose a different function for initial values.
* 

* 
* [1.x.114]
* 
*   [1.x.115]  [1.x.116]
* 

* 
*  As announced in the introduction, we implement two different permeability tensor fields. Each of them we put into a namespace of its own, so that it will be easy later to replace use of one by the other in the code.
* 

* 
*   [1.x.117]  [1.x.118]
* 

* 
*  The first function for the permeability was the one that models a single curving crack. It was already used at the end of  [2.x.157] , and its functional form is given in the introduction of the present tutorial program. As in some previous programs, we have to declare a (seemingly unnecessary) default constructor of the KInverse class to avoid warnings from some compilers:
* 

* 
* [1.x.119]
* 
*   [1.x.120]  [1.x.121]
* 

* 
*  This function does as announced in the introduction, i.e. it creates an overlay of exponentials at random places. There is one thing worth considering for this class. The issue centers around the problem that the class creates the centers of the exponentials using a random function. If we therefore created the centers each time we create an object of the present type, we would get a different list of centers each time. That's not what we expect from classes of this type: they should reliably represent the same function.   
*   The solution to this problem is to make the list of centers a static member variable of this class, i.e. there exists exactly one such variable for the entire program, rather than for each object of this type. That's exactly what we are going to do.   
*   The next problem, however, is that we need a way to initialize this variable. Since this variable is initialized at the beginning of the program, we can't use a regular member function for that since there may not be an object of this type around at the time. The C++ standard therefore says that only non-member and static member functions can be used to initialize a static variable. We use the latter possibility by defining a function  [2.x.158]  that computes the list of center points when called.   
*   Note that this class works just fine in both 2d and 3d, with the only difference being that we use more points in 3d: by experimenting we find that we need more exponentials in 3d than in 2d (we have more ground to cover, after all, if we want to keep the distance between centers roughly equal), so we choose 40 in 2d and 100 in 3d. For any other dimension, the function does presently not know what to do so simply throws an exception indicating exactly this.
* 

* 
* [1.x.122]
* 
*   [1.x.123]  [1.x.124]
* 

* 
*  There are two more pieces of data that we need to describe, namely the inverse mobility function and the saturation curve. Their form is also given in the introduction:
* 

* 
* [1.x.125]
* 
*   [1.x.126]  [1.x.127]
* 

* 
*  The linear solvers we use are also completely analogous to the ones used in  [2.x.159] . The following classes are therefore copied verbatim from there. Note that the classes here are not only copied from  [2.x.160] , but also duplicate classes in deal.II. In a future version of this example, they should be replaced by an efficient method, though. There is a single change: if the size of a linear system is small, i.e. when the mesh is very coarse, then it is sometimes not sufficient to set a maximum of  [2.x.161]  CG iterations before the solver in the  [2.x.162]  function converges. (This is, of course, a result of numerical round-off, since we know that on paper, the CG method converges in at most  [2.x.163]  steps.) As a consequence, we set the maximum number of iterations equal to the maximum of the size of the linear system and 200.
* 

* 
* [1.x.128]
* 
*   [1.x.129]  [1.x.130]
* 

* 
*  Here now the implementation of the main class. Much of it is actually copied from  [2.x.164] , so we won't comment on it in much detail. You should try to get familiar with that program first, then most of what is happening here should be mostly clear.
* 

* 
*   [1.x.131]  [1.x.132]
* 

* 
*  First for the constructor. We use  [2.x.165]  spaces. For initializing the DiscreteTime object, we don't set the time step size in the constructor because we don't have its value yet. The time step size is initially set to zero, but it will be computed before it is needed to increment time, as described in a subsection of the introduction. The time object internally prevents itself from being incremented when  [2.x.166] , forcing us to set a non-zero desired size for  [2.x.167]  before advancing time.
* 

* 
* [1.x.133]
* 
*   [1.x.134]  [1.x.135]
* 

* 
*  This next function starts out with well-known functions calls that create and refine a mesh, and then associate degrees of freedom with it. It does all the same things as in  [2.x.168] , just now for three components instead of two.
* 

* 
* [1.x.136]
* 
*   [1.x.137]  [1.x.138]
* 

* 
*  This is the function that assembles the linear system, or at least everything except the (1,3) block that depends on the still-unknown velocity computed during this time step (we deal with this in  [2.x.169] ). Much of it is again as in  [2.x.170] , but we have to deal with some nonlinearity this time.  However, the top of the function is pretty much as usual (note that we set matrix and right hand side to zero at the beginning &mdash; something we didn't have to do for stationary problems since there we use each matrix object only once and it is empty at the beginning anyway).   
*   Note that in its present form, the function uses the permeability implemented in the  [2.x.171]  class. Switching to the single curved crack permeability function is as simple as just changing the namespace name.
* 

* 
* [1.x.139]
* 
*  Here's the first significant difference: We have to get the values of the saturation function of the previous time step at the quadrature points. To this end, we can use the  [2.x.172]  (previously already used in  [2.x.173] ,  [2.x.174]  and  [2.x.175] ), a function that takes a solution vector and returns a list of function values at the quadrature points of the present cell. In fact, it returns the complete vector-valued solution at each quadrature point, i.e. not only the saturation but also the velocities and pressure:
* 

* 
* [1.x.140]
* 
*  Then we also have to get the values of the pressure right hand side and of the inverse permeability tensor at the quadrature points:
* 

* 
* [1.x.141]
* 
*  With all this, we can now loop over all the quadrature points and shape functions on this cell and assemble those parts of the matrix and right hand side that we deal with in this function. The individual terms in the contributions should be self-explanatory given the explicit form of the bilinear form stated in the introduction:
* 

* 
* [1.x.142]
* 
*  Next, we also have to deal with the pressure boundary values. This, again is as in  [2.x.176] :
* 

* 
* [1.x.143]
* 
*  The final step in the loop over all cells is to transfer local contributions into the global matrix and right hand side vector:
* 

* 
* [1.x.144]
* 
*  So much for assembly of matrix and right hand side. Note that we do not have to interpolate and apply boundary values since they have all been taken care of in the weak form already.
* 

* 
*  
*  
*  [1.x.145]  [1.x.146]
* 

* 
*  As explained in the introduction, we can only evaluate the right hand side of the saturation equation once the velocity has been computed. We therefore have this separate function to this end.
* 

* 
* [1.x.147]
* 
*  First for the cell terms. These are, following the formulas in the introduction,  [2.x.177] , where  [2.x.178]  is the saturation component of the test function:
* 

* 
* [1.x.148]
* 
*  Secondly, we have to deal with the flux parts on the face boundaries. This was a bit more involved because we first have to determine which are the influx and outflux parts of the cell boundary. If we have an influx boundary, we need to evaluate the saturation on the other side of the face (or the boundary values, if we are at the boundary of the domain).         
*   All this is a bit tricky, but has been explained in some detail already in  [2.x.179] . Take a look there how this is supposed to work!
* 

* 
* [1.x.149]
* 
*   [1.x.150]  [1.x.151]
* 

* 
*  After all these preparations, we finally solve the linear system for velocity and pressure in the same way as in  [2.x.180] . After that, we have to deal with the saturation equation (see below):
* 

* 
* [1.x.152]
* 
*  First the pressure, using the pressure Schur complement of the first two equations:
* 

* 
* [1.x.153]
* 
*  Now the velocity:
* 

* 
* [1.x.154]
* 
*  Finally, we have to take care of the saturation equation. The first business we have here is to determine the time step using the formula in the introduction. Knowing the shape of our domain and that we created the mesh by regular subdivision of cells, we can compute the diameter of each of our cells quite easily (in fact we use the linear extensions in coordinate directions of the cells, not the diameter). Note that we will learn a more general way to do this in  [2.x.181] , where we use the  [2.x.182]  function.     
*   The maximal velocity we compute using a helper function to compute the maximal velocity defined below, and with all this we can evaluate our new time step length. We use the method  [2.x.183]  to suggest the new calculated value of the time step to the DiscreteTime object. In most cases, the time object uses the exact provided value to increment time. It some case, the step size may be modified further by the time object. For example, if the calculated time increment overshoots the end time, it is truncated accordingly.
* 

* 
* [1.x.155]
* 
*  The next step is to assemble the right hand side, and then to pass everything on for solution. At the end, we project back saturations onto the physically reasonable range:
* 

* 
* [1.x.156]
* 
*   [1.x.157]  [1.x.158]
* 

* 
*  There is nothing surprising here. Since the program will do a lot of time steps, we create an output file only every fifth time step and skip all other time steps at the top of the file already.   
*   When creating file names for output close to the bottom of the function, we convert the number of the time step to a string representation that is padded by leading zeros to four digits. We do this because this way all output file names have the same length, and consequently sort well when creating a directory listing.
* 

* 
* [1.x.159]
* 
*   [1.x.160]  [1.x.161]
* 

* 
*  In this function, we simply run over all saturation degrees of freedom and make sure that if they should have left the physically reasonable range, that they be reset to the interval  [2.x.184] . To do this, we only have to loop over all saturation components of the solution vector; these are stored in the block 2 (block 0 are the velocities, block 1 are the pressures).   
*   It may be instructive to note that this function almost never triggers when the time step is chosen as mentioned in the introduction. However, if we choose the timestep only slightly larger, we get plenty of values outside the proper range. Strictly speaking, the function is therefore unnecessary if we choose the time step small enough. In a sense, the function is therefore only a safety device to avoid situations where our entire solution becomes unphysical because individual degrees of freedom have become unphysical a few time steps earlier.
* 

* 
* [1.x.162]
* 
*   [1.x.163]  [1.x.164]
* 

* 
*  The following function is used in determining the maximal allowable time step. What it does is to loop over all quadrature points in the domain and find what the maximal magnitude of the velocity is.
* 

* 
* [1.x.165]
* 
*   [1.x.166]  [1.x.167]
* 

* 
*  This is the final function of our main class. Its brevity speaks for itself. There are only two points worth noting: First, the function projects the initial values onto the finite element space at the beginning; the  [2.x.185]  function doing this requires an argument indicating the hanging node constraints. We have none in this program (we compute on a uniformly refined mesh), but the function requires the argument anyway, of course. So we have to create a constraint object. In its original state, constraint objects are unsorted, and have to be sorted (using the  [2.x.186]  function) before they can be used. This is what we do here, and which is why we can't simply call the  [2.x.187]  function with an anonymous temporary object  [2.x.188]  as the second argument.   
*   The second point worth mentioning is that we only compute the length of the present time step in the middle of solving the linear system corresponding to each time step. We can therefore output the present time of a time step only at the end of the time step. We increment time by calling the method  [2.x.189]  inside the loop. Since we are reporting the time and dt after we increment it, we have to call the method  [2.x.190]  instead of  [2.x.191]  After many steps, when the simulation reaches the end time, the last dt is chosen by the DiscreteTime class in such a way that the last step finishes exactly at the end time.
* 

* 
* [1.x.168]
* 
*   [1.x.169]  [1.x.170]
* 

* 
*  That's it. In the main function, we pass the degree of the finite element space to the constructor of the TwoPhaseFlowProblem object.  Here, we use zero-th degree elements, i.e.  [2.x.192] . The rest is as in all the other programs.
* 

* 
* [1.x.171]
* [1.x.172][1.x.173]
* 

* The code as presented here does not actually compute the resultsfound on the web page. The reason is, that even on a decentcomputer it runs more than a day. If you want to reproduce theseresults, modify the end time of the DiscreteTime object to `250` within theconstructor of TwoPhaseFlowProblem.
* If we run the program, we get the following kind of output:
* [1.x.174]
* As we can see, the time step is pretty much constant right from the start,which indicates that the velocities in the domain are not strongly dependenton changes in saturation, although they certainly are through the factor [2.x.193]  in the pressure equation.
* Our second observation is that the number of CG iterations needed to solve thepressure Schur complement equation drops from 22 to 17 between the first andthe second time step (in fact, it remains around 17 for the rest of thecomputations). The reason is actually simple: Before we solve for the pressureduring a time step, we don't reset the  [2.x.194]  variable tozero. The pressure (and the other variables) therefore have the previous timestep's values at the time we get into the CG solver. Since the velocities andpressures don't change very much as computations progress, the previous timestep's pressure is actually a good initial guess for this time step'spressure. Consequently, the number of iterations we need once we have computedthe pressure once is significantly reduced.
* The final observation concerns the number of iterations needed to solve forthe saturation, i.e. one. This shouldn't surprise us too much: the matrix wehave to solve with is the mass matrix. However, this is the mass matrix forthe  [2.x.195]  element of piecewise constants where no element couples with thedegrees of freedom on neighboring cells. The matrix is therefore a diagonalone, and it is clear that we should be able to invert this matrix in a singleCG iteration.
* 

* With all this, here are a few movies that show how the saturation progressesover time. First, this is for the single crack model, as implemented in the [2.x.196]  class:
*  [2.x.197] 
* As can be seen, the water rich fluid snakes its way mostly along thehigh-permeability zone in the middle of the domain, whereas the rest of thedomain is mostly impermeable. This and the next movie are generated using [2.x.198] , leading to a  [2.x.199]  mesh with some16,000 cells and about 66,000 unknowns in total.
* 

* The second movie shows the saturation for the random medium model of class [2.x.200] , where we have randomly distributedcenters of high permeability and fluid hops from one of these zones tothe next:
*  [2.x.201] 
* 

* Finally, here is the same situation in three space dimensions, on a mesh with [2.x.202] , which produces a mesh of some 32,000 cellsand 167,000 degrees of freedom:
*  [2.x.203] 
* To repeat these computations, all you have to do is to change the line
* [1.x.175]
* in the main function to
* [1.x.176]
* The visualization uses a cloud technique, where the saturation is indicated bycolored but transparent clouds for each cell. This way, one can also seesomewhat what happens deep inside the domain. A different way of visualizingwould have been to show isosurfaces of the saturation evolving overtime. There are techniques to plot isosurfaces transparently, so that one cansee several of them at the same time like the layers of an onion.
* So why don't we show such isosurfaces? The problem lies in the way isosurfacesare computed: they require that the field to be visualized is continuous, sothat the isosurfaces can be generated by following contours at least across asingle cell. However, our saturation field is piecewise constant anddiscontinuous. If we wanted to plot an isosurface for a saturation  [2.x.204] ,chances would be that there is no single point in the domain where thatsaturation is actually attained. If we had to define isosurfaces in thatcontext at all, we would have to take the interfaces between cells, where oneof the two adjacent cells has a saturation greater than and the other cell asaturation less than 0.5. However, it appears that most visualization programsare not equipped to do this kind of transformation.
* 

* [1.x.177][1.x.178][1.x.179]
* 

* There are a number of areas where this program can be improved. Three of themare listed below. All of them are, in fact, addressed in a tutorial programthat forms the continuation of the current one:  [2.x.205] .
* 

* [1.x.180][1.x.181]
* 

* At present, the program is not particularly fast: the 2d random mediumcomputation took about a day for the 1,000 or so time steps. The corresponding3d computation took almost two days for 800 time steps. The reason why itisn't faster than this is twofold. First, we rebuild the entire matrix inevery time step, although some parts such as the  [2.x.206] ,  [2.x.207] , and  [2.x.208]  blocksnever change.
* Second, we could do a lot better with the solver andpreconditioners. Presently, we solve the Schur complement  [2.x.209] with a CG method, using  [2.x.210]  as apreconditioner. Applying this preconditioner is expensive, since it involvessolving a linear system each time. This may have been appropriate for  [2.x.211]  " [2.x.212] ", where we have to solve the entire problem onlyonce. However, here we have to solve it hundreds of times, and in such casesit is worth considering a preconditioner that is more expensive to set up thefirst time, but cheaper to apply later on.
* One possibility would be to realize that the matrix we use as preconditioner, [2.x.213]  is still sparse, and symmetric on top ofthat. If one looks at the flow field evolve over time, we also see that while [2.x.214]  changes significantly over time, the pressure hardly does and consequently [2.x.215] . In other words, the matrix for the first time step should be a goodpreconditioner also for all later time steps.  With a bit ofback-and-forthing, it isn't hard to actually get a representation of it as aSparseMatrix object. We could then hand it off to the SparseMIC class to forma sparse incomplete Cholesky decomposition. To form this decomposition isexpensive, but we have to do it only once in the first time step, and can thenuse it as a cheap preconditioner in the future. We could do better even byusing the SparseDirectUMFPACK class that produces not only an incomplete, buta complete decomposition of the matrix, which should yield an even betterpreconditioner.
* Finally, why use the approximation  [2.x.216]  toprecondition  [2.x.217] ? The latter matrix, after all, is the mixedform of the Laplace operator on the pressure space, for which we use linearelements. We could therefore build a separate matrix  [2.x.218]  on the side thatdirectly corresponds to the non-mixed formulation of the Laplacian, forexample using the bilinear form  [2.x.219] . We could then form an incomplete or completedecomposition of this non-mixed matrix and use it as a preconditioner of themixed form.
* Using such techniques, it can reasonably be expected that the solution processwill be faster by at least an order of magnitude.
* 

* [1.x.182][1.x.183]
* 

* In the introduction we have identified the time step restriction[1.x.184]that has to hold globally, i.e. for all  [2.x.220] . After discretization, wesatisfy it by choosing[1.x.185]
* This restriction on the time step is somewhat annoying: the finer we make themesh the smaller the time step; in other words, we get punished twice: eachtime step is more expensive to solve and we have to do more time steps.
* This is particularly annoying since the majority of the additional work isspent solving the implicit part of the equations, i.e. the pressure-velocitysystem, whereas it is the hyperbolic transport equation for the saturationthat imposes the time step restriction.
* To avoid this bottleneck, people have invented a number of approaches. Forexample, they may only re-compute the pressure-velocity field every few timesteps (or, if you want, use different time step sizes for thepressure/velocity and saturation equations). This keeps the time steprestriction on the cheap explicit part while it makes the solution of theimplicit part less frequent. Experiments in this direction arecertainly worthwhile; one starting point for such an approach is the paper byZhangxin Chen, Guanren Huan and Baoyan Li: [1.x.186], Transport in Porous Media, 54 (2004),pp. 361&mdash;376. There are certainly many other papers on this topic as well, butthis one happened to land on our desk a while back.
* 

* 
* [1.x.187][1.x.188]
* 

* Adaptivity would also clearly help. Looking at the movies, one clearly seesthat most of the action is confined to a relatively small part of the domain(this particularly obvious for the saturation, but also holds for thevelocities and pressures). Adaptivity can therefore be expected to keep thenecessary number of degrees of freedom low, or alternatively increase theaccuracy.
* On the other hand, adaptivity for time dependent problems is not a trivialthing: we would have to change the mesh every few time steps, and we wouldhave to transport our present solution to the next mesh every time we changeit (something that the SolutionTransfer class can help with). These are notinsurmountable obstacles, but they do require some additional coding and morethan we felt comfortable was worth packing into this tutorial program.
* 

* [1.x.189][1.x.190] [2.x.221] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-22_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] ,  [2.x.2] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34][1.x.35][1.x.36][1.x.37][1.x.38][1.x.39][1.x.40][1.x.41][1.x.42][1.x.43][1.x.44]
*  [2.x.3] 
* [1.x.45]
* 

* 
* [1.x.46][1.x.47][1.x.48]
* 

* This program deals with the Stokes system of equations which reads asfollows in non-dimensionalized form:[1.x.49]
* where  [2.x.4]  denotes the velocity of a fluid,  [2.x.5]  is itspressure,  [2.x.6]  are external forces, and [2.x.7]   is therank-2 tensor of symmetrized gradients; a component-wise definitionof it is  [2.x.8] .
* The Stokes equations describe the steady-state motion of aslow-moving, viscous fluid such as honey, rocks in the earth mantle,or other cases where inertia does not play a significant role. If afluid is moving fast enough that inertia forces are significantcompared to viscous friction, the Stokes equations are no longervalid; taking into account inertia effects then leads to thenonlinear Navier-Stokes equations. However, in this tutorial program,we will focus on the simpler Stokes system.
* Note that when deriving the more general compressible Navier-Stokes equations,the diffusion is modeled as the divergence of the stress tensor[1.x.50]
* where  [2.x.9]  is the viscosity of the fluid. With the assumption of  [2.x.10] (assume constant viscosity and non-dimensionalize the equation by dividing out [2.x.11] ) and assuming incompressibility ( [2.x.12] ), wearrive at the formulation from above:[1.x.51]
* A different formulation uses the Laplace operator ( [2.x.13] )instead of the symmetrized gradient. A big difference here is that thedifferent components of the velocity do not couple. If you assume additionalregularity of the solution  [2.x.14]  (second partial derivatives exist andare continuous), the formulations are equivalent:[1.x.52]
* This is because the  [2.x.15] th entry of   [2.x.16]  is given by:[1.x.53]
* If you can not assume the above mentioned regularity, or if your viscosity isnot a constant, the equivalence no longer holds. Therefore, we decided tostick with the more physically accurate symmetric tensor formulation in thistutorial.
* 

* To be well-posed, we will have to add boundary conditions to theequations. What boundary conditions are readily possible here willbecome clear once we discuss the weak form of the equations.
* The equations covered here fall into the class of vector-valued problems. Atoplevel overview of this topic can be found in the  [2.x.17]  module.
* 

* [1.x.54][1.x.55]
* 

* The weak form of the equations is obtained by writing it in vectorform as[1.x.56]
* forming the dot product from the left with a vector-valued testfunction  [2.x.18]  and integratingover the domain  [2.x.19] , yielding the following set of equations:[1.x.57]
* which has to hold for all test functions  [2.x.20] .
* A generally good rule of thumb is that if one [1.x.58] reduce howmany derivatives are taken on any variable in the formulation, thenone [1.x.59] in fact do that using integration by parts. (This ismotivated by the theory of [1.x.60], and in particular the difference betweenstrong and [1.x.61].) We have already done that for the Laplace equation,where we have integrated the second derivative by parts to obtain theweak formulation that has only one derivative on both test and trialfunction.
* In the current context, we integrate by parts the second term:[1.x.62]
* Likewise, we integrate by parts the first term to obtain[1.x.63]
* where the scalar product between two tensor-valued quantities is heredefined as[1.x.64]
* Using this, we have now reduced the requirements on our variables tofirst derivatives for  [2.x.21]  and no derivatives at allfor  [2.x.22] .
* Because the scalar product between a general tensor like [2.x.23]  and a symmetric tensor like [2.x.24]  equals the scalar product between thesymmetrized forms of the two, we can also write the bilinear formabove as follows:[1.x.65]
* We will deal with the boundary terms in the next section, but it is alreadyclear from the domain terms[1.x.66]
* of the bilinear form that the Stokes equations yield a symmetric bilinearform, and consequently a symmetric (if indefinite) system matrix.
* 

* [1.x.67][1.x.68]
* 

*  [2.x.25] ( [2.x.26] 
* The weak form just derived immediately presents us with differentpossibilities for imposing boundary conditions: [2.x.27]  [2.x.28] Dirichlet velocity boundary conditions: On a part     [2.x.29]  we may impose Dirichlet conditions    on the velocity  [2.x.30] :
*     [1.x.69]
*     Because test functions  [2.x.31]  come from the tangent space of    the solution variable, we have that  [2.x.32]  on  [2.x.33]     and consequently that    [1.x.70]
*     In other words, as usual, strongly imposed boundary values do not    appear in the weak form.
*     It is noteworthy that if we impose Dirichlet boundary values on the entire    boundary, then the pressure is only determined up to a constant. An    algorithmic realization of that would use similar tools as have been seen in     [2.x.34] .
*  [2.x.35] Neumann-type or natural boundary conditions: On the rest of the boundary     [2.x.36] , let us re-write the    boundary terms as follows:    [1.x.71]
*     In other words, on the Neumann part of the boundary we can    prescribe values for the total stress:    [1.x.72]
*     If the boundary is subdivided into Dirichlet and Neumann parts     [2.x.37] , this then leads to the following weak form:    [1.x.73]
* 
* 

*  [2.x.38] Robin-type boundary conditions: Robin boundary conditions are a mixture of    Dirichlet and Neumann boundary conditions. They would read    [1.x.74]
*     with a rank-2 tensor (matrix)  [2.x.39] . The associated weak form is    [1.x.75]
* 
*  [2.x.40] Partial boundary conditions: It is possible to combine Dirichlet and    Neumann boundary conditions by only enforcing each of them for certain    components of the velocity. For example, one way to impose artificial    boundary conditions is to require that the flow is perpendicular to the    boundary, i.e. the tangential component  [2.x.41]  be zero, thereby constraining     [2.x.42] -1 components of the velocity. The remaining component can    be constrained by requiring that the normal component of the normal    stress be zero, yielding the following set of boundary conditions:    [1.x.76]
* 
*     An alternative to this is when one wants the flow to be [1.x.77]    rather than perpendicular to the boundary (in deal.II, the     [2.x.43]  function can do this for    you). This is frequently the case for problems with a free boundary    (e.g. at the surface of a river or lake if vertical forces of the flow are    not large enough to actually deform the surface), or if no significant    friction is exerted by the boundary on the fluid (e.g. at the interface    between earth mantle and earth core where two fluids meet that are    stratified by different densities but that both have small enough    viscosities to not introduce much tangential stress on each other).    In formulas, this means that    [1.x.78]
*     the first condition (which needs to be imposed strongly) fixing a single    component of the velocity, with the second (which would be enforced in the    weak form) fixing the remaining two components. [2.x.44] 
* Despite this wealth of possibilities, we will only use Dirichlet and(homogeneous) Neumann boundary conditions in this tutorial program.
* 

* [1.x.79][1.x.80]
* 

* As developed above, the weak form of the equations with Dirichlet and Neumannboundary conditions on  [2.x.45]  and  [2.x.46]  reads like this: find [2.x.47]  so that[1.x.81]
* for all test functions [2.x.48] .
* These equations represent a symmetric [1.x.82]. It is well knownthat then a solution only exists if the function spaces in which we search fora solution have to satisfy certain conditions, typically referred to as theBabuska-Brezzi or Ladyzhenskaya-Babuska-Brezzi (LBB) conditions. The continuousfunction spaces above satisfy these. However, when we discretize the equations byreplacing the continuous variables and test functions by finite elementfunctions in finite dimensional spaces  [2.x.49] , we have to make sure that  [2.x.50]  also satisfy the LBBconditions. This is similar to what we had to do in  [2.x.51] .
* For the Stokes equations, there are a number of possible choices to ensurethat the finite element spaces are compatible with the LBB condition. A simpleand accurate choice that we will use here is  [2.x.52] , i.e. use elements one order higher for the velocities than for thepressures.
* This then leads to the following discrete problem: find  [2.x.53]  sothat[1.x.83]
* for all test functions  [2.x.54] . Assembling the linear systemassociated with this problem follows the same lines used in  [2.x.55] " [2.x.56] ",  [2.x.57] , and explained in detail in the  [2.x.58]  module.
* 

* 
* [1.x.84][1.x.85]
* 

* The weak form of the discrete equations naturally leads to the followinglinear system for the nodal values of the velocity and pressure fields:[1.x.86]
* Like in  [2.x.59]  and  [2.x.60] , we will solve thissystem of equations by forming the Schur complement, i.e. we will first findthe solution  [2.x.61]  of[1.x.87]
* and then[1.x.88]
* The way we do this is pretty much exactly like we did in these previoustutorial programs, i.e. we use the same classes  [2.x.62] and  [2.x.63]  again. There are two significant differences,however:
*  [2.x.64]  [2.x.65] First, in the mixed Laplace equation we had to deal with the question of howto precondition the Schur complement  [2.x.66] , which was spectrallyequivalent to the Laplace operator on the pressure space (because  [2.x.67] represents the gradient operator,  [2.x.68]  its adjoint  [2.x.69] , and  [2.x.70] the identity (up to the material parameter  [2.x.71] ), so  [2.x.72]  issomething like  [2.x.73] ). Consequently, thematrix is badly conditioned for small mesh sizes and we had to come up with anelaborate preconditioning scheme for the Schur complement.
*  [2.x.74] Second, every time we multiplied with  [2.x.75]  we had to solve with themass matrix  [2.x.76] . This wasn't particularly difficult, however, since the massmatrix is always well conditioned and so simple to invert using CG and alittle bit of preconditioning. [2.x.77] In other words, preconditioning the inner solver for  [2.x.78]  was simple whereaspreconditioning the outer solver for  [2.x.79]  was complicated.
* Here, the situation is pretty much exactly the opposite. The difference stemsfrom the fact that the matrix at the heart of the Schur complement does notstem from the identity operator but from a variant of the Laplace operator, [2.x.80]  (where  [2.x.81]  is the symmetric gradient)acting on a vector field. In the investigation of this issuewe largely follow the paper D. Silvester and A. Wathen:"Fast iterative solution of stabilised Stokes systems part II. Usinggeneral block preconditioners." (SIAM J. Numer. Anal., 31 (1994),pp. 1352-1367), which is available online [1.x.89].Principally, the difference in the matrix at the heart of the Schurcomplement has two consequences:
*  [2.x.82]  [2.x.83] First, it makes the outer preconditioner simple: the Schur complementcorresponds to the operator  [2.x.84]  on the pressure space; forgetting about the fact that we deal withsymmetric gradients instead of the regular one, the Schur complement issomething like  [2.x.85] , which, even if not mathematicallyentirely concise, is spectrally equivalent to the identity operator (aheuristic argument would be to commute the operators into [2.x.86] ). It turns out that it isn't easy to solvethis Schur complement in a straightforward way with the CG method:using no preconditioner, the condition number of the Schur complement matrixdepends on the size ratios of the largest to the smallest cells, and one stillneeds on the order of 50-100 CG iterations. However, there is a simple cure:precondition with the mass matrix on the pressure space and we get down to anumber between 5-15 CG iterations, pretty much independently of the structureof the mesh (take a look at the [1.x.90] of thisprogram to see that indeed the number of CG iterations does not change as werefine the mesh).
* So all we need in addition to what we already have is the mass matrix on thepressure variables and we will store it in a separate object.
* 

* 
*  [2.x.87] While the outer preconditioner has become simpler compared to themixed Laplace case discussed in  [2.x.88] , the issue ofthe inner solver has become more complicated. In the mixed Laplacediscretization, the Schur complement has the form  [2.x.89] . Thus,every time we multiplied with the Schur complement, we had to solve alinear system  [2.x.90] ; this isn't too complicated there, however,since the mass matrix  [2.x.91]  on the pressure space is well-conditioned.
* 

* On the other hand, for the Stokes equation we consider here, the Schurcomplement is  [2.x.92]  where the matrix  [2.x.93]  is related to theLaplace operator (it is, in fact, the matrix corresponding to thebilinear form  [2.x.94] ). Thus,solving with  [2.x.95]  is a lot more complicated: the matrix is badlyconditioned and we know that we need many iterations unless we have avery good preconditioner. What is worse, we have to solve with  [2.x.96] every time we multiply with the Schur complement, which is 5-15 timesusing the preconditioner described above.
* Because we have to solve with  [2.x.97]  several times, it pays off to spenda bit more time once to create a good preconditioner for thismatrix. So here's what we're going to do: if in 2d, we use theultimate preconditioner, namely a direct sparse LU decomposition ofthe matrix. This is implemented using the SparseDirectUMFPACK classthat uses the UMFPACK direct solver to compute the decomposition. Touse it, you will have to build deal.II with UMFPACK support (which is thedefault); see the [1.x.91]for instructions. With this, the inner solver converges in one iteration.
* In 2d, we can do this sort of thing because even reasonably large problemsrarely have more than a few 100,000 unknowns with relatively few nonzeroentries per row. Furthermore, the bandwidth of matrices in 2d is  [2.x.98]  and therefore moderate. For such matrices, sparse factors can becomputed in a matter of a few seconds. (As a point of reference, computing thesparse factors of a matrix of size  [2.x.99]  and bandwidth  [2.x.100]  takes  [2.x.101]  operations. In 2d, this is  [2.x.102] ; though this is a highercomplexity than, for example, assembling the linear system which takes  [2.x.103] , the constant for computing the decomposition is so small that itdoesn't become the dominating factor in the entire program until we get tovery large %numbers of unknowns in the high 100,000s or more.)
* The situation changes in 3d, because there we quickly have many moreunknowns and the bandwidth of matrices (which determines the number ofnonzero entries in sparse LU factors) is  [2.x.104] , and thereare many more entries per row as well. This makes using a sparsedirect solver such as UMFPACK inefficient: only for problem sizes of afew 10,000 to maybe 100,000 unknowns can a sparse decomposition becomputed using reasonable time and memory resources.
* What we do in that case is to use an incomplete LU decomposition (ILU) as apreconditioner, rather than actually computing complete LU factors. As it sohappens, deal.II has a class that does this: SparseILU. Computing the ILUtakes a time that only depends on the number of nonzero entries in the sparsematrix (or that we are willing to fill in the LU factors, if these should bemore than the ones in the matrix), but is independent of the bandwidth of thematrix. It is therefore an operation that can efficiently also be computed in3d. On the other hand, an incomplete LU decomposition, by definition, does notrepresent an exact inverse of the matrix  [2.x.105] . Consequently, preconditioningwith the ILU will still require more than one iteration, unlikepreconditioning with the sparse direct solver. The inner solver will thereforetake more time when multiplying with the Schur complement: an unavoidabletrade-off. [2.x.106] 
* In the program below, we will make use of the fact that the SparseILU andSparseDirectUMFPACK classes have a very similar interface and can be usedinterchangeably. All that we need is a switch class that, depending on thedimension, provides a type that is either of the two classes mentionedabove. This is how we do that:
* [1.x.92]
* 
* From here on, we can refer to the type <code>typename [2.x.107]  and automatically get the correctpreconditioner class. Because of the similarity of the interfaces of the twoclasses, we will be able to use them interchangeably using the same syntax inall places.
* 

* [1.x.93][1.x.94]
* 

* The discussions above showedone* way in which the linear system thatresults from the Stokes equations can be solved, and because thetutorial programs are teaching tools that makes sense. But is this theway this system of equationsshould* be solved?
* The answer to this is no. The primary bottleneck with the approach,already identified above, is that we have to repeatedly solve linearsystems with  [2.x.108]  inside the Schur complement, and because we don'thave a good preconditioner for the Schur complement, these solves justhave to happen too often. A better approach is to use a blockdecomposition, which is based on an observation of Silvester andWathen  [2.x.109]  and explained in much greater detail in [2.x.110]  . An implementation of this alternative approach isdiscussed below, in the section on a [1.x.95] in the results section of this program.
* 

* [1.x.96][1.x.97]
* 

* Above, we have claimed that the linear system has the form[1.x.98]
* i.e., in particular that there is a zero block at the bottom right of thematrix. This then allowed us to write the Schur complement as [2.x.111] . But this is not quite correct.
* Think of what would happen if there are constraints on somepressure variables (see the [2.x.112]  "Constraints on degrees of freedom" documentationmodule), for example because we use adaptivelyrefined meshes and continuous pressure finite elements so that thereare hanging nodes. Another cause for such constraints are Dirichletboundary conditions on the pressure. Then the AffineConstraintsclass, upon copying the local contributions to the matrix into theglobal linear system will zero out rows and columns correspondingto constrained degrees of freedom and put a positive entry onthe diagonal. (You can think of this entry as being one forsimplicity, though in reality it is a value of the same orderof magnitude as the other matrix entries.) In other words,the bottom right block is really not empty at all: It hasa few entries on the diagonal, one for each constrainedpressure degree of freedom, and a correct descriptionof the linear system we have to solve is that it has theform[1.x.99]
* where  [2.x.113]  is the zero matrix with the exception of thepositive diagonal entries for the constrained degrees offreedom. The correct Schur complement would then in factbe the matrix  [2.x.114]  instead of the onestated above.
* Thinking about this makes us, first, realize that theresulting Schur complement is now indefinite because [2.x.115]  is symmetric and positive definite whereas [2.x.116]  is a positive semidefinite, and subtracting the latterfrom the former may no longer be positive definite. Thisis annoying because we could no longer employ the ConjugateGradient method on this true Schur complement. That said, we couldfix the issue in  [2.x.117]  bysimply puttingnegative* values onto the diagonal for the constrainedpressure variables
* 
*  -  because we really only put something nonzeroto ensure that the resulting matrix is not singular; we really didn'tcare whether that entry is positive or negative. So if the entrieson the diagonal of  [2.x.118]  were negative, then  [2.x.119]  would again be asymmetric and positive definite matrix.
* But, secondly, the code below doesn't actually do any of that: Ithappily solves the linear system with the wrong Schur complement [2.x.120]  that just ignores the issue altogether. Whydoes this even work? To understand why this is so, recall thatwhen writing local contributions into the global matrix, [2.x.121]  zeros out therows and columns that correspond to constrained degrees of freedom.This means that  [2.x.122]  has some zero rows, and  [2.x.123]  zero columns.As a consequence, if one were to multiply out what the entriesof  [2.x.124]  are, one would realize that it has zero rows and columnsfor all constrained pressure degrees of freedom, including azero on the diagonal. The nonzero entries of  [2.x.125]  would fitinto exactly those zero diagonal locations, and ensure that  [2.x.126] is invertible. Not doing so, strictly speaking, means that  [2.x.127] remains singular: It is symmetric and positive definite on thesubset of non-constrained pressure degrees of freedom, andsimply the zero matrix on the constrained pressures. Whydoes the Conjugate Gradient method work for this matrix?Because  [2.x.128] also makes sure that the right hand side entries thatcorrespond to these zero rows of the matrix arealso*
zero, i.e., the right hand side is compatible.
* What this means is that whatever the values of the solutionvector for these constrained pressure degrees of freedom,these rows will always have a zero residual and, if onewere to consider what the CG algorithm does internally, justnever produce any updates to the solution vector. In otherwords, the CG algorithm justignores* these rows, despite thefact that the matrix is singular. This only works because thesedegrees of freedom are entirely decoupled from the rest of thelinear system (because the entire row and corresponding columnare zero). At the end of the solution process, the constrainedpressure values in the solution vector therefore remain exactlyas they were when we started the call to the solver; they arefinally overwritten with their correct values when we call [2.x.129]  after the CG solver is done.
* The upshot of this discussion is that the assumption that thebottom right block of the big matrix is zero is a bitsimplified, but that just going with it does not actually leadto any practical problems worth addressing.
* 

* [1.x.100][1.x.101]
* 

* The domain, right hand side and boundary conditions we implement below relateto a problem in geophysics: there, one wants to compute the flow field ofmagma in the earth's interior under a mid-ocean rift. Rifts are places wheretwo continental plates are very slowly drifting apart (a few centimeters peryear at most), leaving a crack in the earth crust that is filled with magmafrom below. Without trying to be entirely realistic, we model this situationby solving the following set of equations and boundary conditions on thedomain  [2.x.130] :[1.x.102]
* and using natural boundary conditions  [2.x.131]  everywhere else. In other words, at theleft part of the top surface we prescribe that the fluid moves with thecontinental plate to the left at speed  [2.x.132] , that it moves to the right on theright part of the top surface, and impose natural flow conditions everywhereelse. If we are in 2d, the description is essentially the same, with theexception that we omit the second component of all vectors stated above.
* As will become apparent in the [1.x.103], theflow field will pull material from below and move it to the left and rightends of the domain, as expected. The discontinuity of velocity boundaryconditions will produce a singularity in the pressure at the center of the topsurface that sucks material all the way to the top surface to fill the gapleft by the outward motion of material at this location.
* 

* [1.x.104][1.x.105]
* 

* [1.x.106][1.x.107]
* 

* In all the previous tutorial programs, we used the AffineConstraints object merelyfor handling hanging node constraints (with exception of  [2.x.133] ). However,the class can also be used to implement Dirichlet boundary conditions, as wewill show in this program, by fixing some node values  [2.x.134] . Note thatthese are inhomogeneous constraints, and we have to pay some specialattention to that. The way we are going to implement this is to first readin the boundary values into the AffineConstraints object by using the call
* [1.x.108]
* 
* very similar to how we were making the list of boundary nodesbefore (note that we set Dirichlet conditions only on boundaries withboundary flag 1). The actual application of the boundary values is thenhandled by the AffineConstraints object directly, without any additionalinterference.
* We could then proceed as before, namely by filling the matrix, and thencalling a condense function on the constraints object of the form
* [1.x.109]
* 
* Note that we call this on the system matrix and system right hand sidesimultaneously, since resolving inhomogeneous constraints requires knowledgeabout both the matrix entries and the right hand side. For efficiencyreasons, though, we choose another strategy: all the constraints collectedin the AffineConstraints object can be resolved on the fly while writing local datainto the global matrix, by using the call
* [1.x.110]
* 
* This technique is further discussed in the  [2.x.135]  tutorialprogram. All we need to know here is that this functions does three thingsat once: it writes the local data into the global matrix and right handside, it distributes the hanging node constraints and additionallyimplements (inhomogeneous) Dirichlet boundary conditions. That's nice, isn'tit?
* We can conclude that the AffineConstraints class provides an alternative to using [2.x.136]  for implementing Dirichlet boundaryconditions.
* 

* [1.x.111][1.x.112][1.x.113]
* Frequently, a sparse matrix contains a substantial amount of elements thatactually are zero when we are about to start a linear solve. Such elements areintroduced when we eliminate constraints or implement Dirichlet conditions,where we usually delete all entries in constrained rows and columns, i.e., weset them to zero. The fraction of elements that are present in the sparsitypattern, but do not really contain any information, can be up to one fourthof the total number of elements in the matrix for the 3D applicationconsidered in this tutorial program. Remember that matrix-vector products orpreconditioners operate on all the elements of a sparse matrix (even thosethat are zero), which is an inefficiency we will avoid here.
* An advantage of directly resolving constrained degrees of freedom is that wecan avoid having most of the entries that are going to be zero in our sparsematrix &mdash; we do not need constrained entries during matrix construction(as opposed to the traditional algorithms, which first fill the matrix, andonly resolve constraints afterwards). This will save both memory and timewhen forming matrix-vector products. The way we are going to do that is topass the information about constraints to the function that generates thesparsity pattern, and then set a <tt>false</tt> argument specifying that wedo not intend to use constrained entries:
* [1.x.114]
* This functions obviates, by the way, also the call to the<tt>condense()</tt> function on the sparsity pattern.
* 

* [1.x.115][1.x.116]
* 

* The program developed below has seen a lot of TLC. We have run it over andover under profiling tools (mainly [1.x.117]'s cachegrind and callgrindtools, as well as the KDE [1.x.118] program forvisualization) to see where the bottlenecks are. This has paid off: throughthis effort, the program has become about four times as fast whenconsidering the runtime of the refinement cycles zero through three,reducing the overall number of CPU instructions executed from869,574,060,348 to 199,853,005,625. For higher refinement levels, the gainis probably even larger since some algorithms that are not  [2.x.137] have been eliminated.
* Essentially, there are currently two algorithms in the program that do notscale linearly with the number of degrees of freedom: renumbering of degreesof freedom (which is  [2.x.138] , and the linear solver (which is [2.x.139] ). As for the first, while reordering degrees of freedommay not scale linearly, it is an indispensable part of the overall algorithmas it greatly improves the quality of the sparse ILU, easily making up forthe time spent on computing the renumbering; graphs and timings todemonstrate this are shown in the documentation of the DoFRenumberingnamespace, also underlining the choice of the Cuthill-McKee reorderingalgorithm chosen below.
* As for the linear solver: as mentioned above, our implementation here uses aSchur complement formulation. This is not necessarily the very best choicebut demonstrates various important techniques available in deal.II. Thequestion of which solver is best is again discussed in the [1.x.119]of this program, along with code showing alternative solvers and acomparison of their results.
* Apart from this, many other algorithms have been tested and improved duringthe creation of this program. For example, in building the sparsity pattern,we originally used a (now no longer existing) BlockCompressedSparsityPatternobject that added one element at a time; however, its data structures were poorlyadapted for the large numbers of nonzero entries per row created by ourdiscretization in 3d, leading to a quadratic behavior. Replacing the internalalgorithms in deal.II to set many elements at a time, and using aBlockCompressedSimpleSparsityPattern (which has, as of early 2015, been in turnreplaced by BlockDynamicSparsityPattern) as a better adapted data structure,removed this bottleneck at the price of a slightly higher memoryconsumption. Likewise, the implementation of the decomposition step in theSparseILU class was very inefficient and has been replaced by one that isabout 10 times faster. Even the vmult function of the SparseILU has beenimproved to save about twenty percent of time. Small improvements wereapplied here and there. Moreover, the AffineConstraints object has been usedto eliminate a lot of entries in the sparse matrix that are eventually goingto be zero, see [1.x.120].
* A profile of how many CPU instructions are spent at the variousdifferent places in the program during refinement cycleszero through three in 3d is shown here:
*  [2.x.140] 
* As can be seen, at this refinement level approximately three quarters of theinstruction count is spent on the actual solver (the  [2.x.141]  callson the left, the  [2.x.142]  call in the middle for the Schurcomplement solve, and another box representing the multiplications withSparseILU and SparseMatrix in the solve for [1.x.121]). About one fifth ofthe instruction count is spent on matrix assembly and sparse ILU computation(box in the lower right corner) and the rest on other things. Since floatingpoint operations such as in the  [2.x.143]  calls typically take muchlonger than many of the logical operations and table lookups in matrixassembly, the fraction of the run time taken up by matrix assembly isactually significantly less than the fraction of instructions, as willbecome apparent in the comparison we make in the results section.
* For higher refinement levels, the boxes representing the solver as well asthe blue box at the top right stemming from reordering algorithm are goingto grow at the expense of the other parts of the program, since they don'tscale linearly. The fact that at this moderate refinement level (3168 cellsand 93176 degrees of freedom) the linear solver already makes up about threequarters of the instructions is a good sign that most of the algorithms usedin this program are well-tuned and that major improvements in speeding upthe program are most likely not to come from hand-optimizing individualaspects but by changing solver algorithms. We will address this point in thediscussion of results below as well.
* As a final point, and as a point of reference, the following picture alsoshows how the profile looked at an early stage of optimizing this program:
*  [2.x.144] 
* As mentioned above, the runtime of this version was about four times as long asfor the first profile, with the SparseILU decomposition taking up about 30% ofthe instruction count, and operations an early, inefficient version ofDynamicSparsityPattern about 10%. Both these bottlenecks have since beencompletely removed.
* 

*  [1.x.122] [1.x.123]
*   [1.x.124]  [1.x.125]
* 

* 
*  As usual, we start by including some well-known files:
* 

* 
* [1.x.126]
* 
*  Then we need to include the header file for the sparse direct solver UMFPACK:
* 

* 
* [1.x.127]
* 
*  This includes the library for the incomplete LU factorization that will be used as a preconditioner in 3D:
* 

* 
* [1.x.128]
* 
*  This is C++:
* 

* 
* [1.x.129]
* 
*  As in all programs, the namespace dealii is included:
* 

* 
* [1.x.130]
* 
*   [1.x.131]  [1.x.132]
* 

* 
*  As explained in the introduction, we are going to use different preconditioners for two and three space dimensions, respectively. We distinguish between them by the use of the spatial dimension as a template parameter. See  [2.x.145]  for details on templates. We are not going to create any preconditioner object here, all we do is to create class that holds a local alias determining the preconditioner class so we can write our program in a dimension-independent way.
* 

* 
* [1.x.133]
* 
*  In 2D, we are going to use a sparse direct solver as preconditioner:
* 

* 
* [1.x.134]
* 
*  And the ILU preconditioning in 3D, called by SparseILU:
* 

* 
* [1.x.135]
* 
*   [1.x.136]  [1.x.137]
* 

* 
*  This is an adaptation of  [2.x.146] , so the main class and the data types are nearly the same as used there. The only difference is that we have an additional member  [2.x.147] , that is used for preconditioning the Schur complement, and a corresponding sparsity pattern  [2.x.148] . In addition, instead of relying on LinearOperator, we implement our own InverseMatrix class.   
*   In this example we also use adaptive grid refinement, which is handled in analogy to  [2.x.149] . According to the discussion in the introduction, we are also going to use the AffineConstraints object for implementing Dirichlet boundary conditions. Hence, we change the name  [2.x.150] .
* 

* 
* [1.x.138]
* 
*  This one is new: We shall use a so-called shared pointer structure to access the preconditioner. Shared pointers are essentially just a convenient form of pointers. Several shared pointers can point to the same object (just like regular pointers), but when the last shared pointer object to point to a preconditioner object is deleted (for example if a shared pointer object goes out of scope, if the class of which it is a member is destroyed, or if the pointer is assigned a different preconditioner object) then the preconditioner object pointed to is also destroyed. This ensures that we don't have to manually track in how many places a preconditioner object is still referenced, it can never create a memory leak, and can never produce a dangling pointer to an already destroyed object:
* 

* 
* [1.x.139]
* 
*   [1.x.140]  [1.x.141]
* 

* 
*  As in  [2.x.151]  and most other example programs, the next task is to define the data for the PDE: For the Stokes problem, we are going to use natural boundary values on parts of the boundary (i.e. homogeneous Neumann-type) for which we won't have to do anything special (the homogeneity implies that the corresponding terms in the weak form are simply zero), and boundary conditions on the velocity (Dirichlet-type) on the rest of the boundary, as described in the introduction.   
*   In order to enforce the Dirichlet boundary values on the velocity, we will use the  [2.x.152]  function as usual which requires us to write a function object with as many components as the finite element has. In other words, we have to define the function on the  [2.x.153] -space, but we are going to filter out the pressure component when interpolating the boundary values.
* 

* 
*  The following function object is a representation of the boundary values described in the introduction:
* 

* 
* [1.x.142]
* 
*  We implement similar functions for the right hand side which for the current example is simply zero:
* 

* 
* [1.x.143]
* 
*   [1.x.144]  [1.x.145]
* 

* 
*  The linear solvers and preconditioners are discussed extensively in the introduction. Here, we create the respective objects that will be used.
* 

* 
*   [1.x.146]  [1.x.147] The  [2.x.154]  class represents the data structure for an inverse matrix. Unlike  [2.x.155] , we implement this with a class instead of the helper function inverse_linear_operator() we will apply this class to different kinds of matrices that will require different preconditioners (in  [2.x.156]  we only used a non-identity preconditioner for the mass matrix). The types of matrix and preconditioner are passed to this class via template parameters, and matrix and preconditioner objects of these types will then be passed to the constructor when an  [2.x.157]  object is created. The member function  [2.x.158]  is obtained by solving a linear system:
* 

* 
* [1.x.148]
* 
*  This is the implementation of the  [2.x.159]  function.
* 

* 
*  In this class we use a rather large tolerance for the solver control. The reason for this is that the function is used very frequently, and hence, any additional effort to make the residual in the CG solve smaller makes the solution more expensive. Note that we do not only use this class as a preconditioner for the Schur complement, but also when forming the inverse of the Laplace matrix &ndash; which is hence directly responsible for the accuracy of the solution itself, so we can't choose a too large tolerance, either.
* 

* 
* [1.x.149]
* 
*   [1.x.150]  [1.x.151]
* 

* 
*  This class implements the Schur complement discussed in the introduction. It is in analogy to  [2.x.160] .  Though, we now call it with a template parameter  [2.x.161]  in order to access that when specifying the respective type of the inverse matrix class. As a consequence of the definition above, the declaration  [2.x.162]  now contains the second template parameter for a preconditioner class as above, which affects the  [2.x.163]  as well.
* 

* 
* [1.x.152]
* 
*   [1.x.153]  [1.x.154]
* 

* 
*   [1.x.155]  [1.x.156]
* 

* 
*  The constructor of this class looks very similar to the one of  [2.x.164] . The constructor initializes the variables for the polynomial degree, triangulation, finite element system and the dof handler. The underlying polynomial functions are of order  [2.x.165]  for the vector-valued velocity components and of order  [2.x.166]  for the pressure.  This gives the LBB-stable element pair  [2.x.167] , often referred to as the Taylor-Hood element.   
*   Note that we initialize the triangulation with a MeshSmoothing argument, which ensures that the refinement of cells is done in a way that the approximation of the PDE solution remains well-behaved (problems arise if grids are too unstructured), see the documentation of  [2.x.168]  for details.
* 

* 
* [1.x.157]
* 
*   [1.x.158]  [1.x.159]
* 

* 
*  Given a mesh, this function associates the degrees of freedom with it and creates the corresponding matrices and vectors. At the beginning it also releases the pointer to the preconditioner object (if the shared pointer pointed at anything at all at this point) since it will definitely not be needed any more after this point and will have to be re-computed after assembling the matrix, and unties the sparse matrices from their sparsity pattern objects.   
*   We then proceed with distributing degrees of freedom and renumbering them: In order to make the ILU preconditioner (in 3D) work efficiently, it is important to enumerate the degrees of freedom in such a way that it reduces the bandwidth of the matrix, or maybe more importantly: in such a way that the ILU is as close as possible to a real LU decomposition. On the other hand, we need to preserve the block structure of velocity and pressure already seen in  [2.x.169]  and  [2.x.170] . This is done in two steps: First, all dofs are renumbered to improve the ILU and then we renumber once again by components. Since  [2.x.171]  does not touch the renumbering within the individual blocks, the basic renumbering from the first step remains. As for how the renumber degrees of freedom to improve the ILU: deal.II has a number of algorithms that attempt to find orderings to improve ILUs, or reduce the bandwidth of matrices, or optimize some other aspect. The DoFRenumbering namespace shows a comparison of the results we obtain with several of these algorithms based on the testcase discussed here in this tutorial program. Here, we will use the traditional Cuthill-McKee algorithm already used in some of the previous tutorial programs.  In the [1.x.160] we're going to discuss this issue in more detail.
* 

* 
*  There is one more change compared to previous tutorial programs: There is no reason in sorting the  [2.x.172]  velocity components individually. In fact, rather than first enumerating all  [2.x.173] -velocities, then all  [2.x.174] -velocities, etc, we would like to keep all velocities at the same location together and only separate between velocities (all components) and pressures. By default, this is not what the  [2.x.175]  function does: it treats each vector component separately; what we have to do is group several components into "blocks" and pass this block structure to that function. Consequently, we allocate a vector  [2.x.176]  with as many elements as there are components and describe all velocity components to correspond to block 0, while the pressure component will form block 1:
* 

* 
* [1.x.161]
* 
*  Now comes the implementation of Dirichlet boundary conditions, which should be evident after the discussion in the introduction. All that changed is that the function already appears in the setup functions, whereas we were used to see it in some assembly routine. Further down below where we set up the mesh, we will associate the top boundary where we impose Dirichlet boundary conditions with boundary indicator 1.  We will have to pass this boundary indicator as second argument to the function below interpolating boundary values.  There is one more thing, though.  The function describing the Dirichlet conditions was defined for all components, both velocity and pressure. However, the Dirichlet conditions are to be set for the velocity only.  To this end, we use a ComponentMask that only selects the velocity components. The component mask is obtained from the finite element by specifying the particular components we want. Since we use adaptively refined grids, the affine constraints object needs to be first filled with hanging node constraints generated from the DoF handler. Note the order of the two functions &mdash; we first compute the hanging node constraints, and then insert the boundary values into the constraints object. This makes sure that we respect H<sup>1</sup> conformity on boundaries with hanging nodes (in three space dimensions), where the hanging node needs to dominate the Dirichlet boundary values.
* 

* 
* [1.x.162]
* 
*  In analogy to  [2.x.177] , we count the dofs in the individual components. We could do this in the same way as there, but we want to operate on the block structure we used already for the renumbering: The function  [2.x.178]  does the same as  [2.x.179] , but now grouped as velocity and pressure block via  [2.x.180] .
* 

* 
* [1.x.163]
* 
*  The next task is to allocate a sparsity pattern for the system matrix we will create and one for the preconditioner matrix. We could do this in the same way as in  [2.x.181] , i.e. directly build an object of type SparsityPattern through  [2.x.182]  However, there is a major reason not to do so: In 3D, the function  [2.x.183]  yields a conservative but rather large number for the coupling between the individual dofs, so that the memory initially provided for the creation of the sparsity pattern of the matrix is far too much
* 
*  -  so much actually that the initial sparsity pattern won't even fit into the physical memory of most systems already for moderately-sized 3D problems, see also the discussion in  [2.x.184] . Instead, we first build temporary objects that use a different data structure that doesn't require allocating more memory than necessary but isn't suitable for use as a basis of SparseMatrix or BlockSparseMatrix objects; in a second step we then copy these objects into objects of type BlockSparsityPattern. This is entirely analogous to what we already did in  [2.x.185]  and  [2.x.186] . In particular, we make use of the fact that we will never write into the  [2.x.187]  block of the system matrix and that this is the only block to be filled for the preconditioner matrix.     
*   All this is done inside new scopes, which means that the memory of  [2.x.188]  will be released once the information has been copied to  [2.x.189] .
* 

* 
* [1.x.164]
* 
*  Finally, the system matrix, the preconsitioner matrix, the solution and the right hand side vector are created from the block structure similar to the approach in  [2.x.190] :
* 

* 
* [1.x.165]
* 
*   [1.x.166]  [1.x.167]
* 

* 
*  The assembly process follows the discussion in  [2.x.191]  and in the introduction. We use the well-known abbreviations for the data structures that hold the local matrices, right hand side, and global numbering of the degrees of freedom for the present cell.
* 

* 
* [1.x.168]
* 
*  Next, we need two objects that work as extractors for the FEValues object. Their use is explained in detail in the report on  [2.x.192]  vector_valued :
* 

* 
* [1.x.169]
* 
*  As an extension over  [2.x.193]  and  [2.x.194] , we include a few optimizations that make assembly much faster for this particular problem. The improvements are based on the observation that we do a few calculations too many times when we do as in  [2.x.195] : The symmetric gradient actually has  [2.x.196]  different values per quadrature point, but we extract it  [2.x.197]  times from the FEValues object
* 
*  - for both the loop over  [2.x.198]  and the inner loop over  [2.x.199] . In 3d, that means evaluating it  [2.x.200]  instead of  [2.x.201]  times, a not insignificant difference.     
*   So what we're going to do here is to avoid such repeated calculations by getting a vector of rank-2 tensors (and similarly for the divergence and the basis function value on pressure) at the quadrature point prior to starting the loop over the dofs on the cell. First, we create the respective objects that will hold these values. Then, we start the loop over all cells and the loop over the quadrature points, where we first extract these values. There is one more optimization we implement here: the local matrix (as well as the global one) is going to be symmetric, since all the operations involved are symmetric with respect to  [2.x.202]  and  [2.x.203] . This is implemented by simply running the inner loop not to  [2.x.204] , the index of the outer loop.
* 

* 
* [1.x.170]
* 
*  Now finally for the bilinear forms of both the system matrix and the matrix we use for the preconditioner. Recall that the formulas for these two are

* 
* [1.x.171]
*  and

* 
* [1.x.172]
*  respectively, where  [2.x.205]  and  [2.x.206]  are the velocity and pressure components of the  [2.x.207] th shape function. The various terms above are then easily recognized in the following implementation:
* 

* 
* [1.x.173]
* 
*  Note that in the implementation of (1) above, `operator*` is overloaded for symmetric tensors, yielding the scalar product between the two tensors.                 
*   For the right-hand side we use the fact that the shape functions are only non-zero in one component (because our elements are primitive).  Instead of multiplying the tensor representing the dim+1 values of shape function i with the whole right-hand side vector, we only look at the only non-zero component. The function  [2.x.208]  will return which component this shape function lives in (0=x velocity, 1=y velocity, 2=pressure in 2d), which we use to pick out the correct component of the right-hand side vector to multiply with.
* 

* 
* [1.x.174]
* 
*  Before we can write the local data into the global matrix (and simultaneously use the AffineConstraints object to apply Dirichlet boundary conditions and eliminate hanging node constraints, as we discussed in the introduction), we have to be careful about one thing, though. We have only built half of the local matrices because of symmetry, but we're going to save the full matrices in order to use the standard functions for solving. This is done by flipping the indices in case we are pointing into the empty part of the local matrices.
* 

* 
* [1.x.175]
* 
*  Before we're going to solve this linear system, we generate a preconditioner for the velocity-velocity matrix, i.e.,  [2.x.209]  in the system matrix. As mentioned above, this depends on the spatial dimension. Since the two classes described by the  [2.x.210]  alias have the same interface, we do not have to do anything different whether we want to use a sparse direct solver or an ILU:
* 

* 
* [1.x.176]
* 
*   [1.x.177]  [1.x.178]
* 

* 
*  After the discussion in the introduction and the definition of the respective classes above, the implementation of the  [2.x.211]  function is rather straight-forward and done in a similar way as in  [2.x.212] . To start with, we need an object of the  [2.x.213]  class that represents the inverse of the matrix A. As described in the introduction, the inverse is generated with the help of an inner preconditioner of type  [2.x.214] .
* 

* 
* [1.x.179]
* 
*  This is as in  [2.x.215] . We generate the right hand side  [2.x.216]  for the Schur complement and an object that represents the respective linear operation  [2.x.217] , now with a template parameter indicating the preconditioner
* 
*  - in accordance with the definition of the class.
* 

* 
* [1.x.180]
* 
*  The usual control structures for the solver call are created...
* 

* 
* [1.x.181]
* 
*  Now to the preconditioner to the Schur complement. As explained in the introduction, the preconditioning is done by a mass matrix in the pressure variable.       
*   Actually, the solver needs to have the preconditioner in the form  [2.x.218] , so we need to create an inverse operation. Once again, we use an object of the class  [2.x.219] , which implements the  [2.x.220]  operation that is needed by the solver.  In this case, we have to invert the pressure mass matrix. As it already turned out in earlier tutorial programs, the inversion of a mass matrix is a rather cheap and straight-forward operation (compared to, e.g., a Laplace matrix). The CG method with ILU preconditioning converges in 5-10 steps, independently on the mesh size.  This is precisely what we do here: We choose another ILU preconditioner and take it along to the InverseMatrix object via the corresponding template parameter.  A CG solver is then called within the vmult operation of the inverse matrix.       
*   An alternative that is cheaper to build, but needs more iterations afterwards, would be to choose a SSOR preconditioner with factor 1.2. It needs about twice the number of iterations, but the costs for its generation are almost negligible.
* 

* 
* [1.x.182]
* 
*  With the Schur complement and an efficient preconditioner at hand, we can solve the respective equation for the pressure (i.e. block 0 in the solution vector) in the usual way:
* 

* 
* [1.x.183]
* 
*  After this first solution step, the hanging node constraints have to be distributed to the solution in order to achieve a consistent pressure field.
* 

* 
* [1.x.184]
* 
*  As in  [2.x.221] , we finally need to solve for the velocity equation where we plug in the solution to the pressure equation. This involves only objects we already know
* 
*  - so we simply multiply  [2.x.222]  by  [2.x.223] , subtract the right hand side and multiply by the inverse of  [2.x.224] . At the end, we need to distribute the constraints from hanging nodes in order to obtain a consistent flow field:
* 

* 
* [1.x.185]
* 
*   [1.x.186]  [1.x.187]
* 

* 
*  The next function generates graphical output. In this example, we are going to use the VTK file format.  We attach names to the individual variables in the problem:  [2.x.225]  components of velocity and  [2.x.226]  to the pressure.   
*   Not all visualization programs have the ability to group individual vector components into a vector to provide vector plots; in particular, this holds for some VTK-based visualization programs. In this case, the logical grouping of components into vectors should already be described in the file containing the data. In other words, what we need to do is provide our output writers with a way to know which of the components of the finite element logically form a vector (with  [2.x.227]  components in  [2.x.228]  space dimensions) rather than letting them assume that we simply have a bunch of scalar fields.  This is achieved using the members of the  [2.x.229]  namespace: as with the filename, we create a vector in which the first  [2.x.230]  components refer to the velocities and are given the tag  [2.x.231]  we finally push one tag  [2.x.232]  to describe the grouping of the pressure variable.
* 

* 
*  The rest of the function is then the same as in  [2.x.233] .
* 

* 
* [1.x.188]
* 
*   [1.x.189]  [1.x.190]
* 

* 
*  This is the last interesting function of the  [2.x.234]  class.  As indicated by its name, it takes the solution to the problem and refines the mesh where this is needed. The procedure is the same as in the respective step in  [2.x.235] , with the exception that we base the refinement only on the change in pressure, i.e., we call the Kelly error estimator with a mask object of type ComponentMask that selects the single scalar component for the pressure that we are interested in (we get such a mask from the finite element class by specifying the component we want). Additionally, we do not coarsen the grid again:
* 

* 
* [1.x.191]
* 
*   [1.x.192]  [1.x.193]
* 

* 
*  The last step in the Stokes class is, as usual, the function that generates the initial grid and calls the other functions in the respective order.   
*   We start off with a rectangle of size  [2.x.236]  (in 2d) or  [2.x.237]  (in 3d), placed in  [2.x.238]  as  [2.x.239]  or  [2.x.240] , respectively. It is natural to start with equal mesh size in each direction, so we subdivide the initial rectangle four times in the first coordinate direction. To limit the scope of the variables involved in the creation of the mesh to the range where we actually need them, we put the entire block between a pair of braces:
* 

* 
* [1.x.194]
* 
*  A boundary indicator of 1 is set to all boundaries that are subject to Dirichlet boundary conditions, i.e.  to faces that are located at 0 in the last coordinate direction. See the example description above for details.
* 

* 
* [1.x.195]
* 
*  We then apply an initial refinement before solving for the first time. In 3D, there are going to be more degrees of freedom, so we refine less there:
* 

* 
* [1.x.196]
* 
*  As first seen in  [2.x.241] , we cycle over the different refinement levels and refine (except for the first cycle), setup the degrees of freedom and matrices, assemble, solve and create output:
* 

* 
* [1.x.197]
* 
*   [1.x.198]  [1.x.199]
* 

* 
*  The main function is the same as in  [2.x.242] . We pass the element degree as a parameter and choose the space dimension at the well-known template slot.
* 

* 
* [1.x.200]
* [1.x.201][1.x.202][1.x.203]
* 

* [1.x.204][1.x.205]
* 

* [1.x.206][1.x.207]
* 

* Running the program with the space dimension set to 2 in the  [2.x.243] function yields the following output (in "release mode", [2.x.244] 
* [1.x.208]
* 
* The entire computation above takes about 2 seconds on a reasonablyquick (for 2015 standards) machine.
* What we see immediately from this is that the number of (outer)iterations does not increase as we refine the mesh. This confirms thestatement in the introduction that preconditioning the Schurcomplement with the mass matrix indeed yields a matrix spectrallyequivalent to the identity matrix (i.e. with eigenvalues bounded aboveand below independently of the mesh size or the relative sizes ofcells). In other words, the mass matrix and the Schur complement arespectrally equivalent.
* In the images below, we show the grids for the first six refinementsteps in the program.  Observe how the grid is refined in regionswhere the solution rapidly changes: On the upper boundary, we haveDirichlet boundary conditions that are
* 
*  -  in the left half of the lineand 1 in the right one, so there is an abrupt change at  [2.x.245] . Likewise,there are changes from Dirichlet to Neumann data in the two uppercorners, so there is need for refinement there as well:
*  [2.x.246] 
* Finally, following is a plot of the flow field. It shows fluidtransported along with the moving upper boundary and being replaced bymaterial coming from below:
*  [2.x.247] 
* This plot uses the capability of VTK-based visualization programs (inthis case of VisIt) to show vector data; this is the result of usdeclaring the velocity components of the finite element in use to be aset of vector components, rather than independent scalar components inthe  [2.x.248]  function of thistutorial program.
* 

* 
* [1.x.209][1.x.210]
* 

* In 3d, the screen output of the program looks like this:
* [1.x.211]
* 
* Again, we see that the number of outer iterations does not increase aswe refine the mesh. Nevertheless, the compute time increasessignificantly: for each of the iterations above separately, it takes about0.14 seconds, 0.63 seconds, 4.8 seconds, 35 seconds, 2 minutes and 33 seconds,and 13 minutes and 12 seconds. This overall superlinear (in the number ofunknowns) increase in runtime is due to the fact that our inner solver is not [2.x.249] : a simple experiment shows that as we keep refining the mesh, theaverage number of ILU-preconditioned CG iterations to invert thevelocity-velocity block  [2.x.250]  increases.
* We will address the question of how possibly to improve our solver [1.x.212].
* As for the graphical output, the grids generated during the solutionlook as follow:
*  [2.x.251] 
* Again, they show essentially the location of singularities introducedby boundary conditions. The vector field computed makes for aninteresting graph:
*  [2.x.252] 
* The isocontours shown here as well are those of the pressurevariable, showing the singularity at the point of discontinuousvelocity boundary conditions.
* 

* 
* [1.x.213][1.x.214]
* 

* As explained during the generation of the sparsity pattern, it isimportant to have the numbering of degrees of freedom in mind whenusing preconditioners like incomplete LU decompositions. This is mostconveniently visualized using the distribution of nonzero elements inthe stiffness matrix.
* If we don't do anything special to renumber degrees of freedom (i.e.,without using  [2.x.253]  but with using [2.x.254]  to ensure that degrees of freedom areappropriately sorted into their corresponding blocks of the matrix andvector), then we get the following image after the first adaptiverefinement in two dimensions:
*  [2.x.255] 
* In order to generate such a graph, you have to insert a piece ofcode like the following to the end of the setup step.
* [1.x.215]
* 
* It is clearly visible that the nonzero entries are spread over almost thewhole matrix.  This makes preconditioning by ILU inefficient: ILU generates aGaussian elimination (LU decomposition) without fill-in elements, which meansthat more tentative fill-ins left out will result in a worse approximation ofthe complete decomposition.
* In this program, we have thus chosen a more advanced renumbering ofcomponents.  The renumbering with  [2.x.256]  and groupingthe components into velocity and pressure yields the following output:
*  [2.x.257] 
* It is apparent that the situation has improved a lot. Most of the elements arenow concentrated around the diagonal in the (0,0) block in the matrix. Similareffects are also visible for the other blocks. In this case, the ILUdecomposition will be much closer to the full LU decomposition, which improvesthe quality of the preconditioner. (It may be interesting to note that thesparse direct solver UMFPACK does some %internal renumbering of the equationsbefore actually generating a sparse LU decomposition; that procedure leads toa very similar pattern to the one we got from the Cuthill-McKee algorithm.)
* Finally, we want to have a closerlook at a sparsity pattern in 3D. We show only the (0,0) block of thematrix, again after one adaptive refinement. Apart from the fact that the matrixsize has increased, it is also visible that there are many more entriesin the matrix. Moreover, even for the optimized renumbering, there will be aconsiderable amount of tentative fill-in elements. This illustrates why UMFPACKis not a good choice in 3D
* 
*  - a full decomposition needs many new entries that eventually won't fit into the physical memory (RAM):
*  [2.x.258] 
* 

* 
* [1.x.216][1.x.217]
* 

* [1.x.218][1.x.219][1.x.220]
* We have seen in the section of computational results that the number of outeriterations does not depend on the mesh size, which is optimal in a sense ofscalability. This does, however, not apply to the solver as a whole, asmentioned above:We did not look at the number of inner iterations when generating the inverse ofthe matrix  [2.x.259]  and the mass matrix  [2.x.260] . Of course, this is unproblematic inthe 2D case where we precondition  [2.x.261]  with a direct solver and the [2.x.262]  operation of the inverse matrix structure will converge inone single CG step, but this changes in 3D where we only use an ILUpreconditioner.  There, the number of required preconditioned CG steps toinvert  [2.x.263]  increases as the mesh is refined, and each  [2.x.264] operation involves on average approximately 14, 23, 36, 59, 75 and 101 innerCG iterations in the refinement steps shown above. (On the other hand,the number of iterations for applying the inverse pressure mass matrix isalways around five, both in two and three dimensions.)  To summarize, most workis spent on solving linear systems with the same matrix  [2.x.265]  over and over again.What makes this look even worse is the fact that weactually invert a matrix that is about 95 percent the size of the total systemmatrix and stands for 85 percent of the non-zero entries in the sparsitypattern. Hence, the natural question is whether it is reasonable to solve alinear system with matrix  [2.x.266]  for about 15 times when calculating the solutionto the block system.
* The answer is, of course, that we can do that in a few other (most of the timebetter) ways.Nevertheless, it has to be remarked that an indefinite system as the oneat hand puts indeed much higherdemands on the linear algebra than standard elliptic problems as we have seenin the early tutorial programs. The improvements are still ratherunsatisfactory, if one compares with an elliptic problem of similarsize. Either way, we will introduce below a number of improvements to thelinear solver, a discussion that we will re-consider again with additionaloptions in the  [2.x.267]  program.
* [1.x.221][1.x.222][1.x.223]A first attempt to improve the speed of the linear solution process is to choosea dof reordering that makes the ILU being closer to a full LU decomposition, asalready mentioned in the in-code comments. The DoFRenumbering namespace comparesseveral choices for the renumbering of dofs for the Stokes equations. The bestresult regarding the computing time was found for the King ordering, which isaccessed through the call  [2.x.268]  With thatprogram, the inner solver needs considerably less operations, e.g. about 62inner CG iterations for the inversion of  [2.x.269]  at cycle 4 compared to about 75iterations with the standard Cuthill-McKee-algorithm. Also, the computing timeat cycle 4 decreased from about 17 to 11 minutes for the  [2.x.270] call. However, the King ordering (and the orderings provided by the [2.x.271]  namespace in general) has a serious drawback
* 
*  - it usesmuch more memory than the in-build deal versions, since it acts on abstractgraphs rather than the geometry provided by the triangulation. In the presentcase, the renumbering takes about 5 times as much memory, which yields aninfeasible algorithm for the last cycle in 3D with 1.2 millionunknowns.
* [1.x.224][1.x.225]
* Another idea to improve the situation even more would be to choose apreconditioner that makes CG for the (0,0) matrix  [2.x.272]  converge in amesh-independent number of iterations, say 10 to 30. We have seen such acandidate in  [2.x.273] : multigrid.
* [1.x.226][1.x.227]
* [1.x.228]Even with a good preconditioner for  [2.x.274] , we stillneed to solve of the same linear system repeatedly (with differentright hand sides, though) in order to make the Schur complement solveconverge. The approach we are going to discuss here is how inner iterationand outer iteration can be combined. If we persist in calculating the Schurcomplement, there is no other possibility.
* The alternative is to attack the block system at once and use an approximateSchur complement as efficient preconditioner. The idea is asfollows: If we find a block preconditioner  [2.x.275]  such that the matrix[1.x.229]
* is simple, then an iterative solver with that preconditioner will converge in afew iterations. Using the Schur complement  [2.x.276] , one finds that[1.x.230]
* would appear to be a good choice since[1.x.231]
* This is the approach taken by the paper by Silvester and Wathen referencedto in the introduction (with the exception that Silvester and Wathen useright preconditioning). In this case, a Krylov-based iterative method wouldconverge in one step only if exact inverses of  [2.x.277]  and  [2.x.278]  were applied,since all the eigenvalues are one (and the number of iterations in such amethod is bounded by the number of distinct eigenvalues). Below, we willdiscuss the choice of an adequate solver for this problem. First, we aregoing to have a closer look at the implementation of the preconditioner.
* Since  [2.x.279]  is aimed to be a preconditioner only, we shall use approximations tothe inverse of the Schur complement  [2.x.280]  and the matrix  [2.x.281] . Hence, the Schurcomplement will be approximated by the pressure mass matrix  [2.x.282] , and we usea preconditioner to  [2.x.283]  (without an InverseMatrix class around it) forapproximating  [2.x.284] .
* Here comes the class that implements the block Schurcomplement preconditioner. The  [2.x.285]  operation for block vectorsaccording to the derivation above can be specified by three successiveoperations:
* [1.x.232]
* 
* Since we act on the whole block system now, we have to live with onedisadvantage: we need to perform the solver iterations onthe full block system instead of the smaller pressure space.
* Now we turn to the question which solver we should use for the blocksystem. The first observation is that the resulting preconditioned matrix cannotbe solved with CG since it is neither positive definite nor symmetric.
* The deal.II libraries implement several solvers that are appropriate for theproblem at hand. One choice is the solver  [2.x.286]  "BiCGStab", whichwas used for the solution of the unsymmetric advection problem in  [2.x.287] . Thesecond option, the one we are going to choose, is  [2.x.288]  "GMRES"(generalized minimum residual). Both methods have their pros and cons
* 
*  - thereare problems where one of the two candidates clearly outperforms the other, andvice versa.[1.x.233]'sarticle on the GMRES method gives a comparative presentation.A more comprehensive and well-founded comparison can be read e.g. in the book byJ.W. Demmel (Applied Numerical Linear Algebra, SIAM, 1997, section 6.6.6).
* For our specific problem with the ILU preconditioner for  [2.x.289] , we certainly needto perform hundreds of iterations on the block system for large problem sizes(we won't beat CG!). Actually, this disfavors GMRES: During the GMRESiterations, a basis of Krylov vectors is successively built up and someoperations are performed on these vectors. The more vectors are in this basis,the more operations and memory will be needed. The number of operations scalesas  [2.x.290]  and memory as  [2.x.291] , where  [2.x.292]  is the number ofvectors in the Krylov basis and  [2.x.293]  the size of the (block) matrix.To not let these demands grow excessively, deal.II limits the size  [2.x.294]  of thebasis to 30 vectors by default.Then, the basis is rebuilt. This implementation of the GMRES method is calledGMRES(k), with default  [2.x.295] . What we have gained by this restriction,namely a bound on operations and memory requirements, will be compensated bythe fact that we use an incomplete basis
* 
*  - this will increase the number ofrequired iterations.
* BiCGStab, on the other hand, won't get slower when many iterations are needed(one iteration uses only results from one preceding step andnot all the steps as GMRES). Besides the fact the BiCGStab is more expensive perstep since two matrix-vector products are needed (compared to one forCG or GMRES), there is one main reason which makes BiCGStab not appropriate forthis problem: The preconditioner applies the inverse of the pressuremass matrix by using the InverseMatrix class. Since the application of theinverse matrix to a vector is done only in approximative way (an exact inverseis too expensive), this will also affect the solver. In the case of BiCGStab,the Krylov vectors will not be orthogonal due to that perturbation. Whilethis is uncritical for a small number of steps (up to about 50), it ruins theperformance of the solver when these perturbations have grown to a significantmagnitude in the coarse of iterations.
* We did some experiments with BiCGStab and found it tobe faster than GMRES up to refinement cycle 3 (in 3D), but it became very slowfor cycles 4 and 5 (even slower than the original Schur complement), so thesolver is useless in this situation. Choosing a sharper tolerance for theinverse matrix class ( [2.x.296]  instead of [2.x.297] ) made BiCGStab perform well also for cycle 4,but did not change the failure on the very large problems.
* GMRES is of course also effected by the approximate inverses, but it is not assensitive to orthogonality and retains a relatively good performance also forlarge sizes, see the results below.
* With this said, we turn to the realization of the solver call with GMRES with [2.x.298]  temporary vectors:
* [1.x.234]
* 
* Obviously, one needs to add the include file  [2.x.299] "<lac/solver_gmres.h>" in order to make this run.We call the solver with a BlockVector template in order to enableGMRES to operate on block vectors and matrices.Note also that we need to set the (1,1) block in the systemmatrix to zero (we saved the pressure mass matrix there which is not part of theproblem) after we copied the information to another matrix.
* Using the Timer class, we collect some statistics that compare the runtimeof the block solver with the one from the problem implementation above.Besides the solution with the two options we also check if the solutionsof the two variants are close to each other (i.e. this solver gives indeed thesame solution as we had before) and calculate the infinitynorm of the vector difference.
* Let's first see the results in 2D:
* [1.x.235]
* 
* We see that there is no huge difference in the solution time between theblock Schur complement preconditioner solver and the Schur complementitself. The reason is simple: we used a direct solve as preconditioner for [2.x.300] 
* 
*  - so we cannot expect any gain by avoiding the inner iterations. We seethat the number of iterations has slightly increased for GMRES, but all inall the two choices are fairly similar.
* The picture of course changes in 3D:
* [1.x.236]
* 
* Here, the block preconditioned solver is clearly superior to the Schurcomplement, but the advantage gets less for more mesh points. This isbecause GMRES(k) scales worse with the problem size than CG, as we discussedabove.  Nonetheless, the improvement by a factor of 3-6 for moderate problemsizes is quite impressive.
* 

* [1.x.237][1.x.238]
* An ultimate linear solver for this problem could be imagined as acombination of an optimalpreconditioner for  [2.x.301]  (e.g. multigrid) and the block preconditionerdescribed above, which is the approach taken in the  [2.x.302] and  [2.x.303]  tutorial programs (where we use an algebraic multigridmethod) and  [2.x.304]  (where we use a geometric multigrid method).
* 

* [1.x.239][1.x.240]
* Another possibility that can be taken into account is to not set up a blocksystem, but rather solve the system of velocity and pressure all at once. Theoptions are direct solve with UMFPACK (2D) or GMRES with ILUpreconditioning (3D). It should be straightforward to try that.
* 

* 
* [1.x.241][1.x.242]
* 

* The program can of course also serve as a basis to compute the flow in moreinteresting cases. The original motivation to write this program was for it tobe a starting point for some geophysical flow problems, such as themovement of magma under places where continental plates drift apart (forexample mid-ocean ridges). Of course, in such places, the geometry is morecomplicated than the examples shown above, but it is not hard to accommodatefor that.
* For example, by using the following modification of the boundary valuesfunction
* [1.x.243]
* and the following way to generate the mesh as the domain [2.x.305] 
* [1.x.244]
* then we get images where the fault line is curved: [2.x.306] 
* 

* [1.x.245][1.x.246] [2.x.307] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-23_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22]
* [1.x.23][1.x.24][1.x.25]
* 

*  [2.x.2] 
* This is the first of a number of tutorial programs that will finallycover "real" time-dependent problems, not the slightly odd form of timedependence found in  [2.x.3]  or the DAE model of  [2.x.4] . In particular, this program introducesthe wave equation in a bounded domain. Later,  [2.x.5] will consider an example of absorbing boundary conditions, and  [2.x.6]  " [2.x.7] " a kind of nonlinear wave equation producingsolutions called solitons.
* The wave equation in its prototypical form reads as follows: find [2.x.8]  that satisfies[1.x.26]
* Note that since this is an equation with second-order timederivatives, we need to pose two initial conditions, one for the valueand one for the time derivative of the solution.
* Physically, the equation describes the motion of an elastic medium. In2-d, one can think of how a membrane moves if subjected to aforce. The Dirichlet boundary conditions above indicate that themembrane is clamped at the boundary at a height  [2.x.9]  (this heightmight be moving as well &mdash; think of people holding a blanket andshaking it up and down). The first initial condition equals theinitial deflection of the membrane, whereas the second one gives itsvelocity. For example, one could think of pushing the membrane downwith a finger and then letting it go at  [2.x.10]  (nonzero deflection butzero initial velocity), or hitting it with a hammer at  [2.x.11]  (zerodeflection but nonzero velocity). Both cases would induce motion inthe membrane.
* 

* [1.x.27][1.x.28]
* 

* [1.x.29][1.x.30]
* There is a long-standing debate in the numerical analysis communityover whether a discretization of time dependent equations shouldinvolve first discretizing the time variable leading to a stationaryPDE at each time step that is then solved using standard finiteelement techniques (this is called the Rothe method), or whetherone should first discretize the spatial variables, leading to a largesystem of ordinary differential equations that can then be handled byone of the usual ODE solvers (this is called the method of lines).
* Both of these methods have advantages and disadvantages.Traditionally, people have preferred the method of lines, since itallows to use the very well developed machinery of high-order ODEsolvers available for the rather stiff ODEs resulting from thisapproach, including step length control and estimation of the temporalerror.
* On the other hand, Rothe's method becomes awkward when usinghigher-order time stepping method, since one then has to write down aPDE that couples the solution of the present time step not only withthat at the previous time step, but possibly also even earliersolutions, leading to a significant number of terms.
* For these reasons, the method of lines was the method of choice for along time. However, it has one big drawback: if we discretize thespatial variable first, leading to a large ODE system, we have tochoose a mesh once and for all. If we are willing to do this, thenthis is a legitimate and probably superior approach.
* If, on the other hand, we are looking at the wave equation and manyother time dependent problems, we find that the character of asolution changes as time progresses. For example, for the waveequation, we may have a single wave travelling through the domain,where the solution is smooth or even constant in front of and behindthe wave &mdash; adaptivity would be really useful for such cases, but thekey is that the area where we need to refine the mesh changes fromtime step to time step!
* If we intend to go that way, i.e. choose a different mesh for eachtime step (or set of time steps), then the method of lines is notappropriate any more: instead of getting one ODE system with a numberof variables equal to the number of unknowns in the finite elementmesh, our number of unknowns now changes all the time, a fact thatstandard ODE solvers are certainly not prepared to deal with atall. On the other hand, for the Rothe method, we just get a PDE foreach time step that we may choose to discretize independently of themesh used for the previous time step; this approach is not withoutperils and difficulties, but at least is a sensible and well-definedprocedure.
* For all these reasons, for the present program, we choose to use theRothe method for discretization, i.e. we first discretize in time andthen in space. We will not actually use adaptive meshes at all, sincethis involves a large amount of additional code, but we will commenton this some more in the [1.x.31].
* 

* [1.x.32][1.x.33]
* 

* Given these considerations, here is how we will proceed: let us firstdefine a simple time stepping method for this second order problem,and then in a second step do the spatial discretization, i.e. we willfollow Rothe's approach.
* For the first step, let us take a little detour first: in order todiscretize a second time derivative, we can either discretize itdirectly, or we can introduce an additional variable and transform thesystem into a first order system. In many cases, this turns out to beequivalent, but dealing with first order systems is often simpler. Tothis end, let us introduce[1.x.34]and call this variable the [1.x.35] for obvious reasons. We canthen reformulate the original wave equation as follows:[1.x.36]
* The advantage of this formulation is that it now only contains firsttime derivatives for both variables, for which it is simple to writedown time stepping schemes. Note that we do not have boundaryconditions for  [2.x.12]  at first. However, we could enforce  [2.x.13]  on the boundary. It turns out in numerical examples that thisis actually necessary: without doing so the solution doesn't look particularlywrong, but the Crank-Nicolson scheme does not conserve energy if one doesn'tenforce these boundary conditions.
* With this formulation, let us introduce the following timediscretization where a superscript  [2.x.14]  indicates the number of a timestep and  [2.x.15]  is the length of the present time step:[1.x.37]Note how we introduced a parameter  [2.x.16]  here. If we chose [2.x.17] , for example, the first equation would reduce to [2.x.18] , which is well-known as theforward or explicit Euler method. On the other hand, if we set [2.x.19] , then we would get [2.x.20] , which corresponds to thebackward or implicit Euler method. Both these methods are first orderaccurate methods. They are simple to implement, but they are notreally very accurate.
* The third case would be to choose  [2.x.21] . The first of theequations above would then read  [2.x.22] . This method is known asthe Crank-Nicolson method and has the advantage that it is secondorder accurate. In addition, it has the nice property that itpreserves the energy in the solution (physically, the energy is thesum of the kinetic energy of the particles in the membrane plus thepotential energy present due to the fact that it is locally stretched;this quantity is a conserved one in the continuous equation, but mosttime stepping schemes do not conserve it after timediscretization). Since  [2.x.23]  also appears in the equation for  [2.x.24] ,the Crank-Nicolson scheme is also implicit.
* In the program, we will leave  [2.x.25]  as a parameter, so that it willbe easy to play with it. The results section will show some numericalevidence comparing the different schemes.
* The equations above (called the [1.x.38] equationsbecause we have only discretized the time, but not space), can besimplified a bit by eliminating  [2.x.26]  from the first equation andrearranging terms. We then get[1.x.39]In this form, we see that if we are given the solution [2.x.27]  of the previous timestep, that we can then solve forthe variables  [2.x.28]  separately, i.e. one at a time. This isconvenient. In addition, we recognize that the operator in the firstequation is positive definite, and the second equation looksparticularly simple.
* 

* [1.x.40][1.x.41]
* 

* We have now derived equations that relate the approximate(semi-discrete) solution  [2.x.29]  and its time derivative  [2.x.30]  attime  [2.x.31]  with the solutions  [2.x.32]  of the previoustime step at  [2.x.33] . The next step is to also discretize thespatial variable using the usual finite element methodology. To thisend, we multiply each equation with a test function, integrate overthe entire domain, and integrate by parts where necessary. This leadsto[1.x.42]
* It is then customary to approximate  [2.x.34] , where  [2.x.35]  are the shape functions usedfor the discretization of the  [2.x.36] -th time step and  [2.x.37]  are theunknown nodal values of the solution. Similarly,  [2.x.38] . Finally, we have the solutions ofthe previous time step,  [2.x.39]  and  [2.x.40] . Note that since the solution of the previoustime step has already been computed by the time we get to time step [2.x.41] ,  [2.x.42]  are known. Furthermore, note that the solutionsof the previous step may have been computed on a different mesh, sowe have to use shape functions  [2.x.43] .
* If we plug these expansions into above equations and test with thetest functions from the present mesh, we get the following linearsystem:[1.x.43]where[1.x.44]
* 
* If we solve these two equations, we can move the solution one stepforward and go on to the next time step.
* It is worth noting that if we choose the same mesh on each time step(as we will in fact do in the program below), then we have the sameshape functions on time step  [2.x.44]  and  [2.x.45] ,i.e.  [2.x.46] . Consequently, we get [2.x.47]  and  [2.x.48] . On the other hand, if we hadused different shape functions, then we would have to computeintegrals that contain shape functions defined on two meshes. This is asomewhat messy process that we omit here, but that is treated in somedetail in  [2.x.49] .
* Under these conditions (i.e. a mesh that doesn't change), one can optimize thesolution procedure a bit by basically eliminating the solution of the secondlinear system. We will discuss this in the introduction of the  [2.x.50] " [2.x.51] " program.
* [1.x.45][1.x.46]
* 

* One way to compare the quality of a time stepping scheme is to see whether thenumerical approximation preserves conservation properties of the continuousequation. For the wave equation, the natural quantity to look at is theenergy. By multiplying the wave equation by  [2.x.52] , integrating over  [2.x.53] ,and integrating by parts where necessary, we find that[1.x.47]By consequence, in absence of body forces and constant boundary values, we getthat[1.x.48]is a conserved quantity, i.e. one that doesn't change with time. Wewill compute this quantity after each timestep. It is straightforward to see that if we replace  [2.x.54]  by its finiteelement approximation, and  [2.x.55]  by the finiteelement approximation of the velocity  [2.x.56] , then[1.x.49]As we will see in the results section, the Crank-Nicolson scheme does indeedconserve the energy, whereas neither the forward nor the backward Euler schemedo.
* 

* [1.x.50][1.x.51]
* 

* One of the reasons why the wave equation is nasty to solve numerically is thatexplicit time discretizations are only stable if the time step is smallenough. In particular, it is coupled to the spatial mesh width  [2.x.57] . For thelowest order discretization we use here, the relationship reads[1.x.52]where  [2.x.58]  is the wave speed, which in our formulation of the wave equation hasbeen normalized to one. Consequently, unless we use the implicit schemes with [2.x.59] , our solutions will not be numerically stable if we violate thisrestriction. Implicit schemes do not have this restriction for stability, butthey become inaccurate if the time step is too large.
* This condition was first recognized by Courant, Friedrichs, and Lewy &mdash;in 1928, long before computers became available for numericalcomputations! (This result appeared in the German language articleR. Courant, K. Friedrichs and H. Lewy: [1.x.53], MathematischeAnnalen, vol. 100, no. 1, pages 32-74, 1928.)This condition on the time step is most frequently just referredto as the [1.x.54] condition. Intuitively, the CFL condition saysthat the time step must not be larger than the time it takes a wave tocross a single cell.
* In the program, we will refine the square [2.x.60]  seven times uniformly, giving a mesh size of  [2.x.61] , whichis what we set the time step to. The fact that we set the time step and meshsize individually in two different places is error prone: it is too easy torefine the mesh once more but forget to also adjust the time step.  [2.x.62]  " [2.x.63] " shows a better way how to keep these things in sync.
* 

* [1.x.55][1.x.56]
* 

* Although the program has all the hooks to deal with nonzero initial andboundary conditions and body forces, we take a simple case where the domain isa square  [2.x.64]  and[1.x.57]
* This corresponds to a membrane initially at rest and clamped all around, wheresomeone is waving a part of the clamped boundary once up and down, therebyshooting a wave into the domain.
* 

*  [1.x.58] [1.x.59]
*   [1.x.60]  [1.x.61]
* 

* 
*  We start with the usual assortment of include files that we've seen in so many of the previous tests:
* 

* 
* [1.x.62]
* 
*  Here are the only three include files of some new interest: The first one is already used, for example, for the  [2.x.65]  and  [2.x.66]  functions. However, we here use another function in that class,  [2.x.67]  to compute our initial values as the  [2.x.68]  projection of the continuous initial values. Furthermore, we use  [2.x.69]  to generate the integrals  [2.x.70] . These were previously always generated by hand in  [2.x.71]  or similar functions in application code. However, we're too lazy to do that here, so simply use a library function:
* 

* 
* [1.x.63]
* 
*  In a very similar vein, we are also too lazy to write the code to assemble mass and Laplace matrices, although it would have only taken copying the relevant code from any number of previous tutorial programs. Rather, we want to focus on the things that are truly new to this program and therefore use the  [2.x.72]  and  [2.x.73]  functions. They are declared here:
* 

* 
* [1.x.64]
* 
*  Finally, here is an include file that contains all sorts of tool functions that one sometimes needs. In particular, we need the  [2.x.74]  class that, given an integer argument, returns a string representation of it. It is particularly useful since it allows for a second parameter indicating the number of digits to which we want the result padded with leading zeros. We will use this to write output files that have the form  [2.x.75]  denotes the number of the time step and always consists of three digits even if we are still in the single or double digit time steps.
* 

* 
* [1.x.65]
* 
*  The last step is as in all previous programs:
* 

* 
* [1.x.66]
* 
*   [1.x.67]  [1.x.68]
* 

* 
*  Next comes the declaration of the main class. It's public interface of functions is like in most of the other tutorial programs. Worth mentioning is that we now have to store four matrices instead of one: the mass matrix  [2.x.76] , the Laplace matrix  [2.x.77] , the matrix  [2.x.78]  used for solving for  [2.x.79] , and a copy of the mass matrix with boundary conditions applied used for solving for  [2.x.80] . Note that it is a bit wasteful to have an additional copy of the mass matrix around. We will discuss strategies for how to avoid this in the section on possible improvements.   
*   Likewise, we need solution vectors for  [2.x.81]  as well as for the corresponding vectors at the previous time step,  [2.x.82] . The  [2.x.83]  will be used for whatever right hand side vector we have when solving one of the two linear systems in each time step. These will be solved in the two functions  [2.x.84]  and  [2.x.85] .   
*   Finally, the variable  [2.x.86]  is used to indicate the parameter  [2.x.87]  that is used to define which time stepping scheme to use, as explained in the introduction. The rest is self-explanatory.
* 

* 
* [1.x.69]
* 
*   [1.x.70]  [1.x.71]
* 

* 
*  Before we go on filling in the details of the main class, let us define the equation data corresponding to the problem, i.e. initial and boundary values for both the solution  [2.x.88]  and its time derivative  [2.x.89] , as well as a right hand side class. We do so using classes derived from the Function class template that has been used many times before, so the following should not be a surprise.   
*   Let's start with initial values and choose zero for both the value  [2.x.90]  as well as its time derivative, the velocity  [2.x.91] :
* 

* 
* [1.x.72]
* 
*  Secondly, we have the right hand side forcing term. Boring as we are, we choose zero here as well:
* 

* 
* [1.x.73]
* 
*  Finally, we have boundary values for  [2.x.92]  and  [2.x.93] . They are as described in the introduction, one being the time derivative of the other:
* 

* 
* [1.x.74]
* 
*   [1.x.75]  [1.x.76]
* 

* 
*  The implementation of the actual logic is actually fairly short, since we relegate things like assembling the matrices and right hand side vectors to the library. The rest boils down to not much more than 130 lines of actual code, a significant fraction of which is boilerplate code that can be taken from previous example programs (e.g. the functions that solve linear systems, or that generate output).   
*   Let's start with the constructor (for an explanation of the choice of time step, see the section on Courant, Friedrichs, and Lewy in the introduction):
* 

* 
* [1.x.77]
* 
*   [1.x.78]  [1.x.79]
* 

* 
*  The next function is the one that sets up the mesh, DoFHandler, and matrices and vectors at the beginning of the program, i.e. before the first time step. The first few lines are pretty much standard if you've read through the tutorial programs at least up to  [2.x.94] :
* 

* 
* [1.x.80]
* 
*  Then comes a block where we have to initialize the 3 matrices we need in the course of the program: the mass matrix, the Laplace matrix, and the matrix  [2.x.95]  used when solving for  [2.x.96]  in each time step.     
*   When setting up these matrices, note that they all make use of the same sparsity pattern object. Finally, the reason why matrices and sparsity patterns are separate objects in deal.II (unlike in many other finite element or linear algebra classes) becomes clear: in a significant fraction of applications, one has to hold several matrices that happen to have the same sparsity pattern, and there is no reason for them not to share this information, rather than re-building and wasting memory on it several times.     
*   After initializing all of these matrices, we call library functions that build the Laplace and mass matrices. All they need is a DoFHandler object and a quadrature formula object that is to be used for numerical integration. Note that in many respects these functions are better than what we would usually do in application programs, for example because they automatically parallelize building the matrices if multiple processors are available in a machine: for more information see the documentation of WorkStream or the  [2.x.97]  "Parallel computing with multiple processors" module. The matrices for solving linear systems will be filled in the run() method because we need to re-apply boundary conditions every time step.
* 

* 
* [1.x.81]
* 
*  The rest of the function is spent on setting vector sizes to the correct value. The final line closes the hanging node constraints object. Since we work on a uniformly refined mesh, no constraints exist or have been computed (i.e. there was no need to call  [2.x.98]  as in other programs), but we need a constraints object in one place further down below anyway.
* 

* 
* [1.x.82]
* 
*   [1.x.83]  [1.x.84]
* 

* 
*  The next two functions deal with solving the linear systems associated with the equations for  [2.x.99]  and  [2.x.100] . Both are not particularly interesting as they pretty much follow the scheme used in all the previous tutorial programs.   
*   One can make little experiments with preconditioners for the two matrices we have to invert. As it turns out, however, for the matrices at hand here, using Jacobi or SSOR preconditioners reduces the number of iterations necessary to solve the linear system slightly, but due to the cost of applying the preconditioner it is no win in terms of run-time. It is not much of a loss either, but let's keep it simple and just do without:
* 

* 
* [1.x.85]
* 
*   [1.x.86]  [1.x.87]
* 

* 
*  Likewise, the following function is pretty much what we've done before. The only thing worth mentioning is how here we generate a string representation of the time step number padded with leading zeros to 3 character length using the  [2.x.101]  function's second argument.
* 

* 
* [1.x.88]
* 
*  Like  [2.x.102] , since we write output at every time step (and the system we have to solve is relatively easy), we instruct DataOut to use the zlib compression algorithm that is optimized for speed instead of disk usage since otherwise plotting the output becomes a bottleneck:
* 

* 
* [1.x.89]
* 
*   [1.x.90]  [1.x.91]
* 

* 
*  The following is really the only interesting function of the program. It contains the loop over all time steps, but before we get to that we have to set up the grid, DoFHandler, and matrices. In addition, we have to somehow get started with initial values. To this end, we use the  [2.x.103]  function that takes an object that describes a continuous function and computes the  [2.x.104]  projection of this function onto the finite element space described by the DoFHandler object. Can't be any simpler than that:
* 

* 
* [1.x.92]
* 
*  The next thing is to loop over all the time steps until we reach the end time ( [2.x.105]  in this case). In each time step, we first have to solve for  [2.x.106] , using the equation  [2.x.107]   [2.x.108]   [2.x.109] . Note that we use the same mesh for all time steps, so that  [2.x.110]  and  [2.x.111] . What we therefore have to do first is to add up  [2.x.112]  and the forcing terms, and put the result into the  [2.x.113]  vector. (For these additions, we need a temporary vector that we declare before the loop to avoid repeated memory allocations in each time step.)     
*   The one thing to realize here is how we communicate the time variable to the object describing the right hand side: each object derived from the Function class has a time field that can be set using the  [2.x.114]  and read by  [2.x.115]  In essence, using this mechanism, all functions of space and time are therefore considered functions of space evaluated at a particular time. This matches well what we typically need in finite element programs, where we almost always work on a single time step at a time, and where it never happens that, for example, one would like to evaluate a space-time function for all times at any given spatial location.
* 

* 
* [1.x.93]
* 
*  After so constructing the right hand side vector of the first equation, all we have to do is apply the correct boundary values. As for the right hand side, this is a space-time function evaluated at a particular time, which we interpolate at boundary nodes and then use the result to apply boundary values as we usually do. The result is then handed off to the solve_u() function:
* 

* 
* [1.x.94]
* 
*  The matrix for solve_u() is the same in every time steps, so one could think that it is enough to do this only once at the beginning of the simulation. However, since we need to apply boundary values to the linear system (which eliminate some matrix rows and columns and give contributions to the right hand side), we have to refill the matrix in every time steps before we actually apply boundary data. The actual content is very simple: it is the sum of the mass matrix and a weighted Laplace matrix:
* 

* 
* [1.x.95]
* 
*  The second step, i.e. solving for  [2.x.116] , works similarly, except that this time the matrix on the left is the mass matrix (which we copy again in order to be able to apply boundary conditions, and the right hand side is  [2.x.117]  plus forcing terms. Boundary values are applied in the same way as before, except that now we have to use the BoundaryValuesV class:
* 

* 
* [1.x.96]
* 
*  Finally, after both solution components have been computed, we output the result, compute the energy in the solution, and go on to the next time step after shifting the present solution into the vectors that hold the solution at the previous time step. Note the function  [2.x.118]  that can compute  [2.x.119]  and  [2.x.120]  in one step, saving us the expense of a temporary vector and several lines of code:
* 

* 
* [1.x.97]
* 
*   [1.x.98]  [1.x.99]
* 

* 
*  What remains is the main function of the program. There is nothing here that hasn't been shown in several of the previous programs:
* 

* 
* [1.x.100]
* [1.x.101][1.x.102]
* 

* When the program is run, it produces the following output:
* [1.x.103]
* 
* What we see immediately is that the energy is a constant at least after [2.x.121]  (until which the boundary source term  [2.x.122]  is nonzero, injectingenergy into the system).
* In addition to the screen output, the program writes the solution of each timestep to an output file. If we process them adequately and paste them into amovie, we get the following:
*  [2.x.123] 
* The movie shows the generated wave nice traveling through the domain and back,being reflected at the clamped boundary. Some numerical noise is trailing thewave, an artifact of a too-large mesh size that can be reduced by reducing themesh width and the time step.
* 

* [1.x.104][1.x.105][1.x.106]
* 

* If you want to explore a bit, try out some of the following things: [2.x.124]    [2.x.125] Varying  [2.x.126] . This gives different time stepping schemes, some of  which are stable while others are not. Take a look at how the energy  evolves.
*    [2.x.127] Different initial and boundary conditions, right hand sides.
*    [2.x.128] More complicated domains or more refined meshes. Remember that the time  step needs to be bounded by the mesh width, so changing the mesh should  always involve also changing the time step. We will come back to this issue  in  [2.x.129] .
*    [2.x.130] Variable coefficients: In real media, the wave speed is often  variable. In particular, the "real" wave equation in realistic media would  read  [1.x.107]  where  [2.x.131]  is the density of the material, and  [2.x.132]  is related to the  stiffness coefficient. The wave speed is then  [2.x.133] .
*   To make such a change, we would have to compute the mass and Laplace  matrices with a variable coefficient. Fortunately, this isn't too hard: the  functions  [2.x.134]  and   [2.x.135]  have additional default parameters that can  be used to pass non-constant coefficient functions to them. The required  changes are therefore relatively small. On the other hand, care must be  taken again to make sure the time step is within the allowed range.
*    [2.x.136] In the in-code comments, we discussed the fact that the matrices for  solving for  [2.x.137]  and  [2.x.138]  need to be reset in every time because of  boundary conditions, even though the actual content does not change. It is  possible to avoid copying by not eliminating columns in the linear systems,  which is implemented by appending a  [2.x.139]  argument to the call: 
* [1.x.108]
* 
*    [2.x.140] deal.II being a library that supports adaptive meshes it would of course be  nice if this program supported change the mesh every few time steps. Given the  structure of the solution &mdash; a wave that travels through the domain &mdash;  it would seem appropriate if we only refined the mesh where the wave currently is,  and not simply everywhere. It is intuitively clear that we should be able to  save a significant amount of cells this way. (Though upon further thought one  realizes that this is really only the case in the initial stages of the simulation.  After some time, for wave phenomena, the domain is filled with reflections of  the initial wave going in every direction and filling every corner of the domain.  At this point, there is in general little one can gain using local mesh  refinement.)
*   To make adaptively changing meshes possible, there are basically two routes.  The "correct" way would be to go back to the weak form we get using Rothe's  method. For example, the first of the two equations to be solved in each time  step looked like this:  [1.x.109]  Now, note that we solve for  [2.x.141]  on mesh  [2.x.142] , and  consequently the test functions  [2.x.143]  have to be from the space   [2.x.144]  as well. As discussed in the introduction, terms like   [2.x.145]  then require us to integrate the solution of the  previous step (which may have been computed on a different mesh   [2.x.146] ) against the test functions of the current mesh,  leading to a matrix  [2.x.147] . This process of integrating shape  functions from different meshes is, at best, awkward. It can be done  but because it is difficult to ensure that  [2.x.148]  and   [2.x.149]  differ by at most one level of refinement, one  has to recursively match cells from both meshes. It is feasible to  do this, but it leads to lengthy and not entirely obvious code.
*   The second approach is the following: whenever we change the mesh,  we simply interpolate the solution from the last time step on the old  mesh to the new mesh, using the SolutionTransfer class. In other words,  instead of the equation above, we would solve  [1.x.110]  where  [2.x.150]  interpolates a given function onto mesh  [2.x.151] .  This is a much simpler approach because, in each time step, we no  longer have to worry whether  [2.x.152]  were computed on the  same mesh as we are using now or on a different mesh. Consequently,  the only changes to the code necessary are the addition of a function  that computes the error, marks cells for refinement, sets up a  SolutionTransfer object, transfers the solution to the new mesh, and  rebuilds matrices and right hand side vectors on the new mesh. Neither  the functions building the matrices and right hand sides, nor the  solvers need to be changed.
*   While this second approach is, strictly speaking,  not quite correct in the Rothe framework (it introduces an addition source  of error, namely the interpolation), it is nevertheless what  almost everyone solving time dependent equations does. We will use this  method in  [2.x.153] , for example. [2.x.154] 
* 

* [1.x.111][1.x.112] [2.x.155] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-24_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20]
* [1.x.21][1.x.22][1.x.23]
* 

* This program grew out of a student project by Xing Jin at Texas A&amp;MUniversity. Most of the work for this program is by her. Some of the work onthis tutorial program has been funded by NSF under grant DMS-0604778.
* The program is part of a project that aims to simulate thermoacoustictomography imaging. In thermoacoustic tomography, pulsed electromagneticenergy is delivered into biological issues. Tissues absorb some of this energyand those parts of the tissue that absorb the most energy generatethermoacoustic waves through thermoelastic expansion. For imaging, one usesthat different kinds of tissue, most importantly healthy and diseased tissue,absorb different amounts of energy and therefore expand at differentrates. The experimental setup is to measure the amplitude of the pressurewaves generated by these sources on the surface of the tissue and try toreconstruct the source distributions, which is indicative for the distributionof absorbers and therefore of different kinds of tissue. Part of this projectis to compare simulated data with actual measurements, so one has to solve the"forward problem", i.e. the wave equation that describes the propagation ofpressure waves in tissue. This program is therefore a continuation of  [2.x.2]  " [2.x.3] ", where the wave equation was first introduced.
* 

* [1.x.24][1.x.25]
* 

* The temperature at a given location, neglecting thermal diffusion, can bestated as
* [1.x.26]
* Here  [2.x.4]  is the density;  [2.x.5]  is the specificheat;  [2.x.6]  is the temperature rise dueto the delivered microwave energy; and  [2.x.7]  is the heatingfunction defined as the thermal energy per time and volume transformed fromdeposited microwave energy.
* Let us assume that tissues have heterogeneous dielectric properties buthomogeneous acoustic properties. The basic acoustic generation equation in anacoustically homogeneous medium can be described as follows: if  [2.x.8]  is thevector-valued displacement, then tissue certainly reacts to changes inpressure by acceleration:[1.x.27]Furthermore, it contracts due to excess pressure and expands based on changes in temperature:[1.x.28]Here,  [2.x.9]  is a thermoexpansion coefficient.
* Let us now make the assumption that heating only happens on a timescale much shorter than wave propagation through tissue (i.e. the temporallength of the microwave pulse that heats the tissue is much shorter than thetime it takes a wave to cross the domain). In that case, the heatingrate  [2.x.10]  can be written as  [2.x.11]  (where  [2.x.12]  is a map of absorption strengths formicrowave energy and  [2.x.13]  is the Dirac delta function), which togetherwith the first equation above will yieldan instantaneous jump in the temperature  [2.x.14]  at time  [2.x.15] .Using this assumption, and taking all equations together, we canrewrite and combine the above as follows:[1.x.29]where  [2.x.16] .
* This somewhat strange equation with the derivative of a Dirac delta functionon the right hand side can be rewritten as an initial value problem as follows:[1.x.30]
* (A derivation of this transformation into an initial value problem is given atthe end of this introduction as an appendix.)
* In the inverse problem, it is the initial condition  [2.x.17]  thatone would like to recover, since it is a map of absorption strengths formicrowave energy, and therefore presumably an indicator to discern healthyfrom diseased tissue.
* In real application, the thermoacoustic source is very small as compared tothe medium.  The propagation path of the thermoacoustic waves can then beapproximated as from the source to the infinity. Furthermore, detectors areonly a limited distance from the source. One only needs to evaluate the valueswhen the thermoacoustic waves pass through the detectors, although they docontinue beyond. This is therefore a problem where we are only interested in asmall part of an infinite medium, and we do not want waves generated somewhereto be reflected at the boundary of the domain which we considerinteresting. Rather, we would like to simulate only that part of the wavefield that is contained inside the domain of interest, and waves that hit theboundary of that domain to simply pass undisturbed through the boundary. Inother words, we would like the boundary to absorb any waves that hit it.
* In general, this is a hard problem: Good absorbing boundary conditions arenonlinear and/or numerically very expensive. We therefore opt for a simplefirst order approximation to absorbing boundary conditions that reads[1.x.31]Here,  [2.x.18]  is the normal derivative atthe boundary. It should be noted that this is not a particularly good boundarycondition, but it is one of the very few that are reasonably simple to implement.
* 

* [1.x.32][1.x.33]
* 

* As in  [2.x.19] , one first introduces a second variable, which isdefined as the derivative of the pressure potential:[1.x.34]
* With the second variable, one then transforms the forward problem intotwo separate equations:[1.x.35]
* with initial conditions:[1.x.36]
* Note that we have introduced a right hand side  [2.x.20]  here to showhow to derive these formulas in the general case, although in the applicationto the thermoacoustic problem  [2.x.21] .
* The semi-discretized, weak version of this model, using the general  [2.x.22]  schemeintroduced in  [2.x.23]  is then:[1.x.37]
* where  [2.x.24]  is an arbitrary test function, and where we have used theabsorbing boundary condition to integrate by parts:absorbing boundary conditions are incorporated into the weak form by using[1.x.38]
* From this we obtain the discrete model by introducing a finite number of shapefunctions, and get[1.x.39]
* The matrices  [2.x.25]  and  [2.x.26]  are here as in  [2.x.27] , and theboundary mass matrix[1.x.40]results from the use of absorbing boundary conditions.
* Above two equations can be rewritten in a matrix form with the pressure and its derivative asan unknown vector:[1.x.41]
* where[1.x.42]
* By simple transformations, one then obtains two equations forthe pressure potential and its derivative, just as in the previous tutorial program:[1.x.43]
* 
* 

* [1.x.44][1.x.45]
* 

* Compared to  [2.x.28] , this programs adds the treatment of asimple absorbing boundary conditions. In addition, it deals with data obtainedfrom actual experimental measurements. To this end, we need to evaluate thesolution at points at which the experiment also evaluates a real pressurefield. We will see how to do that using the  [2.x.29]  functionfurther down below.
* 

* 
* [1.x.46][1.x.47]
* 

* In the derivation of the initial value problem for the wave equation, weinitially found that the equation had the derivative of a Dirac delta functionas a right hand side:[1.x.48]In order to see how to transform this single equation into the usual statementof a PDE with initial conditions, let us make the assumption that thephysically quite reasonable medium is at rest initially, i.e.  [2.x.30]  for  [2.x.31] . Next, let us formthe indefinite integral with respect to time of both sides:[1.x.49]This immediately leads to the statement[1.x.50]where  [2.x.32]  is such that  [2.x.33] . Next, we form the (definite) integral over time from  [2.x.34]  to [2.x.35]  to find[1.x.51]If we use the property of the delta function that  [2.x.36] , and assume that  [2.x.37]  is a continuous function in time, we findas we let  [2.x.38]  go to zero that[1.x.52]In other words, using that  [2.x.39] , we retrieve the initialcondition[1.x.53]At the same time, we know that for every  [2.x.40]  the delta function is zero, sofor  [2.x.41]  we get the equation[1.x.54]Consequently, we have obtained a representation of the wave equation and oneinitial condition from the original somewhat strange equation.
* Finally, because we here have an equation with two time derivatives, we stillneed a second initial condition. To this end, let us go back to the equation[1.x.55]and integrate it in time from  [2.x.42]  to  [2.x.43] . This leads to[1.x.56]Using integration by parts of the form[1.x.57]where we use that  [2.x.44]  and inserting  [2.x.45] , wesee that in fact[1.x.58]
* Now, let  [2.x.46] . Assuming that  [2.x.47]  is a continuous function intime, we see that[1.x.59]and consequently[1.x.60]However, we have assumed that  [2.x.48] .Consequently, we obtain as the second initial condition that[1.x.61]completing the system of equations.
* 

*  [1.x.62] [1.x.63]
*   [1.x.64]  [1.x.65]
* 

* 
*  The following have all been covered previously:
* 

* 
* [1.x.66]
* 
*  This is the only new one: We will need a library function defined in the namespace GridTools that computes the minimal cell diameter.
* 

* 
* [1.x.67]
* 
*  The last step is as in all previous programs:
* 

* 
* [1.x.68]
* 
*   [1.x.69]  [1.x.70]
* 

* 
*  The first part of the main class is exactly as in  [2.x.49]  (except for the name):
* 

* 
* [1.x.71]
* 
*  Here's what's new: first, we need that boundary mass matrix  [2.x.50]  that came out of the absorbing boundary condition. Likewise, since this time we consider a realistic medium, we must have a measure of the wave speed  [2.x.51]  that will enter all the formulas with the Laplace matrix (which we still define as  [2.x.52] ):
* 

* 
* [1.x.72]
* 
*  The last thing we have to take care of is that we wanted to evaluate the solution at a certain number of detector locations. We need an array to hold these locations, declared here and filled in the constructor:
* 

* 
* [1.x.73]
* 
*   [1.x.74]  [1.x.75]
* 

* 
*  As usual, we have to define our initial values, boundary conditions, and right hand side functions. Things are a bit simpler this time: we consider a problem that is driven by initial conditions, so there is no right hand side function (though you could look up in  [2.x.53]  to see how this can be done). Secondly, there are no boundary conditions: the entire boundary of the domain consists of absorbing boundary conditions. That only leaves initial conditions, and there things are simple too since for this particular application only nonzero initial conditions for the pressure are prescribed, not for the velocity (which is zero at the initial time).   
*   So this is all we need: a class that specifies initial conditions for the pressure. In the physical setting considered in this program, these are small absorbers, which we model as a series of little circles where we assume that the pressure surplus is one, whereas no absorption and therefore no pressure surplus is everywhere else. This is how we do things (note that if we wanted to expand this program to not only compile but also to run, we would have to initialize the sources with three-dimensional source locations):
* 

* 
* [1.x.76]
* 
*   [1.x.77]  [1.x.78]
* 

* 
*  Let's start again with the constructor. Setting the member variables is straightforward. We use the acoustic wave speed of mineral oil (in millimeters per microsecond, a common unit in experimental biomedical imaging) since this is where many of the experiments we want to compare the output with are made in. The Crank-Nicolson scheme is used again, i.e. theta is set to 0.5. The time step is later selected to satisfy  [2.x.54] : here we initialize it to an invalid number.
* 

* 
* [1.x.79]
* 
*  The second task in the constructor is to initialize the array that holds the detector locations. The results of this program were compared with experiments in which the step size of the detector spacing is 2.25 degree, corresponding to 160 detector locations. The radius of the scanning circle is selected to be half way between the center and the boundary to avoid that the remaining reflections from the imperfect boundary condition spoils our numerical results.     
*   The locations of the detectors are then calculated in clockwise order. Note that the following of course only works if we are computing in 2d, a condition that we guard with an assertion. If we later wanted to run the same program in 3d, we would have to add code here for the initialization of detector locations in 3d. Due to the assertion, there is no way we can forget to do this.
* 

* 
* [1.x.80]
* 
*   [1.x.81]  [1.x.82]
* 

* 
*  The following system is pretty much what we've already done in  [2.x.55] , but with two important differences. First, we have to create a circular (or spherical) mesh around the origin, with a radius of 1. This nothing new: we've done so before in  [2.x.56]  and  [2.x.57] , where we also explain how the PolarManifold or SphericalManifold object places new points on concentric circles when a cell is refined, which we will use here as well.   
*   One thing we had to make sure is that the time step satisfies the CFL condition discussed in the introduction of  [2.x.58] . Back in that program, we ensured this by hand by setting a timestep that matches the mesh width, but that was error prone because if we refined the mesh once more we would also have to make sure the time step is changed. Here, we do that automatically: we ask a library function for the minimal diameter of any cell. Then we set  [2.x.59] . The only problem is: what exactly is  [2.x.60] ? The point is that there is really no good theory on this question for the wave equation. It is known that for uniformly refined meshes consisting of rectangles,  [2.x.61]  is the minimal edge length. But for meshes on general quadrilaterals, the exact relationship appears to be unknown, i.e. it is unknown what properties of cells are relevant for the CFL condition. The problem is that the CFL condition follows from knowledge of the smallest eigenvalue of the Laplace matrix, and that can only be computed analytically for simply structured meshes.   
*   The upshot of all this is that we're not quite sure what exactly we should take for  [2.x.62] . The function  [2.x.63]  computes the minimal diameter of all cells. If the cells were all squares or cubes, then the minimal edge length would be the minimal diameter divided by  [2.x.64] . We simply generalize this, without theoretical justification, to the case of non-uniform meshes.   
*   The only other significant change is that we need to build the boundary mass matrix. We will comment on this further down below.
* 

* 
* [1.x.83]
* 
*  The second difference, as mentioned, to  [2.x.65]  is that we need to build the boundary mass matrix that grew out of the absorbing boundary conditions.     
*   A first observation would be that this matrix is much sparser than the regular mass matrix, since none of the shape functions with purely interior support contribute to this matrix. We could therefore optimize the storage pattern to this situation and build up a second sparsity pattern that only contains the nonzero entries that we need. There is a trade-off to make here: first, we would have to have a second sparsity pattern object, so that costs memory. Secondly, the matrix attached to this sparsity pattern is going to be smaller and therefore requires less memory; it would also be faster to perform matrix-vector multiplications with it. The final argument, however, is the one that tips the scale: we are not primarily interested in performing matrix-vector with the boundary matrix alone (though we need to do that for the right hand side vector once per time step), but mostly wish to add it up to the other matrices used in the first of the two equations since this is the one that is going to be multiplied with once per iteration of the CG method, i.e. significantly more often. It is now the case that the  [2.x.66]  class allows to add one matrix to another, but only if they use the same sparsity pattern (the reason being that we can't add nonzero entries to a matrix after the sparsity pattern has been created, so we simply require that the two matrices have the same sparsity pattern).     
*   So let's go with that:
* 

* 
* [1.x.84]
* 
*  The second thing to do is to actually build the matrix. Here, we need to integrate over faces of cells, so first we need a quadrature object that works on  [2.x.67]  dimensional objects. Secondly, the FEFaceValues variant of FEValues that works on faces, as its name suggest. And finally, the other variables that are part of the assembly machinery. All of this we put between curly braces to limit the scope of these variables to where we actually need them.     
*   The actual act of assembling the matrix is then fairly straightforward: we loop over all cells, over all faces of each of these cells, and then do something only if that particular face is at the boundary of the domain. Like this:
* 

* 
* [1.x.85]
* 
*   [1.x.86]  [1.x.87]
* 

* 
*  The following two functions, solving the linear systems for the pressure and the velocity variable, are taken pretty much verbatim (with the exception of the change of name from  [2.x.68]  to  [2.x.69]  of the primary variable) from  [2.x.70] :
* 

* 
* [1.x.88]
* 
*   [1.x.89]  [1.x.90]
* 

* 
*  The same holds here: the function is from  [2.x.71] .
* 

* 
* [1.x.91]
* 
*   [1.x.92]  [1.x.93]
* 

* 
*  This function that does most of the work is pretty much again like in  [2.x.72] , though we make things a bit clearer by using the vectors G1 and G2 mentioned in the introduction. Compared to the overall memory consumption of the program, the introduction of a few temporary vectors isn't doing much harm.   
*   The only changes to this function are: first, that we do not have to project initial values for the velocity  [2.x.73] , since we know that it is zero. And second that we evaluate the solution at the detector locations computed in the constructor. This is done using the  [2.x.74]  function. These values are then written to a file that we open at the beginning of the function.
* 

* 
* [1.x.94]
* 
*   [1.x.95]  [1.x.96]
* 

* 
*  What remains is the main function of the program. There is nothing here that hasn't been shown in several of the previous programs:
* 

* 
* [1.x.97]
* [1.x.98][1.x.99]
* 

* The program writes both graphical data for each time step as well as thevalues evaluated at each detector location to disk. We thendraw them in plots. Experimental data were also collected for comparison.Currently our experiments have only been done in two dimensions bycircularly scanning a single detector. The tissue sample here is a thin slicein the  [2.x.75]  plane ( [2.x.76] ), and we assume that signals from other  [2.x.77] directions won't contribute to the data. Consequently, we only have to compareour experimental data with two dimensional simulated data.
* [1.x.100][1.x.101]
* 

* This movie shows the thermoacoustic waves generated by a single small absorberpropagating in the medium (in our simulation, we assume the medium is mineraloil, which has a acoustic speed of 1.437  [2.x.78] ):
*  [2.x.79] 
* For a single absorber, we of course have to change the [2.x.80]  class accordingly.
* Next, let us compare experimental and computational results. The visualizationuses a technique long used in seismology, where the data of each detector isplotted all in one graph. The way this is done is by offsetting eachdetector's signal a bit compared to the previous one. For example, here is aplot of the first four detectors (from bottom to top, with time inmicroseconds running from left to right) using the source setup used in theprogram, to make things a bit more interesting compared to the present case ofonly a single source:
*  [2.x.81] 
* One thing that can be seen, for example, is that the arrival of the second andfourth signals shifts to earlier times for greater detector numbers (i.e. thetopmost ones), but not the first and the third; this can be interpreted tomean that the origin of these signals must be closer to the latter detectorsthan to the former ones.
* If we stack not only 4, but all 160 detectors in one graph, the individuallines blur, but where they run together they create a pattern of darker orlighter grayscales.  The following two figures show the results obtained atthe detector locations stacked in that way. The left figure is obtained fromexperiments, and the right is the simulated data.In the experiment, a single small strong absorber was embedded inweaker absorbing tissue:
*  [2.x.82] 
* It is obvious that the source location is closer to the detectors at angle [2.x.83] . All the other signals that can be seen in the experimental dataresult from the fact that there are weak absorbers also in the rest of thetissue, which surrounds the signals generated by the small strong absorber inthe center. On the other hand, in the simulated data, we only simulate thesmall strong absorber.
* In reality, detectors have limited bandwidth. The thermoacoustic waves passingthrough the detector will therefore be filtered. By using a high-pass filter(implemented in MATLAB and run against the data file produced by this program),the simulated results can be made to look closer to the experimentaldata:
*  [2.x.84] 
* In our simulations, we see spurious signals behind the main wave thatresult from numerical artifacts. This problem can be alleviated by using finermesh, resulting in the following plot:
*  [2.x.85] 
* 

* 
* [1.x.102][1.x.103]
* 

* To further verify the program, we will also show simulation results formultiple absorbers. This corresponds to the case that is actually implementedin the program. The following movie shows the propagation of the generatedthermoacoustic waves in the medium by multiple absorbers:
*  [2.x.86] 
* Experimental data and our simulated data are compared in the following twofigures: [2.x.87] 
* Note that in the experimental data, the first signal (i.e. the left-most darkline) results from absorption at the tissue boundary, and therefore reachesthe detectors first and before any of the signals from the interior. Thissignal is also faintly visible at the end of the traces, around 30  [2.x.88] ,which indicates that the signal traveled through the entire tissue to reachdetectors at the other side, after all the signals originating from theinterior have reached them.
* As before, the numerical result better matches experimental ones by applying abandwidth filter that matches the actual behavior of detectors (left) and bychoosing a finer mesh (right):
*  [2.x.89] 
* One of the important differences between the left and the right figure is thatthe curves look much less "angular" at the right. The angularity comes fromthe fact that while waves in the continuous equation travel equally fast inall directions, this isn't the case after discretization: there, waves thattravel diagonal to cells move at slightly different speeds to those that moveparallel to mesh lines. This anisotropy leads to wave fronts that aren'tperfectly circular (and would produce sinusoidal signals in the stackedplots), but are bulged out in certain directions. To make things worse, thecircular mesh we use (see for example  [2.x.90]  for a view of thecoarse mesh) is not isotropic either. The net result is that the signal frontsare not sinusoidal unless the mesh is sufficiently fine. The right image is alot better in this respect, though artifacts in the form of trailing spuriouswaves can still be seen.
* 

* [1.x.104][1.x.105] [2.x.91] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-25_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26]
* [1.x.27][1.x.28] [1.x.29]
* 

* This program grew out of a student project by Ivan Christov at Texas A&amp;MUniversity. Most of the work for this program is by him.
* The goal of this program is to solve the sine-Gordon soliton equationin 1, 2 or 3 spatial dimensions. The motivation for solving thisequation is that very little is known about the nature of thesolutions in 2D and 3D, even though the 1D case has been studiedextensively.
* Rather facetiously, the sine-Gordon equation's moniker is a pun on theso-called Klein-Gordon equation, which is a relativistic version ofthe Schrödinger equation for particles with non-zero mass. The resemblance is not justsuperficial, the sine-Gordon equation has been shown to model someunified-field phenomena such as interaction of subatomic particles(see, e.g., Perring &amp; Skyrme in Nuclear %Physics [1.x.30]) and theJosephson (quantum) effect in superconductor junctions (see, e.g., [1.x.31]).Furthermore, from the mathematical standpoint, since the sine-Gordonequation is "completely integrable," it is a candidate for study usingthe usual methods such as the inverse scatteringtransform. Consequently, over the years, many interestingsolitary-wave, and even stationary, solutions to the sine-Gordonequation have been found. In these solutions, particles correspond tolocalized features. For more on the sine-Gordon equation, theinverse scattering transform and other methods for finding analyticalsoliton equations, the reader should consult the following "classical"references on the subject: G. L. Lamb's [1.x.32] (Chapter 5, Section 2) and G. B. Whitham's [1.x.33] (Chapter 17, Sections 10-13).
*  [2.x.2]  We will cover a separate nonlinear equation from quantum  mechanics, the Nonlinear Schr&ouml;dinger Equation, in  [2.x.3] .
* [1.x.34][1.x.35]
* The sine-Gordon initial-boundary-value problem (IBVP) we wish to solveconsists of the following equations:[1.x.36]It is a nonlinear equation similar to the wave equation wediscussed in  [2.x.4]  and  [2.x.5] .We have chosen to enforce zero Neumann boundary conditions in order for wavesto reflect off the boundaries of our domain. It should be noted, however, thatDirichlet boundary conditions are not appropriate for this problem. Eventhough the solutions to the sine-Gordon equation are localized, it only makessense to specify (Dirichlet) boundary conditions at  [2.x.6] , otherwiseeither a solution does not exist or only the trivial solution  [2.x.7]  exists.
* However, the form of the equation above is not ideal for numericaldiscretization. If we were to discretize the second-order timederivative directly and accurately, then  we would need a largestencil (i.e., several time steps would need to be kept in thememory), which could become expensive. Therefore, in complete analogyto what we did in  [2.x.8]  and  [2.x.9] ,we split thesecond-order (in time) sine-Gordon equation into a system of twofirst-order (in time) equations, which we call the split, or velocity,formulation. To this end, by setting  [2.x.10] , it is easy to see that the sine-Gordon equation is equivalent to[1.x.37]
* [1.x.38][1.x.39]
* Now, we can discretize the split formulation in time using the [2.x.11] -method, which has a stencil of only two time steps. Bychoosing a  [2.x.12] , the latter discretization allows us tochoose from a continuum of schemes. In particular, if we pick [2.x.13]  or  [2.x.14] , we obtain the first-order accurate explicitor implicit Euler method, respectively. Another important choice is [2.x.15] , which gives the second-order accurateCrank-Nicolson scheme. Henceforth, a superscript  [2.x.16]  denotes thevalues of the variables at the  [2.x.17]  time step, i.e. at [2.x.18] , where  [2.x.19]  is the (fixed) time step size. Thus,the split formulation of the time-discretized sine-Gordon equation becomes[1.x.40]
* We can simplify the latter via a bit of algebra. Eliminating  [2.x.20]  from the first equation and rearranging, we obtain[1.x.41]
* It may seem as though we can just proceed to discretize the equationsin space at this point. While this is true for the second equation(which is linear in  [2.x.21] ), this would not work for all  [2.x.22]  since thefirst equation above is nonlinear. Therefore, a nonlinear solver must beimplemented, then the equations can be discretized in space and solved.
* To this end, we can use Newton's method. Given the nonlinear equation  [2.x.23] , we produce successive approximations to  [2.x.24]  as follows:[1.x.42]The iteration can be initialized with the old time step, i.e.  [2.x.25] ,and eventually it will produce a solution to the first equation ofthe split formulation (see above). For the time discretization of thesine-Gordon equation under consideration here, we have that[1.x.43]Notice that while  [2.x.26]  is a function,  [2.x.27]  is an operator.
* [1.x.44][1.x.45]
* With hindsight, we choose both the solution and the test space to be  [2.x.28] . Hence, multiplying by a test function  [2.x.29]  and integrating, we obtain the following variational (or weak) formulation of the split formulation (including the nonlinear solver for the first equation) at each time step:[1.x.46]Note that the we have used integration by parts and the zero Neumannboundary conditions on all terms involving the Laplacianoperator. Moreover,  [2.x.30]  and  [2.x.31]  are as defined above,and  [2.x.32]  denotes the usual  [2.x.33]  inner productover the domain  [2.x.34] , i.e.  [2.x.35] . Finally, notice that the first equation is, in fact,the definition of an iterative procedure, so it is solved multipletimes during each time step until a stopping criterion is met.
* [1.x.47][1.x.48]
* Using the Finite Element Method, we discretize the variationalformulation in space. To this end, let  [2.x.36]  be a finite-dimensional [2.x.37] -conforming finite element space ( [2.x.38] ) with nodal basis  [2.x.39] . Now,we can expand all functions in the weak formulation (see above) interms of the nodal basis. Henceforth, we shall denote by a capitalletter the vector of coefficients (in the nodal basis) of a functiondenoted by the same letter in lower case; e.g.,  [2.x.40]  where  [2.x.41]  and  [2.x.42] . Thus, the finite-dimensional version of the variational formulation requires that we solve the following matrix equations at each time step:[1.x.49]
* Above, the matrix  [2.x.43]  and the vector  [2.x.44]  denote the discrete versions of the gadgets discussed above, i.e.,[1.x.50]Again, note that the first matrix equation above is, in fact, thedefinition of an iterative procedure, so it is solved multiple timesuntil a stopping criterion is met. Moreover,  [2.x.45]  is the mass matrix,i.e.  [2.x.46] ,  [2.x.47]  isthe Laplace matrix, i.e.  [2.x.48] ,  [2.x.49]  is the nonlinear term in theequation that defines our auxiliary velocity variable, i.e.  [2.x.50] , and [2.x.51]  is the nonlinear term in the Jacobian matrix of  [2.x.52] ,i.e.  [2.x.53] .
* What solvers can we use for the first equation? Let's look at the matrix wehave to invert:[1.x.51]for some  [2.x.54]  that depends on the present and previous solution. First,note that the matrix is symmetric. In addition, if the time step  [2.x.55]  is smallenough, i.e. if  [2.x.56] , then the matrix is also going to be positivedefinite. In the program below, this will always be the case, so we will usethe Conjugate Gradient method together with the SSOR method aspreconditioner. We should keep in mind, however, that this will failif we happen to use a bigger time step. Fortunately, in that casethe solver will just throw an exception indicating a failure to converge,rather than silently producing a wrong result. If that happens, then we cansimply replace the CG method by something that can handle indefinite symmetricsystems. The GMRES solver is typically the standard method for all "bad"linear systems, but it is also a slow one. Possibly better would be a solverthat utilizes the symmetry, such as, for example, SymmLQ, which is alsoimplemented in deal.II.
* This program uses a clever optimization over  [2.x.57]  and  [2.x.58]  " [2.x.59] ": If you read the above formulas closely, it becomes clearthat the velocity  [2.x.60]  only ever appears in products with the mass matrix. In [2.x.61]  and  [2.x.62] , we were, therefore, a bitwasteful: in each time step, we would solve a linear system with the massmatrix, only to multiply the solution of that system by  [2.x.63]  again in the nexttime step. This can, of course, be avoided, and we do so in this program.
* 

* [1.x.52][1.x.53]
* 

* There are a few analytical solutions for the sine-Gordon equation, both in 1Dand 2D. In particular, the program as is computes the solution to a problemwith a single kink-like solitary wave initial condition.  This solution isgiven by Leibbrandt in \e Phys. \e Rev. \e Lett. \b 41(7), and is implementedin the  [2.x.64]  class.
* It should be noted that this closed-form solution, strictly speaking, only holdsfor the infinite-space initial-value problem (not the Neumanninitial-boundary-value problem under consideration here). However, given thatwe impose \e zero Neumann boundary conditions, we expect that the solution toour initial-boundary-value problem would be close to the solution of theinfinite-space initial-value problem, if reflections of waves off theboundaries of our domain do \e not occur. In practice, this is of course notthe case, but we can at least assume that this were so.
* The constants  [2.x.65]  and  [2.x.66]  in the 2D solution and  [2.x.67] , [2.x.68]  and  [2.x.69]  in the 3D solution are called the B&auml;cklundtransformation parameters. They control such things as the orientation andsteepness of the kink. For the purposes of testing the code against the exactsolution, one should choose the parameters so that the kink is aligned withthe grid.
* The solutions that we implement in the  [2.x.70]  class arethese: [2.x.71]    [2.x.72] In 1D:  [1.x.54]  where we choose  [2.x.73] .
*   In 1D, more interesting analytical solutions are known. Many of them are  listed on http://mathworld.wolfram.com/Sine-GordonEquation.html .
*    [2.x.74] In 2D:  [1.x.55]  where  [2.x.75]  is defined as  [1.x.56]  and where we choose  [2.x.76] .
*    [2.x.77] In 3D:  [1.x.57]  where  [2.x.78]  is defined as  [1.x.58]  and where we choose  [2.x.79] . [2.x.80] 
* 

* Since it makes it easier to play around, the  [2.x.81]  classthat is used to set &mdash; surprise! &mdash; the initial values of oursimulation simply queries the class that describes the exact solution for thevalue at the initial time, rather than duplicating the effort to implement asolution function.
* 

*  [1.x.59] [1.x.60]
*   [1.x.61]  [1.x.62]
* 

* 
*  For an explanation of the include files, the reader should refer to the example programs  [2.x.82]  through  [2.x.83] . They are in the standard order, which is  [2.x.84] 
* 
*  -   [2.x.85] 
* 
*  -   [2.x.86] 
* 
*  -   [2.x.87] 
* 
*  -   [2.x.88] 
* 
*  -   [2.x.89]  (since each of these categories roughly builds upon previous ones), then a few C++ headers for file input/output and string streams.
* 

* 
* [1.x.63]
* 
*  The last step is as in all previous programs:
* 

* 
* [1.x.64]
* 
*   [1.x.65]  [1.x.66]
* 

* 
*  The entire algorithm for solving the problem is encapsulated in this class. As in previous example programs, the class is declared with a template parameter, which is the spatial dimension, so that we can solve the sine-Gordon equation in one, two or three spatial dimensions. For more on the dimension-independent class-encapsulation of the problem, the reader should consult  [2.x.90]  and  [2.x.91] .   
*   Compared to  [2.x.92]  and  [2.x.93] , there isn't anything newsworthy in the general structure of the program (though there is of course in the inner workings of the various functions!). The most notable difference is the presence of the two new functions  [2.x.94]  and  [2.x.95]  that compute the nonlinear contributions to the system matrix and right-hand side of the first equation, as discussed in the Introduction. In addition, we have to have a vector  [2.x.96]  that contains the nonlinear update to the solution vector in each Newton step.   
*   As also mentioned in the introduction, we do not store the velocity variable in this program, but the mass matrix times the velocity. This is done in the  [2.x.97]  variable (the "x" is intended to stand for "times").   
*   Finally, the  [2.x.98]  variable stores the number of time steps to be taken each time before graphical output is to be generated. This is of importance when using fine meshes (and consequently small time steps) where we would run lots of time steps and create lots of output files of solutions that look almost the same in subsequent files. This only clogs up our visualization procedures and we should avoid creating more output than we are really interested in. Therefore, if this variable is set to a value  [2.x.99]  bigger than one, output is generated only every  [2.x.100] th time step.
* 

* 
* [1.x.67]
* 
*   [1.x.68]  [1.x.69]
* 

* 
*  In the following two classes, we first implement the exact solution for 1D, 2D, and 3D mentioned in the introduction to this program. This space-time solution may be of independent interest if one wanted to test the accuracy of the program by comparing the numerical against the analytic solution (note however that the program uses a finite domain, whereas these are analytic solutions for an unbounded domain). This may, for example, be done using the  [2.x.101]  function. Note, again (as was already discussed in  [2.x.102] ), how we describe space-time functions as spatial functions that depend on a time variable that can be set and queried using the  [2.x.103]  and  [2.x.104]  member functions of the FunctionTime base class of the Function class.
* 

* 
* [1.x.70]
* 
*  In the second part of this section, we provide the initial conditions. We are lazy (and cautious) and don't want to implement the same functions as above a second time. Rather, if we are queried for initial conditions, we create an object  [2.x.105] , set it to the correct time, and let it compute whatever values the exact solution has at that time:
* 

* 
* [1.x.71]
* 
*   [1.x.72]  [1.x.73]
* 

* 
*  Let's move on to the implementation of the main class, as it implements the algorithm outlined in the introduction.
* 

* 
*   [1.x.74]  [1.x.75]
* 

* 
*  This is the constructor of the  [2.x.106]  class. It specifies the desired polynomial degree of the finite elements, associates a  [2.x.107]  object (just as in the example programs  [2.x.108]  and  [2.x.109] ), initializes the current or initial time, the final time, the time step size, and the value of  [2.x.110]  for the time stepping scheme. Since the solutions we compute here are time-periodic, the actual value of the start-time doesn't matter, and we choose it so that we start at an interesting time.   
*   Note that if we were to chose the explicit Euler time stepping scheme ( [2.x.111] ), then we must pick a time step  [2.x.112] , otherwise the scheme is not stable and oscillations might arise in the solution. The Crank-Nicolson scheme ( [2.x.113] ) and the implicit Euler scheme ( [2.x.114] ) do not suffer from this deficiency, since they are unconditionally stable. However, even then the time step should be chosen to be on the order of  [2.x.115]  in order to obtain a good solution. Since we know that our mesh results from the uniform subdivision of a rectangle, we can compute that time step easily; if we had a different domain, the technique in  [2.x.116]  using  [2.x.117]  would work as well.
* 

* 
* [1.x.76]
* 
*   [1.x.77]  [1.x.78]
* 

* 
*  This function creates a rectangular grid in  [2.x.118]  dimensions and refines it several times. Also, all matrix and vector members of the  [2.x.119]  class are initialized to their appropriate sizes once the degrees of freedom have been assembled. Like  [2.x.120] , we use  [2.x.121]  functions to generate a mass matrix  [2.x.122]  and a Laplace matrix  [2.x.123]  and store them in the appropriate variables for the remainder of the program's life.
* 

* 
* [1.x.79]
* 
*   [1.x.80]  [1.x.81]
* 

* 
*  This function assembles the system matrix and right-hand side vector for each iteration of Newton's method. The reader should refer to the Introduction for the explicit formulas for the system matrix and right-hand side.   
*   Note that during each time step, we have to add up the various contributions to the matrix and right hand sides. In contrast to  [2.x.124]  and  [2.x.125] , this requires assembling a few more terms, since they depend on the solution of the previous time step or previous nonlinear step. We use the functions  [2.x.126]  and  [2.x.127]  to do this, while the present function provides the top-level logic.
* 

* 
* [1.x.82]
* 
*  First we assemble the Jacobian matrix  [2.x.128] , where  [2.x.129]  is stored in the vector  [2.x.130]  for convenience.
* 

* 
* [1.x.83]
* 
*  Next we compute the right-hand side vector. This is just the combination of matrix-vector products implied by the description of  [2.x.131]  in the introduction.
* 

* 
* [1.x.84]
* 
*   [1.x.85]  [1.x.86]
* 

* 
*  This function computes the vector  [2.x.132] , which appears in the nonlinear term in both equations of the split formulation. This function not only simplifies the repeated computation of this term, but it is also a fundamental part of the nonlinear iterative solver that we use when the time stepping is implicit (i.e.  [2.x.133] ). Moreover, we must allow the function to receive as input an "old" and a "new" solution. These may not be the actual solutions of the problem stored in  [2.x.134] , but are simply the two functions we linearize about. For the purposes of this function, let us call the first two arguments  [2.x.135]  and  [2.x.136]  in the documentation of this class below, respectively.   
*   As a side-note, it is perhaps worth investigating what order quadrature formula is best suited for this type of integration. Since  [2.x.137]  is not a polynomial, there are probably no quadrature formulas that can integrate these terms exactly. It is usually sufficient to just make sure that the right hand side is integrated up to the same order of accuracy as the discretization scheme is, but it may be possible to improve on the constant in the asymptotic statement of convergence by choosing a more accurate quadrature formula.
* 

* 
* [1.x.87]
* 
*  Once we re-initialize our  [2.x.138]  instantiation to the current cell, we make use of the  [2.x.139]  routine to get the values of the "old" data (presumably at  [2.x.140] ) and the "new" data (presumably at  [2.x.141] ) at the nodes of the chosen quadrature formula.
* 

* 
* [1.x.88]
* 
*  Now, we can evaluate  [2.x.142]  using the desired quadrature formula.
* 

* 
* [1.x.89]
* 
*  We conclude by adding up the contributions of the integrals over the cells to the global integral.
* 

* 
* [1.x.90]
* 
*   [1.x.91]  [1.x.92]
* 

* 
*  This is the second function dealing with the nonlinear scheme. It computes the matrix  [2.x.143] , which appears in the nonlinear term in the Jacobian of  [2.x.144] . Just as  [2.x.145] , we must allow this function to receive as input an "old" and a "new" solution, which we again call  [2.x.146]  and  [2.x.147]  below, respectively.
* 

* 
* [1.x.93]
* 
*  Again, first we re-initialize our  [2.x.148]  instantiation to the current cell.
* 

* 
* [1.x.94]
* 
*  Then, we evaluate  [2.x.149]  using the desired quadrature formula.
* 

* 
* [1.x.95]
* 
*  Finally, we add up the contributions of the integrals over the cells to the global integral.
* 

* 
* [1.x.96]
* 
*   [1.x.97]  [1.x.98]
* 

* 
*  As discussed in the Introduction, this function uses the CG iterative solver on the linear system of equations resulting from the finite element spatial discretization of each iteration of Newton's method for the (nonlinear) first equation of the split formulation. The solution to the system is, in fact,  [2.x.150]  so it is stored in  [2.x.151]  in the  [2.x.152]  function.   
*   Note that we re-set the solution update to zero before solving for it. This is not necessary: iterative solvers can start from any point and converge to the correct solution. If one has a good estimate about the solution of a linear system, it may be worthwhile to start from that vector, but as a general observation it is a fact that the starting point doesn't matter very much: it has to be a very, very good guess to reduce the number of iterations by more than a few. It turns out that for this problem, using the previous nonlinear update as a starting point actually hurts convergence and increases the number of iterations needed, so we simply set it to zero.   
*   The function returns the number of iterations it took to converge to a solution. This number will later be used to generate output on the screen showing how many iterations were needed in each nonlinear iteration.
* 

* 
* [1.x.99]
* 
*   [1.x.100]  [1.x.101]
* 

* 
*  This function outputs the results to a file. It is pretty much identical to the respective functions in  [2.x.153]  and  [2.x.154] :
* 

* 
* [1.x.102]
* 
*   [1.x.103]  [1.x.104]
* 

* 
*  This function has the top-level control over everything: it runs the (outer) time-stepping loop, the (inner) nonlinear-solver loop, and outputs the solution after each time step.
* 

* 
* [1.x.105]
* 
*  To acknowledge the initial condition, we must use the function  [2.x.155]  to compute  [2.x.156] . To this end, below we will create an object of type  [2.x.157] ; note that when we create this object (which is derived from the  [2.x.158]  class), we set its internal time variable to  [2.x.159] , to indicate that the initial condition is a function of space and time evaluated at  [2.x.160] .     
*   Then we produce  [2.x.161]  by projecting  [2.x.162]  onto the grid using  [2.x.163] . We have to use the same construct using hanging node constraints as in  [2.x.164] : the  [2.x.165]  function requires a hanging node constraints object, but to be used we first need to close it:
* 

* 
* [1.x.106]
* 
*  For completeness, we output the zeroth time step to a file just like any other time step.
* 

* 
* [1.x.107]
* 
*  Now we perform the time stepping: at every time step we solve the matrix equation(s) corresponding to the finite element discretization of the problem, and then advance our solution according to the time stepping formulas we discussed in the Introduction.
* 

* 
* [1.x.108]
* 
*  At the beginning of each time step we must solve the nonlinear equation in the split formulation via Newton's method
* 
*  - - i.e. solve for  [2.x.166]  then compute  [2.x.167]  and so on. The stopping criterion for this nonlinear iteration is that  [2.x.168] . Consequently, we need to record the norm of the residual in the first iteration.         
*   At the end of each iteration, we output to the console how many linear solver iterations it took us. When the loop below is done, we have (an approximation of)  [2.x.169] .
* 

* 
* [1.x.109]
* 
*  Upon obtaining the solution to the first equation of the problem at  [2.x.170] , we must update the auxiliary velocity variable  [2.x.171] . However, we do not compute and store  [2.x.172]  since it is not a quantity we use directly in the problem. Hence, for simplicity, we update  [2.x.173]  directly:
* 

* 
* [1.x.110]
* 
*  Oftentimes, in particular for fine meshes, we must pick the time step to be quite small in order for the scheme to be stable. Therefore, there are a lot of time steps during which "nothing interesting happens" in the solution. To improve overall efficiency
* 
*  -  in particular, speed up the program and save disk space
* 
*  -  we only output the solution every  [2.x.174]  time steps:
* 

* 
* [1.x.111]
* 
*   [1.x.112]  [1.x.113]
* 

* 
*  This is the main function of the program. It creates an object of top-level class and calls its principal function. If exceptions are thrown during the execution of the run method of the  [2.x.175]  class, we catch and report them here. For more information about exceptions the reader should consult  [2.x.176] .
* 

* 
* [1.x.114]
* [1.x.115][1.x.116]
* The explicit Euler time stepping scheme  ( [2.x.177] ) performs adequately for the problems we wish to solve. Unfortunately, a rather small time step has to be chosen due to stability issues
* 
*  - -  [2.x.178]  appears to work for most the simulations we performed. On the other hand, the Crank-Nicolson scheme ( [2.x.179] ) is unconditionally stable, and (at least for the case of the 1D breather) we can pick the time step to be as large as  [2.x.180]  without any ill effects on the solution. The implicit Euler scheme ( [2.x.181] ) is "exponentially damped," so it is not a good choice for solving the sine-Gordon equation, which is conservative. However, some of the damped schemes in the continuum that is offered by the  [2.x.182] -method were useful for eliminating spurious oscillations due to boundary effects.
* In the simulations below, we solve the sine-Gordon equation on the interval  [2.x.183]  in 1D and on the square  [2.x.184]  in 2D. Ineach case, the respective grid is refined uniformly 6 times, i.e.  [2.x.185] .
* [1.x.117][1.x.118]
* The first example we discuss is the so-called 1D (stationary) breathersolution of the sine-Gordon equation. The breather has the followingclosed-form expression, as mentioned in the Introduction:[1.x.119]where  [2.x.186] ,  [2.x.187]  and  [2.x.188]  are constants. In the simulation below, we have chosen  [2.x.189] ,  [2.x.190] ,  [2.x.191] . Moreover, it is know that the period of oscillation of the breather is  [2.x.192] , hence we have chosen  [2.x.193]  and  [2.x.194]  so that we can observe three oscillations of the solution. Then, taking  [2.x.195] ,  [2.x.196]  and  [2.x.197] , the program computed the following solution.
*  [2.x.198] 
* Though not shown how to do this in the program, another way to visualize the(1+1)-d solution is to use output generated by the DataOutStack class; itallows to "stack" the solutions of individual time steps, so that we get2D space-time graphs from 1D time-dependentsolutions. This produces the space-time plot below instead of the animationabove.
*  [2.x.199] 
* Furthermore, since the breather is an analytical solution of the sine-Gordonequation, we can use it to validate our code, although we have to assume thatthe error introduced by our choice of Neumann boundary conditions is smallcompared to the numerical error. Under this assumption, one could use the [2.x.200]  function to compute the difference betweenthe numerical solution and the function described by the [2.x.201]  class of this program. For thesimulation shown in the two images above, the  [2.x.202]  norm of the error in thefinite element solution at each time step remained on the order of [2.x.203] . Hence, we can conclude that the numerical method has beenimplemented correctly in the program.
* 

* [1.x.120][1.x.121]
* 

* The only analytical solution to the sine-Gordon equation in (2+1)D that can be found in the literature is the so-called kink solitary wave. It has the following closed-form expression:  [1.x.122]with  [1.x.123]where  [2.x.204] ,  [2.x.205]  and  [2.x.206]  are constants. In the simulation belowwe have chosen  [2.x.207] . Notice that if  [2.x.208]  the kink isstationary, hence it would make a good solution against which we canvalidate the program in 2D because no reflections off the boundary of thedomain occur.
* The simulation shown below was performed with  [2.x.209] ,  [2.x.210] ,  [2.x.211] ,  [2.x.212]  and  [2.x.213] . The  [2.x.214]  norm of the error of the finite element solution at each time step remained on the order of  [2.x.215] , showing that the program is working correctly in 2D, as well as 1D. Unfortunately, the solution is not very interesting, nonetheless we have included a snapshot of it below for completeness.
*  [2.x.216] 
* Now that we have validated the code in 1D and 2D, we move to a problem where the analytical solution is unknown.
* To this end, we rotate the kink solution discussed above about the  [2.x.217] axis: we let   [2.x.218] . The latter results in asolitary wave that is not aligned with the grid, so reflections occurat the boundaries of the domain immediately. For the simulation shownbelow, we have taken  [2.x.219] , [2.x.220] ,  [2.x.221] ,  [2.x.222]  and  [2.x.223] . Moreover, we hadto pick  [2.x.224]  because for any  [2.x.225] oscillations arose at the boundary, which are likely due to the schemeand not the equation, thus picking a value of  [2.x.226]  a good bit intothe "exponentially damped" spectrum of the time stepping schemesassures these oscillations are not created.
*  [2.x.227] 
* Another interesting solution to the sine-Gordon equation (which cannot beobtained analytically) can be produced by using two 1D breathers to constructthe following separable 2D initial condition:[1.x.124]where  [2.x.228] ,  [2.x.229]  as in the 1D case we discussedabove. For the simulation shown below, we have chosen  [2.x.230] , [2.x.231] ,  [2.x.232]  and  [2.x.233] . The solution is pretty interesting
* 
*  - - it acts like a breather (as far as the pictures are concerned); however,it appears to break up and reassemble, rather than just oscillate.
*  [2.x.234] 
* 

* [1.x.125][1.x.126][1.x.127]
* 

* It is instructive to change the initial conditions. Most choices will not leadto solutions that stay localized (in the soliton community, suchsolutions are called "stationary", though the solution does changewith time), but lead to solutions where the wave-likecharacter of the equation dominates and a wave travels away from the locationof a localized initial condition. For example, it is worth playing around withthe  [2.x.235]  class, by replacing the call to the [2.x.236]  class by something like this function:[1.x.128]if  [2.x.237] , and  [2.x.238]  outside this region.
* A second area would be to investigate whether the scheme isenergy-preserving. For the pure wave equation, discussed in  [2.x.239]  " [2.x.240] ", this is the case if we choose the time steppingparameter such that we get the Crank-Nicolson scheme. One could do asimilar thing here, noting that the energy in the sine-Gordon solutionis defined as[1.x.129](We use  [2.x.241]  instead of  [2.x.242]  in the formula to ensure that allcontributions to the energy are positive, and so that decaying solutions havefinite energy on unbounded domains.)
* Beyond this, there are two obvious areas:
* 
*  - Clearly, adaptivity (i.e. time-adaptive grids) would be of interest  to problems like these. Their complexity leads us to leave this out  of this program again, though the general comments in the  introduction of  [2.x.243]  " [2.x.244] " remain true.
* 
*  - Faster schemes to solve this problem. While computers today are  plenty fast enough to solve 2d and, frequently, even 3d stationary  problems within not too much time, time dependent problems present  an entirely different class of problems. We address this topic in   [2.x.245]  where we show how to solve this problem in parallel and  without assembling or inverting any matrix at all.
* 

* [1.x.130][1.x.131] [2.x.246] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-26_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22]
* [1.x.23][1.x.24][1.x.25]
* 

*  [2.x.2] ( [2.x.3] 
* 

* This program implements the heat equation
* [1.x.26]
* In some sense, this equation is simpler than the ones we have discussed in thepreceding programs  [2.x.4] ,  [2.x.5] ,  [2.x.6] , namely the wave equation. Thisis due to the fact that the heat equation smoothes out the solution over time,and is consequently more forgiving in many regards. For example, when usingimplicit time stepping methods, we can actually take large time steps, we haveless trouble with the small disturbances we introduce through adapting themesh every few time steps, etc.
* Our goal here will be to solve the equations above using the theta-scheme thatdiscretizes the equation in time using the following approach, where we wouldlike  [2.x.7]  to approximate  [2.x.8]  at some time  [2.x.9] :
* [1.x.27]
* Here,  [2.x.10]  is the time step size. The theta-scheme generalizesthe explicit Euler ( [2.x.11] ), implicit Euler ( [2.x.12] ) andCrank-Nicolson ( [2.x.13] ) time discretizations. Since the latter hasthe highest convergence order, we will choose  [2.x.14]  in the programbelow, but make it so that playing with this parameter remains simple. (If youare interested in playing with higher order methods, take a look at  [2.x.15] .)
* Given this time discretization, space discretization happens as it alwaysdoes, by multiplying with test functions, integrating by parts, and thenrestricting everything to a finite dimensional subspace. This yields thefollowing set of fully discrete equations after multiplying through with [2.x.16] :
* [1.x.28]
* where  [2.x.17]  is the mass matrix and  [2.x.18]  is the stiffness matrix that results fromdiscretizing the Laplacian. Bringing all known quantities to the right handside yields the linear system we have to solve in every step:
* [1.x.29]
* The linear system on the left hand side is symmetric and positive definite, sowe should have no trouble solving it with the Conjugate Gradient method.
* We can start the iteration above if we have the set of nodal coefficients [2.x.19]  at the initial time. Here, we take the ones we get by interpolating theinitial values  [2.x.20]  onto the mesh used for the first time step. Wewill also need to choose a time step; we will here just choose it as fixed,but clearly advanced simulators will want to choose it adaptively. We willbriefly come back to this in the [1.x.30].
* 

* [1.x.31][1.x.32]
* 

* When solving the wave equation and its variants in the previous few programs,we kept the mesh fixed. Just as for stationary equations, one can make a goodcase that this is not the smartest approach and that significant savings canbe had by adapting the mesh. There are, however, significant difficultiescompared to the stationary case. Let us go through them in turn:
*  [2.x.21]    [2.x.22] [1.x.33]: For stationary problems, the  general approach is "make the mesh as fine as it is necessary". For problems  with singularities, this often leads to situations where we get many levels  of refinement into corners or along interfaces. The very first tutorial to  use adaptive meshes,  [2.x.23] , is a point in case already.
*   However, for time dependent problems, we typically need to choose the time  step related to the mesh size. For explicit time discretizations, this is  obvious, since we need to respect a CFL condition that ties the time step  size to the smallest mesh size. For implicit time discretizations, no such  hard restriction exists, but in practice we still want to make the time step  smaller if we make the mesh size smaller since we typically have error  estimates of the form  [2.x.24]  where  [2.x.25]  are the  convergence orders of the time and space discretization, respectively. We  can only make the error small if we decrease both terms. Ideally, an  estimate like this would suggest to choose  [2.x.26] . Because, at  least for problems with non-smooth solutions, the error is typically  localized in the cells with the smallest mesh size, we have to indeed choose   [2.x.27] , using the [1.x.34] mesh size.
*   The consequence is that refining the mesh further in one place implies not  only the moderate additional effort of increasing the number of degrees of  freedom slightly, but also the much larger effort of having the solve the  [1.x.35] linear system more often because of the smaller time step.
*   In practice, one typically deals with this by acknowledging that we can not  make the time step arbitrarily small, and consequently can not make the  local mesh size arbitrarily small. Rather, we set a maximal level of  refinement and when we flag cells for refinement, we simply do not refine  those cells whose children would exceed this maximal level of refinement.
*   There is a similar problem in that we will choose a right hand side that  will switch on in different parts of the domain at different times. To avoid  being caught flat footed with too coarse a mesh in areas where we suddenly  need a finer mesh, we will also enforce in our program a [1.x.36] mesh  refinement level.
*    [2.x.28] [1.x.37]: Let us consider again the  semi-discrete equations we have written down above: 
* [1.x.38]
*   We can here consider  [2.x.29]  as data since it has presumably been computed  before. Now, let us replace 
* [1.x.39]
*   multiply with test functions  [2.x.30]  and integrate by parts  where necessary. In a process as outlined above, this would yield 
* [1.x.40]
*   Now imagine that we have changed the mesh between time steps  [2.x.31]  and   [2.x.32] . Then the problem is that the basis functions we use for  [2.x.33]  and   [2.x.34]  are different! This pertains to the terms on the right hand side,  the first of which we could more clearly write as (the second follows the  same pattern) 
* [1.x.41]
*   If the meshes used in these two time steps are the same, then   [2.x.35]  forms a square mass matrix   [2.x.36] . However, if the meshes are not the same, then in general the matrix  is rectangular. Worse, it is difficult to even compute these integrals  because if we loop over the cells of the mesh at time step  [2.x.37] , then we need  to evaluate  [2.x.38]  at the quadrature points of these cells, but  they do not necessarily correspond to the cells of the mesh at time step   [2.x.39]  and  [2.x.40]  is not defined via these cells; the same of  course applies if we wanted to compute the integrals via integration on the  cells of mesh  [2.x.41] .
*   In any case, what we have to face is a situation where we need to integrate  shape functions defined on two different meshes. This can be done, and is in  fact demonstrated in  [2.x.42] , but the process is at best described by the  word "awkward".
*   In practice, one does not typically want to do this. Rather, we avoid the  whole situation by interpolating the solution from the old to the new mesh  every time we adapt the mesh. In other words, rather than solving the  equations above, we instead solve the problem 
* [1.x.42]
*   where  [2.x.43]  is the interpolation operator onto the finite element space  used in time step  [2.x.44] . This is not the optimal approach since it introduces  an additional error besides time and space discretization, but it is a  pragmatic one that makes it feasible to do time adapting meshes. [2.x.45] 
* 

* 
* [1.x.43][1.x.44]
* 

* There are a number of things one can typically get wrong when implementing afinite element code. In particular, for time dependent problems, the followingare common sources of bugs:
* 
*  - The time integration, for example by getting the coefficients in front of  the terms involving the current and previous time steps wrong (e.g., mixing  up a factor  [2.x.46]  for  [2.x.47] ).
* 
*  - Handling the right hand side, for example forgetting a factor of  [2.x.48]  or   [2.x.49] .
* 
*  - Mishandling the boundary values, again for example forgetting a factor of   [2.x.50]  or  [2.x.51] , or forgetting to apply nonzero boundary values not only  to the right hand side but also to the system matrix.
* A less common problem is getting the initial conditions wrong because one cantypically see that it is wrong by just outputting the first time step. In anycase, in order to verify the correctness of the code, it is helpful to have atesting protocol that allows us to verify each of these componentsseparately. This means:
* 
*  - Testing the code with nonzero initial conditions but zero right hand side  and boundary values and verifying that the time evolution is correct.
* 
*  - Then testing with zero initial conditions and boundary values but nonzero  right hand side and again ensuring correctness.
* 
*  - Finally, testing with zero initial conditions and right hand side but  nonzero boundary values.
* This sounds complicated, but fortunately, for linear partial differentialequations without coefficients (or constant coefficients) like the one here,there is a fairly standard protocol that rests on the following observation:if you choose as your domain a square  [2.x.52]  (or, with slightmodifications, a rectangle), then the exact solution can be written as
* [1.x.45]
* (with integer constants  [2.x.53] )if only the initial condition, right hand side and boundary values are allof the form  [2.x.54]  as well. This is due to the factthat the function  [2.x.55]  is an eigenfunction of theLaplace operator and allows us to compute things like the time factor  [2.x.56] analytically and, consequently, compare with what we get numerically.
* As an example, let us consider the situation where we have [2.x.57]  and [2.x.58] . With the claim (ansatz) of the form for [2.x.59]  above, we get that
* [1.x.46]
* For this to be equal to  [2.x.60] , we need that
* [1.x.47]
* and due to the initial conditions,  [2.x.61] . This differential equation can beintegrated to yield
* [1.x.48]
* In other words, if the initial condition is a product of sines, then thesolution has exactly the same shape of a product of sines that decays to zerowith a known time dependence. This is something that is easy to test if youhave a sufficiently fine mesh and sufficiently small time step.
* What is typically going to happen if you get the time integration scheme wrong(e.g., by having the wrong factors of  [2.x.62]  or  [2.x.63]  in front of the variousterms) is that you don't get the right temporal behavior of thesolution. Double check the various factors until you get the rightbehavior. You may also want to verify that the temporal decay rate (asdetermined, for example, by plotting the value of the solution at a fixedpoint) does not double or halve each time you double or halve the time step ormesh size. You know that it's not the handling of theboundary conditions or right hand side because these were both zero.
* If you have so verified that the time integrator is correct, take thesituation where the right hand side is nonzero but the initial conditions arezero:  [2.x.64]  and [2.x.65] . Again,
* [1.x.49]
* and for this to be equal to  [2.x.66] , we need that
* [1.x.50]
* and due to the initial conditions,  [2.x.67] . Integrating this equation in timeyields
* [1.x.51]
* 
* Again, if you have the wrong factors of  [2.x.68]  or  [2.x.69]  in front of the righthand side terms you will either not get the right temporal behavior of thesolution, or it will converge to a maximum value other than [2.x.70] .
* Once we have verified that the time integration and right hand side handlingare correct using this scheme, we can go on to verifying that we have theboundary values correct, using a very similar approach.
* 

* 
* [1.x.52][1.x.53]
* 

* Solving the heat equation on a simple domain with a simple right hand sidealmost always leads to solutions that are exceedingly boring, since theybecome very smooth very quickly and then do not move very much anymore. Rather, we here solve the equation on the L-shaped domain with zeroDirichlet boundary values and zero initial conditions, but as right hand sidewe choose
* [1.x.54]
* Here,
* [1.x.55]
* In other words, in every period of length  [2.x.71] , the right hand side firstflashes on in domain 1, then off completely, then on in domain 2, then offcompletely again. This pattern is probably best observed via the littleanimation of the solution shown in the [1.x.56].
* If you interpret the heat equation as finding the spatially and temporallyvariable temperature distribution of a conducting solid, then the test caseabove corresponds to an L-shaped body where we keep the boundary at zerotemperature, and heat alternatingly in two parts of the domain. While heatingis in effect, the temperature rises in these places, after which it diffusesand diminishes again. The point of these initial conditions is that theyprovide us with a solution that has singularities both in time (when sourcesswitch on and off) as well as time (at the reentrant corner as well as at theedges and corners of the regions where the source acts).
* 

*  [1.x.57] [1.x.58]
*  The program starts with the usual include files, all of which you should have seen before by now:
* 

* 
* [1.x.59]
* 
*  Then the usual placing of all content of this program into a namespace and the importation of the deal.II namespace into the one we will work in:
* 

* 
* [1.x.60]
* 
*   [1.x.61]  [1.x.62]   
*   The next piece is the declaration of the main class of this program. It follows the well trodden path of previous examples. If you have looked at  [2.x.72] , for example, the only thing worth noting here is that we need to build two matrices (the mass and Laplace matrix) and keep the current and previous time step's solution. We then also need to store the current time, the size of the time step, and the number of the current time step. The last of the member variables denotes the theta parameter discussed in the introduction that allows us to treat the explicit and implicit Euler methods as well as the Crank-Nicolson method and other generalizations all in one program.   
*   As far as member functions are concerned, the only possible surprise is that the  [2.x.73]  function takes arguments for the minimal and maximal mesh refinement level. The purpose of this is discussed in the introduction.
* 

* 
* [1.x.63]
* 
*   [1.x.64]  [1.x.65]
* 

* 
*  In the following classes and functions, we implement the various pieces of data that define this problem (right hand side and boundary values) that are used in this program and for which we need function objects. The right hand side is chosen as discussed at the end of the introduction. For boundary values, we choose zero values, but this is easily changed below.
* 

* 
* [1.x.66]
* 
*   [1.x.67]  [1.x.68]   
*   It is time now for the implementation of the main class. Let's start with the constructor which selects a linear element, a time step constant at 1/500 (remember that one period of the source on the right hand side was set to 0.2 above, so we resolve each period with 100 time steps) and chooses the Crank Nicolson method by setting  [2.x.74] .
* 

* 
* [1.x.69]
* 
*   [1.x.70]  [1.x.71]   
*   The next function is the one that sets up the DoFHandler object, computes the constraints, and sets the linear algebra objects to their correct sizes. We also compute the mass and Laplace matrix here by simply calling two functions in the library.   
*   Note that we do not take the hanging node constraints into account when assembling the matrices (both functions have an AffineConstraints argument that defaults to an empty object). This is because we are going to condense the constraints in run() after combining the matrices for the current time-step.
* 

* 
* [1.x.72]
* 
*   [1.x.73]  [1.x.74]   
*   The next function is the one that solves the actual linear system for a single time step. There is nothing surprising here:
* 

* 
* [1.x.75]
* 
*   [1.x.76]  [1.x.77]   
*   Neither is there anything new in generating graphical output other than the fact that we tell the DataOut object what the current time and time step number is, so that this can be written into the output file:
* 

* 
* [1.x.78]
* 
*   [1.x.79]  [1.x.80]   
*   This function is the interesting part of the program. It takes care of the adaptive mesh refinement. The three tasks this function performs is to first find out which cells to refine/coarsen, then to actually do the refinement and eventually transfer the solution vectors between the two different grids. The first task is simply achieved by using the well-established Kelly error estimator on the solution. The second task is to actually do the remeshing. That involves only basic functions as well, such as the  [2.x.75]  that refines those cells with the largest estimated error that together make up 60 per cent of the error, and coarsens those cells with the smallest error that make up for a combined 40 per cent of the error. Note that for problems such as the current one where the areas where something is going on are shifting around, we want to aggressively coarsen so that we can move cells around to where it is necessary.   
*   As already discussed in the introduction, too small a mesh leads to too small a time step, whereas too large a mesh leads to too little resolution. Consequently, after the first two steps, we have two loops that limit refinement and coarsening to an allowable range of cells:
* 

* 
* [1.x.81]
* 
*  These two loops above are slightly different but this is easily explained. In the first loop, instead of calling  [2.x.76]  we may as well have called  [2.x.77] . The two calls should yield the same iterator since iterators are sorted by level and there should not be any cells on levels higher than on level  [2.x.78] . In fact, this very piece of code makes sure that this is the case.
* 

* 
*  As part of mesh refinement we need to transfer the solution vectors from the old mesh to the new one. To this end we use the SolutionTransfer class and we have to prepare the solution vectors that should be transferred to the new grid (we will lose the old grid once we have done the refinement so the transfer has to happen concurrently with refinement). At the point where we call this function, we will have just computed the solution, so we no longer need the old_solution variable (it will be overwritten by the solution just after the mesh may have been refined, i.e., at the end of the time step; see below). In other words, we only need the one solution vector, and we copy it to a temporary object where it is safe from being reset when we further down below call  [2.x.79] .     
*   Consequently, we initialize a SolutionTransfer object by attaching it to the old DoF handler. We then prepare the triangulation and the data vector for refinement (in this order).
* 

* 
* [1.x.82]
* 
*  Now everything is ready, so do the refinement and recreate the DoF structure on the new grid, and finally initialize the matrix structures and the new vectors in the  [2.x.80]  function. Next, we actually perform the interpolation of the solution from old to new grid. The final step is to apply the hanging node constraints to the solution vector, i.e., to make sure that the values of degrees of freedom located on hanging nodes are so that the solution is continuous. This is necessary since SolutionTransfer only operates on cells locally, without regard to the neighborhood.
* 

* 
* [1.x.83]
* 
*   [1.x.84]  [1.x.85]   
*   This is the main driver of the program, where we loop over all time steps. At the top of the function, we set the number of initial global mesh refinements and the number of initial cycles of adaptive mesh refinement by repeating the first time step a few times. Then we create a mesh, initialize the various objects we will work with, set a label for where we should start when re-running the first time step, and interpolate the initial solution onto out mesh (we choose the zero function here, which of course we could do in a simpler way by just setting the solution vector to zero). We also output the initial time step once.   
*  

* 
*  [2.x.81]  If you're an experienced programmer, you may be surprised that we use a  [2.x.82]  statement in this piece of code!  [2.x.83]  statements are not particularly well liked any more since Edsgar Dijkstra, one of the greats of computer science, wrote a letter in 1968 called "Go To Statement considered harmful" (see [1.x.86]). The author of this code subscribes to this notion whole-heartedly:  [2.x.84]  is hard to understand. In fact, deal.II contains virtually no occurrences: excluding code that was essentially transcribed from books and not counting duplicated code pieces, there are 3 locations in about 600,000 lines of code at the time this note is written; we also use it in 4 tutorial programs, in exactly the same context as here. Instead of trying to justify the occurrence here, let's first look at the code and we'll come back to the issue at the end of function.
* 

* 
* [1.x.87]
* 
*  Then we start the main loop until the computed time exceeds our end time of 0.5. The first task is to build the right hand side of the linear system we need to solve in each time step. Recall that it contains the term  [2.x.85] . We put these terms into the variable system_rhs, with the help of a temporary vector:
* 

* 
* [1.x.88]
* 
*  The second piece is to compute the contributions of the source terms. This corresponds to the term  [2.x.86] . The following code calls  [2.x.87]  to compute the vectors  [2.x.88] , where we set the time of the right hand side (source) function before we evaluate it. The result of this all ends up in the forcing_terms variable:
* 

* 
* [1.x.89]
* 
*  Next, we add the forcing terms to the ones that come from the time stepping, and also build the matrix  [2.x.89]  that we have to invert in each time step. The final piece of these operations is to eliminate hanging node constrained degrees of freedom from the linear system:
* 

* 
* [1.x.90]
* 
*  There is one more operation we need to do before we can solve it: boundary values. To this end, we create a boundary value object, set the proper time to the one of the current time step, and evaluate it as we have done many times before. The result is used to also set the correct boundary values in the linear system:
* 

* 
* [1.x.91]
* 
*  With this out of the way, all we have to do is solve the system, generate graphical data, and...
* 

* 
* [1.x.92]
* 
*  ...take care of mesh refinement. Here, what we want to do is (i) refine the requested number of times at the very beginning of the solution procedure, after which we jump to the top to restart the time iteration, (ii) refine every fifth time step after that.         
*   The time loop and, indeed, the main part of the program ends with starting into the next time step by setting old_solution to the solution we have just computed.
* 

* 
* [1.x.93]
* 
*  Now that you have seen what the function does, let us come back to the issue of the  [2.x.90] . In essence, what the code does is something like this:  [2.x.91]  Here, the condition "happy with the result" is whether we'd like to keep the current mesh or would rather refine the mesh and start over on the new mesh. We could of course replace the use of the  [2.x.92]  by the following:  [2.x.93]  This has the advantage of getting rid of the  [2.x.94]  but the disadvantage of having to duplicate the code that implements the "solve timestep" and "postprocess" operations in two different places. This could be countered by putting these parts of the code (sizable chunks in the actual implementation above) into their own functions, but a  [2.x.95]  loop with a  [2.x.96]  statement is not really all that much easier to read or understand than a  [2.x.97] .
* 

* 
*  In the end, one might simply agree that [1.x.96]  [2.x.98]  statements are a bad idea but be pragmatic and state that there may be occasions where they can help avoid code duplication and awkward control flow. This may be one of these places, and it matches the position Steve McConnell takes in his excellent book "Code Complete"  [2.x.99]  about good programming practices (see the mention of this book in the introduction of  [2.x.100] ) that spends a surprising ten pages on the question of  [2.x.101]  in general.
* 

* 
*  
*  
*  [1.x.97]  [1.x.98]
* 

* 
*  Having made it this far,  there is, again, nothing much to discuss for the main function of this program: it looks like all such functions since  [2.x.102] .
* 

* 
* [1.x.99]
* [1.x.100][1.x.101]
* 

* As in many of the tutorials, the actual output of the program matters lessthan how we arrived there. Nonetheless, here it is:
* [1.x.102]
* 
* Maybe of more interest is a visualization of the solution and the mesh on whichit was computed:
*  [2.x.103] 
* The movie shows how the two sources switch on and off and how the mesh reactsto this. It is quite obvious that the mesh as is is probably not the best wecould come up with. We'll get back to this in the next section.
* 

* [1.x.103][1.x.104][1.x.105]
* 

* There are at least two areas where one can improve this program significantly:adaptive time stepping and a better choice of the mesh.
* [1.x.106][1.x.107]
* 

* Having chosen an implicit time stepping scheme, we are not bound by anyCFL-like condition on the time step. Furthermore, because the time scales onwhich change happens on a given cell in the heat equation are not bound to thecells diameter (unlike the case with the wave equation, where we had a fixedspeed of information transport that couples the temporal and spatial scales),we can choose the time step as we please. Or, better, choose it as we deemnecessary for accuracy.
* Looking at the solution, it is clear that the action does not happen uniformlyover time: a lot is changing around the time we switch on a source, thingsbecome less dramatic once a source is on for a little while, and we enter along phase of decline when both sources are off. During these times, we couldsurely get away with a larger time step than before without sacrificing toomuch accuracy.
* The literature has many suggestions on how to choose the time step sizeadaptively. Much can be learned, for example, from the way ODE solvers choosetheir time steps. One can also be inspired by a posteriori error estimatorsthat can, ideally, be written in a way that the consist of a temporal and aspatial contribution to the overall error. If the temporal one is too large,we should choose a smaller time step. Ideas in this direction can be found,for example, in the PhD thesis of a former principal developer of deal.II,Ralf Hartmann, published by the University of Heidelberg, Germany, in 2002.
* 

* [1.x.108][1.x.109]
* 

* We here use one of the simpler time stepping methods, namely the second orderin time Crank-Nicolson method. However, more accurate methods such asRunge-Kutta methods are available and should be used as they do not representmuch additional effort. It is not difficult to implement this for the currentprogram, but a more systematic treatment is also given in  [2.x.104] .
* 

* [1.x.110][1.x.111]
* 

* If you look at the meshes in the movie above, it is clear that they are notparticularly well suited to the task at hand. In fact, they look ratherrandom.
* There are two factors at play. First, there are some islands where cellshave been refined but that are surrounded by non-refined cells (and thereare probably also a few occasional coarsened islands). These are not terrible,as they most of the time do not affect the approximation quality of the mesh,but they also don't help because so many of their additional degrees offreedom are in fact constrained by hanging node constraints. That said,this is easy to fix: the Triangulation class takes an argument to itsconstructor indicating a level of "mesh smoothing". Passing one of manypossible flags, this instructs the triangulation to refine some additionalcells, or not to refine some cells, so that the resulting mesh does not havethese artifacts.
* The second problem is more severe: the mesh appears to lag the solution.The underlying reason is that we only adapt the mesh once every fifthtime step, and only allow for a single refinement in these cases. Whenever asource switches on, the solution had been very smooth in this area before andthe mesh was consequently rather coarse. This implies that the next time stepwhen we refine the mesh, we will get one refinement level more in this area,and five time steps later another level, etc. But this is not enough: first,we should refine immediately when a source switches on (after all, in thecurrent context we at least know what the right hand side is), and we shouldallow for more than one refinement level. Of course, all of this can be doneusing deal.II, it just requires a bit of algorithmic thinking in how to makethis work!
* 

* [1.x.112][1.x.113]
* 

* To increase the accuracy and resolution of your simulation in time, onetypically decreases the time step size  [2.x.105] . If you start playing aroundwith the time step in this particular example, you will notice that thesolution becomes partly negative, if  [2.x.106]  is below a certain threshold.This is not what we would expect to happen (in nature).
* To get an idea of this behavior mathematically, let us consider a general,fully discrete problem:
* [1.x.114]
* The general form of the  [2.x.107] th equation then reads:
* [1.x.115]
* where  [2.x.108]  is the set of degrees of freedom that DoF  [2.x.109]  couples with (i.e.,for which either the matrix  [2.x.110]  or matrix  [2.x.111]  has a nonzero entry at position [2.x.112] ). If all coefficientsfulfill the following conditions:
* [1.x.116]
* all solutions  [2.x.113]  keep their sign from the previous ones  [2.x.114] , andconsequently from the initial values  [2.x.115] . See e.g.[1.x.117]for more information on positivity preservation.
* Depending on the PDE to solve and the time integration scheme used, one isable to deduce conditions for the time step  [2.x.116] . For the heat equation withthe Crank-Nicolson scheme,[1.x.118] havetranslated it to the following ones:
* [1.x.119]
* where  [2.x.117]  denotes the mass matrix and  [2.x.118]  the stiffnessmatrix with  [2.x.119]  for  [2.x.120] , respectively. With [2.x.121] , we can formulate bounds for the global time step  [2.x.122]  asfollows:
* [1.x.120]
* In other words, the time step is constrained by [1.x.121] in case of a Crank-Nicolson scheme. These bounds should beconsidered along with the CFL condition to ensure significance of the performedsimulations.
* Being unable to make the time step as small as we want to get moreaccuracy without losing the positivity property is annoying. It raisesthe question of whether we can at least [1.x.122] the minimal time stepwe can choose  to ensure positivity preservation in this particular tutorial.Indeed, we can usethe SparseMatrix objects for both mass and stiffness that are created viathe MatrixCreator functions. Iterating through each entry via SparseMatrixIteratorslets us check for diagonal and off-diagonal entries to set a proper time stepdynamically. For quadratic matrices, the diagonal element is stored as thefirst member of a row (see SparseMatrix documentation). An exemplary codesnippet on how to grab the entries of interest from the  [2.x.123] is shown below.
* [1.x.123]
* 
* Using the information so computed, we can bound the time step via the formulasabove.
* 

* [1.x.124][1.x.125] [2.x.124] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-27_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34]
* [1.x.35][1.x.36][1.x.37]
* 

* This tutorial program attempts to show how to use  [2.x.2] -finite element methodswith deal.II. It solves the Laplace equation and so builds only on the firstfew tutorial programs, in particular on  [2.x.3]  for dimensionindependent programming and  [2.x.4]  for adaptive mesh refinement.
* The  [2.x.5] -finite element method was proposed in the early 1980s byBabu&scaron;ka and Guo as an alternative to either(i) mesh refinement (i.e., decreasing the mesh parameter  [2.x.6]  in a finiteelement computation) or (ii) increasing the polynomial degree  [2.x.7]  used forshape functions. It is based on the observation that increasing the polynomialdegree of the shape functions reduces the approximation error if the solutionis sufficiently smooth. On the other hand, it is well knownthat even for the generally well-behaved class of elliptic problems, higherdegrees of regularity can not be guaranteed in the vicinity of boundaries,corners, or where coefficients are discontinuous; consequently, theapproximation can not be improved in these areas by increasing the polynomialdegree  [2.x.8]  but only by refining the mesh, i.e., by reducing the mesh size [2.x.9] . These differing means to reduce theerror have led to the notion of  [2.x.10] -finite elements, where the approximatingfinite element spaces are adapted to have a high polynomial degree  [2.x.11] wherever the solution is sufficiently smooth, while the mesh width  [2.x.12]  isreduced at places wherever the solution lacks regularity. It wasalready realized in the first papers on this method that  [2.x.13] -finite elementscan be a powerful tool that can guarantee that the error is reduced not onlywith some negative power of the number of degrees of freedom, but in factexponentially.
* In order to implement this method, we need several things above and beyondwhat a usual finite element program needs, and in particular above what wehave introduced in the tutorial programs leading up to  [2.x.14] . In particular,we will have to discuss the following aspects: [2.x.15]    [2.x.16] Instead of using the same finite element on all cells, we now will want  a collection of finite element objects, and associate each cell with one  of these objects in this collection. [2.x.17] 
*    [2.x.18] Degrees of freedom will then have to be allocated on each cell depending  on what finite element is associated with this particular cell. Constraints  will have to be generated in the same way as for hanging nodes, but we now  also have to deal with the case where two neighboring cells have different  finite elements assigned. [2.x.19] 
*    [2.x.20] We will need to be able to assemble cell and face contributions  to global matrices and right hand side vectors. [2.x.21] 
*    [2.x.22] After solving the resulting linear system, we will want to  analyze the solution. In particular, we will want to compute error  indicators that tell us whether a given cell should be refined  and/or whether the polynomial degree of the shape functions used on  it should be increased. [2.x.23]  [2.x.24] 
* We will discuss all these aspects in the following subsections of thisintroduction. It will not come as a big surprise that most of thesetasks are already well supported by functionality provided by thedeal.II, and that we will only have to provide the logic of what theprogram should do, not exactly how all this is going to happen.
* In deal.II, the  [2.x.25] -functionality is largely packaged intothe hp-namespace. This namespace provides classes that handle [2.x.26] -discretizations, assembling matrices and vectors, and othertasks. We will get to know many of them further down below. Inaddition, most of the functions in the DoFTools, and VectorToolsnamespaces accept  [2.x.27] -objects in addition to the non- [2.x.28] -ones. Much ofthe  [2.x.29] -implementation is also discussed in the  [2.x.30]  documentationmodule and the links found there.
* It may be worth giving a slightly larger perspective at the end ofthis first part of the introduction.  [2.x.31] -functionality has beenimplemented in a number of different finite element packages (see, forexample, the list of references cited in the  [2.x.32]  "hp-paper").However, by and large, most of these packages have implemented it onlyfor the (i) the 2d case, and/or (ii) the discontinuous Galerkinmethod. The latter is a significant simplification becausediscontinuous finite elements by definition do not require continuityacross faces between cells and therefore do not require the specialtreatment otherwise necessary whenever finite elements of differentpolynomial degree meet at a common face. In contrast, deal.IIimplements the most general case, i.e., it allows for continuous anddiscontinuous elements in 1d, 2d, and 3d, and automatically handlesthe resulting complexity. In particular, it handles computing theconstraints (similar to hanging node constraints) of elements ofdifferent degree meeting at a face or edge. The many algorithmic anddata structure techniques necessary for this are described in the [2.x.33]  "hp-paper" for those interested in such detail.
* We hope that providing such a general implementation will help explorethe potential of  [2.x.34] -methods further.
* 

* 
* [1.x.38][1.x.39]
* 

* Now on again to the details of how to use the  [2.x.35] -functionality indeal.II. The first aspect we have to deal with is that now we do nothave only a single finite element any more that is used on all cells,but a number of different elements that cells can choose to use. Forthis, deal.II introduces the concept of a [1.x.40], implemented in the class  [2.x.36]  In essence,such a collection acts like an object of type [2.x.37] , but with a few more bellsand whistles and a memory management better suited to the task athand. As we will later see, we will also use similar quadraturecollections, and &mdash; although we don't use them here &mdash; thereis also the concept of mapping collections. All of these classes aredescribed in the  [2.x.38]  overview.
* In this tutorial program, we will use continuous Lagrange elements oforders 2 through 7 (in 2d) or 2 through 5 (in 3d). The collection ofused elements can then be created as follows:
* [1.x.41]
* 
* 

* 
* [1.x.42][1.x.43][1.x.44]
* 

* The next task we have to consider is what to do with the list offinite element objects we want to use. In previous tutorial programs,starting with  [2.x.39] , we have seen that the DoFHandlerclass is responsible for making the connection between a mesh(described by a Triangulation object) and a finite element, byallocating the correct number of degrees of freedom for each vertex,face, edge, and cell of the mesh.
* The situation here is a bit more complicated since we do not just havea single finite element object, but rather may want to use differentelements on different cells. We therefore need two things: (i) aversion of the DoFHandler class that can deal with this situation, and(ii) a way to tell the DoFHandler which element to use on which cell.
* The first of these two things is implemented in the [1.x.45]-mode ofthe DoFHandler class: rather than associating it with a triangulationand a single finite element object, it is associated with a triangulationand a finite element collection. The second part is achieved by a loopover all cells of this DoFHandler and for each cell setting the indexof the finite element within the collection that shall be used on thiscell. We call the index of the finite element object within thecollection that shall be used on a cell the cell's [1.x.46] to indicate that this is the finite element that is activeon this cell, whereas all the other elements of the collection areinactive on it. The general outline of this reads like this:
* [1.x.47]
* 
* Dots in the call to  [2.x.40]  indicate thatwe will have to have some sort of strategy later on to decide whichelement to use on which cell; we will come back to this later. Themain point here is that the first and last line of this code snippetis pretty much exactly the same as for the non- [2.x.41] -case.
* Another complication arises from the fact that this time we do notsimply have hanging nodes from local mesh refinement, but we also haveto deal with the case that if there are two cells with differentactive finite element indices meeting at a face (for example a Q2 anda Q3 element) then we have to compute additional constraints on thefinite element field to ensure that it is continuous. This isconceptually very similar to how we compute hanging node constraints,and in fact the code looks exactly the same:
* [1.x.48]
* In other words, the  [2.x.42]  deals notonly with hanging node constraints, but also with  [2.x.43] -constraints atthe same time.
* 

* 
* [1.x.49][1.x.50]
* 

* Following this, we have to set up matrices and vectors for the linear systemof the correct size and assemble them. Setting them up works in exactly thesame way as for the non- [2.x.44] -case. Assembling requires a bit more thought.
* The main idea is of course unchanged: we have to loop over all cells, assemblelocal contributions, and then copy them into the global objects. As discussedin some detail first in  [2.x.45] , deal.II has the FEValues class that pullsthe finite element description, mapping, and quadrature formulatogether and aids in evaluating values and gradients of shape functions aswell as other information on each of the quadrature points mapped to the reallocation of a cell. Every time we move on to a new cell we re-initialize thisFEValues object, thereby asking it to re-compute that part of the informationthat changes from cell to cell. It can then be used to sum up localcontributions to bilinear form and right hand side.
* In the context of  [2.x.46] -finite element methods, we have to deal with the factthat we do not use the same finite element object on each cell. In fact, weshould not even use the same quadrature object for all cells, but ratherhigher order quadrature formulas for cells where we use higher order finiteelements. Similarly, we may want to use higher order mappings on such cells aswell.
* To facilitate these considerations, deal.II has a class  [2.x.47]  that doeswhat we need in the current context. The difference is that instead of asingle finite element, quadrature formula, and mapping, it takes collectionsof these objects. It's use is very much like the regular FEValues class,i.e., the interesting part of the loop over all cells would look like this:
* [1.x.51]
* 
* In this tutorial program, we will always use a Q1 mapping, so the mappingcollection argument to the  [2.x.48]  construction will be omitted. Insidethe loop, we first initialize the  [2.x.49]  object for the currentcell. The second, third and fourth arguments denote the index within theirrespective collections of the quadrature, mapping, and finite element objectswe wish to use on this cell. These arguments can be omitted (and are in theprogram below), in which case  [2.x.50]  is usedfor this index. The order of these arguments is chosen in this way because onemay sometimes want to pick a different quadrature or mapping object from theirrespective collections, but hardly ever a different finite element than theone in use on this cell, i.e., one with an index different from [2.x.51] . The finite element collection index istherefore the last default argument so that it can be conveniently omitted.
* What this  [2.x.52]  call does is the following: the [2.x.53]  class checks whether it has previously already allocated anon- [2.x.54] -FEValues object for this combination of finite element, quadrature,and mapping objects. If not, it allocates one. It then re-initializes thisobject for the current cell, after which there is now a FEValues object forthe selected finite element, quadrature and mapping usable on the currentcell. A reference to this object is then obtained using the call [2.x.55] , and will be used in theusual fashion to assemble local contributions.
* 

* 
* [1.x.52][1.x.53]
* 

* One of the central pieces of the adaptive finite element method is that weinspect the computed solution (a posteriori) with an indicator that tells uswhich are the cells where the error is largest, and then refine them. In manyof the other tutorial programs, we use the KellyErrorEstimator class to get anindication of the size of the error on a cell, although we also discuss morecomplicated strategies in some programs, most importantly in  [2.x.56] .
* In any case, as long as the decision is only "refine this cell" or "do notrefine this cell", the actual refinement step is not particularlychallenging. However, here we have a code that is capable of hp-refinement,i.e., we suddenly have two choices whenever we detect that the error on acertain cell is too large for our liking: we can refine the cell by splittingit into several smaller ones, or we can increase the polynomial degree of theshape functions used on it. How do we know which is the more promisingstrategy? Answering this question is the central problem in  [2.x.57] -finiteelement research at the time of this writing.
* In short, the question does not appear to be settled in the literature at thistime. There are a number of more or less complicated schemes that address it,but there is nothing like the KellyErrorEstimator that is universally acceptedas a good, even if not optimal, indicator of the error. Most proposals use thefact that it is beneficial to increase the polynomial degree whenever thesolution is locally smooth whereas it is better to refine the mesh wherever itis rough. However, the questions of how to determine the local smoothness ofthe solution as well as the decision when a solution is smooth enough to allowfor an increase in  [2.x.58]  are certainly big and important ones.
* In the following, we propose a simple estimator of the local smoothness of asolution. As we will see in the results section, this estimator has flaws, inparticular as far as cells with local hanging nodes are concerned. Wetherefore do not intend to present the following ideas as a complete solutionto the problem. Rather, it is intended as an idea to approach it that meritsfurther research and investigation. In other words, we do not intend to entera sophisticated proposal into the fray about answers to the generalquestion. However, to demonstrate our approach to  [2.x.59] -finite elements, weneed a simple indicator that does generate some useful information that isable to drive the simple calculations this tutorial program will perform.
* 

* [1.x.54][1.x.55]
* 

* Our approach here is simple: for a function  [2.x.60]  to be in theSobolev space  [2.x.61]  on a cell  [2.x.62] , it has to satisfy the condition[1.x.56]Assuming that the cell  [2.x.63]  is not degenerate, i.e., that the mapping from theunit cell to cell  [2.x.64]  is sufficiently regular, above condition is of courseequivalent to[1.x.57]where  [2.x.65]  is the function  [2.x.66]  mapped back onto the unit cell [2.x.67] . From here, we can do the following: first, let us define theFourier series of  [2.x.68]  as[1.x.58]with Fourier vectors  [2.x.69]  in 2d,  [2.x.70] in 3d, etc, and  [2.x.71] . The coefficients of expansion [2.x.72]  can be obtained using  [2.x.73] -orthogonality of the exponential basis[1.x.59]that leads to the following expression[1.x.60]It becomes clear that we can then write the  [2.x.74]  norm of  [2.x.75]  as[1.x.61]In other words, if this norm is to be finite (i.e., for  [2.x.76]  to be in  [2.x.77] ), we need that[1.x.62]Put differently: the higher regularity  [2.x.78]  we want, the faster theFourier coefficients have to go to zero. If you wonder where theadditional exponent  [2.x.79]  comes from: we would like to makeuse of the fact that  [2.x.80]  if the sequence  [2.x.81]  for any  [2.x.82] . The problem is that wehere have a summation not only over a single variable, but over allthe integer multiples of  [2.x.83]  that are located inside the [2.x.84] -dimensional sphere, because we have vector components  [2.x.85] . In the same way as we prove that the sequence  [2.x.86]  aboveconverges by replacing the sum by an integral over the entire line, wecan replace our  [2.x.87] -dimensional sum by an integral over [2.x.88] -dimensional space. Now we have to note that between distance  [2.x.89] and  [2.x.90] , there are, up to a constant,  [2.x.91]  modes, inmuch the same way as we can transform the volume element  [2.x.92]  into [2.x.93] . Consequently, it is no longer  [2.x.94]  that has to decay as  [2.x.95] , butit is in fact  [2.x.96] . Acomparison of exponents yields the result.
* We can turn this around: Assume we are given a function  [2.x.97]  of unknownsmoothness. Let us compute its Fourier coefficients  [2.x.98] and see how fast they decay. If they decay as[1.x.63]then consequently the function we had here was in  [2.x.99] .
* 

* [1.x.64][1.x.65]
* 

* So what do we have to do to estimate the local smoothness of  [2.x.100]  ona cell  [2.x.101] ? Clearly, the first step is to compute the Fourier coefficientsof our solution. Fourier series being infinite series, we simplify ourtask by only computing the first few terms of the series, such that [2.x.102]  with a cut-off  [2.x.103] . Let us parenthetically remarkthat we want to choose  [2.x.104]  large enough so that we capture at leastthe variation of those shape functions that vary the most. On theother hand, we should not choose  [2.x.105]  too large: clearly, a finiteelement function, being a polynomial, is in  [2.x.106]  on any givencell, so the coefficients will have to decay exponentially at onepoint; since we want to estimate the smoothness of the function thispolynomial approximates, not of the polynomial itself, we need tochoose a reasonable cutoff for  [2.x.107] . Either way, computing this seriesis not particularly hard: from the definition[1.x.66]we see that we can compute the coefficient  [2.x.108]  as[1.x.67]where  [2.x.109]  is the value of the  [2.x.110] th degree of freedom on thiscell. In other words, we can write it as a matrix-vector product[1.x.68]with the matrix[1.x.69]This matrix is easily computed for a given number of shape functions [2.x.111]  and Fourier modes  [2.x.112] . Consequently, finding thecoefficients  [2.x.113]  is a rather trivial job.To simplify our life even further, we will use  [2.x.114]  class whichdoes exactly this.
* The next task is that we have to estimate how fast these coefficientsdecay with  [2.x.115] . The problem is that, of course, we have onlyfinitely many of these coefficients in the first place. In otherwords, the best we can do is to fit a function  [2.x.116] to our data points  [2.x.117] , for example bydetermining  [2.x.118]  via a least-squares procedure:[1.x.70]However, the problem with this is that it leads to a nonlinearproblem, a fact that we would like to avoid. On the other hand, we cantransform the problem into a simpler one if we try to fit thelogarithm of our coefficients to the logarithm of  [2.x.119] ,like this:[1.x.71]Using the usual facts about logarithms, we see that this yields theproblem[1.x.72]where  [2.x.120] . This is now a problem for which theoptimality conditions  [2.x.121] , are linear in  [2.x.122] . We canwrite these conditions as follows:[1.x.73]This linear system is readily inverted to yield[1.x.74]and[1.x.75]
* This is nothing else but linear regression fit and to do that we will use [2.x.123] While we are not particularly interested in the actual value of [2.x.124] , the formula above gives us a mean to calculate the value ofthe exponent  [2.x.125]  that we can then use to determine that [2.x.126]  is in  [2.x.127]  with  [2.x.128] .
* These steps outlined above are applicable to many different scenarios, whichmotivated the introduction of a generic function [2.x.129]  in deal.II, that combines allthe tasks described in this section in one simple function call. We will use itin the implementation of this program.
* 

* [1.x.76][1.x.77]
* 

* In the formulas above, we have derived the Fourier coefficients  [2.x.130] . Because  [2.x.131]  is a vector, we will get a number of Fouriercoefficients  [2.x.132]  for the same absolute value  [2.x.133] ,corresponding to the Fourier transform in different directions. If we nowconsider a function like  [2.x.134]  then we will find lots of large Fouriercoefficients in  [2.x.135] -direction because the function is non-smooth in thisdirection, but fast-decaying Fourier coefficients in  [2.x.136] -direction because thefunction is smooth there. The question that arises is this: if we simply fitour polynomial decay  [2.x.137]  to [1.x.78] Fourier coefficients,we will fit it to a smoothness [1.x.79]. Isthis what we want? Or would it be better to only consider the largestcoefficient  [2.x.138]  for all  [2.x.139]  with the same magnitude,essentially trying to determine the smoothness of the solution in that spatialdirection in which the solution appears to be roughest?
* One can probably argue for either case. The issue would be of more interest ifdeal.II had the ability to use anisotropic finite elements, i.e., ones that usedifferent polynomial degrees in different spatial directions, as they would beable to exploit the directionally variable smoothness much better. Alas, thiscapability does not exist at the time of writing this tutorial program.
* Either way, because we only have isotopic finite element classes, we adopt theviewpoint that we should tailor the polynomial degree to the lowest amount ofregularity, in order to keep numerical efforts low. Consequently, instead ofusing the formula[1.x.80]To calculate  [2.x.140]  as shown above, we have to slightly modify all sums:instead of summing over all Fourier modes, we only sum over those for whichthe Fourier coefficient is the largest one among all  [2.x.141]  withthe same magnitude  [2.x.142] , i.e., all sums above have to replaced by thefollowing sums:[1.x.81]This is the form we will implement in the program.
* 

* [1.x.82][1.x.83]
* 

* One may ask whether it is a problem that we only compute the Fourier transformon the [1.x.84] (rather than the real cell) of thesolution. After all, we stretch the solution by a factor  [2.x.143]  during thetransformation, thereby shifting the Fourier frequencies by a factor of [2.x.144] . This is of particular concern since we may have neighboring cells withmesh sizes  [2.x.145]  that differ by a factor of 2 if one of them is more refinedthan the other. The concern is also motivated by the fact that, as we will seein the results section below, the estimated smoothness of the solution shouldbe a more or less continuous function, but exhibits jumps at locations wherethe mesh size jumps. It therefore seems natural to ask whether we have tocompensate for the transformation.
* The short answer is "no". In the process outlined above, we attempt to findcoefficients  [2.x.146]  that minimize the sum of squares of the terms[1.x.85]To compensate for the transformation means not attempting to fit a decay [2.x.147]  with respect to the Fourier frequencies  [2.x.148]  [1.x.86], but to fit the coefficients  [2.x.149]  computed on thereference cell [1.x.87], where  [2.x.150]  is the norm of the transformation operator (i.e., somethinglike the diameter of the cell). In other words, we would have to minimize thesum of squares of the terms[1.x.88]instead. However, using fundamental properties of the logarithm, this issimply equivalent to minimizing[1.x.89]In other words, this and the original least squares problem will produce thesame best-fit exponent  [2.x.151] , though the offset will in one case be  [2.x.152] and in the other  [2.x.153] . However, since we are not interested inthe offset at all but only in the exponent, it doesn't matter whether we scaleFourier frequencies in order to account for mesh size effects or not, theestimated smoothness exponent will be the same in either case.
* 

* 
* [1.x.90][1.x.91]
* 

* [1.x.92][1.x.93]
* 

* One of the problems with  [2.x.154] -methods is that the high polynomial degree ofshape functions together with the large number of constrained degrees offreedom leads to matrices with large numbers of nonzero entries in somerows. At the same time, because there are areas where we use low polynomialdegree and consequently matrix rows with relatively few nonzeroentries. Consequently, allocating the sparsity pattern for these matrices is achallenge: we cannot simply assemble a SparsityPattern by starting with anestimate of the bandwidth without using a lot of extra memory.
* The way in which we create a SparsityPattern for the underlying linear system istightly coupled to the strategy we use to enforce constraints. deal.II supportshandling constraints in linear systems in two ways: [2.x.155]    [2.x.156] Assembling the matrix without regard to the constraints and applying them  afterwards with  [2.x.157]  or [2.x.158]    [2.x.159] Applying constraints as we assemble the system with   [2.x.160]  [2.x.161] Most programs built on deal.II use the  [2.x.162]  functionto allocate a DynamicSparsityPattern that takes constraints into account. Thesystem matrix then uses a SparsityPattern copied over from theDynamicSparsityPattern. This method is explained in  [2.x.163]  and used in mosttutorial programs.
* The early tutorial programs use first or second degree finite elements, soremoving entries in the sparsity pattern corresponding to constrained degrees offreedom does not have a large impact on the overall number of zeros explicitlystored by the matrix. However, since as many as a third of the degrees offreedom may be constrained in an hp-discretization (and, with higher degreeelements, these constraints can couple one DoF to as many as ten or twenty otherDoFs), it is worthwhile to take these constraints into consideration since theresulting matrix will be much sparser (and, therefore, matrix-vector products orfactorizations will be substantially faster too).
* 

* [1.x.94][1.x.95]
* 

* A second problem particular to  [2.x.164] -methods arises because we have somany constrained degrees of freedom: typically up to about one thirdof all degrees of freedom (in 3d) are constrained because they eitherbelong to cells with hanging nodes or because they are on cellsadjacent to cells with a higher or lower polynomial degree. This is,in fact, not much more than the fraction of constrained degrees offreedom in non- [2.x.165] -mode, but the difference is that each constrainedhanging node is constrained not only against the two adjacent degreesof freedom, but is constrained against many more degrees of freedom.
* It turns out that the strategy presented first in  [2.x.166]  to eliminate theconstraints while computing the element matrices and vectors with [2.x.167]  is the most efficient approachalso for this case. The alternative strategy to first build the matrix withoutconstraints and then "condensing" away constrained degrees of freedom isconsiderably more expensive. It turns out that building the sparsity patternby this inefficient algorithm requires at least  [2.x.168]  in thenumber of unknowns, whereas an ideal finite element program would of courseonly have algorithms that are linear in the number of unknowns. Timing thesparsity pattern creation as well as the matrix assembly shows that thealgorithm presented in  [2.x.169]  (and used in the code below) is indeed faster.
* In our program, we will also treat the boundary conditions as (possiblyinhomogeneous) constraints and eliminate the matrix rows and columns tothose as well. All we have to do for this is to call the function thatinterpolates the Dirichlet boundary conditions already in the setup phase inorder to tell the AffineConstraints object about them, and then do thetransfer from local to global data on matrix and vector simultaneously. Thisis exactly what we've shown in  [2.x.170] .
* 

* 
* [1.x.96][1.x.97]
* 

* The test case we will solve with this program is a re-take of the one wealready look at in  [2.x.171] : we solve the Laplace equation[1.x.98]in 2d, with  [2.x.172] , and with zero Dirichlet boundary values for [2.x.173] . We do so on the domain  [2.x.174] ,i.e., a square with a square hole in the middle.
* The difference to  [2.x.175]  is of course that we use  [2.x.176] -finiteelements for the solution. The test case is of interest because it hasre-entrant corners in the corners of the hole, at which the solution hassingularities. We therefore expect that the solution will be smooth in theinterior of the domain, and rough in the vicinity of the singularities. Thehope is that our refinement and smoothness indicators will be able to see thisbehavior and refine the mesh close to the singularities, while the polynomialdegree is increased away from it. As we will see in the results section, thisis indeed the case.
* 

*  [1.x.99] [1.x.100]
*   [1.x.101]  [1.x.102]
* 

* 
*  The first few files have already been covered in previous examples and will thus not be further commented on.
* 

* 
* [1.x.103]
* 
*  These are the new files we need. The first and second provide the FECollection and the [1.x.104] version of the FEValues class as described in the introduction of this program. The next one provides the functionality for automatic  [2.x.177] -adaptation, for which we will use the estimation algorithms based on decaying series expansion coefficients that are part of the last two files.
* 

* 
* [1.x.105]
* 
*  The last set of include files are standard C++ headers.
* 

* 
* [1.x.106]
* 
*  Finally, this is as in previous programs:
* 

* 
* [1.x.107]
* 
*   [1.x.108]  [1.x.109]
* 

* 
*  The main class of this program looks very much like the one already used in the first few tutorial programs, for example the one in  [2.x.178] . The main difference is that we have merged the refine_grid and output_results functions into one since we will also want to output some of the quantities used in deciding how to refine the mesh (in particular the estimated smoothness of the solution).   
*   As far as member variables are concerned, we use the same structure as already used in  [2.x.179] , but we need collections instead of individual finite element, quadrature, and face quadrature objects. We will fill these collections in the constructor of the class. The last variable,  [2.x.180] , indicates the maximal polynomial degree of shape functions used.
* 

* 
* [1.x.110]
* 
*   [1.x.111]  [1.x.112]   
*   Next, let us define the right hand side function for this problem. It is  [2.x.181]  in 1d,  [2.x.182]  in 2d, and so on.
* 

* 
* [1.x.113]
* 
*   [1.x.114]  [1.x.115]
* 

* 
*   [1.x.116]  [1.x.117]
* 

* 
*  The constructor of this class is fairly straightforward. It associates the DoFHandler object with the triangulation, and then sets the maximal polynomial degree to 7 (in 1d and 2d) or 5 (in 3d and higher). We do so because using higher order polynomial degrees becomes prohibitively expensive, especially in higher space dimensions.   
*   Following this, we fill the collections of finite element, and cell and face quadrature objects. We start with quadratic elements, and each quadrature formula is chosen so that it is appropriate for the matching finite element in the  [2.x.183]  object.
* 

* 
* [1.x.118]
* 
*   [1.x.119]  [1.x.120]
* 

* 
*  The destructor is unchanged from what we already did in  [2.x.184] :
* 

* 
* [1.x.121]
* 
*   [1.x.122]  [1.x.123]   
*   This function is again a verbatim copy of what we already did in  [2.x.185] . Despite function calls with exactly the same names and arguments, the algorithms used internally are different in some aspect since the dof_handler variable here is in  [2.x.186] -mode.
* 

* 
* [1.x.124]
* 
*   [1.x.125]  [1.x.126]
* 

* 
*  This is the function that assembles the global matrix and right hand side vector from the local contributions of each cell. Its main working is as has been described in many of the tutorial programs before. The significant deviations are the ones necessary for [1.x.127] finite element methods. In particular, that we need to use a collection of FEValues object (implemented through the  [2.x.187]  class), and that we have to eliminate constrained degrees of freedom already when copying local contributions into global objects. Both of these are explained in detail in the introduction of this program.   
*   One other slight complication is the fact that because we use different polynomial degrees on different cells, the matrices and vectors holding local contributions do not have the same size on all cells. At the beginning of the loop over all cells, we therefore each time have to resize them to the correct size (given by  [2.x.188] ). Because these classes are implemented in such a way that reducing the size of a matrix or vector does not release the currently allocated memory (unless the new size is zero), the process of resizing at the beginning of the loop will only require re-allocation of memory during the first few iterations. Once we have found in a cell with the maximal finite element degree, no more re-allocations will happen because all subsequent  [2.x.189]  calls will only set the size to something that fits the currently allocated memory. This is important since allocating memory is expensive, and doing so every time we visit a new cell would take significant compute time.
* 

* 
* [1.x.128]
* 
*   [1.x.129]  [1.x.130]
* 

* 
*  The function solving the linear system is entirely unchanged from previous examples. We simply try to reduce the initial residual (which equals the  [2.x.190]  norm of the right hand side) by a certain factor:
* 

* 
* [1.x.131]
* 
*   [1.x.132]  [1.x.133]
* 

* 
*  After solving the linear system, we will want to postprocess the solution. Here, all we do is to estimate the error, estimate the local smoothness of the solution as described in the introduction, then write graphical output, and finally refine the mesh in both  [2.x.191]  and  [2.x.192]  according to the indicators computed before. We do all this in the same function because we want the estimated error and smoothness indicators not only for refinement, but also include them in the graphical output.
* 

* 
* [1.x.134]
* 
*  Let us start with computing estimated error and smoothness indicators, which each are one number for each active cell of our triangulation. For the error indicator, we use the KellyErrorEstimator class as always.
* 

* 
* [1.x.135]
* 
*  Estimating the smoothness is performed with the method of decaying expansion coefficients as outlined in the introduction. We will first need to create an object capable of transforming the finite element solution on every single cell into a sequence of Fourier series coefficients. The SmoothnessEstimator namespace offers a factory function for such a  [2.x.193]  object that is optimized for the process of estimating smoothness. The actual determination of the decay of Fourier coefficients on every individual cell then happens in the last function.
* 

* 
* [1.x.136]
* 
*  Next we want to generate graphical output. In addition to the two estimated quantities derived above, we would also like to output the polynomial degree of the finite elements used on each of the elements on the mesh.     
*   The way to do that requires that we loop over all cells and poll the active finite element index of them using  [2.x.194] . We then use the result of this operation and query the finite element collection for the finite element with that index, and finally determine the polynomial degree of that element. The result we put into a vector with one element per cell. The DataOut class requires this to be a vector of  [2.x.195] , even though our values are all integers, so that is what we use:
* 

* 
* [1.x.137]
* 
*  With now all data vectors available
* 
*  -  solution, estimated errors and smoothness indicators, and finite element degrees
* 
*  - , we create a DataOut object for graphical output and attach all data:
* 

* 
* [1.x.138]
* 
*  The final step in generating output is to determine a file name, open the file, and write the data into it (here, we use VTK format):
* 

* 
* [1.x.139]
* 
*  After this, we would like to actually refine the mesh, in both  [2.x.196]  and  [2.x.197] . The way we are going to do this is as follows: first, we use the estimated error to flag those cells for refinement that have the largest error. This is what we have always done:
* 

* 
* [1.x.140]
* 
*  Next we would like to figure out which of the cells that have been flagged for refinement should actually have  [2.x.198]  increased instead of  [2.x.199]  decreased. The strategy we choose here is that we look at the smoothness indicators of those cells that are flagged for refinement, and increase  [2.x.200]  for those with a smoothness larger than a certain relative threshold. In other words, for every cell for which (i) the refinement flag is set, (ii) the smoothness indicator is larger than the threshold, and (iii) we still have a finite element with a polynomial degree higher than the current one in the finite element collection, we will assign a future FE index that corresponds to a polynomial with degree one higher than it currently is. The following function is capable of doing exactly this. Absent any better strategies, we will set the threshold via interpolation between the minimal and maximal smoothness indicators on cells flagged for refinement. Since the corner singularities are strongly localized, we will favor  [2.x.201] - over  [2.x.202] -refinement quantitatively. We achieve this with a low threshold by setting a small interpolation factor of 0.2. In the same way, we deal with cells that are going to be coarsened and decrease their polynomial degree when their smoothness indicator is below the corresponding threshold determined on cells to be coarsened.
* 

* 
* [1.x.141]
* 
*  The above function only determines whether the polynomial degree will change via future FE indices, but does not manipulate the  [2.x.203] -refinement flags. So for cells that are flagged for both refinement categories, we prefer  [2.x.204] - over  [2.x.205] -refinement. The following function call ensures that only one of  [2.x.206] - or  [2.x.207] -refinement is imposed, and not both at once.
* 

* 
* [1.x.142]
* 
*  For grid adaptive refinement, we ensure a 2:1 mesh balance by limiting the difference of refinement levels of neighboring cells to one by calling  [2.x.208]  We would like to achieve something similar for the p-levels of neighboring cells: levels of future finite elements are not allowed to differ by more than a specified difference. With its default parameters, a call of  [2.x.209]  ensures that their level difference is limited to one. This will not necessarily decrease the number of hanging nodes in the domain, but makes sure that high order polynomials are not constrained to much lower polynomials on faces, e.g., fifth order to second order polynomials.
* 

* 
* [1.x.143]
* 
*  At the end of this procedure, we then refine the mesh. During this process, children of cells undergoing bisection inherit their mother cell's finite element index. Further, future finite element indices will turn into active ones, so that the new finite elements will be assigned to cells after the next call of  [2.x.210] 
* 

* 
* [1.x.144]
* 
*   [1.x.145]  [1.x.146]
* 

* 
*  The following function is used when creating the initial grid. The grid we would like to create is actually similar to the one from  [2.x.211] , i.e., the square domain with the square hole in the middle. It can be generated by exactly the same function. However, since its implementation is only a specialization of the 2d case, we will present a different way of creating this domain which is dimension independent.   
*   We first create a hypercube triangulation with enough cells so that it already holds our desired domain  [2.x.212] , subdivided into  [2.x.213]  cells. We then remove those cells in the center of the domain by testing the coordinate values of the vertices on each cell. In the end, we refine the so created grid globally as usual.
* 

* 
* [1.x.147]
* 
*   [1.x.148]  [1.x.149]
* 

* 
*  This function implements the logic of the program, as did the respective function in most of the previous programs already, see for example  [2.x.214] .   
*   Basically, it contains the adaptive loop: in the first iteration create a coarse grid, and then set up the linear system, assemble it, solve, and postprocess the solution including mesh refinement. Then start over again. In the meantime, also output some information for those staring at the screen trying to figure out what the program does:
* 

* 
* [1.x.150]
* 
*   [1.x.151]  [1.x.152]
* 

* 
*  The main function is again verbatim what we had before: wrap creating and running an object of the main class into a  [2.x.215]  block and catch whatever exceptions are thrown, thereby producing meaningful output if anything should go wrong:
* 

* 
* [1.x.153]
* [1.x.154][1.x.155]
* 

* In this section, we discuss a few results produced from running thecurrent tutorial program. More results, in particular the extension to3d calculations and determining how much compute time the individualcomponents of the program take, are given in the  [2.x.216]  "hp-paper".
* When run, this is what the program produces:
* [1.x.156]
* 
* The first thing we learn from this is that the number of constrained degreesof freedom is on the order of 20-25% of the total number of degrees offreedom, at least on the later grids when we have elements of relativelyhigh order (in 3d, the fraction of constrained degrees of freedom can be upto 30%). This is, in fact, on the same order of magnitude as fornon- [2.x.217] -discretizations. For example, in the last step of the  [2.x.218] program, we have 18353 degrees of freedom, 4432 of which areconstrained. The difference is that in the latter program, each constrainedhanging node is constrained against only the two adjacent degrees offreedom, whereas in the  [2.x.219] -case, constrained nodes are constrained againstmany more degrees of freedom. Note also that the current program alsoincludes nodes subject to Dirichlet boundary conditions in the list ofconstraints. In cycle 0, all the constraints are actually because ofboundary conditions.
* Of maybe more interest is to look at the graphical output. First, here is thesolution of the problem:
* <img src="https://www.dealii.org/images/steps/developer/ [2.x.220] -solution.png"     alt="Elevation plot of the solution, showing the lack of regularity near          the interior (reentrant) corners."     width="200" height="200">
* Secondly, let us look at the sequence of meshes generated:
*  [2.x.221] 
* It is clearly visible how the mesh is refined near the corner singularities,as one would expect it. More interestingly, we should be curious to see thedistribution of finite element polynomial degrees to these mesh cells, wherethe lightest color corresponds to degree two and the darkest one correspondsto degree seven:
*  [2.x.222] 
* While this is certainly not a perfect arrangement, it does make some sense: weuse low order elements close to boundaries and corners where regularity islow. On the other hand, higher order elements are used where (i) the error wasat one point fairly large, i.e., mainly in the general area around the cornersingularities and in the top right corner where the solution is large, and(ii) where the solution is smooth, i.e., far away from the boundary.
* This arrangement of polynomial degrees of course follows from our smoothnessestimator. Here is the estimated smoothness of the solution, with darker colorsindicating least smoothness and lighter indicating the smoothest areas:
*  [2.x.223] 
* The primary conclusion one can draw from this is that the loss of regularity atthe internal corners is a highly localized phenomenon; it only seems to impactthe cells adjacent to the corner itself, so when we refine the mesh the blackcoloring is no longer visible. Besides the corners, this sequence of plotsimplies that the smoothness estimates are somewhat independent of the meshrefinement, particularly when we are far away from boundaries.It is also obvious that the smoothness estimates are independent of the actualsize of the solution (see the picture of the solution above), as it should be.A point of larger concern, however, is that one realizes on closer inspectionthat the estimator we have overestimates the smoothness of the solution oncells with hanging nodes. This in turn leads to higher polynomial degrees inthese areas, skewing the allocation of finite elements onto cells.
* We have no good explanation for this effect at the moment. One theory is thatthe numerical solution on cells with hanging nodes is, of course, constrainedand therefore not entirely free to explore the function space to get close tothe exact solution. This lack of degrees of freedom may manifest itself byyielding numerical solutions on these cells with suppressed oscillation,meaning a higher degree of smoothness. The estimator picks this signal up andthe estimated smoothness overestimates the actual value. However, a definiteanswer to what is going on currently eludes the authors of this program.
* The bigger question is, of course, how to avoid this problem. Possibilitiesinclude estimating the smoothness not on single cells, but cell assemblies orpatches surrounding each cell. It may also be possible to find simplecorrection factors for each cell depending on the number of constraineddegrees of freedom it has. In either case, there are ample opportunities forfurther research on finding good  [2.x.224] -refinement criteria. On the other hand,the main point of the current program was to demonstrate using the [2.x.225] -technology in deal.II, which is unaffected by our use of a possiblesub-optimal refinement criterion.
* 

* 
* [1.x.157][1.x.158][1.x.159]
* 

* [1.x.160][1.x.161]
* 

* This tutorial demonstrates only one particular strategy to decide between  [2.x.226] - and [2.x.227] -adaptation. In fact, there are many more ways to automatically decide on theadaptation type, of which a few are already implemented in deal.II: [2.x.228]    [2.x.229] [1.x.162] This is the strategy currently  implemented in this tutorial. For more information on this strategy, see  the general documentation of the  [2.x.230]  namespace. [2.x.231] 
*    [2.x.232] [1.x.163] This strategy is quite similar  to the current one, but uses Legendre series expansion rather than the  Fourier one: instead of sinusoids as basis functions, this strategy uses  Legendre polynomials. Of course, since we approximate the solution using a  finite-dimensional polynomial on each cell, the expansion of the solution in  Legendre polynomials is also finite and, consequently, when we talk about the  "decay" of this expansion, we can only consider the finitely many nonzero  coefficients of this expansion, rather than think about it in asymptotic terms.  But, if we have enough of these coefficients, we can certainly think of the  decay of these coefficients as characteristic of the decay of the coefficients  of the exact solution (which is, in general, not polynomial and so will have an  infinite Legendre expansion), and considering the coefficients we have should  reveal something about the properties of the exact solution.
*   The transition from the Fourier strategy to the Legendre one is quite simple:  You just need to change the series expansion class and the corresponding  smoothness estimation function to be part of the proper namespaces   [2.x.233]  and  [2.x.234]  This strategy is used  in  [2.x.235] . For the theoretical background of this strategy, consult the  general documentation of the  [2.x.236]  namespace, as well  as  [2.x.237]  ,  [2.x.238]  and  [2.x.239]  . [2.x.240] 
*    [2.x.241] [1.x.164] The last strategy is quite different  from the other two. In theory, we know how the error will converge  after changing the discretization of the function space. With   [2.x.242] -refinement the solution converges algebraically as already pointed  out in  [2.x.243] . If the solution is sufficiently smooth, though, we  expect that the solution will converge exponentially with increasing  polynomial degree of the finite element. We can compare a proper  prediction of the error with the actual error in the following step to  see if our choice of adaptation type was justified.
*   The transition to this strategy is a bit more complicated. For this, we need  an initialization step with pure  [2.x.244] - or  [2.x.245] -refinement and we need to  transfer the predicted errors over adapted meshes. The extensive  documentation of the  [2.x.246]  function describes not  only the theoretical details of this approach, but also presents a blueprint  on how to implement this strategy in your code. For more information, see   [2.x.247]  .
*   Note that with this particular function you cannot predict the error for  the next time step in time-dependent problems. Therefore, this strategy  cannot be applied to this type of problem without further ado. Alternatively,  the following approach could be used, which works for all the other  strategies as well: start each time step with a coarse mesh, keep refining  until happy with the result, and only then move on to the next time step. [2.x.248]  [2.x.249] 
* Try implementing one of these strategies into this tutorial and observe thesubtle changes to the results. You will notice that all strategies arecapable of identifying the singularities near the reentrant corners andwill perform  [2.x.250] -refinement in these regions, while preferring  [2.x.251] -refinementin the bulk domain. A detailed comparison of these strategies is presentedin  [2.x.252]  .
* 

* [1.x.165][1.x.166]
* 

* All functionality presented in this tutorial already works for bothsequential and parallel applications. It is possible without too mucheffort to change to either the  [2.x.253]  or the [2.x.254]  classes. If you feel eager to tryit, we recommend reading  [2.x.255]  for the former and  [2.x.256]  for thelatter case first for further background information on the topic, andthen come back to this tutorial to try out your newly acquired skills.
* We go one step further in  [2.x.257] : Here, we combine hp-adapative andMatrixFree methods in combination with [2.x.258]  objects.
* 

* [1.x.167][1.x.168] [2.x.259] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-28_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34][1.x.35][1.x.36][1.x.37][1.x.38][1.x.39]
*  [2.x.2] 
* [1.x.40][1.x.41][1.x.42]
*  [2.x.3] 
* 

* [1.x.43][1.x.44] [1.x.45]
* In this example, we intend to solve the multigroup diffusion approximation ofthe neutron transport equation. Essentially, the way to view this is as follows: In anuclear reactor, neutrons are speeding around at different energies, getabsorbed or scattered, or start a new fissionevent. If viewed at long enough length scales, the movement of neutrons can beconsidered a diffusion process.
* A mathematical description of this would group neutrons into energy bins, andconsider the balance equations for the neutron fluxes in each of thesebins, or energy groups. The scattering, absorption, and fission events wouldthen be operators within the diffusion equation describing the neutronfluxes. Assume we have energy groups  [2.x.4] , where by convention weassume that the neutrons with the highest energy are in group 1 and those withthe lowest energy in group  [2.x.5] . Then the neutron flux of each group satisfies thefollowing equations:[1.x.46]
* augmented by appropriate boundary conditions. Here,  [2.x.6]  is the velocity ofneutrons within group  [2.x.7] . In other words, the change intime in flux of neutrons in group  [2.x.8]  is governed by the followingprocesses: [2.x.9]  [2.x.10]  Diffusion  [2.x.11] . Here,  [2.x.12]  is the  (spatially variable) diffusion coefficient. [2.x.13]  Absorption  [2.x.14]  (note the  negative sign). The coefficient  [2.x.15]  is called the [1.x.47]. [2.x.16]  Nuclear fission  [2.x.17] .  The production of neutrons of energy  [2.x.18]  is  proportional to the flux of neutrons of energy  [2.x.19]  times the  probability  [2.x.20]  that neutrons of energy  [2.x.21]  cause a fission  event times the number  [2.x.22]  of neutrons produced in each fission event  times the probability that a neutron produced in this event has energy   [2.x.23] .  [2.x.24]  is called the [1.x.48] and   [2.x.25]  the [1.x.49]. We will denote the term   [2.x.26]  as the [1.x.50] in the program. [2.x.27]  Scattering  [2.x.28]   of neutrons of energy  [2.x.29]  producing neutrons  of energy  [2.x.30] .  [2.x.31]  is called the [1.x.51]. The case of elastic, in-group scattering  [2.x.32]  exists, too, but  we subsume this into the removal cross section. The case  [2.x.33]  is called  down-scattering, since a neutron loses energy in such an event. On the  other hand,  [2.x.34]  corresponds to up-scattering: a neutron gains energy in  a scattering event from the thermal motion of the atoms surrounding it;  up-scattering is therefore only an important process for neutrons with  kinetic energies that are already on the same order as the thermal kinetic  energy (i.e. in the sub  [2.x.35]  range). [2.x.36]  An extraneous source  [2.x.37] . [2.x.38] 
* For realistic simulations in reactor analysis, one may want to split thecontinuous spectrum of neutron energies into many energy groups, often up to 100.However, if neutron energy spectra are known well enough for some type ofreactor (for example Pressurized Water Reactors, PWR), it is possible to obtainsatisfactory results with only 2 energy groups.
* In the program shown in this tutorial program, we provide the structure tocompute with as many energy groups as desired. However, to keep computingtimes moderate and in order to avoid tabulating hundreds of coefficients, weonly provide the coefficients for above equations for a two-group simulation,i.e.  [2.x.39] . We do, however, consider a realistic situation by assuming thatthe coefficients are not constant, but rather depend on the materials that areassembled into reactor fuel assemblies in rather complicated ways (seebelow).
* 

* [1.x.52][1.x.53]
* 

* If we consider all energy groups at once, we may write above equations in thefollowing operator form:[1.x.54]
* where  [2.x.40]  are sinking, fission, and scattering operators,respectively.  [2.x.41]  here includes both the diffusion and removal terms. Notethat  [2.x.42]  is symmetric, whereas  [2.x.43]  and  [2.x.44]  are not.
* It is well known that this equation admits a stable solution if alleigenvalues of the operator  [2.x.45]  are negative. This can be readily seen bymultiplying the equation by  [2.x.46]  and integrating over the domain, leading to[1.x.55]
* Stability means that the solution does not grow, i.e. we want the left handside to be less than zero, which is the case if the eigenvalues of theoperator on the right are all negative. For obvious reasons, it isnot very desirable if a nuclear reactor produces neutron fluxes that growexponentially, so eigenvalue analyses are the bread-and-butter of nuclearengineers. The main point of the program is therefore to consider theeigenvalue problem[1.x.56]
* where we want to make sure that all eigenvalues are positive. Note that  [2.x.47] ,being the diffusion operator plus the absorption (removal), is positivedefinite; the condition that all eigenvalues are positive therefore means thatwe want to make sure that fission and inter-group scattering are weak enoughto not shift the spectrum into the negative.
* In nuclear engineering, one typically looks at a slightly differentformulation of the eigenvalue problem. To this end, we do not just multiplywith  [2.x.48]  and integrate, but rather multiply with  [2.x.49] . We thenget the following evolution equation:[1.x.57]
* Stability is then guaranteed if the eigenvalues of the following problem areall negative:[1.x.58]
* which is equivalent to the eigenvalue problem[1.x.59]
* The typical formulation in nuclear engineering is to write this as[1.x.60]
* where  [2.x.50] .Intuitively,  [2.x.51]  is something like the multiplicationfactor for neutrons per typical time scale and should be less than or equal toone for stable operation of a reactor: if it is less than one, the chain reaction willdie down, whereas nuclear bombs for example have a  [2.x.52] -eigenvalue larger thanone. A stable reactor should have  [2.x.53] .
* For those who wonder how this can be achieved in practice withoutinadvertently getting slightly larger than one and triggering a nuclear bomb:first, fission processes happen on different time scales. While most neutronsare released very quickly after a fission event, a small number of neutronsare only released by daughter nuclei after several further decays, up to 10-60seconds after the fission was initiated. If one is therefore slightly beyond [2.x.54] , one therefore has many seconds to react until all theneutrons created in fission re-enter the fission cycle. Nevertheless, controlrods in nuclear reactors absorbing neutrons
* 
*  -  and therefore reducing [2.x.55] 
* 
*  -  are designed in such a way that they are all the way inthe reactor in at most 2 seconds.
* One therefore has on the order of 10-60 seconds to regulate the nuclear reactionif  [2.x.56]  should be larger than one for some time, as indicated bya growing neutron flux. Regulation can be achieved by continuously monitoringthe neutron flux, and if necessary increase or reduce neutron flux by movingneutron-absorbing control rods a few millimeters into or out of thereactor. On a longer scale, the water cooling the reactor contains boron, agood neutron absorber. Every few hours, boron concentrations are adjusted byadding boron or diluting the coolant.
* Finally, some of the absorption and scattering reactions have somestability built in; for example, higher neutron fluxes result in locallyhigher temperatures, which lowers the density of water and therefore reducesthe number of scatterers that are necessary to moderate neutrons from high tolow energies before they can start fission events themselves.
* In this tutorial program, we solve above  [2.x.57] -eigenvalue problem for two energygroups, and we are looking for the largest multiplication factor [2.x.58] , which is proportional to the inverse of the minimumeigenvalue plus one. To solve the eigenvalue problem, we generallyuse a modified version of the [1.x.61]. The algorithm lookslike this:
*  [2.x.59]  [2.x.60]  Initialize  [2.x.61]  and  [2.x.62]  with  [2.x.63]   and  [2.x.64]  and let  [2.x.65] .
*  [2.x.66]  Define the so-called [1.x.62] by  [1.x.63]
* 
*  [2.x.67]  Solve for all group fluxes  [2.x.68]  using  [1.x.64]
* 
*  [2.x.69]  Update  [1.x.65]
* 
*  [2.x.70]  Compare  [2.x.71]  with  [2.x.72] .  If the change greater than a prescribed tolerance then set  [2.x.73]  repeat  the iteration starting at step 2, otherwise end the iteration. [2.x.74] 
* Note that in this scheme, we do not solve group fluxes exactly in each poweriteration, but rather consider previously compute  [2.x.75]  only fordown-scattering events  [2.x.76] . Up-scattering is only treated by using olditerators  [2.x.77] , in essence assuming that the scatteringoperator is triangular. This is physically motivated since up-scattering doesnot play a too important role in neutron scattering. In addition, practicesshows that the inverse power iteration is stable even using thissimplification.
* Note also that one can use lots of extrapolation techniques to accelerate thepower iteration laid out above. However, none of these are implemented in thisexample.
* 

* [1.x.66][1.x.67]
* 

* One may wonder whether it is appropriate to solve for the solutions of theindividual energy group equations on the same meshes. The question boils downto this: will  [2.x.78]  and  [2.x.79]  have similar smoothness properties? Ifthis is the case, then it is appropriate to use the same mesh for the two; atypical application could be chemical combustion, where typically theconcentrations of all or most chemical species change rapidly within the flamefront. As it turns out, and as will be apparent by looking at thegraphs shown in the results section of this tutorial program, this isn't thecase here, however: since the diffusion coefficient is different for differentenergy groups, fast neutrons (in bins with a small group number  [2.x.80] ) have a verysmooth flux function, whereas slow neutrons (in bins with a large groupnumber) are much more affected by the local material properties and have acorrespondingly rough solution if the coefficient are rough as in the case wecompute here. Consequently, we will want to use different meshes to computeeach energy group.
* This has two implications that we will have to consider: First, we need tofind a way to refine the meshes individually. Second, assembling the sourceterms for the inverse power iteration, where we have to integrate solution [2.x.81]  defined on mesh  [2.x.82]  against the shape functions defined onmesh  [2.x.83] , becomes a much more complicated task.
* 

* [1.x.68][1.x.69]
* 

* We use the usual paradigm: solve on a given mesh, then evaluate an errorindicator for each cell of each mesh we have. Because it is so convenient, weagain use the a posteriori error estimator by Kelly, Gago, Zienkiewiczand Babuska which approximates the error per cell by integrating the jump ofthe gradient of the solution along the faces of each cell. Using this, weobtain indicators[1.x.70]
* where  [2.x.84]  is the triangulation used in the solution of [2.x.85] . The question is what to do with this. For one, it is clear thatrefining only those cells with the highest error indicators might lead to badresults. To understand this, it is important to realize that  [2.x.86] scales with the second derivative of  [2.x.87] . In other words, if we have twoenergy groups  [2.x.88]  whose solutions are equally smooth but where one islarger by a factor of 10,000, for example, then only the cells of that meshwill be refined, whereas the mesh for the solution of small magnitude willremain coarse. This is probably not what one wants, since we can consider bothcomponents of the solution equally important.
* In essence, we would therefore have to scale  [2.x.89]  by an importancefactor  [2.x.90]  that says how important it is to resolve  [2.x.91]  to any givenaccuracy. Such important factors can be computed using duality techniques(see, for example, the  [2.x.92]  tutorial program, and thereference to the book by Bangerth and Rannacher cited there). We won't gothere, however, and simply assume that all energy groups are equallyimportant, and will therefore normalize the error indicators  [2.x.93]  forgroup  [2.x.94]  by the maximum of the solution  [2.x.95] . We then refine the cellswhose errors satisfy[1.x.71]
* and coarsen the cells where[1.x.72]
* We chose  [2.x.96]  and  [2.x.97]  in the code. Note that this will,of course, lead to different meshes for the different energy groups.
* The strategy above essentially means the following: If for energy group  [2.x.98] there are many cells  [2.x.99]  on which the error is large, forexample because the solution is globally very rough, then many cells will beabove the threshold. On the other hand, if there are a few cells with largeand many with small errors, for example because the solution is overall rathersmooth except at a few places, then only the few cells with large errors willbe refined. Consequently, the strategy allows for meshes that track the globalsmoothness properties of the corresponding solutions rather well.
* 

* [1.x.73][1.x.74]
* 

* As pointed out above, the multigroup refinement strategy results indifferent meshes for the different solutions  [2.x.100] . So what's the problem?In essence it goes like this: in step 3 of the eigenvalue iteration, we haveform the weak form for the equation to compute  [2.x.101]  as usual bymultiplication with test functions  [2.x.102]  defined on the mesh forenergy group  [2.x.103] ; in the process, we have tocompute the right hand side vector that contains terms of the following form:[1.x.75]
* where  [2.x.104]  is one of the coefficient functions  [2.x.105]  or [2.x.106]  used in the right hand sideof eigenvalue equation. The difficulty now is that  [2.x.107]  is defined onthe mesh for energy group  [2.x.108] , i.e. it can be expanded as [2.x.109] , with basis functions [2.x.110]  defined on mesh  [2.x.111] . The contribution to the right handside can therefore be written as[1.x.76]
* On the other hand, the test functions  [2.x.112]  are defined on mesh [2.x.113] . This means that we can't just split the integral  [2.x.114]  into integralsover the cells of either mesh  [2.x.115]  or  [2.x.116] , since the respectively other basisfunctions may not be defined on these cells.
* The solution to this problem lies in the fact that both the meshes for  [2.x.117]  and [2.x.118]  are derived by adaptive refinement from a common coarse mesh. We cantherefore always find a set of cells, which we denote by  [2.x.119] , that satisfy the following conditions: [2.x.120]  [2.x.121]  the union of the cells covers the entire domain, and [2.x.122]  a cell  [2.x.123]  is active on at least  one of the two meshes. [2.x.124] A way to construct this set is to take each cell of coarse mesh and do thefollowing steps: (i) if the cell is active on either  [2.x.125]  or [2.x.126] , then add this cell to the set; (ii) otherwise, i.e. ifthis cell has children on both meshes, then do step (i) for each of thechildren of this cell. In fact, deal.II has a function [2.x.127]  that computes exactly this setof cells that are active on at least one of two meshes.
* With this, we can write above integral as follows:[1.x.77]
*  In the code, wecompute the right hand side in the function [2.x.128] , where (among other things) weloop over the set of common most refined cells, calling the function [2.x.129]  on each pair ofthese cells.
* By construction, there are now three cases to be considered: [2.x.130]  [2.x.131]  The cell  [2.x.132]  is active on both meshes, i.e. both the basis  functions  [2.x.133]  as well as  [2.x.134]  are defined on  [2.x.135] . [2.x.136]  The cell  [2.x.137]  is active on mesh  [2.x.138] , but not  [2.x.139] , i.e. the   [2.x.140]   are defined on  [2.x.141] , whereas the  [2.x.142]  are defined  on children of  [2.x.143] . [2.x.144]  The cell  [2.x.145]  is active on mesh  [2.x.146] , but not  [2.x.147] , with opposite  conclusions than in (ii). [2.x.148] 
* To compute the right hand side above, we then need to have different code forthese three cases, as follows: [2.x.149]  [2.x.150]  If the cell  [2.x.151]  is active on both meshes, then we can directly  evaluate the integral. In fact, we don't even have to bother with the basis  functions  [2.x.152] , since all we need is the values of  [2.x.153]  at  the quadrature points. We can do this using the   [2.x.154]  function. This is done directly in  the  [2.x.155]  function.
*  [2.x.156]  If the cell  [2.x.157]  is active on mesh  [2.x.158] , but not  [2.x.159] , then the  basis functions  [2.x.160]  are only defined either on the children   [2.x.161] , or on children of these children if cell  [2.x.162]   is refined more than once on mesh  [2.x.163] .
*   Let us assume for a second that  [2.x.164]  is only once more refined on mesh  [2.x.165]   than on mesh  [2.x.166] . Using the fact that we use embedded finite element spaces  where each basis function on one mesh can be written as a linear combination  of basis functions on the next refined mesh, we can expand the restriction  of  [2.x.167]  to child cell  [2.x.168]  into the basis functions defined on that  child cell (i.e. on cells on which the basis functions  [2.x.169]  are  defined):  [1.x.78]
*   Here, and in the following, summation over indices appearing twice is  implied. The matrix  [2.x.170]  is the matrix that interpolated data from a cell  to its  [2.x.171] -th child.
*   Then we can write the contribution of cell  [2.x.172]  to the right hand side  component  [2.x.173]  as  [1.x.79]
*   In matrix notation, this can be written as  [1.x.80]
*   where  [2.x.174]  is  the weighted mass matrix on child  [2.x.175]  of cell  [2.x.176] .
*   The next question is what happens if a child  [2.x.177]  of  [2.x.178]  is not  active. Then, we have to apply the process recursively, i.e. we have to  interpolate the basis functions  [2.x.179]  onto child  [2.x.180]  of  [2.x.181] , then  onto child  [2.x.182]  of that cell, onto child  [2.x.183]  of that one, etc,  until we find an active cell. We then have to sum up all the contributions  from all the children, grandchildren, etc, of cell  [2.x.184] , with contributions  of the form  [1.x.81]
*   or  [1.x.82]
*   etc. We do this process recursively, i.e. if we sit on cell  [2.x.185]  and see that  it has children on grid  [2.x.186] , then we call a function   [2.x.187]  with an identity matrix; the function will  multiply it's argument from the left with the prolongation matrix; if the  cell has further children, it will call itself with this new matrix,  otherwise it will perform the integration.
*  [2.x.188]  The last case is where  [2.x.189]  is active on mesh  [2.x.190]  but not mesh   [2.x.191] . In that case, we have to express basis function  [2.x.192]  in  terms of the basis functions defined on the children of cell  [2.x.193] , rather  than  [2.x.194]  as before. This of course works in exactly the same  way. If the children of  [2.x.195]  are active on mesh  [2.x.196] , then  leading to the expression  [1.x.83]
*   In matrix notation, this expression now reads as  [1.x.84]
*   and correspondingly for cases where cell  [2.x.197]  is refined more than once on  mesh  [2.x.198] :  [1.x.85]
*   or  [1.x.86]
*   etc. In other words, the process works in exactly the same way as before,  except that we have to take the transpose of the prolongation matrices and  need to multiply it to the mass matrix from the other side. [2.x.199] 
* 

* The expressions for cases (ii) and (iii) can be understood as repeatedlyinterpolating either the left or right basis functions in the scalar product [2.x.200]  onto child cells, and then finallyforming the inner product (the mass matrix) on the final cell. To make thesymmetry in these cases more obvious, we can write them like this: for case(ii), we have[1.x.87]
* whereas for case (iii) we get[1.x.88]
* 
* 

* 
* [1.x.89][1.x.90]
* 

* A nuclear reactor core is composed of different types of assemblies. Anassembly is essentially the smallest unit that can be moved in and out of areactor, and is usually rectangular or square. However, assemblies are notfixed units, as they are assembled from a complex lattice of different fuelrods, control rods, and instrumentation elements that are held in placerelative to each other by spacers that are permanently attached to the rods.To make things more complicated, there are different kinds of assemblies thatare used at the same time in a reactor, where assemblies differ in the typeand arrangement of rods they are made up of.
* Obviously, the arrangement of assemblies as well as the arrangement of rodsinside them affect the distribution of neutron fluxes in the reactor (a factthat will be obvious by looking at the solution shown below in the resultssections of this program). Fuel rods, for example, differ from each other inthe enrichment of U-235 or Pu-239. Control rods, on the other hand, have zerofission, but nonzero scattering and absorption cross sections.
* This whole arrangement would make the description or spatially dependentmaterial parameters very complicated. It will not become much simpler, but wewill make one approximation: we merge the volume inhabited by each cylindricalrod and the surrounding water into volumes of quadratic cross section intoso-called `pin cells' for which homogenized material data are obtained withnuclear database and knowledge of neutron spectrum. The homogenization makesall material data piecewise constant on the solution domain for a reactor withfresh fuel. Spatially dependent material parameters are then looked up for thequadratic assembly in which a point is located, and then for the quadratic pincell within this assembly.
* In this tutorial program, we simulate a quarter of a reactor consisting of  [2.x.201]  assemblies. We use symmetry (Neumann) boundary conditions to reducethe problem to one quarter of the domain, and consequently only simulate a [2.x.202]  set of assemblies. Two of them will be UO [2.x.203]  fuel, the othertwo of them MOX fuel. Each of these assemblies consists of  [2.x.204]  rodsof different compositions. In total, we therefore create a  [2.x.205] lattice of rods. To make things simpler later on, we reflect this fact bycreating a coarse mesh of  [2.x.206]  cells (even though the domain is asquare, for which we would usually use a single cell). In deal.II, each cellhas a  [2.x.207]  which one may use to associated each cell with aparticular number identifying the material from which this cell's volume ismade of; we will use this material ID to identify which of the 8 differentkinds of rods that are used in this testcase make up a particular cell. Notethat upon mesh refinement, the children of a cell inherit the material ID,making it simple to track the material even after mesh refinement.
* The arrangement of the rods will be clearly visible in the images shown inthe results section. The cross sections for materials and for both energygroups are taken from a OECD/NEA benchmark problem. The detailed configurationand material data is given in the code.
* 

* [1.x.91][1.x.92]
* 

* As a coarse overview of what exactly the program does, here is the basiclayout: starting on a coarse mesh that is the same for each energy group, wecompute inverse eigenvalue iterations to compute the  [2.x.208] -eigenvalue on a givenset of meshes. We stop these iterations when the change in the eigenvaluedrops below a certain tolerance, and then write out the meshes and solutionsfor each energy group for inspection by a graphics program. Because the meshesfor the solutions are different, we have to generate a separate output filefor each energy group, rather than being able to add all energy groupsolutions into the same file.
* After this, we evaluate the error indicators as explained in one of the sectionsabove for each of the meshes, and refine and coarsen the cells of each meshindependently. Since the eigenvalue iterations are fairly expensive, we don'twant to start all over on the new mesh; rather, we use the SolutionTransferclass to interpolate the solution on the previous mesh to the next one uponmesh refinement. A simple experiment will convince you that this is a lotcheaper than if we omitted this step. After doing so, we resume our eigenvalueiterations on the next set of meshes.
* The program is controlled by a parameter file, using the ParameterHandlerclass. We will show aparameter file in the results section of this tutorial. For the moment sufficeit to say that it controls the polynomial degree of the finite elements used,the number of energy groups (even though all that is presently implemented arethe coefficients for a 2-group problem), the tolerance where to stop theinverse eigenvalue iteration, and the number of refinement cycles we will do.
* 

*  [1.x.93] [1.x.94]
*   [1.x.95]  [1.x.96]
* 

* 
*  We start with a bunch of include files that have already been explained in previous tutorial programs. One new one is  [2.x.209] : This is the first example program that uses the Timer class. The Timer keeps track of both the elapsed wall clock time (that is, the amount of time that a clock mounted on the wall would measure) and CPU clock time (the amount of time that the current process uses on the CPUs). We will use a Timer below to measure how much CPU time each grid refinement cycle takes.
* 

* 
* [1.x.97]
* 
*  We use the next include file to access block vectors which provide us a convenient way to manage solution and right hand side vectors of all energy groups:
* 

* 
* [1.x.98]
* 
*  This include file is for transferring solutions from one mesh to another different mesh. We use it when we are initializing solutions after each mesh iteration:
* 

* 
* [1.x.99]
* 
*  When integrating functions defined on one mesh against shape functions defined on a different mesh, we need a function  [2.x.210]  (as discussed in the introduction) which is defined in the following header file:
* 

* 
* [1.x.100]
* 
*  We use a little utility class from boost to save the state of an output stream (see the  [2.x.211]  function below):
* 

* 
* [1.x.101]
* 
*  Here are two more C++ standard headers that we use to define list data types as well as to fine-tune the output we generate:
* 

* 
* [1.x.102]
* 
*  The last step is as in all previous programs:
* 

* 
* [1.x.103]
* 
*   [1.x.104]  [1.x.105]
* 

* 
*  First up, we need to define a class that provides material data (including diffusion coefficients, removal cross sections, scattering cross sections, fission cross sections and fission spectra) to the main class.   
*   The parameter to the constructor determines for how many energy groups we set up the relevant tables. At present, this program only includes data for 2 energy groups, but a more sophisticated program may be able to initialize the data structures for more groups as well, depending on how many energy groups are selected in the parameter file.   
*   For each of the different coefficient types, there is one function that returns the value of this coefficient for a particular energy group (or combination of energy groups, as for the distribution cross section  [2.x.212]  or scattering cross section  [2.x.213] ). In addition to the energy group or groups, these coefficients depend on the type of fuel or control rod, as explained in the introduction. The functions therefore take an additional parameter,  [2.x.214]  material_id, that identifies the particular kind of rod. Within this program, we use  [2.x.215]  different kinds of rods.   
*   Except for the scattering cross section, each of the coefficients therefore can be represented as an entry in a two-dimensional array of floating point values indexed by the energy group number as well as the material ID. The Table class template is the ideal way to store such data. Finally, the scattering coefficient depends on both two energy group indices and therefore needs to be stored in a three-dimensional array, for which we again use the Table class, where this time the first template argument (denoting the dimensionality of the array) of course needs to be three:
* 

* 
* [1.x.106]
* 
*  The constructor of the class is used to initialize all the material data arrays. It takes the number of energy groups as an argument (an throws an error if that value is not equal to two, since at presently only data for two energy groups is implemented; however, using this, the function remains flexible and extendable into the future). In the member initialization part at the beginning, it also resizes the arrays to their correct sizes.   
*   At present, material data is stored for 8 different types of material. This, as well, may easily be extended in the future.
* 

* 
* [1.x.107]
* 
*  Next are the functions that return the coefficient values for given materials and energy groups. All they do is to make sure that the given arguments are within the allowed ranges, and then look the respective value up in the corresponding tables:
* 

* 
* [1.x.108]
* 
*  The function computing the fission distribution cross section is slightly different, since it computes its value as the product of two other coefficients. We don't need to check arguments here, since this already happens when we call the two other functions involved, even though it would probably not hurt either:
* 

* 
* [1.x.109]
* 
*   [1.x.110]  [1.x.111]
* 

* 
*  The first interesting class is the one that contains everything that is specific to a single energy group. To group things that belong together into individual objects, we declare a structure that holds the Triangulation and DoFHandler objects for the mesh used for a single energy group, and a number of other objects and member functions that we will discuss in the following sections.   
*   The main reason for this class is as follows: for both the forward problem (with a specified right hand side) as well as for the eigenvalue problem, one typically solves a sequence of problems for a single energy group each, rather than the fully coupled problem. This becomes understandable once one realizes that the system matrix for a single energy group is symmetric and positive definite (it is simply a diffusion operator), whereas the matrix for the fully coupled problem is generally nonsymmetric and not definite. It is also very large and quite full if more than a few energy groups are involved.   
*   Let us first look at the equation to solve in the case of an external right hand side (for the time independent case): [1.x.112]
*    
*   We would typically solve this equation by moving all the terms on the right hand side with  [2.x.216]  to the left hand side, and solving for  [2.x.217] . Of course, we don't know  [2.x.218]  yet, since the equations for those variables include right hand side terms involving  [2.x.219] . What one typically does in such situations is to iterate: compute [1.x.113]
*    
*   In other words, we solve the equation one by one, using values for  [2.x.220]  from the previous iteration  [2.x.221]  if  [2.x.222]  and already computed values for  [2.x.223]  from the present iteration if  [2.x.224] .   
*   When computing the eigenvalue, we do a very similar iteration, except that we have no external right hand side and that the solution is scaled after each iteration as explained in the introduction.   
*   In either case, these two cases can be treated jointly if all we do is to equip the following class with these abilities: (i) form the left hand side matrix, (ii) form the in-group right hand side contribution, i.e. involving the extraneous source, and (iii) form that contribution to the right hand side that stems from group  [2.x.225] . This class does exactly these tasks (as well as some book-keeping, such as mesh refinement, setting up matrices and vectors, etc). On the other hand, the class itself has no idea how many energy groups there are, and in particular how they interact, i.e. the decision of how the outer iteration looks (and consequently whether we solve an eigenvalue or a direct problem) is left to the NeutronDiffusionProblem class further down below in this program.   
*   So let us go through the class and its interface:
* 

* 
* [1.x.114]
* 
*   [1.x.115]  [1.x.116]     
*   The class has a good number of public member functions, since its the way it operates is controlled from the outside, and therefore all functions that do something significant need to be called from another class. Let's start off with book-keeping: the class obviously needs to know which energy group it represents, which material data to use, and from what coarse grid to start. The constructor takes this information and initializes the relevant member variables with that (see below).     
*   Then we also need functions that set up the linear system, i.e. correctly size the matrix and its sparsity pattern, etc, given a finite element object to use. The  [2.x.226]  function does that. Finally, for this initial block, there are two functions that return the number of active cells and degrees of freedom used in this object
* 
*  -  using this, we can make the triangulation and DoF handler member variables private, and do not have to grant external use to it, enhancing encapsulation:
* 

* 
* [1.x.117]
* 
*  Then there are functions that assemble the linear system for each iteration and the present energy group. Note that the matrix is independent of the iteration number, so only has to be computed once for each refinement cycle. The situation is a bit more involved for the right hand side that has to be updated in each inverse power iteration, and that is further complicated by the fact that computing it may involve several different meshes as explained in the introduction. To make things more flexible with regard to solving the forward or the eigenvalue problem, we split the computation of the right hand side into a function that assembles the extraneous source and in-group contributions (which we will call with a zero function as source terms for the eigenvalue problem) and one that computes contributions to the right hand side from another energy group:
* 

* 
* [1.x.118]
* 
*  Next we need a set of functions that actually compute the solution of a linear system, and do something with it (such as computing the fission source contribution mentioned in the introduction, writing graphical information to an output file, computing error indicators, or actually refining the grid based on these criteria and thresholds for refinement and coarsening). All these functions will later be called from the driver class  [2.x.227] , or any other class you may want to implement to solve a problem involving the neutron flux equations:
* 

* 
* [1.x.119]
* 
*   [1.x.120]  [1.x.121]     
*   As is good practice in object oriented programming, we hide most data members by making them private. However, we have to grant the class that drives the process access to the solution vector as well as the solution of the previous iteration, since in the power iteration, the solution vector is scaled in every iteration by the present guess of the eigenvalue we are looking for:
* 

* 
* [1.x.122]
* 
*   [1.x.123]  [1.x.124]     
*   The rest of the data members are private. Compared to all the previous tutorial programs, the only new data members are an integer storing which energy group this object represents, and a reference to the material data object that this object's constructor gets passed from the driver class. Likewise, the constructor gets a reference to the finite element object we are to use.     
*   Finally, we have to apply boundary values to the linear system in each iteration, i.e. quite frequently. Rather than interpolating them every time, we interpolate them once on each new mesh and then store them along with all the other data of this class:
* 

* 
* [1.x.125]
* 
*   [1.x.126]  [1.x.127]     
*   There is one private member function in this class. It recursively walks over cells of two meshes to compute the cross-group right hand side terms. The algorithm for this is explained in the introduction to this program. The arguments to this function are a reference to an object representing the energy group against which we want to integrate a right hand side term, an iterator to a cell of the mesh used for the present energy group, an iterator to a corresponding cell on the other mesh, and the matrix that interpolates the degrees of freedom from the coarser of the two cells to the finer one:
* 

* 
* [1.x.128]
* 
*   [1.x.129]  [1.x.130]
* 

* 
*  The first few functions of this class are mostly self-explanatory. The constructor only sets a few data members and creates a copy of the given triangulation as the base for the triangulation used for this energy group. The next two functions simply return data from private data members, thereby enabling us to make these data members private.
* 

* 
* [1.x.131]
* 
*   [1.x.132]  [1.x.133]   
*   The first "real" function is the one that sets up the mesh, matrices, etc, on the new mesh or after mesh refinement. We use this function to initialize sparse system matrices, and the right hand side vector. If the solution vector has never been set before (as indicated by a zero size), we also initialize it and set it to a default value. We don't do that if it already has a non-zero size (i.e. this function is called after mesh refinement) since in that case we want to preserve the solution across mesh refinement (something we do in the  [2.x.228]  function).
* 

* 
* [1.x.134]
* 
*  At the end of this function, we update the list of boundary nodes and their values, by first clearing this list and the re-interpolating boundary values (remember that this function is called after first setting up the mesh, and each time after mesh refinement).     
*   To understand the code, it is necessary to realize that we create the mesh using the  [2.x.229]  function (in  [2.x.230] ) where we set the last parameter to  [2.x.231] . This means that boundaries of the domain are "colored", i.e. the four (or six, in 3d) sides of the domain are assigned different boundary indicators. As it turns out, the bottom boundary gets indicator zero, the top one boundary indicator one, and left and right boundaries get indicators two and three, respectively.     
*   In this program, we simulate only one, namely the top right, quarter of a reactor. That is, we want to interpolate boundary conditions only on the top and right boundaries, while do nothing on the bottom and left boundaries (i.e. impose natural, no-flux Neumann boundary conditions). This is most easily generalized to arbitrary dimension by saying that we want to interpolate on those boundaries with indicators 1, 3, ..., which we do in the following loop (note that calls to  [2.x.232]  are additive, i.e. they do not first clear the boundary value map):
* 

* 
* [1.x.135]
* 
*   [1.x.136]  [1.x.137]   
*   Next we need functions assembling the system matrix and right hand sides. Assembling the matrix is straightforward given the equations outlined in the introduction as well as what we've seen in previous example programs. Note the use of  [2.x.233]  to get at the kind of material from which a cell is made up of. Note also how we set the order of the quadrature formula so that it is always appropriate for the finite element in use.   
*   Finally, note that since we only assemble the system matrix here, we can't yet eliminate boundary values (we need the right hand side vector for this). We defer this to the  [2.x.234]  function, at which point all the information is available.
* 

* 
* [1.x.138]
* 
*   [1.x.139]  [1.x.140]   
*   As explained in the documentation of the  [2.x.235]  class, we split assembling the right hand side into two parts: the ingroup and the cross-group couplings. First, we need a function to assemble the right hand side of one specific group here, i.e. including an extraneous source (that we will set to zero for the eigenvalue problem) as well as the ingroup fission contributions.  (In-group scattering has already been accounted for with the definition of removal cross section.) The function's workings are pretty standard as far as assembling right hand sides go, and therefore does not require more comments except that we mention that the right hand side vector is set to zero at the beginning of the function
* 
*  -  something we are not going to do for the cross-group terms that simply add to the right hand side vector.
* 

* 
* [1.x.141]
* 
*   [1.x.142]  [1.x.143]   
*   The more interesting function for assembling the right hand side vector for the equation of a single energy group is the one that couples energy group  [2.x.236]  and  [2.x.237] . As explained in the introduction, we first have to find the set of cells common to the meshes of the two energy groups. First we call  [2.x.238]  to obtain this list of pairs of common cells from both meshes. Both cells in a pair may not be active but at least one of them is. We then hand each of these cell pairs off to a function that computes the right hand side terms recursively.   
*   Note that ingroup coupling is handled already before, so we exit the function early if  [2.x.239] .
* 

* 
* [1.x.144]
* 
*   [1.x.145]  [1.x.146]   
*   This is finally the function that handles assembling right hand side terms on potentially different meshes recursively, using the algorithm described in the introduction. The function takes a reference to the object representing energy group  [2.x.240] , as well as iterators to corresponding cells in the meshes for energy groups  [2.x.241]  and  [2.x.242] . At first, i.e. when this function is called from the one above, these two cells will be matching cells on two meshes; however, one of the two may be further refined, and we will call the function recursively with one of the two iterators replaced by one of the children of the original cell.   
*   The last argument is the matrix product matrix  [2.x.243]  from the introduction that interpolates from the coarser of the two cells to the finer one. If the two cells match, then this is the identity matrix
* 
*  -  exactly what we pass to this function initially.   
*   The function has to consider two cases: that both of the two cells are not further refined, i.e. have no children, in which case we can finally assemble the right hand side contributions of this pair of cells; and that one of the two cells is further refined, in which case we have to keep recursing by looping over the children of the one cell that is not active. These two cases will be discussed below:
* 

* 
* [1.x.147]
* 
*  The first case is that both cells are no further refined. In that case, we can assemble the relevant terms (see the introduction). This involves assembling the mass matrix on the finer of the two cells (in fact there are two mass matrices with different coefficients, one for the fission distribution cross section  [2.x.244]  and one for the scattering cross section  [2.x.245] ). This is straight forward, but note how we determine which of the two cells is the finer one by looking at the refinement level of the two cells:
* 

* 
* [1.x.148]
* 
*  Now we have all the interpolation (prolongation) matrices as well as local mass matrices, so we only have to form the product [1.x.149] or [1.x.150] depending on which of the two cells is the finer. We do this using either the matrix-vector product provided by the  [2.x.246]  function, or the product with the transpose matrix using  [2.x.247] . After doing so, we transfer the result into the global right hand side vector of energy group  [2.x.248] .
* 

* 
* [1.x.151]
* 
*  The alternative is that one of the two cells is further refined. In that case, we have to loop over all the children, multiply the existing interpolation (prolongation) product of matrices from the left with the interpolation from the present cell to its child (using the matrix-matrix multiplication function  [2.x.249] ), and then hand the result off to this very same function again, but with the cell that has children replaced by one of its children:
* 

* 
* [1.x.152]
* 
*   [1.x.153]  [1.x.154]   
*   In the (inverse) power iteration, we use the integrated fission source to update the  [2.x.250] -eigenvalue. Given its definition, the following function is essentially self-explanatory:
* 

* 
* [1.x.155]
* 
*   [1.x.156]  [1.x.157]   
*   Next a function that solves the linear system assembled before. Things are pretty much standard, except that we delayed applying boundary values until we get here, since in all the previous functions we were still adding up contributions the right hand side vector.
* 

* 
* [1.x.158]
* 
*   [1.x.159]  [1.x.160]   
*   Mesh refinement is split into two functions. The first estimates the error for each cell, normalizes it by the magnitude of the solution, and returns it in the vector given as an argument. The calling function collects all error indicators from all energy groups, and computes thresholds for refining and coarsening cells.
* 

* 
* [1.x.161]
* 
*   [1.x.162]  [1.x.163]   
*   The second part is to refine the grid given the error indicators compute in the previous function and error thresholds above which cells shall be refined or below which cells shall be coarsened. Note that we do not use any of the functions in  [2.x.251]  here, but rather set refinement flags ourselves.   
*   After setting these flags, we use the SolutionTransfer class to move the solution vector from the old to the new mesh. The procedure used here is described in detail in the documentation of that class:
* 

* 
* [1.x.164]
* 
*  enforce constraints to make the interpolated solution conforming on the new mesh:
* 

* 
* [1.x.165]
* 
*   [1.x.166]  [1.x.167]   
*   The last function of this class outputs meshes and solutions after each mesh iteration. This has been shown many times before. The only thing worth pointing out is the use of the  [2.x.252]  function to convert an integer into its string representation. The second argument of that function denotes how many digits we shall use
* 
*  -  if this value was larger than one, then the number would be padded by leading zeros.
* 

* 
* [1.x.168]
* 
*   [1.x.169]  [1.x.170]
* 

* 
*  This is the main class of the program, not because it implements all the functionality (in fact, most of it is implemented in the  [2.x.253]  class) but because it contains the driving algorithm that determines what to compute and when. It is mostly as shown in many of the other tutorial programs in that it has a public  [2.x.254]  function and private functions doing all the rest. In several places, we have to do something for all energy groups, in which case we will start tasks for each group to let these things run in parallel if deal.II was configured for multithreading.  For strategies of parallelization, take a look at the  [2.x.255]  module.   
*   The biggest difference to previous example programs is that we also declare a nested class that has member variables for all the run-time parameters that can be passed to the program in an input file. Right now, these are the number of energy groups, the number of refinement cycles, the polynomial degree of the finite element to be used, and the tolerance used to determine when convergence of the inverse power iteration has occurred. In addition, we have a constructor of this class that sets all these values to their default values, a function  [2.x.256]  that describes to the ParameterHandler class what parameters are accepted in the input file, and a function  [2.x.257]  that can extract the values of these parameters from a ParameterHandler object. See also  [2.x.258]  for another example of using ParameterHandler.
* 

* 
* [1.x.171]
* 
*   [1.x.172]  [1.x.173]
* 

* 
*  There are not that many member functions in this class since most of the functionality has been moved into the  [2.x.259]  class and is simply called from the  [2.x.260]  member function of this class. The ones that remain have self-explanatory names:
* 

* 
* [1.x.174]
* 
*   [1.x.175]  [1.x.176]
* 

* 
*  Next, we have a few member variables. In particular, these are (i) a reference to the parameter object (owned by the main function of this program, and passed to the constructor of this class), (ii) an object describing the material parameters for the number of energy groups requested in the input file, and (iii) the finite element to be used by all energy groups:
* 

* 
* [1.x.177]
* 
*  Furthermore, we have (iv) the value of the computed eigenvalue at the present iteration. This is, in fact, the only part of the solution that is shared between all energy groups
* 
*  -  all other parts of the solution, such as neutron fluxes are particular to one or the other energy group, and are therefore stored in objects that describe a single energy group:
* 

* 
* [1.x.178]
* 
*  The last computational object (v) is an array of pointers to the energy group objects. The length of this array is, of course, equal to the number of energy groups specified in the parameter file.
* 

* 
* [1.x.179]
* 
*  Finally (vi) we have a file stream to which we will save summarized output.
* 

* 
* [1.x.180]
* 
*   [1.x.181]  [1.x.182]
* 

* 
*  Before going on to the implementation of the outer class, we have to implement the functions of the parameters structure. This is pretty straightforward and, in fact, looks pretty much the same for all such parameters classes using the ParameterHandler capabilities. We will therefore not comment further on this:
* 

* 
* [1.x.183]
* 
*   [1.x.184]  [1.x.185]
* 

* 
*  Now for the  [2.x.261]  class. The constructor and destructor have nothing of much interest:
* 

* 
* [1.x.186]
* 
*   [1.x.187]  [1.x.188]   
*   The first function of interest is the one that sets up the geometry of the reactor core. This is described in more detail in the introduction.   
*   The first part of the function defines geometry data, and then creates a coarse mesh that has as many cells as there are fuel rods (or pin cells, for that matter) in that part of the reactor core that we simulate. As mentioned when interpolating boundary values above, the last parameter to the  [2.x.262]  function specifies that sides of the domain shall have unique boundary indicators that will later allow us to determine in a simple way which of the boundaries have Neumann and which have Dirichlet conditions attached to them.
* 

* 
* [1.x.189]
* 
*  The second part of the function deals with material numbers of pin cells of each type of assembly. Here, we define four different types of assembly, for which we describe the arrangement of fuel rods in the following tables.     
*   The assemblies described here are taken from the benchmark mentioned in the introduction and are (in this order):  [2.x.263]   [2.x.264] 'UX' Assembly: UO2 fuel assembly with 24 guide tubes and a central Moveable Fission Chamber  [2.x.265] 'UA' Assembly: UO2 fuel assembly with 24 AIC and a central Moveable Fission Chamber  [2.x.266] 'PX' Assembly: MOX fuel assembly with 24 guide tubes and a central Moveable Fission Chamber  [2.x.267] 'R' Assembly: a reflector.   [2.x.268]      
*   Note that the numbers listed here and taken from the benchmark description are, in good old Fortran fashion, one-based. We will later subtract one from each number when assigning materials to individual cells to convert things into the C-style zero-based indexing.
* 

* 
* [1.x.190]
* 
*  After the description of the materials that make up an assembly, we have to specify the arrangement of assemblies within the core. We use a symmetric pattern that in fact only uses the 'UX' and 'PX' assemblies:
* 

* 
* [1.x.191]
* 
*  We are now in a position to actually set material IDs for each cell. To this end, we loop over all cells, look at the location of the cell's center, and determine which assembly and fuel rod this would be in. (We add a few checks to see that the locations we compute are within the bounds of the arrays in which we have to look up materials.) At the end of the loop, we set material identifiers accordingly:
* 

* 
* [1.x.192]
* 
*  With the coarse mesh so initialized, we create the appropriate number of energy group objects and let them initialize their individual meshes with the coarse mesh generated above:
* 

* 
* [1.x.193]
* 
*   [1.x.194]  [1.x.195]   
*   In the eigenvalue computation, we need to calculate total fission neutron source after each power iteration. The total power then is used to renew k-effective.   
*   Since the total fission source is a sum over all the energy groups, and since each of these sums can be computed independently, we actually do this in parallel. One of the problems is that the function in the  [2.x.269]  class that computes the fission source returns a value. We would like to add these values together in the loop itself: ideally, each task would compute its value and then immediately add it to the total. Concurrently summing values in this way requires two features:  [2.x.270]   [2.x.271] We need a way of storing a value such that multiple threads can read and write into concurrently in a way that prevents data races (i.e., thread-safe reading and writing). [2.x.272]   [2.x.273] We need a way to increment such a value that is also thread-safe. [2.x.274]   [2.x.275]    
*   The first feature is available through the template class  [2.x.276] . However, the second feature, implemented by  [2.x.277] , is only available in C++20 and later: since deal.II supports older versions of the C++ language standard we cannot use this feature yet. Hence, instead, we simply write each group's value out to an entry in a vector and sum the values at the end of the function.
* 

* 
* [1.x.196]
* 
*   [1.x.197]  [1.x.198]   
*   The next function lets the individual energy group objects refine their meshes. Much of this, again, is a task that can be done independently in parallel: first, let all the energy group objects calculate their error indicators in parallel, then compute the maximum error indicator over all energy groups and determine thresholds for refinement and coarsening of cells, and then ask all the energy groups to refine their meshes accordingly, again in parallel.
* 

* 
* [1.x.199]
* 
*  The destructor of  [2.x.278]  joins all threads so we know that the computation is done by the time we exit the scope.
* 

* 
*  

* 
* [1.x.200]
* 
*   [1.x.201]  [1.x.202]   
*   Finally, this is the function where the meat is: iterate on a sequence of meshes, and on each of them do a power iteration to compute the eigenvalue.   
*   Given the description of the algorithm in the introduction, there is actually not much to comment on:
* 

* 
* [1.x.203]
* 
*  We would like to change the output precision for just this function and restore the state of  [2.x.279]  when this function returns. Hence, we need a way to undo the output format change. Boost provides a convenient way to save the state of an output stream and restore it at the end of the current block (when the destructor of  [2.x.280]  is called) with the  [2.x.281]  class, which we use here.
* 

* 
* [1.x.204]
* 
*  We calculate the error below by the change in k_eff (i.e., the difference between k_eff_old,
* 

* 
* [1.x.205]
* 
*  We will measure the CPU time that each cycle takes below. The constructor for Timer calls  [2.x.282]  so once we create a timer we can query it for information. Since many parts of this loop are parallelized with tasks, the CPU time we measure (if we run with more than one thread) will be larger than the wall time.
* 

* 
* [1.x.206]
* 
*  Print out information about the simulation as well as the elapsed CPU time. We can call  [2.x.283]  without first calling  [2.x.284]  to get the elapsed CPU time at the point of calling the function.
* 

* 
* [1.x.207]
* 
*   [1.x.208]  [1.x.209]
* 

* 
*  The last thing in the program in the  [2.x.285]  function. The structure is as in most other tutorial programs, with the only exception that we here handle a parameter file.  To this end, we first look at the command line arguments passed to this function: if no input file is specified on the command line, then use "project.prm", otherwise take the filename given as the first argument on the command line.
* 

* 
*  With this, we create a ParameterHandler object, let the  [2.x.286]  class declare all the parameters it wants to see in the input file (or, take the default values, if nothing is listed in the parameter file), then read the input file, ask the parameters object to extract the values, and finally hand everything off to an object of type  [2.x.287]  for computation of the eigenvalue:
* 

* 
* [1.x.210]
* [1.x.211][1.x.212]
* 

* We can run the program with the following input file:
* [1.x.213]
* The output of this program then consists of the console output, a filenamed `convergence_table' to record main results of mesh iteration,and the graphical output in vtu format.
* The console output looks like this:
* [1.x.214]
* 
* We see that power iteration does converge faster after cycle 0 due to the initializationwith solution from last mesh iteration.The contents of `convergence_table' are,
* [1.x.215]
* The meanings of columns are: number of mesh iteration, numbers of degrees of freedom of fast energy group, numbers of DoFs of thermal group, convergedk-effective and the ratio between maximum of fast flux and maximum of thermal one.
* The grids of fast and thermal energy groups at mesh iteration #9 lookas follows:
*  [2.x.288] &nbsp; [2.x.289] 
* We see that the grid of thermal group is much finer than the one of fast group.The solutions on these grids are, (Note: flux are normalized with total fissionsource equal to 1)
*  [2.x.290] &nbsp; [2.x.291] 
* Then we plot the convergence data with polynomial order being equal to 1,2 and 3.
*  [2.x.292] 
* The estimated `exact' k-effective = 0.906834721253 which is simply from lastmesh iteration of polynomial order 3 minus 2e-10. We see that h-adaptive calculationsdeliver an algebraic convergence. And the higher polynomial order is, the faster meshiteration converges. In our problem, we need smaller number of DoFs to achieve sameaccuracy with higher polynomial order.
* 

* [1.x.216][1.x.217] [2.x.293] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-29_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22]
*  [2.x.2] 
* [1.x.23]
* [1.x.24] In order to run this program, deal.II must be configured to usethe UMFPACK sparse direct solver. Refer to the [1.x.25] for instructions how to do this.
* 

* [1.x.26][1.x.27][1.x.28]
* 

* 
* A question that comes up frequently is how to solve problems involving complexvalued functions with deal.II. For many problems, instead of working withcomplex valued finite elements directly, it is often more convenient to split complex valuedfunctions into their real and imaginary parts and use separate scalar finiteelement fields for discretizing each one of them. Basically this amounts toviewing a single complex valued equation as a system of two real valuedequations. This short example demonstrates how this can be implemented indeal.II by using an  [2.x.3]  object to stack two finite elementfields representing real and imaginary parts. (The opposite approach,keeping everything complex-valued, is demonstrated in a differenttutorial program: see  [2.x.4]  for this.)When split into real and imaginary parts, the equations covered herefall into the class of vector-valued problems. A toplevel overview ofthis topic can be found in the  [2.x.5]  module.
* In addition to this discussion, we also discuss the ParameterHandlerclass, which provides a convenient way for reading parameters from aconfiguration file at runtime without the need to recompile theprogram code.
* 

* [1.x.29][1.x.30]
* 

* The original purpose of this program is to simulate the focusing propertiesof an ultrasound wave generated by a transducer lens with variablegeometry. Recent applications in medical imaging use ultrasound waves not onlyfor imaging purposes, but also to excite certain local effects in amaterial, like changes in optical properties, that can then be measured byother imaging techniques. A vital ingredient for these methods is the abilityto focus the intensity of the ultrasound wave in a particular part of thematerial, ideally in a point, to be able to examine the properties of thematerial at that particular location.
* To derive a model for this problem, we think of ultrasound as a pressure wavegoverned by the wave equation:[1.x.31]where  [2.x.6]  is the wave speed (that for simplicity we assume to be constant),  [2.x.7] . The boundary [2.x.8]  is divided into two parts  [2.x.9]  and [2.x.10] , with  [2.x.11]  representing thetransducer lens and  [2.x.12]  an absorbing boundary (that is, we want tochoose boundary conditions on  [2.x.13]  in such a way that they imitate alarger domain). On  [2.x.14] , the transducer generates a wave of constantfrequency  [2.x.15]  and constant amplitude (that we chose to be 1 here):[1.x.32]
* If there are no other (interior or boundary) sources, and since the onlysource has frequency  [2.x.16] , then the solution admits a separation ofvariables of the form  [2.x.17] . The complex-valued function  [2.x.18]  describes the spatialdependency of amplitude and phase (relative to the source) of the waves offrequency  [2.x.19] , with the amplitude being the quantity that we areinterested in. By plugging this form of the solution into the wave equation,we see that for  [2.x.20]  we have[1.x.33]
* 
* For finding suitable conditions on  [2.x.21]  that model an absorbingboundary, consider a wave of the form  [2.x.22]  withfrequency  [2.x.23]  traveling in direction  [2.x.24] . In orderfor  [2.x.25]  to solve the wave equation,  [2.x.26]  musthold. Suppose that this wave hits the boundary in  [2.x.27]  at a rightangle, i.e.  [2.x.28]  with  [2.x.29]  denoting the outer unit normal of [2.x.30]  in  [2.x.31] . Then at  [2.x.32] , this wave satisfies the equation[1.x.34]Hence, by enforcing the boundary condition[1.x.35]waves that hit the boundary  [2.x.33]  at a right angle will be perfectlyabsorbed. On the other hand, those parts of the wave field that do not hit aboundary at a right angle do not satisfy this condition and enforcing it as aboundary condition will yield partial reflections, i.e. only parts of the wavewill pass through the boundary as if it wasn't here whereas the remainingfraction of the wave will be reflected back into the domain.
* If we are willing to accept this as a sufficient approximation to an absorbingboundary we finally arrive at the following problem for  [2.x.34] :[1.x.36]
* This is a Helmholtz equation (similar to the one in  [2.x.35] , but this time with''the bad sign'') with Dirichlet data on  [2.x.36]  and mixed boundaryconditions on  [2.x.37] . Because of the condition on  [2.x.38] , we cannot justtreat the equations for real and imaginary parts of  [2.x.39]  separately. What we cando however is to view the PDE for  [2.x.40]  as a system of two PDEs for the real andimaginary parts of  [2.x.41] , with the boundary condition on  [2.x.42]  representingthe coupling terms between the two components of the system. This works alongthe following lines: Let  [2.x.43] , then in termsof  [2.x.44]  and  [2.x.45]  we have the following system:[1.x.37]
* 
* For test functions  [2.x.46]  with  [2.x.47] , afterthe usual multiplication, integration over  [2.x.48]  and applying integration byparts, we get the weak formulation[1.x.38]
* 
* We choose finite element spaces  [2.x.49]  and  [2.x.50]  with bases  [2.x.51]  and look for approximate solutions[1.x.39]Plugging into the variational form yields the equation system[1.x.40]In matrix notation:[1.x.41](One should not be fooled by the right hand side being zero here, that isbecause we haven't included the Dirichlet boundary data yet.)Because of the alternating sign in the off-diagonal blocks, we can alreadysee that this system is non-symmetric, in fact it is even indefinite.Of course, there is no necessity to choose the spaces  [2.x.52]  and  [2.x.53]  to bethe same. However, we expect real and imaginary part of the solution tohave similar properties and will therefore indeed take  [2.x.54]  in theimplementation, and also use the same basis functions  [2.x.55]  forboth spaces. The reason for the notation using different symbols is just thatit allows us to distinguish between shape functions for  [2.x.56]  and  [2.x.57] , as thisdistinction plays an important role in the implementation.
* 

* [1.x.42][1.x.43]
* 

* For the computations, we will consider wave propagation in the unit square,with ultrasound generated by a transducer lens that is shaped like a segmentof the circle with center at  [2.x.58]  and aradius slightly greater than  [2.x.59] ; this shape should lead to a focusing of the soundwave at the center of the circle. Varying  [2.x.60]  changes the "focus" of the lensand affects the spatial distribution of the intensity of  [2.x.61] , where our mainconcern is how well  [2.x.62]  is focused.
* In the program below, we will implement the complex-valued Helmholtz equationsusing the formulation with split real and imaginary parts. We will alsodiscuss how to generate a domain that looks like a square with a slight bulgesimulating the transducer (in the [2.x.63]  function), and how togenerate graphical output that not only contains the solution components  [2.x.64]  and [2.x.65] , but also the magnitude  [2.x.66]  directly in the output file (in [2.x.67] ). Finally, we use theParameterHandler class to easily read parameters like the focal distance  [2.x.68] ,wave speed  [2.x.69] , frequency  [2.x.70] , and a number of other parameters from aninput file at run-time, rather than fixing those parameters in the source codewhere we would have to re-compile every time we want to change parameters.
* 

*  [1.x.44] [1.x.45]
*   [1.x.46]  [1.x.47]
* 

* 
*  The following header files have all been discussed before:
* 

* 
*  

* 
* [1.x.48]
* 
*  This header file contains the necessary declarations for the ParameterHandler class that we will use to read our parameters from a configuration file:
* 

* 
* [1.x.49]
* 
*  For solving the linear system, we'll use the sparse LU-decomposition provided by UMFPACK (see the SparseDirectUMFPACK class), for which the following header file is needed.  Note that in order to compile this tutorial program, the deal.II-library needs to be built with UMFPACK support, which is enabled by default:
* 

* 
* [1.x.50]
* 
*  The FESystem class allows us to stack several FE-objects to one compound, vector-valued finite element field. The necessary declarations for this class are provided in this header file:
* 

* 
* [1.x.51]
* 
*  Finally, include the header file that declares the Timer class that we will use to determine how much time each of the operations of our program takes:
* 

* 
* [1.x.52]
* 
*  As the last step at the beginning of this program, we put everything that is in this program into its namespace and, within it, make everything that is in the deal.II namespace globally available, without the need to prefix everything with  [2.x.71] :
* 

* 
* [1.x.53]
* 
*   [1.x.54]  [1.x.55]
* 

* 
*  First we define a class for the function representing the Dirichlet boundary values. This has been done many times before and therefore does not need much explanation.   
*   Since there are two values  [2.x.72]  and  [2.x.73]  that need to be prescribed at the boundary, we have to tell the base class that this is a vector-valued function with two components, and the  [2.x.74]  function and its cousin  [2.x.75]  must return vectors with two entries. In our case the function is very simple, it just returns 1 for the real part  [2.x.76]  and 0 for the imaginary part  [2.x.77]  regardless of the point where it is evaluated.
* 

* 
* [1.x.56]
* 
*   [1.x.57]  [1.x.58]
* 

* 
*  The next class is responsible for preparing the ParameterHandler object and reading parameters from an input file.  It includes a function  [2.x.78]  that declares all the necessary parameters and a  [2.x.79]  function that is called from outside to initiate the parameter reading process.
* 

* 
* [1.x.59]
* 
*  The constructor stores a reference to the ParameterHandler object that is passed to it:
* 

* 
* [1.x.60]
* 
*   [1.x.61]  [1.x.62]
* 

* 
*  The  [2.x.80]  function declares all the parameters that our ParameterHandler object will be able to read from input files, along with their types, range conditions and the subsections they appear in. We will wrap all the entries that go into a section in a pair of braces to force the editor to indent them by one level, making it simpler to read which entries together form a section:
* 

* 
* [1.x.63]
* 
*  Parameters for mesh and geometry include the number of global refinement steps that are applied to the initial coarse mesh and the focal distance  [2.x.81]  of the transducer lens. For the number of refinement steps, we allow integer values in the range  [2.x.82] , where the omitted second argument to the  [2.x.83]  object denotes the half-open interval.  For the focal distance any number greater than zero is accepted:
* 

* 
* [1.x.64]
* 
*  The next subsection is devoted to the physical parameters appearing in the equation, which are the frequency  [2.x.84]  and wave speed  [2.x.85] . Again, both need to lie in the half-open interval  [2.x.86]  represented by calling the  [2.x.87]  class with only the left end-point as argument:
* 

* 
* [1.x.65]
* 
*  Last but not least we would like to be able to change some properties of the output, like filename and format, through entries in the configuration file, which is the purpose of the last subsection:
* 

* 
* [1.x.66]
* 
*  Since different output formats may require different parameters for generating output (like for example, postscript output needs viewpoint angles, line widths, colors etc), it would be cumbersome if we had to declare all these parameters by hand for every possible output format supported in the library. Instead, each output format has a  [2.x.88]  function, which declares all the parameters specific to that format in an own subsection. The following call of  [2.x.89]  executes  [2.x.90]  for all available output formats, so that for each format an own subsection will be created with parameters declared for that particular output format. (The actual value of the template parameter in the call,  [2.x.91]  above, does not matter here: the function does the same work independent of the dimension, but happens to be in a template-parameter-dependent class.)  To find out what parameters there are for which output format, you can either consult the documentation of the DataOutBase class, or simply run this program without a parameter file present. It will then create a file with all declared parameters set to their default values, which can conveniently serve as a starting point for setting the parameters to the values you desire.
* 

* 
* [1.x.67]
* 
*   [1.x.68]  [1.x.69]
* 

* 
*  This is the main function in the ParameterReader class.  It gets called from outside, first declares all the parameters, and then reads them from the input file whose filename is provided by the caller. After the call to this function is complete, the  [2.x.92]  object can be used to retrieve the values of the parameters read in from the file:
* 

* 
* [1.x.70]
* 
*   [1.x.71]  [1.x.72]
* 

* 
*  As mentioned in the introduction, the quantity that we are really after is the spatial distribution of the intensity of the ultrasound wave, which corresponds to  [2.x.93] . Now we could just be content with having  [2.x.94]  and  [2.x.95]  in our output, and use a suitable visualization or postprocessing tool to derive  [2.x.96]  from the solution we computed. However, there is also a way to output data derived from the solution in deal.II, and we are going to make use of this mechanism here.
* 

* 
*  So far we have always used the  [2.x.97]  function to add vectors containing output data to a DataOut object.  There is a special version of this function that in addition to the data vector has an additional argument of type DataPostprocessor. What happens when this function is used for output is that at each point where output data is to be generated, the  [2.x.98]  or  [2.x.99]  function of the specified DataPostprocessor object is invoked to compute the output quantities from the values, the gradients and the second derivatives of the finite element function represented by the data vector (in the case of face related data, normal vectors are available as well). Hence, this allows us to output any quantity that can locally be derived from the values of the solution and its derivatives.  Of course, the ultrasound intensity  [2.x.100]  is such a quantity and its computation doesn't even involve any derivatives of  [2.x.101]  or  [2.x.102] .
* 

* 
*  In practice, the DataPostprocessor class only provides an interface to this functionality, and we need to derive our own class from it in order to implement the functions specified by the interface. In the most general case one has to implement several member functions but if the output quantity is a single scalar then some of this boilerplate code can be handled by a more specialized class, DataPostprocessorScalar and we can derive from that one instead. This is what the  [2.x.103]  class does:
* 

* 
* [1.x.73]
* 
*  In the constructor, we need to call the constructor of the base class with two arguments. The first denotes the name by which the single scalar quantity computed by this class should be represented in output files. In our case, the postprocessor has  [2.x.104]  as output, so we use "Intensity".   
*   The second argument is a set of flags that indicate which data is needed by the postprocessor in order to compute the output quantities.  This can be any subset of update_values, update_gradients and update_hessians (and, in the case of face data, also update_normal_vectors), which are documented in UpdateFlags.  Of course, computation of the derivatives requires additional resources, so only the flags for data that are really needed should be given here, just as we do when we use FEValues objects. In our case, only the function values of  [2.x.105]  and  [2.x.106]  are needed to compute  [2.x.107] , so we're good with the update_values flag.
* 

* 
* [1.x.74]
* 
*  The actual postprocessing happens in the following function. Its input is an object that stores values of the function (which is here vector-valued) representing the data vector given to  [2.x.108]  evaluated at all evaluation points where we generate output, and some tensor objects representing derivatives (that we don't use here since  [2.x.109]  is computed from just  [2.x.110]  and  [2.x.111] ). The derived quantities are returned in the  [2.x.112]  vector. Remember that this function may only use data for which the respective update flag is specified by  [2.x.113] . For example, we may not use the derivatives here, since our implementation of  [2.x.114]  requests that only function values are provided.
* 

* 
* [1.x.75]
* 
*  The computation itself is straightforward: We iterate over each entry in the output vector and compute  [2.x.115]  from the corresponding values of  [2.x.116]  and  [2.x.117] . We do this by creating a complex number  [2.x.118]  and then calling  [2.x.119]  on the result. (One may be tempted to call  [2.x.120]  but in a historical quirk, the C++ committee decided that  [2.x.121]  should return the [1.x.76] of the absolute value
* 
*  -  thereby not satisfying the properties mathematicians require of something called a "norm".)
* 

* 
* [1.x.77]
* 
*   [1.x.78]  [1.x.79]
* 

* 
*  Finally here is the main class of this program.  It's member functions are very similar to the previous examples, in particular  [2.x.122] , and the list of member variables does not contain any major surprises either. The ParameterHandler object that is passed to the constructor is stored as a reference to allow easy access to the parameters from all functions of the class.  Since we are working with vector valued finite elements, the FE object we are using is of type FESystem.
* 

* 
* [1.x.80]
* 
*  The constructor takes the ParameterHandler object and stores it in a reference. It also initializes the DoF-Handler and the finite element system, which consists of two copies of the scalar Q1 field, one for  [2.x.123]  and one for  [2.x.124] :
* 

* 
* [1.x.81]
* 
*   [1.x.82]  [1.x.83]
* 

* 
*  Here we setup the grid for our domain.  As mentioned in the exposition, the geometry is just a unit square (in 2d) with the part of the boundary that represents the transducer lens replaced by a sector of a circle.
* 

* 
* [1.x.84]
* 
*  First we generate some logging output and start a timer so we can compute execution time when this function is done:
* 

* 
* [1.x.85]
* 
*  Then we query the values for the focal distance of the transducer lens and the number of mesh refinement steps from our ParameterHandler object:
* 

* 
* [1.x.86]
* 
*  Next, two points are defined for position and focal point of the transducer lens, which is the center of the circle whose segment will form the transducer part of the boundary. Notice that this is the only point in the program where things are slightly different in 2D and 3D. Even though this tutorial only deals with the 2D case, the necessary additions to make this program functional in 3D are so minimal that we opt for including them:
* 

* 
* [1.x.87]
* 
*  As initial coarse grid we take a simple unit square with 5 subdivisions in each direction. The number of subdivisions is chosen so that the line segment  [2.x.125]  that we want to designate as the transducer boundary is spanned by a single face. Then we step through all cells to find the faces where the transducer is to be located, which in fact is just the single edge from 0.4 to 0.6 on the x-axis. This is where we want the refinements to be made according to a circle shaped boundary, so we mark this edge with a different manifold indicator. Since we will Dirichlet boundary conditions on the transducer, we also change its boundary indicator.
* 

* 
* [1.x.88]
* 
*  For the circle part of the transducer lens, a SphericalManifold object is used (which, of course, in 2D just represents a circle), with center computed as above.
* 

* 
* [1.x.89]
* 
*  Now global refinement is executed. Cells near the transducer location will be automatically refined according to the circle shaped boundary of the transducer lens:
* 

* 
* [1.x.90]
* 
*  Lastly, we generate some more logging output. We stop the timer and query the number of CPU seconds elapsed since the beginning of the function:
* 

* 
* [1.x.91]
* 
*   [1.x.92]  [1.x.93]   
*   Initialization of the system matrix, sparsity patterns and vectors are the same as in previous examples and therefore do not need further comment. As in the previous function, we also output the run time of what we do here:
* 

* 
* [1.x.94]
* 
*   [1.x.95]  [1.x.96]
* 

* 
*  As before, this function takes care of assembling the system matrix and right hand side vector:
* 

* 
* [1.x.97]
* 
*  First we query wavespeed and frequency from the ParameterHandler object and store them in local variables, as they will be used frequently throughout this function.
* 

* 
*  

* 
* [1.x.98]
* 
*  As usual, for computing integrals ordinary Gauss quadrature rule is used. Since our bilinear form involves boundary integrals on  [2.x.126] , we also need a quadrature rule for surface integration on the faces, which are  [2.x.127]  dimensional:
* 

* 
* [1.x.99]
* 
*  The FEValues objects will evaluate the shape functions for us.  For the part of the bilinear form that involves integration on  [2.x.128] , we'll need the values and gradients of the shape functions, and of course the quadrature weights.  For the terms involving the boundary integrals, only shape function values and the quadrature weights are necessary.
* 

* 
* [1.x.100]
* 
*  As usual, the system matrix is assembled cell by cell, and we need a matrix for storing the local cell contributions as well as an index vector to transfer the cell contributions to the appropriate location in the global system matrix after.
* 

* 
* [1.x.101]
* 
*  On each cell, we first need to reset the local contribution matrix and request the FEValues object to compute the shape functions for the current cell:
* 

* 
* [1.x.102]
* 
*  At this point, it is important to keep in mind that we are dealing with a finite element system with two components. Due to the way we constructed this FESystem, namely as the Cartesian product of two scalar finite element fields, each shape function has only a single nonzero component (they are, in deal.II lingo,  [2.x.129]  GlossPrimitive "primitive").  Hence, each shape function can be viewed as one of the  [2.x.130] 's or  [2.x.131] 's from the introduction, and similarly the corresponding degrees of freedom can be attributed to either  [2.x.132]  or  [2.x.133] . As we iterate through all the degrees of freedom on the current cell however, they do not come in any particular order, and so we cannot decide right away whether the DoFs with index  [2.x.134]  and  [2.x.135]  belong to the real or imaginary part of our solution.  On the other hand, if you look at the form of the system matrix in the introduction, this distinction is crucial since it will determine to which block in the system matrix the contribution of the current pair of DoFs will go and hence which quantity we need to compute from the given two shape functions.  Fortunately, the FESystem object can provide us with this information, namely it has a function  [2.x.136]  that for each local DoF index returns a pair of integers of which the first indicates to which component of the system the DoF belongs. The second integer of the pair indicates which index the DoF has in the scalar base finite element field, but this information is not relevant here. If you want to know more about this function and the underlying scheme behind primitive vector valued elements, take a look at  [2.x.137]  or the  [2.x.138]  module, where these topics are explained in depth.
* 

* 
* [1.x.103]
* 
*  If both DoFs  [2.x.139]  and  [2.x.140]  belong to same component, i.e. their shape functions are both  [2.x.141] 's or both  [2.x.142] 's, the contribution will end up in one of the diagonal blocks in our system matrix, and since the corresponding entries are computed by the same formula, we do not bother if they actually are  [2.x.143]  or  [2.x.144]  shape functions. We can simply compute the entry by iterating over all quadrature points and adding up their contributions, where values and gradients of the shape functions are supplied by our FEValues object.
* 

* 
*  

* 
* [1.x.104]
* 
*  You might think that we would have to specify which component of the shape function we'd like to evaluate when requesting shape function values or gradients from the FEValues object. However, as the shape functions are primitive, they have only one nonzero component, and the FEValues class is smart enough to figure out that we are definitely interested in this one nonzero component.
* 

* 
* [1.x.105]
* 
*  We also have to add contributions due to boundary terms. To this end, we loop over all faces of the current cell and see if first it is at the boundary, and second has the correct boundary indicator associated with  [2.x.145] , the part of the boundary where we have absorbing boundary conditions:
* 

* 
* [1.x.106]
* 
*  These faces will certainly contribute to the off-diagonal blocks of the system matrix, so we ask the FEFaceValues object to provide us with the shape function values on this face:
* 

* 
* [1.x.107]
* 
*  Next, we loop through all DoFs of the current cell to find pairs that belong to different components and both have support on the current face_no:
* 

* 
* [1.x.108]
* 
*  The check whether shape functions have support on a face is not strictly necessary: if we don't check for it we would simply add up terms to the local cell matrix that happen to be zero because at least one of the shape functions happens to be zero. However, we can save that work by adding the checks above.
* 

* 
*  In either case, these DoFs will contribute to the boundary integrals in the off-diagonal blocks of the system matrix. To compute the integral, we loop over all the quadrature points on the face and sum up the contribution weighted with the quadrature weights that the face quadrature rule provides.  In contrast to the entries on the diagonal blocks, here it does matter which one of the shape functions is a  [2.x.146]  and which one is a  [2.x.147] , since that will determine the sign of the entry.  We account for this by a simple conditional statement that determines the correct sign. Since we already checked that DoF  [2.x.148]  and  [2.x.149]  belong to different components, it suffices here to test for one of them to which component it belongs.
* 

* 
* [1.x.109]
* 
*  Now we are done with this cell and have to transfer its contributions from the local to the global system matrix. To this end, we first get a list of the global indices of the this cells DoFs...
* 

* 
* [1.x.110]
* 
*  ...and then add the entries to the system matrix one by one:
* 

* 
* [1.x.111]
* 
*  The only thing left are the Dirichlet boundary values on  [2.x.150] , which is characterized by the boundary indicator 1. The Dirichlet values are provided by the  [2.x.151]  class we defined above:
* 

* 
* [1.x.112]
* 
*   [1.x.113]  [1.x.114]
* 

* 
*  As already mentioned in the introduction, the system matrix is neither symmetric nor definite, and so it is not quite obvious how to come up with an iterative solver and a preconditioner that do a good job on this matrix.  We chose instead to go a different way and solve the linear system with the sparse LU decomposition provided by UMFPACK. This is often a good first choice for 2D problems and works reasonably well even for a large number of DoFs.  The deal.II interface to UMFPACK is given by the SparseDirectUMFPACK class, which is very easy to use and allows us to solve our linear system with just 3 lines of code.
* 

* 
*  Note again that for compiling this example program, you need to have the deal.II library built with UMFPACK support.
* 

* 
* [1.x.115]
* 
*  The code to solve the linear system is short: First, we allocate an object of the right type. The following  [2.x.152]  call provides the matrix that we would like to invert to the SparseDirectUMFPACK object, and at the same time kicks off the LU-decomposition. Hence, this is also the point where most of the computational work in this program happens.
* 

* 
* [1.x.116]
* 
*  After the decomposition, we can use  [2.x.153]  like a matrix representing the inverse of our system matrix, so to compute the solution we just have to multiply with the right hand side vector:
* 

* 
* [1.x.117]
* 
*   [1.x.118]  [1.x.119]
* 

* 
*  Here we output our solution  [2.x.154]  and  [2.x.155]  as well as the derived quantity  [2.x.156]  in the format specified in the parameter file. Most of the work for deriving  [2.x.157]  from  [2.x.158]  and  [2.x.159]  was already done in the implementation of the  [2.x.160]  class, so that the output routine is rather straightforward and very similar to what is done in the previous tutorials.
* 

* 
* [1.x.120]
* 
*  Define objects of our  [2.x.161]  class and a DataOut object:
* 

* 
* [1.x.121]
* 
*  Next we query the output-related parameters from the ParameterHandler. The  [2.x.162]  call acts as a counterpart to the  [2.x.163]  call in  [2.x.164] . It collects all the output format related parameters from the ParameterHandler and sets the corresponding properties of the DataOut object accordingly.
* 

* 
* [1.x.122]
* 
*  Now we put together the filename from the base name provided by the ParameterHandler and the suffix which is provided by the DataOut class (the default suffix is set to the right type that matches the one set in the .prm file through parse_parameters()):
* 

* 
* [1.x.123]
* 
*  The solution vectors  [2.x.165]  and  [2.x.166]  are added to the DataOut object in the usual way:
* 

* 
* [1.x.124]
* 
*  For the intensity, we just call  [2.x.167]  again, but this with our  [2.x.168]  object as the second argument, which effectively adds  [2.x.169]  to the output data:
* 

* 
* [1.x.125]
* 
*  The last steps are as before. Note that the actual output format is now determined by what is stated in the input file, i.e. one can change the output format without having to re-compile this program:
* 

* 
* [1.x.126]
* 
*   [1.x.127]  [1.x.128]
* 

* 
*  Here we simply execute our functions one after the other:
* 

* 
* [1.x.129]
* 
*   [1.x.130]  [1.x.131]
* 

* 
*  Finally the  [2.x.170]  function of the program. It has the same structure as in almost all of the other tutorial programs. The only exception is that we define ParameterHandler and  [2.x.171]  objects, and let the latter read in the parameter values from a textfile called  [2.x.172] . The values so read are then handed over to an instance of the UltrasoundProblem class:
* 

* 
* [1.x.132]
* [1.x.133][1.x.134][1.x.135]
* 

* The current program reads its run-time parameters from an input filecalled  [2.x.173]  that looks like this:
* [1.x.136]
* 
* As can be seen, we set [2.x.174] , which amounts to a focus of the transducer lensat  [2.x.175] ,  [2.x.176] . The coarse mesh is refined 5 times,resulting in 160x160 cells, and the output is written in vtuformat. The parameter reader understands many more parameterspertaining in particular to the generation of output, but weneed none of these parameters here and therefore stick withtheir default values.
* Here's the console output of the program in debug mode:
* [1.x.137]
* 
* (Of course, execution times will differ if you run the programlocally.) The fact that most of the time is spent on assemblingthe system matrix and generating output is due to the many assertionsthat need to be checked in debug mode. In release mode these partsof the program run much faster whereas solving the linear system ishardly sped up at all:
* [1.x.138]
* 
* The graphical output of the program looks as follows:
* 

*  [2.x.177] 
* The first two pictures show the real and imaginary parts of [2.x.178] , whereas the last shows the intensity  [2.x.179] . One can clearlysee that the intensity is focused around the focal point of thelens (0.5, 0.3), and that the focusis rather sharp in  [2.x.180] -direction but more blurred in  [2.x.181] -direction, which is aconsequence of the geometry of the focusing lens, its finite aperture,and the wave nature of the problem.
* Because colorful graphics are always fun, and to stress the focusingeffects some more, here is another set of images highlighting how wellthe intensity is actually focused in  [2.x.182] -direction:
*  [2.x.183] 
* 

* As a final note, the structure of the program makes it easy todetermine which parts of the program scale nicely as the mesh isrefined and which parts don't. Here are the run times for 5, 6, and 7global refinements:
* [1.x.139]
* 
* Each time we refine the mesh once, so the number of cells and degreesof freedom roughly quadruples from each step to the next. As can be seen,generating the grid, setting up degrees of freedom, assembling thelinear system, and generating output scale pretty closely to linear,whereas solving the linear system is an operation that requires 8times more time each time the number of degrees of freedom isincreased by a factor of 4, i.e. it is  [2.x.184] . This canbe explained by the fact that (using optimal ordering) thebandwidth of a finite element matrix is  [2.x.185] ,and the effort to solve a banded linear system using LU decompositionis  [2.x.186] . This also explains why the program does run in 3das well (after changing the dimension on the [2.x.187]  object), but scales very badly andtakes extraordinary patience before it finishes solving the linearsystem on a mesh with appreciable resolution, even though all theother parts of the program scale very nicely.
* 

* 
* [1.x.140][1.x.141][1.x.142]
* 

* An obvious possible extension for this program is to run it in 3d&mdash; after all, the world around us is three-dimensional, andultrasound beams propagate in three-dimensional media. You can trythis by simply changing the template parameter of the principal classin  [2.x.188]  and running it. This won't get you very far,though: certainly not if you do 5 global refinement steps as set inthe parameter file. You'll simply run out of memory as both the mesh(with its  [2.x.189]  cells)and in particular the sparse direct solver take too much memory. Youcan solve with 3 global refinement steps, however, if you have a bitof time: in early 2011, the direct solve takes about half anhour. What you'll notice, however, is that the solution is completelywrong: the mesh size is simply not small enough to resolve thesolution's waves accurately, and you can see this in plots of thesolution. Consequently, this is one of the cases where adaptivity isindispensable if you don't just want to throw a bigger (presumably%parallel) machine at the problem.
* 

* [1.x.143][1.x.144] [2.x.190] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-30_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19]
* [1.x.20][1.x.21][1.x.22]
* 

* 
* [1.x.23][1.x.24]
* 

* This example is devoted to  [2.x.2] anisotropic refinement [2.x.3] , which extends topossibilities of local refinement. In most parts, this is a modification of the [2.x.4]  tutorial program, we use the same DG method for a linear transportequation. This program will cover the following topics: [2.x.5]    [2.x.6]   [2.x.7] Anisotropic refinement [2.x.8] : What is the meaning of anisotropic refinement?   [2.x.9]   [2.x.10] Implementation [2.x.11] : Necessary modifications of code to work with anisotropically refined meshes.   [2.x.12]   [2.x.13] Jump indicator [2.x.14] : A simple indicator for anisotropic refinement in  the context of DG methods. [2.x.15] The discretization itself will not be discussed, and neither willimplementation techniques not specific to anisotropic refinement usedhere. Please refer to  [2.x.16]  for this.
* Please note, at the moment of writing this tutorial program, anisotropicrefinement is only fully implemented for discontinuous Galerkin FiniteElements. This may later change (or may already have).
* 

* 
*  [2.x.17]  While this program is a modification of  [2.x.18] , it is an adaptation ofa version of  [2.x.19]  written early on in the history of deal.II when theMeshWorker framework wasn't available yet. Consequently, it bears littleresemblance to the  [2.x.20]  as it exists now, apart from the fact that itsolves the same equation with the same discretization.
* 

* 
* [1.x.25][1.x.26]
* 

* All the adaptive processes in the preceding tutorial programs were based on [2.x.21] isotropic [2.x.22]  refinement of cells, which cuts all edges in half and formsnew cells of these split edges (plus some additional edges, faces and vertices,of course). In deal.II,  [2.x.23] anisotropic refinement [2.x.24]  refers to the process ofsplitting only part of the edges while leaving the others unchanged. Consider asimple square cell, for example:
* [1.x.27]
* After the usual refinement it will consist of four children and look like this:
* [1.x.28]
* The new anisotropic refinement may take two forms: either we can split the edgeswhich are parallel to the horizontal x-axis, resulting in these two child cells:
* [1.x.29]
* or we can split the two edges which run along the y-axis, resulting again in twochildren, which look that way, however:
* [1.x.30]
* All refinement cases of cells are described by an enumeration [2.x.25]  and the above anisotropiccases are called  [2.x.26]  and  [2.x.27]  for obvious reasons. Theisotropic refinement case is called  [2.x.28]  in 2D and can berequested from the RefinementCase class via [2.x.29] 
* In 3D, there is a third axis which can be split, the z-axis, and thus wehave an additional refinement case  [2.x.30]  here. Isotropic refinement will nowrefine a cell along the x-, y- and z-axes and thus be referred to as  [2.x.31] cut_xyz. Additional cases  [2.x.32]   [2.x.33]  and  [2.x.34]  exist, which refinea cell along two of the axes, but not along the third one. Given a hex cell withx-axis running to the right, y-axis 'into the page' and z-axis to the top,
* [1.x.31]
* we have the isotropic refinement case,
* [1.x.32]
* three anisotropic cases which refine only one axis:
* [1.x.33]
* and three cases which refine two of the three axes:
* [1.x.34]
* For 1D problems, anisotropic refinement can make no difference, as there is onlyone coordinate direction for a cell, so it is not possible to split itin any other way than isotropically.
* [1.x.35][1.x.36]
* Adaptive local refinement is used to obtain fine meshes which are well adaptedto solving the problem at hand efficiently. In short, the size of cells whichproduce a large error is reduced to obtain a better approximation of thesolution to the problem at hand. However, a lot of problems contain anisotropicfeatures. Prominent examples are shocks or boundary layers in compressibleviscous flows. An efficient mesh approximates these features with cells of higher aspect ratiowhich are oriented according to the mentioned features. Using only isotropicrefinement, the aspect ratios of the original mesh cells are preserved, as theyare inherited by the children of a cell. Thus, starting from an isotropic mesh, aboundary layer will be refined in order to catch the rapid variation of the flowfield in the wall normal direction, thus leading to cells with very small edgelengths both in normal and tangential direction. Usually, much higher edgelengths in tangential direction and thus significantly less cells could be usedwithout a significant loss in approximation accuracy. An anisotropicrefinement process can modify the aspect ratio from mother to child cells by afactor of two for each refinement step. In the course of several refinements,the aspect ratio of the fine cells can be optimized, saving a considerablenumber of cells and correspondingly degrees of freedom and thus computationalresources, memory as well as CPU time.
* [1.x.37][1.x.38]
* 

* Most of the time, when we do finite element computations, we only consider onecell at a time, for example to calculate cell contributions to the globalmatrix, or to interpolate boundary values. However, sometimes we have to lookat how cells are related in our algorithms. Relationships between cells comein two forms: neighborship and mother-child relationship. For the case ofisotropic refinement, deal.II uses certain conventions (invariants) for cellrelationships that are always maintained. For example, a refined cell alwayshas exactly  [2.x.35]  children. And (except for the 1d case), two neighboringcells may differ by at most one refinement level: they are equally oftenrefined or one of them is exactly once more refined, leaving exactly onehanging node on the common face. Almost all of the time these invariants areonly of concern in the internal implementation of the library. However, thereare cases where knowledge of them is also relevant to an application program.
* In the current context, it is worth noting that the kind of mesh refinementaffects some of the most fundamental assumptions. Consequently, some of theusual code found in application programs will need modifications to exploitthe features of meshes which were created using anisotropicrefinement. For those interested in how deal.II evolved, it may be ofinterest that the loosening of such invariants required someincompatible changes. For example, the library used to have a member [2.x.36]  that specified how many childrena cell has once it is refined. For isotropic refinement, this numberis equal to  [2.x.37] , as mentioned above. However, for anisotropic refinement, this numberdoes not exist, as is can be either two or four in 2D and two, four or eight in3D, and the member  [2.x.38]  hasconsequently been removed. It has now been replaced by [2.x.39]  which specifies the[1.x.39] number of children a cell can have. How many children arefined cell has was previously available as static information, butnow it depends on the actual refinement state of a cell and can beretrieved using  [2.x.40] a call that works equally well for both isotropic and anisotropicrefinement. A very similar situation can be found forfaces and their subfaces: the pertinent information can be queried using [2.x.41]  or  [2.x.42] ,depending on the context.
* Another important aspect, and the most important one in this tutorial, isthe treatment of neighbor-relations when assembling jump terms on thefaces between cells. Looking at the documentation of theassemble_system functions in  [2.x.43]  we notice, that we need to decide if aneighboring cell is coarser, finer or on the same (refinement) level as ourcurrent cell. These decisions do not work in the same way for anisotropicrefinement as the information given by the  [2.x.44] level [2.x.45]  of a cell is notenough to completely characterize anisotropic cells; for example, arethe terminal children of a two-dimensionalcell that is first cut in  [2.x.46] -direction and whose children are thencut in  [2.x.47] -direction on level 2, or are they on level 1 as they wouldbe if the cell would have been refined once isotropically, resultingin the same set of finest cells?
* After anisotropic refinement, a coarser neighbor is not necessarilyexactly one level below ours, but can pretty much have any levelrelative to the current one; in fact, it can even be on a higherlevel even though it is coarser. Thus the decisionshave to be made on a different basis, whereas the intention of thedecisions stays the same.
* In the following, we will discuss the cases that can happen when wewant to compute contributions to the matrix (or right hand side) ofthe form[1.x.40]or similar; remember that we integrate terms like this using theFEFaceValues and FESubfaceValues classes. We will also show how towrite code that works for both isotropic and anisotropic refinement:
*  [2.x.48] 
*    [2.x.49]   [2.x.50] Finer neighbor [2.x.51] : If we are on an active cell and want  to integrate over a face  [2.x.52] , the first  possibility is that the neighbor behind this face is more refined,  i.e. has children occupying only part of the  common face. In this case, the face  under consideration has to be a refined one, which can determine by  asking  [2.x.53] . If this is true, we need to  loop over  all subfaces and get the neighbors' child behind this subface, so that we can  reinit an FEFaceValues object with the neighbor and an FESubfaceValues object  with our cell and the respective subface.
*   For isotropic refinement, this kind is reasonably simple because we  know that an invariant of the isotropically refined adaptive meshes  in deal.II is that neighbors can only differ by exactly one  refinement level. However, this isn't quite true any more for  anisotropically refined meshes, in particular in 3d; there,  the active cell we are interested on the other side of  [2.x.54]  might not  actually be a child of our  neighbor, but perhaps a grandchild or even a farther offspring. Fortunately,  this complexity is hidden in the internals of the library. All we need to do  is call the  [2.x.55]   function. Still, in 3D there are two cases which need special consideration:   [2.x.56]      [2.x.57]  If the neighbor is refined more than once anisotropically, it might be  that here are not two or four but actually three subfaces to  consider. Imagine  the following refinement process of the (two-dimensional) face of  the (three-dimensional) neighbor cell we are considering: first the  face is refined along x, later on only the left subface is refined along y.
* [1.x.41]
*      Here the number of subfaces is three. It is important to note the subtle  differences between, for a face,  [2.x.58]  and   [2.x.59]  The first function returns the number of  immediate children, which would be two for the above example, whereas the  second returns the number of active offspring (i.e., including children,  grandchildren, and further descendants), which is the correct three in  the example above. Using  [2.x.60]  works for  isotropic and anisotropic as well as 2D and 3D cases, so it should always be  used. It should be noted that if any of the cells behind the two  small subfaces on the left side of the rightmost image is further  refined, then the current cell (i.e. the side from which we are  viewing this common face) is going to be refined as well: this is so  because otherwise the invariant of having only one hanging node per  edge would be violated.
*      [2.x.61]  It might be, that the neighbor is coarser, but still has children which  are finer than our current cell. This situation can occur if two equally  coarse cells are refined, where one of the cells has two children at the face  under consideration and the other one four. The cells in the next graphic are  only separated from each other to show the individual refinement cases.
* [1.x.42]
* 
*   Here, the left two cells resulted from an anisotropic bisection of  the mother cell in  [2.x.62] -direction, whereas the right four cells  resulted from a simultaneous anisotropic refinement in both the  [2.x.63] -  and  [2.x.64] -directions.  The left cell marked with # has two finer neighbors marked with +, but the  actual neighbor of the left cell is the complete right mother cell, as the  two cells marked with + are finer and their direct mother is the one  large cell.   [2.x.65] 
*   However, fortunately,  [2.x.66]  takes care of  these situations by itself, if you loop over the correct number of subfaces,  in the above example this is two. The  [2.x.67]  function  takes care of this too, so that the resulting state is always correct. There  is one little caveat, however: For reiniting the neighbors FEFaceValues object  you need to know the index of the face that points toward the current  cell. Usually you assume that the neighbor you get directly is as coarse or as  fine as you, if it has children, thus this information can be obtained with   [2.x.68]  If the neighbor is coarser, however, you  would have to use the first value in  [2.x.69]   instead. In order to make this easy for you, there is   [2.x.70]  which does the correct thing for you and  returns the desired result.
*    [2.x.71]   [2.x.72] Neighbor is as fine as our cell [2.x.73] : After we ruled out all cases in  which there are finer children, we only need to decide, whether the neighbor  is coarser here. For this, there is the   [2.x.74]  function which returns a boolean. In  order to get the relevant case of a neighbor of the same coarseness we would  use  [2.x.75] . The code inside this  block can be left untouched. However, there is one thing to mention here: If  we want to use a rule, which cell should assemble certain terms on a given  face we might think of the rule presented in  [2.x.76] . We know that we have to  leave out the part about comparing our cell's level with that of the neighbor  and replace it with the test for a coarser neighbor presented above. However,  we also have to consider the possibility that neighboring cells of same  coarseness have the same index (on different levels). Thus we have to include  the case where the cells have the same index, and give an additional  condition, which of the cells should assemble the terms, e.g. we can choose  the cell with lower level. The details of this concept can be seen in the  implementation below.
*    [2.x.77]   [2.x.78] Coarser neighbor [2.x.79] : The remaining case is obvious: If there are no  refined neighbors and the neighbor is not as fine as the current cell, then it must  be coarser. Thus we can leave the old condition phrase, simply using   [2.x.80] . The  [2.x.81]   function takes care of all the complexity of anisotropic refinement combined  with possible non standard face orientation, flip and rotation on general 3D meshes.
*  [2.x.82] 
* [1.x.43][1.x.44]
* When a triangulation is refined, cells which were not flagged for refinement maybe refined nonetheless. This is due to additional smoothing algorithms which areeither necessary or requested explicitly. In particular, the restriction that therebe at most one hanging node on each edge frequently forces the refinement of additionalcells neighboring ones that are already finer and are flagged forfurther refinement.
* However, deal.II also implements a number of algorithms that make surethat resulting meshes are smoother than just the bare minimum, forexample ensuring that there are no isolated refined cells surroundedby non-refined ones, since the additional degrees of freedom on theseislands would almost all be constrained by hanging nodeconstraints. (See the documentation of the Triangulation class and its [2.x.83]  member for more information on meshsmoothing.)
* Most of the smoothing algorithms that were originally developed forthe isotropic case have been adapted to work in a very similarway for both anisotropic and isotropic refinement. There are twoalgorithms worth mentioning, however: [2.x.84]    [2.x.85]   [2.x.86] : In an isotropic environment,  this algorithm tries to ensure a good approximation quality by reducing the  difference in refinement level of cells meeting at a common vertex. However,  there is no clear corresponding concept for anisotropic refinement, thus this  algorithm may not be used in combination with anisotropic refinement. This  restriction is enforced by an assertion which throws an error as soon as the  algorithm is called on a triangulation which has been refined anisotropically.
*    [2.x.87]   [2.x.88] : If refinement is introduced to  limit the number of hanging nodes, the additional cells are often not needed  to improve the approximation quality. This is especially true for DG  methods. If you set the flag  [2.x.89]  the  smoothing algorithm tries to minimize the number of probably unneeded  additional cells by using anisotropic refinement for the smoothing. If you set  this smoothing flag you might get anisotropically refined cells, even if you  never set a single refinement flag to anisotropic refinement. Be aware that  you should only use this flag, if your code respects the possibility of  anisotropic meshes. Combined with a suitable anisotropic indicator this flag  can help save additional cells and thus effort. [2.x.90] 
* 

* [1.x.45][1.x.46]
* 

* Using the benefits of anisotropic refinement requires an indicator to catchanisotropic features of the solution and exploit them for the refinementprocess. Generally the anisotropic refinement process will consist of severalsteps: [2.x.91]    [2.x.92]  Calculate an error indicator.   [2.x.93]  Use the error indicator to flag cells for refinement, e.g. using a fixed  number or fraction of cells. Those cells will be flagged for isotropic  refinement automatically.   [2.x.94]  Evaluate a distinct anisotropic indicator only on the flagged cells.   [2.x.95]  Use the anisotropic indicator to set a new, anisotropic refinement flag  for cells where this is appropriate, leave the flags unchanged otherwise.   [2.x.96]  Call  [2.x.97]  to perform the  requested refinement, using the requested isotropic and anisotropic flags. [2.x.98] This approach is similar to the one we have used in  [2.x.99] for hp-refinement andhas the great advantage of flexibility: Any error indicator can beused in the anisotropic process, i.e. if you have quite involved a posteriorigoal-oriented error indicators available you can use them as easily as a simpleKelly error estimator. The anisotropic part of the refinement process is notinfluenced by this choice. Furthermore, simply leaving out the third and forthsteps leads to the same isotropic refinement you used to get before anyanisotropic changes in deal.II or your application program.As a last advantage, working onlyon cells flagged for refinement results in a faster evaluation of theanisotropic indicator, which can become noticeable on finer meshes with a lot ofcells if the indicator is quite involved.
* Here, we use a very simple approach which is only applicable to DGmethods. The general idea is quite simple: DG methods allow the discretesolution to jump over the faces of a cell, whereas it is smooth within eachcell. Of course, in the limit we expect that the jumps tend to zero aswe refine the mesh and approximate the true solution better and better.Thus, a large jumpacross a given face indicates that the cell should be refined (at least)orthogonally to that face, whereas a small jump does not lead to thisconclusion. It is possible, of course, that the exact solution is not smooth andthat it also features a jump. In that case, however, a large jump over one faceindicates, that this face is more or less parallel to the jump and in thevicinity of it, thus again we would expect a refinement orthogonal to the faceunder consideration to be effective.
* The proposed indicator calculates the average jump  [2.x.100] , i.e. the mean value ofthe absolute jump  [2.x.101]  of the discrete solution  [2.x.102]  over the two faces [2.x.103] ,  [2.x.104] ,  [2.x.105]  orthogonal to coordinate direction  [2.x.106]  on the unitcell.[1.x.47]If the average jump in one direction is larger than the average of thejumps in the other directions by acertain factor  [2.x.107] , i.e. if [2.x.108] , the cell is refined only along that particulardirection  [2.x.109] , otherwise the cell is refined isotropically.
* Such a criterion is easily generalized to systems of equations: theabsolute value of the jump would be replaced by an appropriate norm ofthe vector-valued jump.
* 

* 
* [1.x.48][1.x.49]
* 

* We solve the linear transport equation presented in  [2.x.110] . The domain isextended to cover  [2.x.111]  in 2D, where the flow field  [2.x.112]  describes acounterclockwise quarter circle around the origin in the right half of thedomain and is parallel to the x-axis in the left part of the domain. The inflowboundary is again located at  [2.x.113]  and along the positive part of the x-axis,and the boundary conditions are chosen as in  [2.x.114] .
* 

*  [1.x.50] [1.x.51]
*  The deal.II include files have already been covered in previous examples and will thus not be further commented on.
* 

* 
* [1.x.52]
* 
*  And this again is C++:
* 

* 
* [1.x.53]
* 
*  The last step is as in all previous programs:
* 

* 
* [1.x.54]
* 
*   [1.x.55]  [1.x.56]   
*   The classes describing equation data and the actual assembly of individual terms are almost entirely copied from  [2.x.115] . We will comment on differences.
* 

* 
* [1.x.57]
* 
*  The flow field is chosen to be a quarter circle with counterclockwise flow direction and with the origin as midpoint for the right half of the domain with positive  [2.x.116]  values, whereas the flow simply goes to the left in the left part of the domain at a velocity that matches the one coming in from the right. In the circular part the magnitude of the flow velocity is proportional to the distance from the origin. This is a difference to  [2.x.117] , where the magnitude was 1 everywhere. the new definition leads to a linear variation of  [2.x.118]  along each given face of a cell. On the other hand, the solution  [2.x.119]  is exactly the same as before.
* 

* 
* [1.x.58]
* 
*   [1.x.59]  [1.x.60]   
*   This declaration of this class is utterly unaffected by our current changes.
* 

* 
* [1.x.61]
* 
*  Likewise, the constructor of the class as well as the functions assembling the terms corresponding to cell interiors and boundary faces are unchanged from before. The function that assembles face terms between cells also did not change because all it does is operate on two objects of type FEFaceValuesBase (which is the base class of both FEFaceValues and FESubfaceValues). Where these objects come from, i.e. how they are initialized, is of no concern to this function: it simply assumes that the quadrature points on faces or subfaces represented by the two objects correspond to the same points in physical space.
* 

* 
* [1.x.62]
* 
*   [1.x.63]  [1.x.64]   
*   This declaration is much like that of  [2.x.120] . However, we introduce a new routine (set_anisotropic_flags) and modify another one (refine_grid).
* 

* 
* [1.x.65]
* 
*  Again we want to use DG elements of degree 1 (but this is only specified in the constructor). If you want to use a DG method of a different degree replace 1 in the constructor by the new degree.
* 

* 
* [1.x.66]
* 
*  This is new, the threshold value used in the evaluation of the anisotropic jump indicator explained in the introduction. Its value is set to 3.0 in the constructor, but it can easily be changed to a different value greater than 1.
* 

* 
* [1.x.67]
* 
*  This is a bool flag indicating whether anisotropic refinement shall be used or not. It is set by the constructor, which takes an argument of the same name.
* 

* 
* [1.x.68]
* 
*  Change here for DG methods of different degrees.
* 

* 
* [1.x.69]
* 
*  As beta is a linear function, we can choose the degree of the quadrature for which the resulting integration is correct. Thus, we choose to use  [2.x.121]  Gauss points, which enables us to integrate exactly polynomials of degree  [2.x.122] , enough for all the integrals we will perform in this program.
* 

* 
* [1.x.70]
* 
*   [1.x.71]  [1.x.72]   
*   We proceed with the  [2.x.123]  function that implements the DG discretization. This function does the same thing as the  [2.x.124]  function from  [2.x.125]  (but without MeshWorker).  The four cases considered for the neighbor-relations of a cell are the same as the isotropic case, namely a) cell is at the boundary, b) there are finer neighboring cells, c) the neighbor is neither coarser nor finer and d) the neighbor is coarser.  However, the way in which we decide upon which case we have are modified in the way described in the introduction.
* 

* 
* [1.x.73]
* 
*  Case (a): The face is at the boundary.
* 

* 
* [1.x.74]
* 
*  Case (b): This is an internal face and the neighbor is refined (which we can test by asking whether the face of the current cell has children). In this case, we will need to integrate over the "sub-faces", i.e., the children of the face of the current cell.                 
*   (There is a slightly confusing corner case: If we are in 1d
* 
*  -  where admittedly the current program and its demonstration of anisotropic refinement is not particularly relevant
* 
*  -  then the faces between cells are always the same: they are just vertices. In other words, in 1d, we do not want to treat faces between cells of different level differently. The condition `face->has_children()` we check here ensures this: in 1d, this function always returns `false`, and consequently in 1d we will not ever go into this `if` branch. But we will have to come back to this corner case below in case (c).)
* 

* 
* [1.x.75]
* 
*  We need to know, which of the neighbors faces points in the direction of our cell. Using the  [2.x.126]  neighbor_face_no function we get this information for both coarser and non-coarser neighbors.
* 

* 
* [1.x.76]
* 
*  Now we loop over all subfaces, i.e. the children and possibly grandchildren of the current face.
* 

* 
* [1.x.77]
* 
*  To get the cell behind the current subface we can use the  [2.x.127]  function. it takes care of all the complicated situations of anisotropic refinement and non-standard faces.
* 

* 
* [1.x.78]
* 
*  The remaining part of this case is unchanged.
* 

* 
* [1.x.79]
* 
*  Case (c). We get here if this is an internal face and if the neighbor is not further refined (or, as mentioned above, we are in 1d in which case we get here for every internal face). We then need to decide whether we want to integrate over the current face. If the neighbor is in fact coarser, then we ignore the face and instead handle it when we visit the neighboring cell and look at the current face (except in 1d, where as mentioned above this is not happening):
* 

* 
* [1.x.80]
* 
*  On the other hand, if the neighbor is more refined, then we have already handled the face in case (b) above (except in 1d). So for 2d and 3d, we just have to decide whether we want to handle a face between cells at the same level from the current side or from the neighboring side.  We do this by introducing a tie-breaker: We'll just take the cell with the smaller index (within the current refinement level). In 1d, we take either the coarser cell, or if they are on the same level, the one with the smaller index within that level. This leads to a complicated condition that, hopefully, makes sense given the description above:
* 

* 
* [1.x.81]
* 
*  Here we know, that the neighbor is not coarser so we can use the usual  [2.x.128]  function. However, we could also use the more general  [2.x.129]  function.
* 

* 
* [1.x.82]
* 
*  We do not need to consider a case (d), as those faces are treated 'from the other side within case (b).
* 

* 
* [1.x.83]
* 
*   [1.x.84]  [1.x.85]   
*   For this simple problem we use the simple Richardson iteration again. The solver is completely unaffected by our anisotropic changes.
* 

* 
* [1.x.86]
* 
*   [1.x.87]  [1.x.88]   
*   We refine the grid according to the same simple refinement criterion used in  [2.x.130] , namely an approximation to the gradient of the solution.
* 

* 
* [1.x.89]
* 
*  We approximate the gradient,
* 

* 
* [1.x.90]
* 
*  and scale it to obtain an error indicator.
* 

* 
* [1.x.91]
* 
*  Then we use this indicator to flag the 30 percent of the cells with highest error indicator to be refined.
* 

* 
* [1.x.92]
* 
*  Now the refinement flags are set for those cells with a large error indicator. If nothing is done to change this, those cells will be refined isotropically. If the  [2.x.131]  flag given to this function is set, we now call the set_anisotropic_flags() function, which uses the jump indicator to reset some of the refinement flags to anisotropic refinement.
* 

* 
* [1.x.93]
* 
*  Now execute the refinement considering anisotropic as well as isotropic refinement flags.
* 

* 
* [1.x.94]
* 
*  Once an error indicator has been evaluated and the cells with largest error are flagged for refinement we want to loop over the flagged cells again to decide whether they need isotropic refinement or whether anisotropic refinement is more appropriate. This is the anisotropic jump indicator explained in the introduction.
* 

* 
* [1.x.95]
* 
*  We want to evaluate the jump over faces of the flagged cells, so we need some objects to evaluate values of the solution on faces.
* 

* 
* [1.x.96]
* 
*  Now we need to loop over all active cells.
* 

* 
* [1.x.97]
* 
*  We only need to consider cells which are flagged for refinement.
* 

* 
* [1.x.98]
* 
*  The four cases of different neighbor relations seen in the assembly routines are repeated much in the same way here.
* 

* 
* [1.x.99]
* 
*  The neighbor is refined.  First we store the information, which of the neighbor's faces points in the direction of our current cell. This property is inherited to the children.
* 

* 
* [1.x.100]
* 
*  Now we loop over all subfaces,
* 

* 
* [1.x.101]
* 
*  get an iterator pointing to the cell behind the present subface...
* 

* 
* [1.x.102]
* 
*  ... and reinit the respective FEFaceValues and FESubFaceValues objects.
* 

* 
* [1.x.103]
* 
*  We obtain the function values
* 

* 
* [1.x.104]
* 
*  as well as the quadrature weights, multiplied by the Jacobian determinant.
* 

* 
* [1.x.105]
* 
*  Now we loop over all quadrature points
* 

* 
* [1.x.106]
* 
*  and integrate the absolute value of the jump of the solution, i.e. the absolute value of the difference between the function value seen from the current cell and the neighboring cell, respectively. We know, that the first two faces are orthogonal to the first coordinate direction on the unit cell, the second two faces are orthogonal to the second coordinate direction and so on, so we accumulate these values into vectors with  [2.x.132]  components.
* 

* 
* [1.x.107]
* 
*  We also sum up the scaled weights to obtain the measure of the face.
* 

* 
* [1.x.108]
* 
*  Our current cell and the neighbor have the same refinement along the face under consideration. Apart from that, we do much the same as with one of the subcells in the above case.
* 

* 
* [1.x.109]
* 
*  Now the neighbor is actually coarser. This case is new, in that it did not occur in the assembly routine. Here, we have to consider it, but this is not overly complicated. We simply use the  [2.x.133]  neighbor_of_coarser_neighbor function, which again takes care of anisotropic refinement and non-standard face orientation by itself.
* 

* 
* [1.x.110]
* 
*  Now we analyze the size of the mean jumps, which we get dividing the jumps by the measure of the respective faces.
* 

* 
* [1.x.111]
* 
*  Now we loop over the  [2.x.134]  coordinate directions of the unit cell and compare the average jump over the faces orthogonal to that direction with the average jumps over faces orthogonal to the remaining direction(s). If the first is larger than the latter by a given factor, we refine only along hat axis. Otherwise we leave the refinement flag unchanged, resulting in isotropic refinement.
* 

* 
* [1.x.112]
* 
*   [1.x.113]  [1.x.114]   
*   The remaining part of the program very much follows the scheme of previous tutorial programs. We output the mesh in VTU format (just as we did in  [2.x.135] , for example), and the visualization output in VTU format as we almost always do.
* 

* 
* [1.x.115]
* 
*  Create the rectangular domain.
* 

* 
* [1.x.116]
* 
*  Adjust the number of cells in different directions to obtain completely isotropic cells for the original mesh.
* 

* 
* [1.x.117]
* 
*  If you want to run the program in 3D, simply change the following line to  [2.x.136] .
* 

* 
* [1.x.118]
* 
*  First, we perform a run with isotropic refinement.
* 

* 
* [1.x.119]
* 
*  Now we do a second run, this time with anisotropic refinement.
* 

* 
* [1.x.120]
* [1.x.121][1.x.122]
* 

* 
* The output of this program consist of the console output, the SVGfiles containing the grids, and the solutions given in VTU format.
* [1.x.123]
* 
* This text output shows the reduction in the number of cells which results fromthe successive application of anisotropic refinement. After the last refinementstep the savings have accumulated so much that almost four times as many cellsand thus degrees of freedom are needed in the isotropic case. The time needed for assemblyscales with a similar factor.
* The first interesting part is of course to see how the meshes look like.On the left are the isotropically refined ones, on the right theanisotropic ones (colors indicate the refinement level of cells):
*  [2.x.137] 
* 

* The other interesting thing is, of course, to see the solution on thesetwo sequences of meshes. Here they are, on the refinement cycles 1 and 4,clearly showing that the solution is indeed composed of [1.x.124] piecewisepolynomials:
*  [2.x.138] 
* We see, that the solution on the anisotropically refined mesh is very similar tothe solution obtained on the isotropically refined mesh. Thus the anisotropicindicator seems to effectively select the appropriate cells for anisotropicrefinement.
* The pictures also explain why the mesh is refined as it is.In the whole left part of the domain refinement is only performed along the [2.x.139] -axis of cells. In the right part of the domain the refinement is dominated byisotropic refinement, as the anisotropic feature of the solution
* 
*  - the jump fromone to zero
* 
*  - is not well aligned with the mesh where the advection directiontakes a turn. However, at the bottom and closest (to the observer) parts of thequarter circle this jumps again becomes more and more alignedwith the mesh and the refinement algorithm reacts by creating anisotropic cellsof increasing aspect ratio.
* It might seem that the necessary alignment of anisotropic features and thecoarse mesh can decrease performance significantly for real worldproblems. That is not wrong in general: If one were, for example, to applyanisotropic refinement to problems in which shocks appear (e.g., theequations solved in  [2.x.140] ), then it many cases the shock is not alignedwith the mesh and anisotropic refinement will help little unless one alsointroduces techniques to move the mesh in alignment with the shocks.On the other hand, many steep features of solutions are due to boundary layers.In those cases, the mesh is already aligned with the anisotropic featuresbecause it is of course aligned with the boundary itself, and anisotropicrefinement will almost always increase the efficiency of computations onadapted grids for these cases.
* 

* [1.x.125][1.x.126] [2.x.141] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-3_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25]
* [1.x.26][1.x.27][1.x.28]
* 

*  [2.x.2] 
* [1.x.29][1.x.30]
* 

* This is the first example where we actually use finite elements to computesomething. Wewill solve a simple version of Poisson's equation with zero boundaryvalues, but a nonzero right hand side:
* [1.x.31]
* We will solve this equation on the square,  [2.x.3] , for whichyou've already learned how to generate a mesh in  [2.x.4]  and  [2.x.5] . Inthis program, we will also only consider the particular case [2.x.6]  and come back to how to implement the more generalcase in the next tutorial program,  [2.x.7] .
* If you've learned about the basics of the finite element method, you willremember the steps we need to take to approximate the solution  [2.x.8]  by a finitedimensional approximation. Specifically, we first need to derive the weak formof the equation above, which we obtain by multiplying the equation by a testfunction  [2.x.9]  [1.x.32] (we will come back to the reason formultiplying from the left and not from the right below) and integrating overthe domain  [2.x.10] :
* [1.x.33]
* This can be integrated by parts:
* [1.x.34]
* The test function  [2.x.11]  has to satisfy the same kind of boundaryconditions (in mathematical terms: it needs to come from the tangent space ofthe set in which we seek the solution), so on the boundary  [2.x.12]  andconsequently the weak form we are looking for reads
* [1.x.35]
* where we have used the common notation  [2.x.13] . The problemthen asks for a function  [2.x.14]  for which this statement is true for all testfunctions  [2.x.15]  from the appropriate space (which here is the space [2.x.16] ).
* Of course we can't find such a function on a computer in the general case, andinstead we seek an approximation  [2.x.17] , where the  [2.x.18]  are unknown expansion coefficients we need to determine(the "degrees of freedom" of this problem), and  [2.x.19]  are thefinite element shape functions we will use. To define these shape functions,we need the following:
* 
*  - A mesh on which to define shape functions. You have already seen how to  generate and manipulate the objects that describe meshes in  [2.x.20]  and   [2.x.21] .
* 
*  - A finite element that describes the shape functions we want to use on the  reference cell (which in deal.II is always the unit interval  [2.x.22] , the  unit square  [2.x.23]  or the unit cube  [2.x.24] , depending on which space  dimension you work in). In  [2.x.25] , we had already used an object of type  FE_Q<2>, which denotes the usual Lagrange elements that define shape  functions by interpolation on support points. The simplest one is  FE_Q<2>(1), which uses polynomial degree 1. In 2d, these are often referred  to as [1.x.36], since they are linear in each of the two coordinates  of the reference cell. (In 1d, they would be [1.x.37] and in 3d  [1.x.38]; however, in the deal.II documentation, we will frequently  not make this distinction and simply always call these functions "linear".)
* 
*  - A DoFHandler object that enumerates all the degrees of freedom on the mesh,  taking the reference cell description the finite element object provides as  the basis. You've also already seen how to do this in  [2.x.26] .
* 
*  - A mapping that tells how the shape functions on the real cell are obtained  from the shape functions defined by the finite element class on the  reference cell. By default, unless you explicitly say otherwise, deal.II  will use a (bi-, tri-)linear mapping for this, so in most cases you don't  have to worry about this step.
* Through these steps, we now have a set of functions  [2.x.27] , and we candefine the weak form of the discrete problem: Find a function  [2.x.28] , i.e., findthe expansion coefficients  [2.x.29]  mentioned above, so that
* [1.x.39]
* Note that we here follow the convention that everything is counted starting atzero, as common in C and C++. This equation can be rewritten as a linearsystem if you insert the representation  [2.x.30]  and then observe that
* [1.x.40]
* With this, the problem reads: Find a vector  [2.x.31]  so that
* [1.x.41]
* where the matrix  [2.x.32]  and the right hand side  [2.x.33]  are defined as
* [1.x.42]
* 
* 

* [1.x.43][1.x.44]
* 

* Before we move on with describing how these quantities can be computed, notethat if we had multiplied the original equation from the [1.x.45] by atest function rather than from the left, then we would have obtained a linearsystem of the form
* [1.x.46]
* with a row vector  [2.x.34] . By transposing this system, this is of courseequivalent to solving
* [1.x.47]
* which here is the same as above since  [2.x.35] . But in general is not,and in order to avoidany sort of confusion, experience has shown that simply getting into the habitof multiplying the equation from the left rather than from the right (as isoften done in the mathematical literature) avoids a common class of errors asthe matrix is automatically correct and does not need to be transposed whencomparing theory and implementation. See  [2.x.36]  for the first example in thistutorial where we have a non-symmetric bilinear form for which it makes adifference whether we multiply from the right or from the left.
* 

* [1.x.48][1.x.49]
* 

* Now we know what we need (namely: objects that hold the matrix andvectors, as well as ways to compute  [2.x.37] ), and we can look at what ittakes to make that happen:
* 
*  - The object for  [2.x.38]  is of type SparseMatrix while those for  [2.x.39]  and  [2.x.40]  are of  type Vector. We will see in the program below what classes are used to solve  linear systems.
* 
*  - We need a way to form the integrals. In the finite element method, this is  most commonly done using quadrature, i.e. the integrals are replaced by a  weighted sum over a set of points on each cell. That is, we first split the  integral over  [2.x.41]  into integrals over all cells, 
* [1.x.50]
*   and then approximate each cell's contribution by quadrature: 
* [1.x.51]
*   where  [2.x.42]  is the  [2.x.43] th quadrature point on cell  [2.x.44] , and  [2.x.45]   the  [2.x.46] th quadrature weight. There are different parts to what is needed in  doing this, and we will discuss them in turn next.
* 
*  - First, we need a way to describe the location  [2.x.47]  of quadrature  points and their weights  [2.x.48] . They are usually mapped from the reference  cell in the same way as shape functions, i.e., implicitly using the  MappingQ1 class or, if you explicitly say so, through one of the other  classes derived from Mapping. The locations and weights on the reference  cell are described by objects derived from the Quadrature base  class. Typically, one chooses a quadrature formula (i.e. a set of points and  weights) so that the quadrature exactly equals the integral in the matrix;  this can be achieved because all factors in the integral are polynomial, and  is done by Gaussian quadrature formulas, implemented in the QGauss class.
* 
*  - We then need something that can help us evaluate  [2.x.49]   on cell  [2.x.50] . This is what the FEValues class does: it takes a finite element  objects to describe  [2.x.51]  on the reference cell, a quadrature object to  describe the quadrature points and weights, and a mapping object (or  implicitly takes the MappingQ1 class) and provides values and derivatives of  the shape functions on the real cell  [2.x.52]  as well as all sorts of other  information needed for integration, at the quadrature points located on  [2.x.53] .
* FEValues really is the central class in the assembly process. One way you canview it is as follows: The FiniteElement and derived classes describe shape[1.x.52], i.e., infinite dimensional objects: functions have values atevery point. We need this for theoretical reasons because we want to performour analysis with integrals over functions. However, for a computer, this is avery difficult concept, since they can in general only deal with a finiteamount of information, and so we replace integrals by sums over quadraturepoints that we obtain by mapping (the Mapping object) using  points defined ona reference cell (the Quadrature object) onto points on the real cell. Inessence, we reduce the problem to one where we only need a finite amount ofinformation, namely shape function values and derivatives, quadrature weights,normal vectors, etc, exclusively at a finite set of points. The FEValues classis the one that brings the three components together and provides this finiteset of information on a particular cell  [2.x.54] . You will see it in action when weassemble the linear system below.
* It is noteworthy that all of this could also be achieved if you simply createdthese three objects yourself in an application program, and juggled theinformation yourself. However, this would neither be simpler (the FEValuesclass provides exactly the kind of information you actually need) nor faster:the FEValues class is highly optimized to only compute on each cell theparticular information you need; if anything can be re-used from the previouscell, then it will do so, and there is a lot of code in that class to makesure things are cached wherever this is advantageous.
* The final piece of this introduction is to mention that after a linearsystem is obtained, it is solved using an iterative solver and thenpostprocessed: we create an output file using the DataOut class that can thenbe visualized using one of the common visualization programs.
*  [2.x.55]  The preceding overview of all the important steps of any finite elementimplementation has its counterpart in deal.II: The library can naturally begrouped into a number of "modules" that cover the basic concepts justoutlined. You can access these modules through the tab at the top of thispage. An overview of the most fundamental groups of concepts is also availableon the [1.x.53].
* 

* [1.x.54][1.x.55]
* 

* Although this is the simplest possible equation you can solve using the finiteelement method, this program shows the basic structure of most finiteelement programs and also serves as the template that almost all of thefollowing programs will essentially follow. Specifically, the main class ofthis program looks like this:
* [1.x.56]
* 
* This follows the object oriented programming mantra of [1.x.57], i.e. we do our best to hide almost all internal details ofthis class in private members that are not accessible to the outside.
* Let's start with the member variables: These follow the building blocks wehave outlined above in the bullet points, namely we need a Triangulation and aDoFHandler object, and a finite element object that describes the kinds ofshape functions we want to use. The second group of objects relate to thelinear algebra: the system matrix and right hand side as well as the solutionvector, and an object that describes the sparsity pattern of the matrix. Thisis all this class needs (and the essentials that any solver for a stationaryPDE requires) and that needs to survive throughout the entire program. Incontrast to this, the FEValues object we need for assembly is only requiredthroughout assembly, and so we create it as a local object in the functionthat does that and destroy it again at its end.
* Secondly, let's look at the member functions. These, as well, already form thecommon structure that almost all following tutorial programs will use: [2.x.56]    [2.x.57]   [2.x.58] : This is what one could call a       [1.x.58]. As its name suggests, it sets up the       object that stores the triangulation. In later examples, it could also       deal with boundary conditions, geometries, etc.   [2.x.59]   [2.x.60] : This then is the function in which all the       other data structures are set up that are needed to solve the       problem. In particular, it will initialize the DoFHandler object and       correctly size the various objects that have to do with the linear       algebra. This function is often separated from the preprocessing       function above because, in a time dependent program, it may be called       at least every few time steps whenever the mesh       is adaptively refined (something we will see how to do in  [2.x.61] ). On       the other hand, setting up the mesh itself in the preprocessing       function above is done only once at the beginning of the program and       is, therefore, separated into its own function.   [2.x.62]   [2.x.63] : This, then is where the contents of the       matrix and right hand side are computed, as discussed at length in the       introduction above. Since doing something with this linear system is       conceptually very different from computing its entries, we separate it       from the following function.   [2.x.64]   [2.x.65] : This then is the function in which we compute the       solution  [2.x.66]  of the linear system  [2.x.67] . In the current program, this       is a simple task since the matrix is so simple, but it will become a       significant part of a program's size whenever the problem is not so       trivial any more (see, for example,  [2.x.68] ,  [2.x.69] , or  [2.x.70]  once       you've learned a bit more about the library).   [2.x.71]   [2.x.72] : Finally, when you have computed a       solution, you probably want to do something with it. For example, you       may want to output it in a format that can be visualized, or you may       want to compute quantities you are interested in: say, heat fluxes in a       heat exchanger, air friction coefficients of a wing, maximum bridge       loads, or simply the value of the numerical solution at a point. This       function is therefore the place for postprocessing your solution. [2.x.73] All of this is held together by the single public function (other than theconstructor), namely the  [2.x.74]  function. It is the one that iscalled from the place where an object of this type is created, and it is theone that calls all the other functions in their proper order. Encapsulatingthis operation into the  [2.x.75]  function, rather than calling allthe other functions from  [2.x.76]  makes sure that youcan change how the separation of concerns within this class isimplemented. For example, if one of the functions becomes too big, you cansplit it up into two, and the only places you have to be concerned aboutchanging as a consequence are within this very same class, and not anywhereelse.
* As mentioned above, you will see this general structure &mdash; sometimes withvariants in spelling of the functions' names, but in essentially this order ofseparation of functionality &mdash; again in many of thefollowing tutorial programs.
* 

* [1.x.59][1.x.60]
* 

* deal.II defines a number of integral %types via alias in namespace  [2.x.77] (In the previous sentence, the word "integral" is used as the [1.x.61]that corresponds to the noun "integer". It shouldn't be confused with the[1.x.62] "integral" that represents the area or volume under a curveor surface. The adjective "integral" is widely used in the C++ world incontexts such as "integral type", "integral constant", etc.)In particular, in this program you will see  [2.x.78]  in a couple ofplaces: an integer type that is used to denote the [1.x.63] index of adegree of freedom, i.e., the index of a particular degree of freedom within theDoFHandler object that is defined on top of a triangulation (as opposed to theindex of a particular degree of freedom within a particular cell). For thecurrent program (as well as almost all of the tutorial programs), you will havea few thousand to maybe a few million unknowns globally (and, for  [2.x.79] elements, you will have 4 [1.x.64] in 2d and 8 in 3d).Consequently, a data type that allows to store sufficiently large numbers forglobal DoF indices is  [2.x.80]  given that it allows to storenumbers between 0 and slightly more than 4 billion (on most systems, whereintegers are 32-bit). In fact, this is what  [2.x.81]  is.
* So, why not just use  [2.x.82]  right away? deal.II used to dothis until version 7.3. However, deal.II supports very large computations (viathe framework discussed in  [2.x.83] ) that may have more than 4 billion unknownswhen spread across a few thousand processors. Consequently, there aresituations where  [2.x.84]  is not sufficiently large and weneed a 64-bit unsigned integral type. To make this possible, we introduced [2.x.85]  which by default is defined as simply <code>unsignedint</code> whereas it is possible to define it as <code>unsigned long longint</code> if necessary, by passing a particular flag during configuration(see the ReadMe file).
* This covers the technical aspect. But there is also a documentation purpose:everywhere in the library and codes that are built on it, if you see a placeusing the data type  [2.x.86]  you immediately know that thequantity that is being referenced is, in fact, a global dof index. No suchmeaning would be apparent if we had just used  [2.x.87]  (whichmay also be a local index, a boundary indicator, a material id,etc.). Immediately knowing what a variable refers to also helps avoid errors:it's quite clear that there must be a bug if you see an object of type [2.x.88]  being assigned to variable of type [2.x.89]  even though they are both represented by unsignedintegers and the compiler will, consequently, not complain.
* In more practical terms what the presence of this type means is that duringassembly, we create a  [2.x.90]  matrix (in 2d, using a  [2.x.91]  element) of thecontributions of the cell we are currently sitting on, and then we need to addthe elements of this matrix to the appropriate elements of the global (system)matrix. For this, we need to get at the global indices of the degrees offreedom that are local to the current cell, for which we will always use thefollowing piece of the code:
* [1.x.65]
* where  [2.x.92]  is declared as
* [1.x.66]
* The name of this variable might be a bit of a misnomer
* 
*  -  it stands for "theglobal indices of those degrees of freedom locally defined on the currentcell"
* 
*  -  but variables that hold this information are universally named thisway throughout the library.
*  [2.x.93]   [2.x.94]  is not the only type defined in this namespace.Rather, there is a whole family, including  [2.x.95]  [2.x.96]  and  [2.x.97]  All of these are alias for integerdata types but, as explained above, they are used throughout the library so that(i) the intent of a variable becomes more easily discerned, and (ii) so that itbecomes possible to change the actual type to a larger one if necessary withouthaving to go through the entire library and figure out whether a particular useof  [2.x.98]  corresponds to, say, a material indicator.
* 

*  [1.x.67] [1.x.68]
*   [1.x.69]  [1.x.70]
* 

* 
*  These include files are already known to you. They declare the classes which handle triangulations and enumeration of degrees of freedom:
* 

* 
* [1.x.71]
* 
*  And this is the file in which the functions are declared that create grids:
* 

* 
* [1.x.72]
* 
*  This file contains the description of the Lagrange interpolation finite element:
* 

* 
* [1.x.73]
* 
*  And this file is needed for the creation of sparsity patterns of sparse matrices, as shown in previous examples:
* 

* 
* [1.x.74]
* 
*  The next two files are needed for assembling the matrix using quadrature on each cell. The classes declared in them will be explained below:
* 

* 
* [1.x.75]
* 
*  The following three include files we need for the treatment of boundary values:
* 

* 
* [1.x.76]
* 
*  We're now almost to the end. The second to last group of include files is for the linear algebra which we employ to solve the system of equations arising from the finite element discretization of the Laplace equation. We will use vectors and full matrices for assembling the system of equations locally on each cell, and transfer the results into a sparse matrix. We will then use a Conjugate Gradient solver to solve the problem, for which we need a preconditioner (in this program, we use the identity preconditioner which does nothing, but we need to include the file anyway):
* 

* 
* [1.x.77]
* 
*  Finally, this is for output to a file and to the console:
* 

* 
* [1.x.78]
* 
*  ...and this is to import the deal.II namespace into the global scope:
* 

* 
* [1.x.79]
* 
*   [1.x.80]  [1.x.81]
* 

* 
*  Instead of the procedural programming of previous examples, we encapsulate everything into a class for this program. The class consists of functions which each perform certain aspects of a finite element program, a `main` function which controls what is done first and what is done next, and a list of member variables.
* 

* 
*  The public part of the class is rather short: it has a constructor and a function `run` that is called from the outside and acts as something like the `main` function: it coordinates which operations of this class shall be run in which order. Everything else in the class, i.e. all the functions that actually do anything, are in the private section of the class:
* 

* 
* [1.x.82]
* 
*  Then there are the member functions that mostly do what their names suggest and whose have been discussed in the introduction already. Since they do not need to be called from outside, they are made private to this class.
* 

* 
*  

* 
* [1.x.83]
* 
*  And finally we have some member variables. There are variables describing the triangulation and the global numbering of the degrees of freedom (we will specify the exact polynomial degree of the finite element in the constructor of this class)...
* 

* 
* [1.x.84]
* 
*  ...variables for the sparsity pattern and values of the system matrix resulting from the discretization of the Laplace equation...
* 

* 
* [1.x.85]
* 
*  ...and variables which will hold the right hand side and solution vectors.
* 

* 
* [1.x.86]
* 
*   [1.x.87]  [1.x.88]
* 

* 
*  Here comes the constructor. It does not much more than first to specify that we want bi-linear elements (denoted by the parameter to the finite element object, which indicates the polynomial degree), and to associate the dof_handler variable to the triangulation we use. (Note that the triangulation isn't set up with a mesh at all at the present time, but the DoFHandler doesn't care: it only wants to know which triangulation it will be associated with, and it only starts to care about an actual mesh once you try to distribute degree of freedom on the mesh using the distribute_dofs() function.) All the other member variables of the Step3 class have a default constructor which does all we want.
* 

* 
* [1.x.89]
* 
*   [1.x.90]  [1.x.91]
* 

* 
*  Now, the first thing we've got to do is to generate the triangulation on which we would like to do our computation and number each vertex with a degree of freedom. We have seen these two steps in  [2.x.99]  and  [2.x.100]  before, respectively.
* 

* 
*  This function does the first part, creating the mesh.  We create the grid and refine all cells five times. Since the initial grid (which is the square  [2.x.101] ) consists of only one cell, the final grid has 32 times 32 cells, for a total of 1024.
* 

* 
*  Unsure that 1024 is the correct number? We can check that by outputting the number of cells using the  [2.x.102]  function on the triangulation.
* 

* 
* [1.x.92]
* 
* 

* 
*  [2.x.103]  We call the  [2.x.104]  function, rather than  [2.x.105]  Here, [1.x.93] means the cells that aren't refined any further. We stress the adjective "active" since there are more cells, namely the parent cells of the finest cells, their parents, etc, up to the one cell which made up the initial grid. Of course, on the next coarser level, the number of cells is one quarter that of the cells on the finest level, i.e. 256, then 64, 16, 4, and 1. If you called  [2.x.106]  instead in the code above, you would consequently get a value of 1365 instead. On the other hand, the number of cells (as opposed to the number of active cells) is not typically of much interest, so there is no good reason to print it.
* 

* 
*  
*  
*  [1.x.94]  [1.x.95]
* 

* 
*  Next we enumerate all the degrees of freedom and set up matrix and vector objects to hold the system data. Enumerating is done by using  [2.x.107]  as we have seen in the  [2.x.108]  example. Since we use the FE_Q class and have set the polynomial degree to 1 in the constructor, i.e. bilinear elements, this associates one degree of freedom with each vertex. While we're at generating output, let us also take a look at how many degrees of freedom are generated:
* 

* 
* [1.x.96]
* 
*  There should be one DoF for each vertex. Since we have a 32 times 32 grid, the number of DoFs should be 33 times 33, or 1089.
* 

* 
*  As we have seen in the previous example, we set up a sparsity pattern by first creating a temporary structure, tagging those entries that might be nonzero, and then copying the data over to the SparsityPattern object that can then be used by the system matrix.
* 

* 
* [1.x.97]
* 
*  Note that the SparsityPattern object does not hold the values of the matrix, it only stores the places where entries are. The entries themselves are stored in objects of type SparseMatrix, of which our variable system_matrix is one.   
*   The distinction between sparsity pattern and matrix was made to allow several matrices to use the same sparsity pattern. This may not seem relevant here, but when you consider the size which matrices can have, and that it may take some time to build the sparsity pattern, this becomes important in large-scale problems if you have to store several matrices in your program.
* 

* 
* [1.x.98]
* 
*  The last thing to do in this function is to set the sizes of the right hand side vector and the solution vector to the right values:
* 

* 
* [1.x.99]
* 
*   [1.x.100]  [1.x.101]
* 

* 
*  
*   The next step is to compute the entries of the matrix and right hand side that form the linear system from which we compute the solution. This is the central function of each finite element program and we have discussed the primary steps in the introduction already.
* 

* 
*  The general approach to assemble matrices and vectors is to loop over all cells, and on each cell compute the contribution of that cell to the global matrix and right hand side by quadrature. The point to realize now is that we need the values of the shape functions at the locations of quadrature points on the real cell. However, both the finite element shape functions as well as the quadrature points are only defined on the reference cell. They are therefore of little help to us, and we will in fact hardly ever query information about finite element shape functions or quadrature points from these objects directly.
* 

* 
*  Rather, what is required is a way to map this data from the reference cell to the real cell. Classes that can do that are derived from the Mapping class, though one again often does not have to deal with them directly: many functions in the library can take a mapping object as argument, but when it is omitted they simply resort to the standard bilinear Q1 mapping. We will go this route, and not bother with it for the moment (we come back to this in  [2.x.109] ,  [2.x.110] , and  [2.x.111] ).
* 

* 
*  So what we now have is a collection of three classes to deal with: finite element, quadrature, and mapping objects. That's too much, so there is one type of class that orchestrates information exchange between these three: the FEValues class. If given one instance of each three of these objects (or two, and an implicit linear mapping), it will be able to provide you with information about values and gradients of shape functions at quadrature points on a real cell.
* 

* 
*  Using all this, we will assemble the linear system for this problem in the following function:
* 

* 
* [1.x.102]
* 
*  Ok, let's start: we need a quadrature formula for the evaluation of the integrals on each cell. Let's take a Gauss formula with two quadrature points in each direction, i.e. a total of four points since we are in 2D. This quadrature formula integrates polynomials of degrees up to three exactly (in 1D). It is easy to check that this is sufficient for the present problem:
* 

* 
* [1.x.103]
* 
*  And we initialize the object which we have briefly talked about above. It needs to be told which finite element we want to use, and the quadrature points and their weights (jointly described by a Quadrature object). As mentioned, we use the implied Q1 mapping, rather than specifying one ourselves explicitly. Finally, we have to tell it what we want it to compute on each cell: we need the values of the shape functions at the quadrature points (for the right hand side  [2.x.112] ), their gradients (for the matrix entries  [2.x.113] ), and also the weights of the quadrature points and the determinants of the Jacobian transformations from the reference cell to the real cells.   
*   This list of what kind of information we actually need is given as a collection of flags as the third argument to the constructor of FEValues. Since these values have to be recomputed, or updated, every time we go to a new cell, all of these flags start with the prefix  [2.x.114]  and then indicate what it actually is that we want updated. The flag to give if we want the values of the shape functions computed is #update_values; for the gradients it is #update_gradients. The determinants of the Jacobians and the quadrature weights are always used together, so only the products (Jacobians times weights, or short  [2.x.115] ) are computed; since we need them, we have to list #update_JxW_values as well:
* 

* 
* [1.x.104]
* 
*  The advantage of this approach is that we can specify what kind of information we actually need on each cell. It is easily understandable that this approach can significantly speed up finite element computations, compared to approaches where everything, including second derivatives, normal vectors to cells, etc are computed on each cell, regardless of whether they are needed or not.   
*  

* 
*  [2.x.116]  The syntax <code>update_values | update_gradients | update_JxW_values</code> is not immediately obvious to anyone not used to programming bit operations in C for years already. First,  [2.x.117]  is the [1.x.105], i.e., it takes two integer arguments that are interpreted as bit patterns and returns an integer in which every bit is set for which the corresponding bit is set in at least one of the two arguments. For example, consider the operation  [2.x.118]  (where the prefix  [2.x.119]  indicates that the number is to be interpreted as a binary number) and  [2.x.120] . Going through each bit and seeing whether it is set in one of the argument, we arrive at  [2.x.121]  or, in decimal notation,  [2.x.122] . The second piece of information you need to know is that the various  [2.x.123]  flags are all integers that have [1.x.106]. For example, assume that  [2.x.124] ,  [2.x.125] ,  [2.x.126] . Then <code>update_values | update_gradients | update_JxW_values = 0b10011 = 19</code>. In other words, we obtain a number that [1.x.107], where each operation corresponds to exactly one bit in the integer that, if equal to one, means that a particular piece should be updated on each cell and, if it is zero, means that we need not compute it. In other words, even though  [2.x.127]  is the [1.x.108], what it really represents is [1.x.109]. Such binary masks are quite common in C programming, but maybe not so in higher level languages like C++, but serve the current purpose quite well.
* 

* 
*  For use further down below, we define a shortcut for a value that will be used very frequently. Namely, an abbreviation for the number of degrees of freedom on each cell (since we are in 2D and degrees of freedom are associated with vertices only, this number is four, but we rather want to write the definition of this variable in a way that does not preclude us from later choosing a different finite element that has a different number of degrees of freedom per cell, or work in a different space dimension).   
*   In general, it is a good idea to use a symbolic name instead of hard-coding these numbers even if you know them, since for example, you may want to change the finite element at some time. Changing the element would have to be done in a different function and it is easy to forget to make a corresponding change in another part of the program. It is better to not rely on your own calculations, but instead ask the right object for the information: Here, we ask the finite element to tell us about the number of degrees of freedom per cell and we will get the correct number regardless of the space dimension or polynomial degree we may have chosen elsewhere in the program.   
*   The shortcut here, defined primarily to discuss the basic concept and not because it saves a lot of typing, will then make the following loops a bit more readable. You will see such shortcuts in many places in larger programs, and `dofs_per_cell` is one that is more or less the conventional name for this kind of object.
* 

* 
* [1.x.110]
* 
*  Now, we said that we wanted to assemble the global matrix and vector cell-by-cell. We could write the results directly into the global matrix, but this is not very efficient since access to the elements of a sparse matrix is slow. Rather, we first compute the contribution of each cell in a small matrix with the degrees of freedom on the present cell, and only transfer them to the global matrix when the computations are finished for this cell. We do the same for the right hand side vector. So let's first allocate these objects (these being local objects, all degrees of freedom are coupling with all others, and we should use a full matrix object rather than a sparse one for the local operations; everything will be transferred to a global sparse matrix later on):
* 

* 
* [1.x.111]
* 
*  When assembling the contributions of each cell, we do this with the local numbering of the degrees of freedom (i.e. the number running from zero through dofs_per_cell-1). However, when we transfer the result into the global matrix, we have to know the global numbers of the degrees of freedom. When we query them, we need a scratch (temporary) array for these numbers (see the discussion at the end of the introduction for the type,  [2.x.128]  used here):
* 

* 
* [1.x.112]
* 
*  Now for the loop over all cells. We have seen before how this works for a triangulation. A DoFHandler has cell iterators that are exactly analogous to those of a Triangulation, but with extra information about the degrees of freedom for the finite element you're using. Looping over the active cells of a degree-of-freedom handler works the same as for a triangulation.   
*   Note that we declare the type of the cell as `const auto &` instead of `auto` this time around. In step 1, we were modifying the cells of the triangulation by flagging them with refinement indicators. Here we're only examining the cells without modifying them, so it's good practice to declare `cell` as `const` in order to enforce this invariant.
* 

* 
* [1.x.113]
* 
*  We are now sitting on one cell, and we would like the values and gradients of the shape functions be computed, as well as the determinants of the Jacobian matrices of the mapping between reference cell and true cell, at the quadrature points. Since all these values depend on the geometry of the cell, we have to have the FEValues object re-compute them on each cell:
* 

* 
* [1.x.114]
* 
*  Next, reset the local cell's contributions to global matrix and global right hand side to zero, before we fill them:
* 

* 
* [1.x.115]
* 
*  Now it is time to start integration over the cell, which we do by looping over all quadrature points, which we will number by q_index.
* 

* 
* [1.x.116]
* 
*  First assemble the matrix: For the Laplace problem, the matrix on each cell is the integral over the gradients of shape function i and j. Since we do not integrate, but rather use quadrature, this is the sum over all quadrature points of the integrands times the determinant of the Jacobian matrix at the quadrature point times the weight of this quadrature point. You can get the gradient of shape function  [2.x.129]  at quadrature point with number q_index by using  [2.x.130] ; this gradient is a 2-dimensional vector (in fact it is of type Tensor [2.x.131]  with here dim=2) and the product of two such vectors is the scalar product, i.e. the product of the two shape_grad function calls is the dot product. This is in turn multiplied by the Jacobian determinant and the quadrature point weight (that one gets together by the call to  [2.x.132]  ). Finally, this is repeated for all shape functions  [2.x.133]  and  [2.x.134] :
* 

* 
* [1.x.117]
* 
*  We then do the same thing for the right hand side. Here, the integral is over the shape function i times the right hand side function, which we choose to be the function with constant value one (more interesting examples will be considered in the following programs).
* 

* 
* [1.x.118]
* 
*  Now that we have the contribution of this cell, we have to transfer it to the global matrix and right hand side. To this end, we first have to find out which global numbers the degrees of freedom on this cell have. Let's simply ask the cell for that information:
* 

* 
* [1.x.119]
* 
*  Then again loop over all shape functions i and j and transfer the local elements to the global matrix. The global numbers can be obtained using local_dof_indices[i]:
* 

* 
* [1.x.120]
* 
*  And again, we do the same thing for the right hand side vector.
* 

* 
* [1.x.121]
* 
*  Now almost everything is set up for the solution of the discrete system. However, we have not yet taken care of boundary values (in fact, Laplace's equation without Dirichlet boundary values is not even uniquely solvable, since you can add an arbitrary constant to the discrete solution). We therefore have to do something about the situation.   
*   For this, we first obtain a list of the degrees of freedom on the boundary and the value the shape function shall have there. For simplicity, we only interpolate the boundary value function, rather than projecting it onto the boundary. There is a function in the library which does exactly this:  [2.x.135]  Its parameters are (omitting parameters for which default values exist and that we don't care about): the DoFHandler object to get the global numbers of the degrees of freedom on the boundary; the component of the boundary where the boundary values shall be interpolated; the boundary value function itself; and the output object.   
*   The component of the boundary is meant as follows: in many cases, you may want to impose certain boundary values only on parts of the boundary. For example, you may have inflow and outflow boundaries in fluid dynamics, or clamped and free parts of bodies in deformation computations of bodies. Then you will want to denote these different parts of the boundary by indicators, and tell the interpolate_boundary_values function to only compute the boundary values on a certain part of the boundary (e.g. the clamped part, or the inflow boundary). By default, all boundaries have a 0 boundary indicator, unless otherwise specified. If sections of the boundary have different boundary conditions, you have to number those parts with different boundary indicators. The function call below will then only determine boundary values for those parts of the boundary for which the boundary indicator is in fact the zero specified as the second argument.   
*   The function describing the boundary values is an object of type Function or of a derived class. One of the derived classes is  [2.x.136]  which describes (not unexpectedly) a function which is zero everywhere. We create such an object in-place and pass it to the  [2.x.137]  function.   
*   Finally, the output object is a list of pairs of global degree of freedom numbers (i.e. the number of the degrees of freedom on the boundary) and their boundary values (which are zero here for all entries). This mapping of DoF numbers to boundary values is done by the  [2.x.138]  class.
* 

* 
* [1.x.122]
* 
*  Now that we got the list of boundary DoFs and their respective boundary values, let's use them to modify the system of equations accordingly. This is done by the following function call:
* 

* 
* [1.x.123]
* 
*   [1.x.124]  [1.x.125]
* 

* 
*  The following function simply solves the discretized equation. As the system is quite a large one for direct solvers such as Gauss elimination or LU decomposition, we use a Conjugate Gradient algorithm. You should remember that the number of variables here (only 1089) is a very small number for finite element computations, where 100.000 is a more usual number.  For this number of variables, direct methods are no longer usable and you are forced to use methods like CG.
* 

* 
* [1.x.126]
* 
*  First, we need to have an object that knows how to tell the CG algorithm when to stop. This is done by using a SolverControl object, and as stopping criterion we say: stop after a maximum of 1000 iterations (which is far more than is needed for 1089 variables; see the results section to find out how many were really used), and stop if the norm of the residual is below  [2.x.139] . In practice, the latter criterion will be the one which stops the iteration:
* 

* 
* [1.x.127]
* 
*  Then we need the solver itself. The template parameter to the SolverCG class is the type of the vectors, and leaving the empty angle brackets would indicate that we are taking the default argument (which is  [2.x.140] ). However, we explicitly mention the template argument:
* 

* 
* [1.x.128]
* 
*  Now solve the system of equations. The CG solver takes a preconditioner as its fourth argument. We don't feel ready to delve into this yet, so we tell it to use the identity operation as preconditioner:
* 

* 
* [1.x.129]
* 
*  Now that the solver has done its job, the solution variable contains the nodal values of the solution function.
* 

* 
* [1.x.130]
* 
*   [1.x.131]  [1.x.132]
* 

* 
*  The last part of a typical finite element program is to output the results and maybe do some postprocessing (for example compute the maximal stress values at the boundary, or the average flux across the outflow, etc). We have no such postprocessing here, but we would like to write the solution to a file.
* 

* 
* [1.x.133]
* 
*  To write the output to a file, we need an object which knows about output formats and the like. This is the DataOut class, and we need an object of that type:
* 

* 
* [1.x.134]
* 
*  Now we have to tell it where to take the values from which it shall write. We tell it which DoFHandler object to use, and the solution vector (and the name by which the solution variable shall appear in the output file). If we had more than one vector which we would like to look at in the output (for example right hand sides, errors per cell, etc) we would add them as well:
* 

* 
* [1.x.135]
* 
*  After the DataOut object knows which data it is to work on, we have to tell it to process them into something the back ends can handle. The reason is that we have separated the frontend (which knows about how to treat DoFHandler objects and data vectors) from the back end (which knows many different output formats) and use an intermediate data format to transfer data from the front- to the backend. The data is transformed into this intermediate format by the following function:
* 

* 
* [1.x.136]
* 
*  Now we have everything in place for the actual output. Just open a file and write the data into it, using VTK format (there are many other functions in the DataOut class we are using here that can write the data in postscript, AVS, GMV, Gnuplot, or some other file formats):
* 

* 
* [1.x.137]
* 
*   [1.x.138]  [1.x.139]
* 

* 
*  Finally, the last function of this class is the main function which calls all the other functions of the  [2.x.141]  class. The order in which this is done resembles the order in which most finite element programs work. Since the names are mostly self-explanatory, there is not much to comment about:
* 

* 
* [1.x.140]
* 
*   [1.x.141]  [1.x.142]
* 

* 
*  This is the main function of the program. Since the concept of a main function is mostly a remnant from the pre-object oriented era before C++ programming, it often does not do much more than creating an object of the top-level class and calling its principle function.
* 

* 
*  Finally, the first line of the function is used to enable output of some diagnostics that deal.II can generate.  The  [2.x.142]  variable (which stands for deal-log, not de-allog) represents a stream to which some parts of the library write output. For example, iterative solvers will generate diagnostics (starting residual, number of solver steps, final residual) as can be seen when running this tutorial program.
* 

* 
*  The output of  [2.x.143]  can be written to the console, to a file, or both. Both are disabled by default since over the years we have learned that a program should only generate output when a user explicitly asks for it. But this can be changed, and to explain how this can be done, we need to explain how  [2.x.144]  works: When individual parts of the library want to log output, they open a "context" or "section" into which this output will be placed. At the end of the part that wants to write output, one exits this section again. Since a function may call another one from within the scope where this output section is open, output may in fact be nested hierarchically into these sections. The LogStream class of which  [2.x.145]  is a variable calls each of these sections a "prefix" because all output is printed with this prefix at the left end of the line, with prefixes separated by colons. There is always a default prefix called "DEAL" (a hint at deal.II's history as the successor of a previous library called "DEAL" and from which the LogStream class is one of the few pieces of code that were taken into deal.II).
* 

* 
*  By default,  [2.x.146]  only outputs lines with zero prefixes
* 
*  -  i.e., all output is disabled because the default "DEAL" prefix is always there. But one can set a different maximal number of prefixes for lines that should be output to something larger, and indeed here we set it to two by calling  [2.x.147]  This means that for all screen output, a context that has pushed one additional prefix beyond the default "DEAL" is allowed to print its output to the screen ("console"), whereas all further nested sections that would have three or more prefixes active would write to  [2.x.148]  but  [2.x.149]  does not forward this output to the screen. Thus, running this example (or looking at the "Results" section), you will see the solver statistics prefixed with "DEAL:CG", which is two prefixes. This is sufficient for the context of the current program, but you will see examples later on (e.g., in  [2.x.150] ) where solvers are nested more deeply and where you may get useful information by setting the depth even higher.
* 

* 
* [1.x.143]
* [1.x.144][1.x.145]
* 

* The output of the program looks as follows:
* [1.x.146]
* 
* The first two lines is what we wrote to  [2.x.151] . The lasttwo lines were generated without our intervention by the CGsolver. The first two lines state the residual at the start of theiteration, while the last line tells us that the solver needed 47iterations to bring the norm of the residual to 5.3e-13, i.e. belowthe threshold 1e-12 which we have set in the `solve' function. We willshow in the next program how to suppress this output, which issometimes useful for debugging purposes, but often clutters up thescreen display.
* Apart from the output shown above, the program generated the file [2.x.152] , which is in the VTK format that is widelyused by many visualization programs today
* 
*  -  including the twoheavy-weights [1.x.147] and[1.x.148] that are the mostcommonly used programs for this purpose today.
* Using VisIt, it is not very difficult to generate a picture of thesolution like this: [2.x.153] It shows both the solution and the mesh, elevated above the  [2.x.154] - [2.x.155]  planebased on the value of the solution at each point. Of course the solutionhere is not particularly exciting, but that is a result of both what theLaplace equation represents and the right hand side  [2.x.156]  wehave chosen for this program: The Laplace equation describes (among manyother uses) the vertical deformation of a membrane subject to an external(also vertical) force. In the current example, the membrane's bordersare clamped to a square frame with no vertical variation; a constantforce density will therefore intuitively lead to a membrane thatsimply bulges upward
* 
*  -  like the one shown above.
* VisIt and Paraview both allow playing with various kinds of visualizationsof the solution. Several video lectures show how to use these programs. [2.x.157] 
* 

* 
* [1.x.149][1.x.150][1.x.151]
* 

* If you want to play around a little bit with this program, here are a fewsuggestions: [2.x.158] 
*  [2.x.159]    [2.x.160]   Change the geometry and mesh: In the program, we have generated a square  domain and mesh by using the  [2.x.161]   function. However, the  [2.x.162]  has a good number of other  functions as well. Try an L-shaped domain, a ring, or other domains you find  there.   [2.x.163] 
*    [2.x.164]   Change the boundary condition: The code uses the  [2.x.165]   function to generate zero boundary conditions. However, you may want to try  non-zero constant boundary values using   [2.x.166]  instead of   [2.x.167]  to have unit Dirichlet boundary  values. More exotic functions are described in the documentation of the  Functions namespace, and you may pick one to describe your particular boundary  values.   [2.x.168] 
*    [2.x.169]  Modify the type of boundary condition: Presently, what happens  is that we use Dirichlet boundary values all around, since the  default is that all boundary parts have boundary indicator zero, and  then we tell the   [2.x.170]  function to  interpolate boundary values to zero on all boundary components with  indicator zero.   [2.x.171]  We can change this behavior if we assign parts  of the boundary different indicators. For example, try this  immediately after calling  [2.x.172]  
* [1.x.152]
* 
*   What this does is it first asks the triangulation to  return an iterator that points to the first active cell. Of course,  this being the coarse mesh for the triangulation of a square, the  triangulation has only a single cell at this moment, and it is  active. Next, we ask the cell to return an iterator to its first  face, and then we ask the face to reset the boundary indicator of  that face to 1. What then follows is this: When the mesh is refined,  faces of child cells inherit the boundary indicator of their  parents, i.e. even on the finest mesh, the faces on one side of the  square have boundary indicator 1. Later, when we get to  interpolating boundary conditions, the   [2.x.173]  call will only produce boundary  values for those faces that have zero boundary indicator, and leave  those faces alone that have a different boundary indicator. What  this then does is to impose Dirichlet boundary conditions on the  former, and homogeneous Neumann conditions on the latter (i.e. zero  normal derivative of the solution, unless one adds additional terms  to the right hand side of the variational equality that deal with  potentially non-zero Neumann conditions). You will see this if you  run the program.
*   An alternative way to change the boundary indicator is to label  the boundaries based on the Cartesian coordinates of the face centers.  For example, we can label all of the cells along the top and  bottom boundaries with a boundary indicator 1 by checking to  see if the cell centers' y-coordinates are within a tolerance  (here 1e-12) of
* 
*  -  and 1. Try this immediately after calling   [2.x.174]  as before: 
* [1.x.153]
*   Although this code is a bit longer than before, it is useful for  complex geometries, as it does not require knowledge of face labels.
*    [2.x.175]   A slight variation of the last point would be to set different boundary  values as above, but then use a different boundary value function for  boundary indicator one. In practice, what you have to do is to add a second  call to  [2.x.176]  for boundary indicator one: 
* [1.x.154]
*   If you have this call immediately after the first one to this function, then  it will interpolate boundary values on faces with boundary indicator 1 to the  unit value, and merge these interpolated values with those previously  computed for boundary indicator 0. The result will be that we will get  discontinuous boundary values, zero on three sides of the square, and one on  the fourth.
*    [2.x.177]   Observe convergence: We will only discuss computing errors in norms in   [2.x.178] , but it is easy to check that computations converge  already here. For example, we could evaluate the value of the solution in a  single point and compare the value for different %numbers of global  refinement (the number of global refinement steps is set in   [2.x.179]  above). To evaluate the  solution at a point, say at  [2.x.180] , we could add the  following code to the  [2.x.181]  function: 
* [1.x.155]
*   For 1 through 9 global refinement steps, we then get the following sequence  of point values:   [2.x.182]   By noticing that the difference between each two consecutive values reduces  by about a factor of 4, we can conjecture that the "correct" value may be   [2.x.183] . In fact, if we assumed this to be  the correct value, we could show that the sequence above indeed shows  [2.x.184]  convergence &mdash; theoretically, the convergence order should be   [2.x.185]  but the symmetry of the domain and the mesh may lead  to the better convergence order observed.
*   A slight variant of this would be to repeat the test with quadratic  elements. All you need to do is to set the polynomial degree of the finite  element to two in the constructor   [2.x.186] .
*    [2.x.187] Convergence of the mean: A different way to see that the solution  actually converges (to something &mdash; we can't tell whether it's really  the correct value!) is to compute the mean of the solution. To this end, add  the following code to  [2.x.188] : 
* [1.x.156]
*   The documentation of the function explains what the second and fourth  parameters mean, while the first and third should be obvious. Doing the same  study again where we change the number of global refinement steps, we get  the following result:   [2.x.189]   Again, the difference between two adjacent values goes down by about a  factor of four, indicating convergence as  [2.x.190] . [2.x.191] 
* 

* 
* [1.x.157][1.x.158]
* 

* %HDF5 is a commonly used format that can be read by many scriptinglanguages (e.g. R or Python). It is not difficult to get deal.II toproduce some %HDF5 files that can then be used in external scripts topostprocess some of the data generated by this program. Here are someideas on what is possible.
* 

* [1.x.159][1.x.160]
* 

* To fully make use of the automation we first need to introduce a private variable for the number ofglobal refinement steps  [2.x.192] , which will be used for the output filename.In  [2.x.193]  with
* [1.x.161]
* The deal.II library has two different %HDF5 bindings, one in the HDF5namespace (for interfacing to general-purpose data files)and another one in DataOut (specifically for writing files for thevisualization of solutions).Although the HDF5 deal.II binding supports both serial and MPI, the %HDF5 DataOut bindingonly supports parallel output.For this reason we need to initialize an MPIcommunicator with only one processor. This is done by adding the following code.
* [1.x.162]
* Next we change the  [2.x.194]  output routine asdescribed in the DataOutBase namespace documentation:
* [1.x.163]
* The resulting file can then be visualized just like the VTK file thatthe original version of the tutorial produces; but, since %HDF5 is amore general file format, it can also easily be processed in scriptinglanguages for other purposes.
* 

* [1.x.164][1.x.165]
* 

* After outputting the solution, the file can be opened again to includemore datasets.  This allows us to keep all the necessary informationof our experiment in a single result file, which can then be read andprocessed by some postprocessing script.(Have a look at  [2.x.195]  for furtherinformation on the possible output options.)
* To make this happen, we first include the necessary header into our file:
* [1.x.166]
* Adding the following lines to the endof our output routine adds the information about the value of thesolution at a particular point, as well as the mean value of thesolution, to our %HDF5 file:
* [1.x.167]
* 
* 

* 
* [1.x.168][1.x.169]
* 

* The data put into %HDF5 files above can then be used from scriptinglanguages for further postprocessing. In the following, let us showhow this can, in particular, be done with the[1.x.170], a widely used language in statistical dataanalysis. (Similar things can also be done in Python, for example.)If you are unfamiliar with R and ggplot2 you could check out the data carpentry course on R[1.x.171].Furthermore, since most search engines struggle with searches of the form "R + topic",we recommend using the specializes service [1.x.172] instead.
* The most prominent difference between R and other languages is thatthe assignment operator (`a = 5`) is typically written as`a <- 5`. As the latter is considered standard we will use it in our examples as well.To open the `.h5` file in R you have to install the [1.x.173] package, which is a part of the Bioconductor package.
* First we will include all necessary packages and have a look at how the data is structured in our file.
* [1.x.174]
* This gives the following output
* [1.x.175]
* The datasets can be accessed by  [2.x.196] . The function [2.x.197]  gives us the dimensions of the matrixthat is used to store our cells.We can see the following three matrices, as well as the twoadditional data points we added. [2.x.198]  [2.x.199]   [2.x.200] : a 4x1024 matrix that stores the  (C++) vertex indices for each cell [2.x.201]   [2.x.202] : a 2x1089 matrix storing the position values (x,y) for our cell vertices [2.x.203]   [2.x.204] : a 1x1089 matrix storing the values of our solution at each vertex [2.x.205] Now we can use this data to generate various plots. Plotting with ggplot2 usually splits into two steps.At first the data needs to be manipulated and added to a  [2.x.206] .After that, a  [2.x.207]  object is constructed and manipulated by adding plot elements to it.
*  [2.x.208]  contain all the information we need to plot our grid.The following code wraps all the data into one dataframe for plotting our grid:
* [1.x.176]
* 
* With the finished dataframe we have everything we need to plot our grid:
* [1.x.177]
* 
* The contents of this file then look as follows (not very exciting, butyou get the idea): [2.x.209] 
* We can also visualize the solution itself, and this is going to lookmore interesting.To make a 2D pseudocolor plot of our solution we will use  [2.x.210] .This function needs a structured grid, i.e. uniform in x and y directions.Luckily our data at this point is structured in the right way.The following code plots a pseudocolor representation of our surface into a new PDF:
* [1.x.178]
* This is now going to look as follows: [2.x.211] 
* For plotting the converge curves we need to re-run the C++ code multiple times with different values for  [2.x.212] starting from 1.Since every file only contains a single data point we need to loop over them and concatenate the results into a single vector.
* [1.x.179]
* As we are not interested in the values themselves but rather in the error compared to a "exact" solution we willassume our highest refinement level to be that solution and omit it from the data.
* [1.x.180]
* Now we have all the data available to generate our plots.It is often useful to plot errors on a log-log scale, which isaccomplished in the following code:
* [1.x.181]
* This results in the following plot that shows how the errors in themean value and the solution value at the chosen point nicely convergeto zero: [2.x.213] 
* 

* [1.x.182][1.x.183] [2.x.214] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-31_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34][1.x.35][1.x.36][1.x.37][1.x.38][1.x.39][1.x.40][1.x.41][1.x.42][1.x.43][1.x.44][1.x.45][1.x.46][1.x.47][1.x.48][1.x.49][1.x.50]
*  [2.x.2] 
* [1.x.51]
* 

* [1.x.52][1.x.53][1.x.54]
* 

* [1.x.55][1.x.56]
* 

* This program deals with an interesting physical problem: how does afluid (i.e., a liquid or gas) behave if it experiences differences inbuoyancy caused by temperature differences? It is clear that thoseparts of the fluid that are hotter (and therefore lighter) are goingto rise up and those that are cooler (and denser) are going to sinkdown with gravity.
* In cases where the fluid moves slowly enough such that inertial effectscan be neglected, the equations that describe such behavior are theBoussinesq equations that read as follows:[1.x.57]
* These equations fall into the class of vector-valued problems (atoplevel overview of this topic can be found in the  [2.x.3]  module).Here,  [2.x.4]  is the velocity field,  [2.x.5]  the pressure, and  [2.x.6] the temperature of the fluid.  [2.x.7]  is the symmetricgradient of the velocity. As can be seen, velocity and pressuresolve a Stokes equation describing the motion of an incompressiblefluid, an equation we have previously considered in  [2.x.8] ; wewill draw extensively on the experience we have gained in that program, inparticular with regard to efficient linear Stokes solvers.
* The forcing term of the fluid motion is the buoyancy of thefluid, expressed as the product of the density  [2.x.9] , the thermal expansioncoefficient  [2.x.10] ,the temperature  [2.x.11]  and the gravity vector  [2.x.12]  pointingdownward. (A derivation of why the right hand side looks like it looksis given in the introduction of  [2.x.13] .)While the first two equations describe how the fluid reacts totemperature differences by moving around, the third equation stateshow the fluid motion affects the temperature field: it is an advectiondiffusion equation, i.e., the temperature is attached to the fluidparticles and advected along in the flow field, with an additionaldiffusion (heat conduction) term. In many applications, the diffusioncoefficient is fairly small, and the temperature equation is in facttransport, not diffusion dominated and therefore in character more hyperbolicthan elliptic; we will have to take this into account when developing a stablediscretization.
* In the equations above, the term  [2.x.14]  on the right hand side denotes theheat sources and may be a spatially and temporally varying function.  [2.x.15] and  [2.x.16]  denote the viscosity and diffusivity coefficients, which we assumeconstant for this tutorial program. The more general case when  [2.x.17]  depends onthe temperature is an important factor in physical applications: Most materialsbecome more fluid as they get hotter (i.e.,  [2.x.18]  decreases with  [2.x.19] );sometimes, as in the case of rock minerals at temperatures close to theirmelting point,  [2.x.20]  may change by orders of magnitude over the typical rangeof temperatures.
* We note that the Stokes equation above could be nondimensionalized byintroducing the [1.x.58]  [2.x.21]  using atypical length scale  [2.x.22] , typical temperature difference  [2.x.23] , density [2.x.24] , thermal diffusivity  [2.x.25] , and thermal conductivity  [2.x.26] . [2.x.27]  is a dimensionless number that describes the ratio of heattransport due to convection induced by buoyancy changes fromtemperature differences, and of heat transport due to thermaldiffusion. A small Rayleigh number implies that buoyancy is not strongrelative to viscosity and fluid motion  [2.x.28]  is slow enough sothat heat diffusion  [2.x.29]  is the dominant heat transportterm. On the other hand, a fluid with a high Rayleigh number will showvigorous convection that dominates heat conduction.
* For most fluids for which we are interested in computing thermalconvection, the Rayleigh number is very large, often  [2.x.30]  orlarger. From the structure of the equations, we see that this willlead to large pressure differences and large velocities. Consequently,the convection term in the convection-diffusion equation for  [2.x.31]  willalso be very large and an accurate solution of this equation willrequire us to choose small time steps. Problems with large Rayleighnumbers are therefore hard to solve numerically for similar reasonsthat make solving the [1.x.59] hard to solve when the [1.x.60] is large.
* Note that a large Rayleigh number does not necessarily involve largevelocities in absolute terms. For example, the Rayleigh number in theearth mantle is larger than  [2.x.32] . Yet thevelocities are small: the material is in fact solid rock but it is sohot and under pressure that it can flow very slowly, on the order ofat most a few centimeters per year. Nevertheless, this can lead tomixing over time scales of many million years, a time scale muchshorter than for the same amount of heat to be distributed by thermalconductivity and a time scale of relevance to affect the evolution of theearth's interior and surface structure.
*  [2.x.33]  If you are interested in using the program as the basis for your ownexperiments, you will also want to take a look at its continuation in [2.x.34] . Furthermore,  [2.x.35]  later was developed into the much larger opensource code ASPECT (see https://aspect.geodynamics.org/ ) that can solve realisticproblems and that you may want to investigate before trying to morph  [2.x.36] into something that can solve whatever you want to solve.
* 

* [1.x.61][1.x.62]
* 

* Since the Boussinesq equations are derived under the assumption that inertiaof the fluid's motion does not play a role, the flow field is at each timeentirely determined by buoyancy difference at that time, not by the flow fieldat previous times. This is reflected by the fact that the first two equationsabove are the steady state Stokes equation that do not contain a timederivative. Consequently, we do not need initial conditions for eithervelocities or pressure. On the other hand, the temperature field does satisfyan equation with a time derivative, so we need initial conditions for  [2.x.37] .
* As for boundary conditions: if  [2.x.38]  then the temperaturesatisfies a second order differential equation that requiresboundary data all around the boundary for all times. These can either be aprescribed boundary temperature  [2.x.39]  (Dirichlet boundaryconditions), or a prescribed thermal flux  [2.x.40] ; in this program, we will use an insulated boundarycondition, i.e., prescribe no thermal flux:  [2.x.41] .
* Similarly, the velocity field requires us to pose boundary conditions. Thesemay be no-slip no-flux conditions  [2.x.42]  on  [2.x.43]  if the fluidsticks to the boundary, or no normal flux conditions  [2.x.44]  if the fluid can flow along but not across the boundary, or any numberof other conditions that are physically reasonable. In this program, we willuse no normal flux conditions.
* 

* [1.x.63][1.x.64]
* 

* Like the equations solved in  [2.x.45] , we here have asystem of differential-algebraic equations (DAE): with respect to the timevariable, only the temperature equation is a differential equationwhereas the Stokes system for  [2.x.46]  and  [2.x.47]  has notime-derivatives and is therefore of the sort of an algebraicconstraint that has to hold at each time instant. The main differenceto  [2.x.48]  is that the algebraic constraint there was amixed Laplace system of the form[1.x.65]
* where now we have a Stokes system[1.x.66]
* where  [2.x.49]  is an operator similar to theLaplacian  [2.x.50]  applied to a vector field.
* Given the similarity to what we have done in  [2.x.51] ,it may not come as a surprise that we choose a similar approach,although we will have to make adjustments for the change in operatorin the top-left corner of the differential operator.
* 

* [1.x.67][1.x.68]
* 

* The structure of the problem as a DAE allows us to use the same strategy aswe have already used in  [2.x.52] , i.e., we use a time lagscheme: we first solve the temperature equation (using an extrapolatedvelocity field), and then insert the new temperature solution into the righthand side of the velocity equation. The way we implement this in our codelooks at things from a slightly different perspective, though. We firstsolve the Stokes equations for velocity and pressure using the temperaturefield from the previous time step, which means that we get the velocity forthe previous time step. In other words, we first solve the Stokes system fortime step  [2.x.53]  as[1.x.69]
* and then the temperature equation with an extrapolated velocity field totime  [2.x.54] .
* In contrast to  [2.x.55] , we'll use a higher order timestepping scheme here, namely the [1.x.70] that replacesthe time derivative  [2.x.56]  by the (one-sided)difference quotient  [2.x.57] with  [2.x.58]  the time step size. This gives the discretized-in-timetemperature equation[1.x.71]
* Note how the temperature equation is solved semi-explicitly: diffusion istreated implicitly whereas advection is treated explicitly using anextrapolation (or forward-projection) of temperature and velocity, includingthe just-computed velocity  [2.x.59] . The forward-projection tothe current time level  [2.x.60]  is derived from a Taylor expansion,  [2.x.61] . We need this projection formaintaining the order of accuracy of the BDF-2 scheme. In other words, thetemperature fields we use in the explicit right hand side are second orderapproximations of the current temperature field &mdash; not quite anexplicit time stepping scheme, but by character not too far away either.
* The introduction of the temperature extrapolation limits the time step by a[1.x.72] just like it was in  [2.x.62] " [2.x.63] ". (We wouldn't have had that stability condition if we treated theadvection term implicitly since the BDF-2 scheme is A-stable, at the pricethat we needed to build a new temperature matrix at each time step.) We willdiscuss the exact choice of time step in the [1.x.73], but for the moment of importance is that this CFL conditionmeans that the time step size  [2.x.64]  may change from time step to timestep, and that we have to modify the above formula slightly. If [2.x.65]  are the time steps sizes of the current and previous timestep, then we use the approximations
* [1.x.74]
* and
* [1.x.75]
* and above equation is generalized as follows:[1.x.76]
* 
* where  [2.x.66]  denotes the extrapolation of velocity [2.x.67]  and temperature  [2.x.68]  to time level  [2.x.69] , using the valuesat the two previous time steps. That's not an easy to read equation, butwill provide us with the desired higher order accuracy. As a consistencycheck, it is easy to verify that it reduces to the same equation as above if [2.x.70] .
* As a final remark we note that the choice of a higher order timestepping scheme of course forces us to keep more time steps in memory;in particular, we here will need to have  [2.x.71]  around, a vectorthat we could previously discard. This seems like a nuisance that wewere able to avoid previously by using only a first order timestepping scheme, but as we will see below when discussing the topic ofstabilization, we will need this vector anyway and so keeping itaround for time discretization is essentially for free and gives usthe opportunity to use a higher order scheme.
* 

* [1.x.77][1.x.78]
* 

* Like solving the mixed Laplace equations, solving the Stokes equationsrequires us to choose particular pairs of finite elements forvelocities and pressure variables. Because this has already been discussed in [2.x.72] , we only cover this topic briefly:Here, we use thestable pair  [2.x.73] . These are continuouselements, so we can form the weak form of the Stokes equation withoutproblem by integrating by parts and substituting continuous functionsby their discrete counterparts:[1.x.79]
* for all test functions  [2.x.74] . The first term of the firstequation is considered as the inner product between tensors, i.e. [2.x.75] .Because the second tensor in this product is symmetric, theanti-symmetric component of  [2.x.76]  plays no role andit leads to the entirely same form if we use the symmetric gradient of [2.x.77]  instead. Consequently, the formulation we consider andthat we implement is[1.x.80]
* 
* This is exactly the same as what we already discussed in [2.x.78]  and there is not much more to say about this here.
* 

* [1.x.81][1.x.82]
* 

* The more interesting question is what to do with the temperatureadvection-diffusion equation. By default, not all discretizations ofthis equation are equally stable unless we either do something likeupwinding, stabilization, or all of this. One way to achieve this isto use discontinuous elements (i.e., the FE_DGQ class that we used, forexample, in the discretization of the transport equation in [2.x.79] , or in discretizing the pressure in [2.x.80]  and  [2.x.81] ) and to define aflux at the interface between cells that takes into accountupwinding. If we had a pure advection problem this would probably bethe simplest way to go. However, here we have some diffusion as well,and the discretization of the Laplace operator with discontinuouselements is cumbersome because of the significant number of additionalterms that need to be integrated on each face betweencells. Discontinuous elements also have the drawback that the use ofnumerical fluxes introduces an additional numerical diffusion thatacts everywhere, whereas we would really like to minimize the effectof numerical diffusion to a minimum and only apply it where it isnecessary to stabilize the scheme.
* A better alternative is therefore to add some nonlinear viscosity tothe model. Essentially, what this does is to transform the temperatureequation from the form[1.x.83]
* to something like[1.x.84]
* where  [2.x.82]  is an addition viscosity (diffusion) term that onlyacts in the vicinity of shocks and other discontinuities.  [2.x.83]  ischosen in such a way that if  [2.x.84]  satisfies the original equations, theadditional viscosity is zero.
* To achieve this, the literature contains a number of approaches. Wewill here follow one developed by Guermond and Popov that builds on asuitably defined residual and a limiting procedure for the additionalviscosity. To this end, let us define a residual  [2.x.85]  as follows:[1.x.85]
* where we will later choose the stabilization exponent  [2.x.86]  fromwithin the range  [2.x.87] . Note that  [2.x.88]  will be zero if  [2.x.89] satisfies the temperature equation, since then the term in parentheseswill be zero. Multiplying terms out, we get the following, entirelyequivalent form:[1.x.86]
* 
* With this residual, we can now define the artificial viscosity asa piecewise constant function defined on each cell  [2.x.90]  with diameter [2.x.91]  separately asfollows:[1.x.87]
* 
* Here,  [2.x.92]  is a stabilization constant (a dimensional analysisreveals that it is unitless and therefore independent of scaling; we willdiscuss its choice in the [1.x.88]) and [2.x.93]  is a normalization constant that must have units [2.x.94] . We will choose it as [2.x.95] ,where  [2.x.96]  is the range of presenttemperature values (remember that buoyancy is driven by temperaturevariations, not the absolute temperature) and  [2.x.97]  is a dimensionlessconstant. To understand why this method works consider this: If on a particularcell  [2.x.98]  the temperature field is smooth, then we expect the residualto be small there (in fact to be on the order of  [2.x.99] ) andthe stabilization term that injects artificial diffusion will there beof size  [2.x.100]  &mdash; i.e., rather small, just as we hope it tobe when no additional diffusion is necessary. On the other hand, if weare on or close to a discontinuity of the temperature field, then theresidual will be large; the minimum operation in the definition of [2.x.101]  will then ensure that the stabilization has size  [2.x.102] &mdash; the optimal amount of artificial viscosity to ensure stability ofthe scheme.
* Whether or not this scheme really works is a good question.Computations by Guermond and Popov have shown that this form ofstabilization actually performs much better than most of the otherstabilization schemes that are around (for example streamlinediffusion, to name only the simplest one). Furthermore, for  [2.x.103]  they can even prove that it produces better convergence ordersfor the linear transport equation than for example streamlinediffusion. For  [2.x.104] , no theoretical results are currentlyavailable, but numerical tests indicate that the resultsare considerably better than for  [2.x.105] .
* A more practical question is how to introduce this artificialdiffusion into the equations we would like to solve. Note that thenumerical viscosity  [2.x.106]  is temperature-dependent, so the equationwe want to solve is nonlinear in  [2.x.107]  &mdash; not what one desires from asimple method to stabilize an equation, and even less so if we realizethat  [2.x.108]  is nondifferentiable in  [2.x.109] . However, there is noreason to despair: we still have to discretize in time and we cantreat the term explicitly.
* In the definition of the stabilization parameter, we approximate the timederivative by  [2.x.110] . This approximation makes only useof available time data and this is the reason why we need to store data of twoprevious time steps (which enabled us to use the BDF-2 scheme withoutadditional storage cost). We could now simply evaluate the rest of theterms at  [2.x.111] , but then the discrete residual would be nothing else thana backward Euler approximation, which is only first order accurate. So, incase of smooth solutions, the residual would be still of the order  [2.x.112] ,despite the second order time accuracy in the outer BDF-2 scheme and thespatial FE discretization. This is certainly not what we want to have(in fact, we desired to have small residuals in regions where the solutionbehaves nicely), so a bit more care is needed. The key to this problemis to observe that the first derivative as we constructed it is actuallycentered at  [2.x.113] . We get the desired second order accurateresidual calculation if we evaluate all spatial terms at  [2.x.114] by using the approximation  [2.x.115] , which meansthat we calculate the nonlinear viscosity as a function of thisintermediate temperature,  [2.x.116] . Note that thisevaluation of the residual is nothing else than a Crank-Nicholson scheme,so we can be sure that now everything is alright. One might wonder whetherit is a problem that the numerical viscosity now is not evaluated attime  [2.x.117]  (as opposed to the rest of the equation). However, this offsetis uncritical: For smooth solutions,  [2.x.118]  will vary continuously,so the error in time offset is  [2.x.119]  times smaller than the nonlinearviscosity itself, i.e., it is a small higher order contribution that isleft out. That's fine because the term itself is already at the level ofdiscretization error in smooth regions.
* Using the BDF-2 scheme introduced above,this yields for the simpler case of uniform time steps of size  [2.x.120] :[1.x.89]
* On the left side of this equation remains the term from the timederivative and the original (physical) diffusion which we treatimplicitly (this is actually a nice term: the matrices that resultfrom the left hand side are the mass matrix and a multiple of theLaplace matrix &mdash; both are positive definite and if the time stepsize  [2.x.121]  is small, the sum is simple to invert). On the right handside, the terms in the first line result from the time derivative; inthe second line is the artificial diffusion at time  [2.x.122] ; the third line contains theadvection term, and the fourth the sources. Note that theartificial diffusion operates on the extrapolatedtemperature at the current time in the same way as we have discussedthe advection works in the section on time stepping.
* The form for nonuniform time steps that we will have to use inreality is a bit more complicated (which is why we showed the simplerform above first) and reads:[1.x.90]
* 
* After settling all these issues, the weak form follows naturally fromthe strong form shown in the last equation, and we immediately arriveat the weak form of the discretized equations:[1.x.91]
* for all discrete test functions  [2.x.123] . Here, the diffusion term has beenintegrated by parts, and we have used that we will impose no thermal flux, [2.x.124] .
* This then results in amatrix equation of form[1.x.92]
* which given the structure of matrix on the left (the sum of twopositive definite matrices) is easily solved using the ConjugateGradient method.
* 

* 
* [1.x.93][1.x.94]
* 

* As explained above, our approach to solving the joint system forvelocities/pressure on the one hand and temperature on the other is to use anoperator splitting where we first solve the Stokes system for the velocitiesand pressures using the old temperature field, and then solve for the newtemperature field using the just computed velocity field. (A moreextensive discussion of operator splitting methods can be found in  [2.x.125] .)
* 

* [1.x.95][1.x.96]
* 

* Solving the linear equations coming from the Stokes system has beendiscussed in great detail in  [2.x.126] . In particular, inthe results section of that program, we have discussed a number ofalternative linear solver strategies that turned out to be moreefficient than the original approach. The best alternativeidentified there we to use a GMRES solver preconditioned by a blockmatrix involving the Schur complement. Specifically, the Stokesoperator leads to a block structured matrix[1.x.97]
* and as discussed there a good preconditioner is[1.x.98]
* where  [2.x.127]  is the Schur complement of the Stokes operator [2.x.128] . Of course, this preconditioner is not useful because wecan't form the various inverses of matrices, but we can use thefollowing as a preconditioner:[1.x.99]
* where  [2.x.129]  are approximations to the inversematrices. In particular, it turned out that  [2.x.130]  is spectrallyequivalent to the mass matrix and consequently replacing  [2.x.131]  by a CG solver applied to the mass matrix on the pressurespace was a good choice. In a small deviation from  [2.x.132] , wehere have a coefficient  [2.x.133]  in the momentum equation, and by the samederivation as there we should arrive at the conclusion that it is the weightedmass matrix with entries  [2.x.134]  thatwe should be using.
* It was more complicated to come up with a good replacement  [2.x.135] , which corresponds to the discretized symmetric Laplacian ofthe vector-valued velocity field, i.e. [2.x.136] .In  [2.x.137]  we used a sparse LU decomposition (using theSparseDirectUMFPACK class) of  [2.x.138]  for  [2.x.139]  &mdash; theperfect preconditioner &mdash; in 2d, but for 3d memory and computetime is not usually sufficient to actually compute this decomposition;consequently, we only use an incomplete LU decomposition (ILU, usingthe SparseILU class) in 3d.
* For this program, we would like to go a bit further. To this end, notethat the symmetrized bilinear form on vector fields, [2.x.140] is not too far away from the nonsymmetrized version, [2.x.141]  (note that the factor 2 has disappeared in this form). The latter,however, has the advantage that the  [2.x.142]  vector componentsof the test functions are not coupled (well, almost, see below),i.e., the resulting matrix is block-diagonal: one block for each vectorcomponent, and each of these blocks is equal to the Laplace matrix forthis vector component. So assuming we order degrees of freedom in sucha way that first all  [2.x.143] -components of the velocity are numbered, thenthe  [2.x.144] -components, and then the  [2.x.145] -components, then the matrix [2.x.146]  that is associated with this slightly different bilinear form hasthe form[1.x.100]
* where  [2.x.147]  is a Laplace matrix of size equal to the number of shape functionsassociated with each component of the vector-valued velocity. With thismatrix, one could be tempted to define our preconditioner for thevelocity matrix  [2.x.148]  as follows:[1.x.101]
* where  [2.x.149]  is a preconditioner for the Laplace matrix &mdash;something where we know very well how to build good preconditioners!
* In reality, the story is not quite as simple: To make the matrix [2.x.150]  definite, we need to make the individual blocks  [2.x.151]  definite by applying boundary conditions. One can try to do so byapplying Dirichlet boundary conditions all around the boundary, andthen the so-defined preconditioner  [2.x.152]  turns out to be agood preconditioner for  [2.x.153]  if the latter matrix results from a Stokesproblem where we also have Dirichlet boundary conditions on thevelocity components all around the domain, i.e., if we enforce  [2.x.154] .
* Unfortunately, this "if" is an "if and only if": in the program belowwe will want to use no-flux boundary conditions of the form  [2.x.155]  (i.e., flow %parallel to the boundary is allowed,but no flux through the boundary). In this case, it turns out that theblock diagonal matrix defined above is not a good preconditionerbecause it neglects the coupling of components at the boundary. Abetter way to do things is therefore if we build the matrix  [2.x.156] as the vector Laplace matrix  [2.x.157]  and then apply the same boundary conditionas we applied to  [2.x.158] . If this is a Dirichlet boundary condition allaround the domain, the  [2.x.159]  will decouple to three diagonal blocksas above, and if the boundary conditions are of the form  [2.x.160]  then this will introduce a coupling of degrees offreedom at the boundary but only there. This, in fact, turns out to bea much better preconditioner than the one introduced above, and hasalmost all the benefits of what we hoped to get.
* 

* To sum this whole story up, we can observe: [2.x.161]    [2.x.162]  Compared to building a preconditioner from the original matrix  [2.x.163]   resulting from the symmetric gradient as we did in  [2.x.164] ,  we have to expect that the preconditioner based on the Laplace bilinear form  performs worse since it does not take into account the coupling between  vector components.
*    [2.x.165] On the other hand, preconditioners for the Laplace matrix are typically  more mature and perform better than ones for vector problems. For example,  at the time of this writing, Algebraic %Multigrid (AMG) algorithms are very  well developed for scalar problems, but not so for vector problems.
*    [2.x.166] In building this preconditioner, we will have to build up the  matrix  [2.x.167]  and its preconditioner. While this means that we  have to store an additional matrix we didn't need before, the  preconditioner  [2.x.168]  is likely going to need much less  memory than storing a preconditioner for the coupled matrix   [2.x.169] . This is because the matrix  [2.x.170]  has only a third of the  entries per row for all rows corresponding to interior degrees of  freedom, and contains coupling between vector components only on  those parts of the boundary where the boundary conditions introduce  such a coupling. Storing the matrix is therefore comparatively  cheap, and we can expect that computing and storing the  preconditioner  [2.x.171]  will also be much cheaper compared to  doing so for the fully coupled matrix. [2.x.172] 
* 

* 
* [1.x.102][1.x.103]
* 

* This is the easy part: The matrix for the temperature equation has the form [2.x.173] , where  [2.x.174]  are mass and stiffness matrices on thetemperature space, and  [2.x.175]  are constants related the time steppingscheme and the current and previous time step. This being the sum of asymmetric positive definite and a symmetric positive semidefinite matrix, theresult is also symmetric positive definite. Furthermore,  [2.x.176]  isa number proportional to the time step, and so becomes small whenever the meshis fine, damping the effect of the then ill-conditioned stiffness matrix.
* As a consequence, inverting this matrix with the Conjugate Gradient algorithm,using a simple preconditioner, is trivial and very cheap compared to invertingthe Stokes matrix.
* 

* 
* [1.x.104][1.x.105]
* 

* [1.x.106][1.x.107]
* 

* One of the things worth explaining up front about the program below is the useof two different DoFHandler objects. If one looks at the structure of theequations above and the scheme for their solution, one realizes that there islittle commonality that keeps the Stokes part and the temperature parttogether. In all previous tutorial programs in which we have discussed  [2.x.177]  "vector-valued problems" we have always only used a singlefinite element with several vector components, and a single DoFHandler object.Sometimes, we have substructured the resulting matrix into blocks tofacilitate particular solver schemes; this was, for example, the case in the [2.x.178]  program for the Stokes equations upon which the currentprogram is based.
* We could of course do the same here. The linear system that we would get wouldlook like this:[1.x.108]
* The problem with this is: We never use the whole matrix at the same time. Infact, it never really exists at the same time: As explained above,  [2.x.179]  and [2.x.180]  depend on the already computed solution  [2.x.181] , in the first case throughthe time step (that depends on  [2.x.182]  because it has to satisfy a CFLcondition). So we can only assemble it once we've already solved the top left [2.x.183]  block Stokes system, and once we've moved on to the temperatureequation we don't need the Stokes part any more; the fact that webuild an object for a matrix that never exists as a whole in memory atany given time led us to jumping through some hoops in  [2.x.184] , solet's not repeat this sort of error. Furthermore, we don'tactually build the matrix  [2.x.185] : Because by the time we get to the temperatureequation we already know  [2.x.186] , and because we have to assemble the right handside  [2.x.187]  at this time anyway, we simply move the term  [2.x.188]  to the righthand side and assemble it along with all the other terms there. What thismeans is that there does not remain a part of the matrix where temperaturevariables and Stokes variables couple, and so a global enumeration of alldegrees of freedom is no longer important: It is enough if we have anenumeration of all Stokes degrees of freedom, and of all temperature degreesof freedom independently.
* In essence, there is consequently not much use in putting [1.x.109]into a block matrix (though there are of course the same good reasons to do sofor the  [2.x.189]  Stokes part), or, for that matter, in putting everythinginto the same DoFHandler object.
* But are there [1.x.110] to doing so? These exist, though they may notbe obvious at first. The main problem is that if we need to create one globalfinite element that contains velocity, pressure, and temperature shapefunctions, and use this to initialize the DoFHandler. But we also use thisfinite element object to initialize all FEValues or FEFaceValues objects thatwe use. This may not appear to be that big a deal, but imagine what happenswhen, for example, we evaluate the residual [2.x.190] that we need to compute the artificial viscosity  [2.x.191] .  Forthis, we need the Laplacian of the temperature, which we compute using thetensor of second derivatives (Hessians) of the shape functions (we have togive the  [2.x.192]  flag to the FEValues object forthis). Now, if we have a finite that contains the shape functions forvelocities, pressures, and temperatures, that means that we have to computethe Hessians of [1.x.111] shape functions, including the many higher ordershape functions for the velocities. That's a lot of computations that we don'tneed, and indeed if one were to do that (as we had in an early version of theprogram), assembling the right hand side took about a quarter of the overallcompute time.
* So what we will do is to use two different finite element objects, one for theStokes components and one for the temperatures. With this come two differentDoFHandlers, two sparsity patterns and two matrices for the Stokes andtemperature parts, etc. And whenever we have to assemble something thatcontains both temperature and Stokes shape functions (in particular the righthand sides of Stokes and temperature equations), then we use two FEValuesobjects initialized with two cell iterators that we walk in %parallel throughthe two DoFHandler objects associated with the same Triangulation object; forthese two FEValues objects, we use of course the same quadrature objects sothat we can iterate over the same set of quadrature points, but each FEValuesobject will get update flags only according to what it actually needs tocompute. In particular, when we compute the residual as above, we only ask forthe values of the Stokes shape functions, but also the Hessians of thetemperature shape functions &mdash; much cheaper indeed, and as it turns out:assembling the right hand side of the temperature equation is now a componentof the program that is hardly measurable.
* With these changes, timing the program yields that only the followingoperations are relevant for the overall run time: [2.x.193]    [2.x.194] Solving the Stokes system: 72% of the run time.   [2.x.195] Assembling the Stokes preconditioner and computing the algebraic      multigrid hierarchy using the Trilinos ML package: 11% of the      run time.   [2.x.196] The function  [2.x.197] : 7%      of overall run time.   [2.x.198] Assembling the Stokes and temperature right hand side vectors as      well as assembling the matrices: 7%. [2.x.199] In essence this means that all bottlenecks apart from the algebraicmultigrid have been removed.
* 

* 
* [1.x.112][1.x.113]
* 

* In much the same way as we used PETSc to support our linear algebra needs in [2.x.200]  and  [2.x.201] , we use interfaces to the [1.x.114] library (see thedeal.II README file for installation instructions) in this program. Trilinosis a very large collection ofeverything that has to do with linear and nonlinear algebra, as well as allsorts of tools around that (and looks like it will grow in many otherdirections in the future as well).
* The main reason for using Trilinos, similar to our exploring PETSc, is that itis a very powerful library that provides a lot more tools than deal.II's ownlinear algebra library. That includes, in particular, the ability to work in%parallel on a cluster, using MPI, and a wider variety of preconditioners. Inthe latter class, one of the most interesting capabilities is the existence ofthe Trilinos ML package that implements an Algebraic Multigrid (AMG)method. We will use this preconditioner to precondition the second orderoperator part of the momentum equation. The ability to solve problems in%parallel will be explored in  [2.x.202] , using the same problem asdiscussed here.
* PETSc, which we have used in  [2.x.203]  and  [2.x.204] , is certainly a powerfullibrary, providing a large number of functions that deal with matrices,vectors, and iterative solvers and preconditioners, along with lots of otherstuff, most of which runs quite well in %parallel. It is, however, a few yearsold already than Trilinos, written in C, and generally not quite as easy touse as some other libraries. As a consequence, deal.II has also acquiredinterfaces to Trilinos, which shares a lot of the same functionality withPETSc. It is, however, a project that is several years younger, is written inC++ and by people who generally have put a significant emphasis on softwaredesign.
* 

* [1.x.115][1.x.116]
* 

* The case we want to solve here is as follows: we solve the Boussinesqequations described above with  [2.x.205] ,i.e., a relatively slow moving fluid that has virtually no thermal diffusiveconductivity and transports heat mainly through convection. On theboundary, we will require no-normal flux for the velocity( [2.x.206] ) and for the temperature( [2.x.207] ). This is one of the cases discussed in theintroduction of  [2.x.208]  and fixes one component of the velocitywhile allowing flow to be %parallel to the boundary. There remain [2.x.209]  components to be fixed, namely the tangential components ofthe normal stress; for these, we choose homogeneous conditions which means thatwe do not have to anything special. Initial conditions are only necessary forthe temperature field, and we choose it to be constant zero.
* The evolution of the problem is then entirely driven by the right hand side [2.x.210]  of the temperature equation, i.e., by heat sources andsinks. Here, we choose a setup invented in advance of a Christmas lecture:real candles are of course prohibited in U.S. class rooms, but virtual onesare allowed. We therefore choose three spherical heat sources unequally spacedclose to the bottom of the domain, imitating three candles. The fluid locatedat these sources, initially at rest, is then heated up and as the temperaturerises gains buoyancy, rising up; more fluid is dragged up and through thesources, leading to three hot plumes that rise up until they are captured bythe recirculation of fluid that sinks down on the outside, replacing the airthat rises due to heating.
* 

*  [1.x.117] [1.x.118]
*   [1.x.119]  [1.x.120]
* 

* 
*  The first step, as always, is to include the functionality of these well-known deal.II library files and some C++ header files.
* 

* 
* [1.x.121]
* 
*  Then we need to include some header files that provide vector, matrix, and preconditioner classes that implement interfaces to the respective Trilinos classes. In particular, we will need interfaces to the matrix and vector classes based on Trilinos as well as Trilinos preconditioners:
* 

* 
* [1.x.122]
* 
*  Finally, here are a few C++ headers that haven't been included yet by one of the aforelisted header files:
* 

* 
* [1.x.123]
* 
*  At the end of this top-matter, we import all deal.II names into the global namespace:
* 

* 
* [1.x.124]
* 
*   [1.x.125]  [1.x.126]
* 

* 
*  Again, the next stage in the program is the definition of the equation data, that is, the various boundary conditions, the right hand sides and the initial condition (remember that we're about to solve a time-dependent system). The basic strategy for this definition is the same as in  [2.x.211] . Regarding the details, though, there are some differences.
* 

* 
*  The first thing is that we don't set any inhomogeneous boundary conditions on the velocity, since as is explained in the introduction we will use no-flux conditions  [2.x.212] . So what is left are  [2.x.213]  conditions for the tangential part of the normal component of the stress tensor,  [2.x.214] ; we assume homogeneous values for these components, i.e., a natural boundary condition that requires no specific action (it appears as a zero term in the right hand side of the weak form).   
*   For the temperature  [2.x.215] , we assume no thermal energy flux, i.e.,  [2.x.216] . This, again, is a boundary condition that does not require us to do anything in particular.   
*   Secondly, we have to set initial conditions for the temperature (no initial conditions are required for the velocity and pressure, since the Stokes equations for the quasi-stationary case we consider here have no time derivatives of the velocity or pressure). Here, we choose a very simple test case, where the initial temperature is zero, and all dynamics are driven by the temperature right hand side.   
*   Thirdly, we need to define the right hand side of the temperature equation. We choose it to be constant within three circles (or spheres in 3d) somewhere at the bottom of the domain, as explained in the introduction, and zero outside.   
*   Finally, or maybe firstly, at the top of this namespace, we define the various material constants we need ( [2.x.217] , density  [2.x.218]  and the thermal expansion coefficient  [2.x.219] ):
* 

* 
* [1.x.127]
* 
*   [1.x.128]  [1.x.129]
* 

* 
*  This section introduces some objects that are used for the solution of the linear equations of the Stokes system that we need to solve in each time step. Many of the ideas used here are the same as in  [2.x.220] , where Schur complement based preconditioners and solvers have been introduced, with the actual interface taken from  [2.x.221]  (in particular the discussion in the "Results" section of  [2.x.222] , in which we introduce alternatives to the direct Schur complement approach). Note, however, that here we don't use the Schur complement to solve the Stokes equations, though an approximate Schur complement (the mass matrix on the pressure space) appears in the preconditioner.
* 

* 
* [1.x.130]
* 
*   [1.x.131]  [1.x.132]
* 

* 
*  This class is an interface to calculate the action of an "inverted" matrix on a vector (using the  [2.x.223]  operation) in the same way as the corresponding class in  [2.x.224] : when the product of an object of this class is requested, we solve a linear equation system with that matrix using the CG method, accelerated by a preconditioner of (templated) class  [2.x.225] .     
*   In a minor deviation from the implementation of the same class in  [2.x.226] , we make the  [2.x.227]  function take any kind of vector type (it will yield compiler errors, however, if the matrix does not allow a matrix-vector product with this kind of vector).     
*   Secondly, we catch any exceptions that the solver may have thrown. The reason is as follows: When debugging a program like this one occasionally makes a mistake of passing an indefinite or nonsymmetric matrix or preconditioner to the current class. The solver will, in that case, not converge and throw a run-time exception. If not caught here it will propagate up the call stack and may end up in  [2.x.228]  where we output an error message that will say that the CG solver failed. The question then becomes: Which CG solver? The one that inverted the mass matrix? The one that inverted the top left block with the Laplace operator? Or a CG solver in one of the several other nested places where we use linear solvers in the current code? No indication about this is present in a run-time exception because it doesn't store the stack of calls through which we got to the place where the exception was generated.     
*   So rather than letting the exception propagate freely up to  [2.x.229]  we realize that there is little that an outer function can do if the inner solver fails and rather convert the run-time exception into an assertion that fails and triggers a call to  [2.x.230] , allowing us to trace back in a debugger how we got to the current place.
* 

* 
* [1.x.133]
* 
*   [1.x.134]  [1.x.135]
* 

* 
*  This is the implementation of the Schur complement preconditioner as described in detail in the introduction. As opposed to  [2.x.231]  and  [2.x.232] , we solve the block system all-at-once using GMRES, and use the Schur complement of the block structured matrix to build a good preconditioner instead.     
*   Let's have a look at the ideal preconditioner matrix  [2.x.233]  described in the introduction. If we apply this matrix in the solution of a linear system, convergence of an iterative GMRES solver will be governed by the matrix [1.x.136] which indeed is very simple. A GMRES solver based on exact matrices would converge in one iteration, since all eigenvalues are equal (any Krylov method takes at most as many iterations as there are distinct eigenvalues). Such a preconditioner for the blocked Stokes system has been proposed by Silvester and Wathen ("Fast iterative solution of stabilised Stokes systems part II.  Using general block preconditioners", SIAM J. Numer. Anal., 31 (1994), pp. 1352-1367).     
*   Replacing  [2.x.234]  by  [2.x.235]  keeps that spirit alive: the product  [2.x.236]  will still be close to a matrix with eigenvalues 1 with a distribution that does not depend on the problem size. This lets us hope to be able to get a number of GMRES iterations that is problem-size independent.     
*   The deal.II users who have already gone through the  [2.x.237]  and  [2.x.238]  tutorials can certainly imagine how we're going to implement this.  We replace the exact inverse matrices in  [2.x.239]  by some approximate inverses built from the InverseMatrix class, and the inverse Schur complement will be approximated by the pressure mass matrix  [2.x.240]  (weighted by  [2.x.241]  as mentioned in the introduction). As pointed out in the results section of  [2.x.242] , we can replace the exact inverse of  [2.x.243]  by just the application of a preconditioner, in this case on a vector Laplace matrix as was explained in the introduction. This does increase the number of (outer) GMRES iterations, but is still significantly cheaper than an exact inverse, which would require between 20 and 35 CG iterations for  [2.x.244] each [2.x.245]  outer solver step (using the AMG preconditioner).     
*   Having the above explanations in mind, we define a preconditioner class with a  [2.x.246]  functionality, which is all we need for the interaction with the usual solver functions further below in the program code.     
*   First the declarations. These are similar to the definition of the Schur complement in  [2.x.247] , with the difference that we need some more preconditioners in the constructor and that the matrices we use here are built upon Trilinos:
* 

* 
* [1.x.137]
* 
*  When using a  [2.x.248]  or a  [2.x.249]  the Vector is initialized using an IndexSet. IndexSet is used not only to resize the  [2.x.250]  but it also associates an index in the  [2.x.251]  with a degree of freedom (see  [2.x.252]  for a more detailed explanation). The function complete_index_set() creates an IndexSet where every valid index is part of the set. Note that this program can only be run sequentially and will throw an exception if used in parallel.
* 

* 
* [1.x.138]
* 
*  Next is the  [2.x.253]  function. We implement the action of  [2.x.254]  as described above in three successive steps.  In formulas, we want to compute  [2.x.255]  where  [2.x.256]  are both vectors with two block components.     
*   The first step multiplies the velocity part of the vector by a preconditioner of the matrix  [2.x.257] , i.e., we compute  [2.x.258] .  The resulting velocity vector is then multiplied by  [2.x.259]  and subtracted from the pressure, i.e., we want to compute  [2.x.260] . This second step only acts on the pressure vector and is accomplished by the residual function of our matrix classes, except that the sign is wrong. Consequently, we change the sign in the temporary pressure vector and finally multiply by the inverse pressure mass matrix to get the final pressure vector, completing our work on the Stokes preconditioner:
* 

* 
* [1.x.139]
* 
*   [1.x.140]  [1.x.141]
* 

* 
*  The definition of the class that defines the top-level logic of solving the time-dependent Boussinesq problem is mainly based on the  [2.x.261]  tutorial program. The main differences are that now we also have to solve for the temperature equation, which forces us to have a second DoFHandler object for the temperature variable as well as matrices, right hand sides, and solution vectors for the current and previous time steps. As mentioned in the introduction, all linear algebra objects are going to use wrappers of the corresponding Trilinos functionality.   
*   The member functions of this class are reminiscent of  [2.x.262] , where we also used a staggered scheme that first solve the flow equations (here the Stokes equations, in  [2.x.263]  Darcy flow) and then update the advected quantity (here the temperature, there the saturation). The functions that are new are mainly concerned with determining the time step, as well as the proper size of the artificial viscosity stabilization.   
*   The last three variables indicate whether the various matrices or preconditioners need to be rebuilt the next time the corresponding build functions are called. This allows us to move the corresponding  [2.x.264]  into the respective function and thereby keeping our main  [2.x.265]  function clean and easy to read.
* 

* 
* [1.x.142]
* 
*   [1.x.143]  [1.x.144]
* 

* 
*   [1.x.145]  [1.x.146]   
*   The constructor of this class is an extension of the constructor in  [2.x.266] . We need to add the various variables that concern the temperature. As discussed in the introduction, we are going to use  [2.x.267]  (Taylor-Hood) elements again for the Stokes part, and  [2.x.268]  elements for the temperature. However, by using variables that store the polynomial degree of the Stokes and temperature finite elements, it is easy to consistently modify the degree of the elements as well as all quadrature formulas used on them downstream. Moreover, we initialize the time stepping as well as the options for matrix assembly and preconditioning:
* 

* 
* [1.x.147]
* 
*   [1.x.148]  [1.x.149]
* 

* 
*  Starting the real functionality of this class is a helper function that determines the maximum ( [2.x.269] ) velocity in the domain (at the quadrature points, in fact). How it works should be relatively obvious to all who have gotten to this point of the tutorial. Note that since we are only interested in the velocity, rather than using  [2.x.270]  to get the values of the entire Stokes solution (velocities and pressures) we use  [2.x.271]  to extract only the velocities part. This has the additional benefit that we get it as a Tensor<1,dim>, rather than some components in a Vector<double>, allowing us to process it right away using the  [2.x.272]  function to get the magnitude of the velocity.   
*   The only point worth thinking about a bit is how to choose the quadrature points we use here. Since the goal of this function is to find the maximal velocity over a domain by looking at quadrature points on each cell. So we should ask how we should best choose these quadrature points on each cell. To this end, recall that if we had a single  [2.x.273]  field (rather than the vector-valued field of higher order) then the maximum would be attained at a vertex of the mesh. In other words, we should use the QTrapezoid class that has quadrature points only at the vertices of cells.   
*   For higher order shape functions, the situation is more complicated: the maxima and minima may be attained at points between the support points of shape functions (for the usual  [2.x.274]  elements the support points are the equidistant Lagrange interpolation points); furthermore, since we are looking for the maximum magnitude of a vector-valued quantity, we can even less say with certainty where the set of potential maximal points are. Nevertheless, intuitively if not provably, the Lagrange interpolation points appear to be a better choice than the Gauss points.   
*   There are now different methods to produce a quadrature formula with quadrature points equal to the interpolation points of the finite element. One option would be to use the  [2.x.275]  function, reduce the output to a unique set of points to avoid duplicate function evaluations, and create a Quadrature object using these points. Another option, chosen here, is to use the QTrapezoid class and combine it with the QIterated class that repeats the QTrapezoid formula on a number of sub-cells in each coordinate direction. To cover all support points, we need to iterate it  [2.x.276]  times since this is the polynomial degree of the Stokes element in use:
* 

* 
* [1.x.150]
* 
*   [1.x.151]  [1.x.152]
* 

* 
*  Next a function that determines the minimum and maximum temperature at quadrature points inside  [2.x.277]  when extrapolated from the two previous time steps to the current one. We need this information in the computation of the artificial viscosity parameter  [2.x.278]  as discussed in the introduction.   
*   The formula for the extrapolated temperature is  [2.x.279] . The way to compute it is to loop over all quadrature points and update the maximum and minimum value if the current value is bigger/smaller than the previous one. We initialize the variables that store the max and min before the loop over all quadrature points by the smallest and the largest number representable as a double. Then we know for a fact that it is larger/smaller than the minimum/maximum and that the loop over all quadrature points is ultimately going to update the initial value with the correct one.   
*   The only other complication worth mentioning here is that in the first time step,  [2.x.280]  is not yet available of course. In that case, we can only use  [2.x.281]  which we have from the initial temperature. As quadrature points, we use the same choice as in the previous function though with the difference that now the number of repetitions is determined by the polynomial degree of the temperature field.
* 

* 
* [1.x.153]
* 
*   [1.x.154]  [1.x.155]
* 

* 
*  The last of the tool functions computes the artificial viscosity parameter  [2.x.282]  on a cell  [2.x.283]  as a function of the extrapolated temperature, its gradient and Hessian (second derivatives), the velocity, the right hand side  [2.x.284]  all on the quadrature points of the current cell, and various other parameters as described in detail in the introduction.   
*   There are some universal constants worth mentioning here. First, we need to fix  [2.x.285] ; we choose  [2.x.286] , a choice discussed in detail in the results section of this tutorial program. The second is the exponent  [2.x.287] ;  [2.x.288]  appears to work fine for the current program, even though some additional benefit might be expected from choosing  [2.x.289] . Finally, there is one thing that requires special casing: In the first time step, the velocity equals zero, and the formula for  [2.x.290]  is not defined. In that case, we return  [2.x.291] , a choice admittedly more motivated by heuristics than anything else (it is in the same order of magnitude, however, as the value returned for most cells on the second time step).   
*   The rest of the function should be mostly obvious based on the material discussed in the introduction:
* 

* 
* [1.x.156]
* 
*   [1.x.157]  [1.x.158]   
*   This is the function that sets up the DoFHandler objects we have here (one for the Stokes part and one for the temperature part) as well as set to the right sizes the various objects required for the linear algebra in this program. Its basic operations are similar to what we do in  [2.x.292] .   
*   The body of the function first enumerates all degrees of freedom for the Stokes and temperature systems. For the Stokes part, degrees of freedom are then sorted to ensure that velocities precede pressure DoFs so that we can partition the Stokes matrix into a  [2.x.293]  matrix. As a difference to  [2.x.294] , we do not perform any additional DoF renumbering. In that program, it paid off since our solver was heavily dependent on ILU's, whereas we use AMG here which is not sensitive to the DoF numbering. The IC preconditioner for the inversion of the pressure mass matrix would of course take advantage of a Cuthill-McKee like renumbering, but its costs are low compared to the velocity portion, so the additional work does not pay off.   
*   We then proceed with the generation of the hanging node constraints that arise from adaptive grid refinement for both DoFHandler objects. For the velocity, we impose no-flux boundary conditions  [2.x.295]  by adding constraints to the object that already stores the hanging node constraints matrix. The second parameter in the function describes the first of the velocity components in the total dof vector, which is zero here. The variable  [2.x.296]  denotes the boundary indicators for which to set the no flux boundary conditions; here, this is boundary indicator zero.   
*   After having done so, we count the number of degrees of freedom in the various blocks:
* 

* 
* [1.x.159]
* 
*  The next step is to create the sparsity pattern for the Stokes and temperature system matrices as well as the preconditioner matrix from which we build the Stokes preconditioner. As in  [2.x.297] , we choose to create the pattern by using the blocked version of DynamicSparsityPattern.     
*   So, we first release the memory stored in the matrices, then set up an object of type BlockDynamicSparsityPattern consisting of  [2.x.298]  blocks (for the Stokes system matrix and preconditioner) or DynamicSparsityPattern (for the temperature part). We then fill these objects with the nonzero pattern, taking into account that for the Stokes system matrix, there are no entries in the pressure-pressure block (but all velocity vector components couple with each other and with the pressure). Similarly, in the Stokes preconditioner matrix, only the diagonal blocks are nonzero, since we use the vector Laplacian as discussed in the introduction. This operator only couples each vector component of the Laplacian with itself, but not with the other vector components. (Application of the constraints resulting from the no-flux boundary conditions will couple vector components at the boundary again, however.)     
*   When generating the sparsity pattern, we directly apply the constraints from hanging nodes and no-flux boundary conditions. This approach was already used in  [2.x.299] , but is different from the one in early tutorial programs where we first built the original sparsity pattern and only then added the entries resulting from constraints. The reason for doing so is that later during assembly we are going to distribute the constraints immediately when transferring local to global dofs. Consequently, there will be no data written at positions of constrained degrees of freedom, so we can let the  [2.x.300]  function omit these entries by setting the last Boolean flag to  [2.x.301] . Once the sparsity pattern is ready, we can use it to initialize the Trilinos matrices. Since the Trilinos matrices store the sparsity pattern internally, there is no need to keep the sparsity pattern around after the initialization of the matrix.
* 

* 
* [1.x.160]
* 
*  The creation of the temperature matrix (or, rather, matrices, since we provide a temperature mass matrix and a temperature stiffness matrix, that will be added together for time discretization) follows the generation of the Stokes matrix &ndash; except that it is much easier here since we do not need to take care of any blocks or coupling between components. Note how we initialize the three temperature matrices: We only use the sparsity pattern for reinitialization of the first matrix, whereas we use the previously generated matrix for the two remaining reinits. The reason for doing so is that reinitialization from an already generated matrix allows Trilinos to reuse the sparsity pattern instead of generating a new one for each copy. This saves both some time and memory.
* 

* 
* [1.x.161]
* 
*  Lastly, we set the vectors for the Stokes solutions  [2.x.302]  and  [2.x.303] , as well as for the temperatures  [2.x.304] ,  [2.x.305]  and  [2.x.306]  (required for time stepping) and all the system right hand sides to their correct sizes and block structure:
* 

* 
* [1.x.162]
* 
*   [1.x.163]  [1.x.164]   
*   This function assembles the matrix we use for preconditioning the Stokes system. What we need are a vector Laplace matrix on the velocity components and a mass matrix weighted by  [2.x.307]  on the pressure component. We start by generating a quadrature object of appropriate order, the FEValues object that can give values and gradients at the quadrature points (together with quadrature weights). Next we create data structures for the cell matrix and the relation between local and global DoFs. The vectors  [2.x.308]  are going to hold the values of the basis functions in order to faster build up the local matrices, as was already done in  [2.x.309] . Before we start the loop over all active cells, we have to specify which components are pressure and which are velocity.
* 

* 
* [1.x.165]
* 
*  The creation of the local matrix is rather simple. There are only a Laplace term (on the velocity) and a mass matrix weighted by  [2.x.310]  to be generated, so the creation of the local matrix is done in two lines. Once the local matrix is ready (loop over rows and columns in the local matrix on each quadrature point), we get the local DoF indices and write the local information into the global matrix. We do this as in  [2.x.311] , i.e., we directly apply the constraints from hanging nodes locally. By doing so, we don't have to do that afterwards, and we don't also write into entries of the matrix that will actually be set to zero again later when eliminating constraints.
* 

* 
* [1.x.166]
* 
*   [1.x.167]  [1.x.168]   
*   This function generates the inner preconditioners that are going to be used for the Schur complement block preconditioner. Since the preconditioners need only to be regenerated when the matrices change, this function does not have to do anything in case the matrices have not changed (i.e., the flag  [2.x.312]  has the value  [2.x.313] ). Otherwise its first task is to call  [2.x.314]  to generate the preconditioner matrices.   
*   Next, we set up the preconditioner for the velocity-velocity matrix  [2.x.315] . As explained in the introduction, we are going to use an AMG preconditioner based on a vector Laplace matrix  [2.x.316]  (which is spectrally close to the Stokes matrix  [2.x.317] ). Usually, the  [2.x.318]  class can be seen as a good black-box preconditioner which does not need any special knowledge. In this case, however, we have to be careful: since we build an AMG for a vector problem, we have to tell the preconditioner setup which dofs belong to which vector component. We do this using the function  [2.x.319]  a function that generates a set of  [2.x.320]  vectors, where each one has ones in the respective component of the vector problem and zeros elsewhere. Hence, these are the constant modes on each component, which explains the name of the variable.
* 

* 
* [1.x.169]
* 
*  Next, we set some more options of the AMG preconditioner. In particular, we need to tell the AMG setup that we use quadratic basis functions for the velocity matrix (this implies more nonzero elements in the matrix, so that a more robust algorithm needs to be chosen internally). Moreover, we want to be able to control how the coarsening structure is build up. The way the Trilinos smoothed aggregation AMG does this is to look which matrix entries are of similar size as the diagonal entry in order to algebraically build a coarse-grid structure. By setting the parameter  [2.x.321]  to 0.02, we specify that all entries that are more than two percent of size of some diagonal pivots in that row should form one coarse grid point. This parameter is rather ad hoc, and some fine-tuning of it can influence the performance of the preconditioner. As a rule of thumb, larger values of  [2.x.322]  will decrease the number of iterations, but increase the costs per iteration. A look at the Trilinos documentation will provide more information on these parameters. With this data set, we then initialize the preconditioner with the matrix we want it to apply to.     
*   Finally, we also initialize the preconditioner for the inversion of the pressure mass matrix. This matrix is symmetric and well-behaved, so we can chose a simple preconditioner. We stick with an incomplete Cholesky (IC) factorization preconditioner, which is designed for symmetric matrices. We could have also chosen an SSOR preconditioner with relaxation factor around 1.2, but IC is cheaper for our example. We wrap the preconditioners into a  [2.x.323]  pointer, which makes it easier to recreate the preconditioner next time around since we do not have to care about destroying the previously used object.
* 

* 
* [1.x.170]
* 
*   [1.x.171]  [1.x.172]   
*   The time lag scheme we use for advancing the coupled Stokes-temperature system forces us to split up the assembly (and the solution of linear systems) into two step. The first one is to create the Stokes system matrix and right hand side, and the second is to create matrix and right hand sides for the temperature dofs, which depends on the result of the linear system for the velocity.   
*   This function is called at the beginning of each time step. In the first time step or if the mesh has changed, indicated by the  [2.x.324] , we need to assemble the Stokes matrix; on the other hand, if the mesh hasn't changed and the matrix is already available, this is not necessary and all we need to do is assemble the right hand side vector which changes in each time step.   
*   Regarding the technical details of implementation, not much has changed from  [2.x.325] . We reset matrix and vector, create a quadrature formula on the cells, and then create the respective FEValues object. For the update flags, we require basis function derivatives only in case of a full assembly, since they are not needed for the right hand side; as always, choosing the minimal set of flags depending on what is currently needed makes the call to  [2.x.326]  further down in the program more efficient.   
*   There is one thing that needs to be commented &ndash; since we have a separate finite element and DoFHandler for the temperature, we need to generate a second FEValues object for the proper evaluation of the temperature solution. This isn't too complicated to realize here: just use the temperature structures and set an update flag for the basis function values which we need for evaluation of the temperature solution. The only important part to remember here is that the same quadrature formula is used for both FEValues objects to ensure that we get matching information when we loop over the quadrature points of the two objects.   
*   The declarations proceed with some shortcuts for array sizes, the creation of the local matrix and right hand side as well as the vector for the indices of the local dofs compared to the global system.
* 

* 
* [1.x.173]
* 
*  Next we need a vector that will contain the values of the temperature solution at the previous time level at the quadrature points to assemble the source term in the right hand side of the momentum equation. Let's call this vector  [2.x.327] .     
*   The set of vectors we create next hold the evaluations of the basis functions as well as their gradients and symmetrized gradients that will be used for creating the matrices. Putting these into their own arrays rather than asking the FEValues object for this information each time it is needed is an optimization to accelerate the assembly process, see  [2.x.328]  for details.     
*   The last two declarations are used to extract the individual blocks (velocity, pressure, temperature) from the total FE system.
* 

* 
* [1.x.174]
* 
*  Now start the loop over all cells in the problem. We are working on two different DoFHandlers for this assembly routine, so we must have two different cell iterators for the two objects in use. This might seem a bit peculiar, since both the Stokes system and the temperature system use the same grid, but that's the only way to keep degrees of freedom in sync. The first statements within the loop are again all very familiar, doing the update of the finite element data as specified by the update flags, zeroing out the local arrays and getting the values of the old solution at the quadrature points. Then we are ready to loop over the quadrature points on the cell.
* 

* 
* [1.x.175]
* 
*  Next we extract the values and gradients of basis functions relevant to the terms in the inner products. As shown in  [2.x.329]  this helps accelerate assembly.             
*   Once this is done, we start the loop over the rows and columns of the local matrix and feed the matrix with the relevant products. The right hand side is filled with the forcing term driven by temperature in direction of gravity (which is vertical in our example).  Note that the right hand side term is always generated, whereas the matrix contributions are only updated when it is requested by the  [2.x.330]  flag.
* 

* 
* [1.x.176]
* 
*  The last step in the loop over all cells is to enter the local contributions into the global matrix and vector structures to the positions specified in  [2.x.331] .  Again, we let the AffineConstraints class do the insertion of the cell matrix elements to the global matrix, which already condenses the hanging node constraints.
* 

* 
* [1.x.177]
* 
*   [1.x.178]  [1.x.179]   
*   This function assembles the matrix in the temperature equation. The temperature matrix consists of two parts, a mass matrix and the time step size times a stiffness matrix given by a Laplace term times the amount of diffusion. Since the matrix depends on the time step size (which varies from one step to another), the temperature matrix needs to be updated every time step. We could simply regenerate the matrices in every time step, but this is not really efficient since mass and Laplace matrix do only change when we change the mesh. Hence, we do this more efficiently by generating two separate matrices in this function, one for the mass matrix and one for the stiffness (diffusion) matrix. We will then sum up the matrix plus the stiffness matrix times the time step size once we know the actual time step.   
*   So the details for this first step are very simple. In case we need to rebuild the matrix (i.e., the mesh has changed), we zero the data structures, get a quadrature formula and a FEValues object, and create local matrices, local dof indices and evaluation structures for the basis functions.
* 

* 
* [1.x.180]
* 
*  Now, let's start the loop over all cells in the triangulation. We need to zero out the local matrices, update the finite element evaluations, and then loop over the rows and columns of the matrices on each quadrature point, where we then create the mass matrix and the stiffness matrix (Laplace terms times the diffusion  [2.x.332] . Finally, we let the constraints object insert these values into the global matrix, and directly condense the constraints into the matrix.
* 

* 
* [1.x.181]
* 
*   [1.x.182]  [1.x.183]   
*   This function does the second part of the assembly work on the temperature matrix, the actual addition of pressure mass and stiffness matrix (where the time step size comes into play), as well as the creation of the velocity-dependent right hand side. The declarations for the right hand side assembly in this function are pretty much the same as the ones used in the other assembly routines, except that we restrict ourselves to vectors this time. We are going to calculate residuals on the temperature system, which means that we have to evaluate second derivatives, specified by the update flag  [2.x.333] .   
*   The temperature equation is coupled to the Stokes system by means of the fluid velocity. These two parts of the solution are associated with different DoFHandlers, so we again need to create a second FEValues object for the evaluation of the velocity at the quadrature points.
* 

* 
* [1.x.184]
* 
*  Next comes the declaration of vectors to hold the old and older solution values (as a notation for time levels  [2.x.334]  and  [2.x.335] , respectively) and gradients at quadrature points of the current cell. We also declare an object to hold the temperature right hand side values ( [2.x.336] ), and we again use shortcuts for the temperature basis functions. Eventually, we need to find the temperature extrema and the diameter of the computational domain which will be used for the definition of the stabilization parameter (we got the maximal velocity as an input to this function).
* 

* 
* [1.x.185]
* 
*  Now, let's start the loop over all cells in the triangulation. Again, we need two cell iterators that walk in parallel through the cells of the two involved DoFHandler objects for the Stokes and temperature part. Within the loop, we first set the local rhs to zero, and then get the values and derivatives of the old solution functions at the quadrature points, since they are going to be needed for the definition of the stabilization parameters and as coefficients in the equation, respectively. Note that since the temperature has its own DoFHandler and FEValues object we get the entire solution at the quadrature point (which is the scalar temperature field only anyway) whereas for the Stokes part we restrict ourselves to extracting the velocity part (and ignoring the pressure part) by using  [2.x.337] .
* 

* 
* [1.x.186]
* 
*  Next, we calculate the artificial viscosity for stabilization according to the discussion in the introduction using the dedicated function. With that at hand, we can get into the loop over quadrature points and local rhs vector components. The terms here are quite lengthy, but their definition follows the time-discrete system developed in the introduction of this program. The BDF-2 scheme needs one more term from the old time step (and involves more complicated factors) than the backward Euler scheme that is used for the first time step. When all this is done, we distribute the local vector into the global one (including hanging node constraints).
* 

* 
* [1.x.187]
* 
*   [1.x.188]  [1.x.189]   
*   This function solves the linear systems of equations. Following the introduction, we start with the Stokes system, where we need to generate our block Schur preconditioner. Since all the relevant actions are implemented in the class  [2.x.338] , all we have to do is to initialize the class appropriately. What we need to pass down is an  [2.x.339]  object for the pressure mass matrix, which we set up using the respective class together with the IC preconditioner we already generated, and the AMG preconditioner for the velocity-velocity matrix. Note that both  [2.x.340]  and  [2.x.341]  are only pointers, so we use  [2.x.342]  to pass down the actual preconditioner objects.   
*   Once the preconditioner is ready, we create a GMRES solver for the block system. Since we are working with Trilinos data structures, we have to set the respective template argument in the solver. GMRES needs to internally store temporary vectors for each iteration (see the discussion in the results section of  [2.x.343] ) &ndash; the more vectors it can use, the better it will generally perform. To keep memory demands in check, we set the number of vectors to 100. This means that up to 100 solver iterations, every temporary vector can be stored. If the solver needs to iterate more often to get the specified tolerance, it will work on a reduced set of vectors by restarting at every 100 iterations.   
*   With this all set up, we solve the system and distribute the constraints in the Stokes system, i.e., hanging nodes and no-flux boundary condition, in order to have the appropriate solution values even at constrained dofs. Finally, we write the number of iterations to the screen.
* 

* 
* [1.x.190]
* 
*  Once we know the Stokes solution, we can determine the new time step from the maximal velocity. We have to do this to satisfy the CFL condition since convection terms are treated explicitly in the temperature equation, as discussed in the introduction. The exact form of the formula used here for the time step is discussed in the results section of this program.     
*   There is a snatch here. The formula contains a division by the maximum value of the velocity. However, at the start of the computation, we have a constant temperature field (we start with a constant temperature, and it will be nonconstant only after the first time step during which the source acts). Constant temperature means that no buoyancy acts, and so the velocity is zero. Dividing by it will not likely lead to anything good.     
*   To avoid the resulting infinite time step, we ask whether the maximal velocity is very small (in particular smaller than the values we encounter during any of the following time steps) and if so rather than dividing by zero we just divide by a small value, resulting in a large but finite time step.
* 

* 
* [1.x.191]
* 
*  Next we set up the temperature system and the right hand side using the function  [2.x.344] .  Knowing the matrix and right hand side of the temperature equation, we set up a preconditioner and a solver. The temperature matrix is a mass matrix (with eigenvalues around one) plus a Laplace matrix (with eigenvalues between zero and  [2.x.345] ) times a small number proportional to the time step  [2.x.346] . Hence, the resulting symmetric and positive definite matrix has eigenvalues in the range  [2.x.347]  (up to constants). This matrix is only moderately ill conditioned even for small mesh sizes and we get a reasonably good preconditioner by simple means, for example with an incomplete Cholesky decomposition preconditioner (IC) as we also use for preconditioning the pressure mass matrix solver. As a solver, we choose the conjugate gradient method CG. As before, we tell the solver to use Trilinos vectors via the template argument  [2.x.348] . Finally, we solve, distribute the hanging node constraints and write out the number of iterations.
* 

* 
* [1.x.192]
* 
*  At the end of this function, we step through the vector and read out the maximum and minimum temperature value, which we also want to output. This will come in handy when determining the correct constant in the choice of time step as discuss in the results section of this program.
* 

* 
* [1.x.193]
* 
*   [1.x.194]  [1.x.195]   
*   This function writes the solution to a VTK output file for visualization, which is done every tenth time step. This is usually quite a simple task, since the deal.II library provides functions that do almost all the job for us. There is one new function compared to previous examples: We want to visualize both the Stokes solution and the temperature as one data set, but we have done all the calculations based on two different DoFHandler objects. Luckily, the DataOut class is prepared to deal with it. All we have to do is to not attach one single DoFHandler at the beginning and then use that for all added vector, but specify the DoFHandler to each vector separately. The rest is done as in  [2.x.349] . We create solution names (that are going to appear in the visualization program for the individual components). The first  [2.x.350]  components are the vector velocity, and then we have pressure for the Stokes part, whereas temperature is scalar. This information is read out using the DataComponentInterpretation helper class. Next, we actually attach the data vectors with their DoFHandler objects, build patches according to the degree of freedom, which are (sub-) elements that describe the data for visualization programs. Finally, we open a file (that includes the time step number) and write the vtk data into it.
* 

* 
* [1.x.196]
* 
*   [1.x.197]  [1.x.198]   
*   This function takes care of the adaptive mesh refinement. The three tasks this function performs is to first find out which cells to refine/coarsen, then to actually do the refinement and eventually transfer the solution vectors between the two different grids. The first task is simply achieved by using the well-established Kelly error estimator on the temperature (it is the temperature we're mainly interested in for this program, and we need to be accurate in regions of high temperature gradients, also to not have too much numerical diffusion). The second task is to actually do the remeshing. That involves only basic functions as well, such as the  [2.x.351]  that refines those cells with the largest estimated error that together make up 80 per cent of the error, and coarsens those cells with the smallest error that make up for a combined 10 per cent of the error.   
*   If implemented like this, we would get a program that will not make much progress: Remember that we expect temperature fields that are nearly discontinuous (the diffusivity  [2.x.352]  is very small after all) and consequently we can expect that a freely adapted mesh will refine further and further into the areas of large gradients. This decrease in mesh size will then be accompanied by a decrease in time step, requiring an exceedingly large number of time steps to solve to a given final time. It will also lead to meshes that are much better at resolving discontinuities after several mesh refinement cycles than in the beginning.   
*   In particular to prevent the decrease in time step size and the correspondingly large number of time steps, we limit the maximal refinement depth of the mesh. To this end, after the refinement indicator has been applied to the cells, we simply loop over all cells on the finest level and unselect them from refinement if they would result in too high a mesh level.
* 

* 
* [1.x.199]
* 
*  As part of mesh refinement we need to transfer the solution vectors from the old mesh to the new one. To this end we use the SolutionTransfer class and we have to prepare the solution vectors that should be transferred to the new grid (we will lose the old grid once we have done the refinement so the transfer has to happen concurrently with refinement). What we definitely need are the current and the old temperature (BDF-2 time stepping requires two old solutions). Since the SolutionTransfer objects only support to transfer one object per dof handler, we need to collect the two temperature solutions in one data structure. Moreover, we choose to transfer the Stokes solution, too, since we need the velocity at two previous time steps, of which only one is calculated on the fly.     
*   Consequently, we initialize two SolutionTransfer objects for the Stokes and temperature DoFHandler objects, by attaching them to the old dof handlers. With this at place, we can prepare the triangulation and the data vectors for refinement (in this order).
* 

* 
* [1.x.200]
* 
*  Now everything is ready, so do the refinement and recreate the dof structure on the new grid, and initialize the matrix structures and the new vectors in the  [2.x.353]  function. Next, we actually perform the interpolation of the solutions between the grids. We create another copy of temporary vectors for temperature (now corresponding to the new grid), and let the interpolate function do the job. Then, the resulting array of vectors is written into the respective vector member variables.     
*   Remember that the set of constraints will be updated for the new triangulation in the setup_dofs() call.
* 

* 
* [1.x.201]
* 
*  After the solution has been transferred we then enforce the constraints on the transferred solution.
* 

* 
* [1.x.202]
* 
*  For the Stokes vector, everything is just the same &ndash; except that we do not need another temporary vector since we just interpolate a single vector. In the end, we have to tell the program that the matrices and preconditioners need to be regenerated, since the mesh has changed.
* 

* 
* [1.x.203]
* 
*   [1.x.204]  [1.x.205]   
*   This function performs all the essential steps in the Boussinesq program. It starts by setting up a grid (depending on the spatial dimension, we choose some different level of initial refinement and additional adaptive refinement steps, and then create a cube in  [2.x.354]  dimensions and set up the dofs for the first time. Since we want to start the time stepping already with an adaptively refined grid, we perform some pre-refinement steps, consisting of all assembly, solution and refinement, but without actually advancing in time. Rather, we use the vilified  [2.x.355]  statement to jump out of the time loop right after mesh refinement to start all over again on the new mesh beginning at the  [2.x.356]  label. (The use of the  [2.x.357]  is discussed in  [2.x.358] .)   
*   Before we start, we project the initial values to the grid and obtain the first data for the  [2.x.359]  vector. Then, we initialize time step number and time step and start the time loop.
* 

* 
* [1.x.206]
* 
*  The first steps in the time loop are all obvious &ndash; we assemble the Stokes system, the preconditioner, the temperature matrix (matrices and preconditioner do actually only change in case we've remeshed before), and then do the solve. Before going on with the next time step, we have to check whether we should first finish the pre-refinement steps or if we should remesh (every fifth time step), refining up to a level that is consistent with initial refinement and pre-refinement steps. Last in the loop is to advance the solutions, i.e., to copy the solutions to the next "older" time level.
* 

* 
* [1.x.207]
* 
*  Do all the above until we arrive at time 100.
* 

* 
* [1.x.208]
* 
*   [1.x.209]  [1.x.210]
* 

* 
*  The main function looks almost the same as in all other programs.
* 

* 
*  There is one difference we have to be careful about. This program uses Trilinos and, typically, Trilinos is configured so that it can run in %parallel using MPI. This doesn't mean that it [1.x.211] to run in %parallel, and in fact this program (unlike  [2.x.360] ) makes no attempt at all to do anything in %parallel using MPI. Nevertheless, Trilinos wants the MPI system to be initialized. We do that be creating an object of type  [2.x.361]  that initializes MPI (if available) using the arguments given to main() (i.e.,  [2.x.362]  and  [2.x.363] ) and de-initializes it again when the object goes out of scope.
* 

* 
* [1.x.212]
* 
*  This program can only be run in serial. Otherwise, throw an exception.
* 

* 
* [1.x.213]
* [1.x.214][1.x.215]
* 

* [1.x.216][1.x.217]
* 

* When you run the program in 2d, the output will look something likethis:<code><pre>Number of active cells: 256 (on 5 levels)Number of degrees of freedom: 3556 (2178+289+1089)
* Timestep 0:  t=0   Assembling...   Rebuilding Stokes preconditioner...   Solving...   0 GMRES iterations for Stokes subsystem.   Time step: 0.919118   9 CG iterations for temperature.   Temperature range:
* 
*  - .16687 1.30011
* Number of active cells: 280 (on 6 levels)Number of degrees of freedom: 4062 (2490+327+1245)
* Timestep 0:  t=0   Assembling...   Rebuilding Stokes preconditioner...   Solving...   0 GMRES iterations for Stokes subsystem.   Time step: 0.459559   9 CG iterations for temperature.   Temperature range:
* 
*  - .0982971 0.598503
* Number of active cells: 520 (on 7 levels)Number of degrees of freedom: 7432 (4562+589+2281)
* Timestep 0:  t=0   Assembling...   Rebuilding Stokes preconditioner...   Solving...   0 GMRES iterations for Stokes subsystem.   Time step: 0.229779   9 CG iterations for temperature.   Temperature range:
* 
*  - .0551098 0.294493
* Number of active cells: 1072 (on 8 levels)Number of degrees of freedom: 15294 (9398+1197+4699)
* Timestep 0:  t=0   Assembling...   Rebuilding Stokes preconditioner...   Solving...   0 GMRES iterations for Stokes subsystem.   Time step: 0.11489   9 CG iterations for temperature.   Temperature range:
* 
*  - .0273524 0.156861
* Number of active cells: 2116 (on 9 levels)Number of degrees of freedom: 30114 (18518+2337+9259)
* Timestep 0:  t=0   Assembling...   Rebuilding Stokes preconditioner...   Solving...   0 GMRES iterations for Stokes subsystem.   Time step: 0.0574449   9 CG iterations for temperature.   Temperature range:
* 
*  - .014993 0.0738328
* Timestep 1:  t=0.0574449   Assembling...   Solving...   56 GMRES iterations for Stokes subsystem.   Time step: 0.0574449   9 CG iterations for temperature.   Temperature range:
* 
*  - .0273934 0.14488
* ...</pre></code>
* In the beginning we refine the mesh several times adaptively andalways return to time step zero to restart on the newly refinedmesh. Only then do we start the actual time iteration.
* The program runs for a while. The temperature field for time steps 0,500, 1000, 1500, 2000, 3000, 4000, and 5000 looks like this (note thatthe color scale used for the temperature is not always the same):
*  [2.x.364] 
* The visualizations shown here were generated using a version of the examplewhich did not enforce the constraints after transferring the mesh.
* As can be seen, we have three heat sources that heat fluid andtherefore produce a buoyancy effect that lets hots pockets of fluidrise up and swirl around. By a chimney effect, the three streams arepressed together by fluid that comes from the outside and wants tojoin the updraft party. Note that because the fluid is initially atrest, those parts of the fluid that were initially over the sourcesreceive a longer heating time than that fluid that is later draggedover the source by the fully developed flow field. It is thereforehotter, a fact that can be seen in the red tips of the threeplumes. Note also the relatively fine features of the flow field, aresult of the sophisticated transport stabilization of the temperatureequation we have chosen.
* In addition to the pictures above, the following ones show theadaptive mesh and the flow field at the same time steps:
*  [2.x.365] 
* 

* [1.x.218][1.x.219]
* 

* The same thing can of course be done in 3d by changing the templateparameter to the BoussinesqFlowProblem object in  [2.x.366] from 2 to 3, so that the output now looks like follows:
* <code><pre>Number of active cells: 64 (on 3 levels)Number of degrees of freedom: 3041 (2187+125+729)
* Timestep 0:  t=0   Assembling...   Rebuilding Stokes preconditioner...   Solving...   0 GMRES iterations for Stokes subsystem.   Time step: 2.45098   9 CG iterations for temperature.   Temperature range:
* 
*  - .675683 4.94725
* Number of active cells: 288 (on 4 levels)Number of degrees of freedom: 12379 (8943+455+2981)
* Timestep 0:  t=0   Assembling...   Rebuilding Stokes preconditioner...   Solving...   0 GMRES iterations for Stokes subsystem.   Time step: 1.22549   9 CG iterations for temperature.   Temperature range:
* 
*  - .527701 2.25764
* Number of active cells: 1296 (on 5 levels)Number of degrees of freedom: 51497 (37305+1757+12435)
* Timestep 0:  t=0   Assembling...   Rebuilding Stokes preconditioner...   Solving...   0 GMRES iterations for Stokes subsystem.   Time step: 0.612745   10 CG iterations for temperature.   Temperature range:
* 
*  - .496942 0.847395
* Number of active cells: 5048 (on 6 levels)Number of degrees of freedom: 192425 (139569+6333+46523)
* Timestep 0:  t=0   Assembling...   Rebuilding Stokes preconditioner...   Solving...   0 GMRES iterations for Stokes subsystem.   Time step: 0.306373   10 CG iterations for temperature.   Temperature range:
* 
*  - .267683 0.497739
* Timestep 1:  t=0.306373   Assembling...   Solving...   27 GMRES iterations for Stokes subsystem.   Time step: 0.306373   10 CG iterations for temperature.   Temperature range:
* 
*  - .461787 0.958679
* ...</pre></code>
* Visualizing the temperature isocontours at time steps 0,50, 100, 150, 200, 300, 400, 500, 600, 700, and 800 yields thefollowing plots:
*  [2.x.367] 
* That the first picture looks like three hedgehogs stems from the fact that ourscheme essentially projects the source times the first time step size onto themesh to obtain the temperature field in the first time step. Since the sourcefunction is discontinuous, we need to expect over- and undershoots from thisproject. This is in fact what happens (it's easier to check this in 2d) andleads to the crumpled appearance of the isosurfaces.  The visualizations shownhere were generated using a version of the example which did not enforce theconstraints after transferring the mesh.
* 

* 
* [1.x.220][1.x.221]
* 

* The program as is has three parameters that we don't have much of atheoretical handle on how to choose in an optimal way. These are: [2.x.368]    [2.x.369] The time step must satisfy a CFL condition       [2.x.370] . Here,  [2.x.371]  is      dimensionless, but what is the right value?   [2.x.372] In the computation of the artificial viscosity,[1.x.222]
*       with  [2.x.373] .      Here, the choice of the dimensionless %numbers  [2.x.374]  is of      interest. [2.x.375] In all of these cases, we will have to expect that the correct choice of eachvalue depends on that of the others, and most likely also on the spacedimension and polynomial degree of the finite element used for thetemperature. Below we'll discuss a few numerical experiments to chooseconstants  [2.x.376]  and  [2.x.377] .
* Below, we will not discuss the choice of  [2.x.378] . In the program, we setit to  [2.x.379] . The reason for this value is abit complicated and has more to do with the history of the programthan reasoning: while the correct formula for the global scalingparameter  [2.x.380]  is shown above, the program (including theversion shipped with deal.II 6.2) initially had a bug in that wecomputed [2.x.381]  instead, wherewe had set the scaling parameter to one. Since we only computed on theunit square/cube where  [2.x.382] , this wasentirely equivalent to using the correct formula with [2.x.383] . Sincethis value for  [2.x.384]  appears to work just fine for the currentprogram, we corrected the formula in the program and set  [2.x.385]  to avalue that reproduces exactly the results we had before. We will,however, revisit this issue again in  [2.x.386] .
* Now, however, back to the discussion of what values of  [2.x.387]  and [2.x.388]  to choose:
* 

* [1.x.223][1.x.224][1.x.225]
* 

* These two constants are definitely linked in some way. The reason is easy tosee: In the case of a pure advection problem, [2.x.389] , anyexplicit scheme has to satisfy a CFL condition of the form [2.x.390] . On the other hand,for a pure diffusion problem, [2.x.391] ,explicit schemes need to satisfy a condition [2.x.392] . So given the form of  [2.x.393]  above, anadvection diffusion problem like the one we have to solve here will result ina condition of the form [2.x.394] .It follows that we have to face the fact that we might want to choose  [2.x.395] larger to improve the stability of the numerical scheme (by increasing theamount of artificial diffusion), but we have to pay a price in the form ofsmaller, and consequently more time steps. In practice, one would thereforelike to choose  [2.x.396]  as small as possible to keep the transport problemsufficiently stabilized while at the same time trying to choose the time stepas large as possible to reduce the overall amount of work.
* The find the right balance, the only way is to do a few computationalexperiments. Here's what we did: We modified the program slightly to allowless mesh refinement (so we don't always have to wait that long) and to choose [2.x.397]  to eliminate the effect of the constant  [2.x.398]  (we know thatsolutions are stable by using this version of  [2.x.399]  as an artificialviscosity, but that we can improve things
* 
*  -  i.e. make the solutionsharper
* 
*  -  by using the more complicated formula for this artificialviscosity). We then run the programfor different values  [2.x.400]  and observe maximal and minimal temperaturesin the domain. What we expect to see is this: If we choose the time step toobig (i.e. choose a  [2.x.401]  bigger than theoretically allowed) then we will getexponential growth of the temperature. If we choose  [2.x.402]  too small, thenthe transport stabilization becomes insufficient and the solution will showsignificant oscillations but not exponential growth.
* 

* [1.x.226][1.x.227]
* 

* Here is what we get for [2.x.403] , and  [2.x.404] , different choices of  [2.x.405] , andbilinear elements ( [2.x.406] ) in 2d:
*  [2.x.407] 
* The way to interpret these graphs goes like this: for  [2.x.408]  and [2.x.409] , we see exponential growth or at least largevariations, but if we choose [2.x.410] or smaller, then the scheme isstable though a bit wobbly. For more artificial diffusion, we can choose [2.x.411] or smaller for  [2.x.412] , [2.x.413] or smaller for  [2.x.414] , and again need [2.x.415] for  [2.x.416]  (this time because much diffusion requires a small timestep).
* So how to choose? If we were simply interested in a large time step, then wewould go with  [2.x.417]  and [2.x.418] .On the other hand, we're also interested in accuracy and here it may be ofinterest to actually investigate what these curves show. To this end note thatwe start with a zero temperature and that our sources are positive &mdash; sowe would intuitively expect that the temperature can never drop belowzero. But it does, a consequence of Gibb's phenomenon when using continuouselements to approximate a discontinuous solution. We can therefore see thatchoosing  [2.x.419]  too small is bad: too little artificial diffusion leads toover- and undershoots that aren't diffused away. On the other hand, for large [2.x.420] , the minimum temperature drops below zero at the beginning but thenquickly diffuses back to zero.
* On the other hand, let's also look at the maximum temperature. Watching themovie of the solution, we see that initially the fluid is at rest. The sourcekeeps heating the same volume of fluid whose temperature increases linearly atthe beginning until its buoyancy is able to move it upwards. The hottest partof the fluid is therefore transported away from the solution and fluid takingits place is heated for only a short time before being moved out of the sourceregion, therefore remaining cooler than the initial bubble. If  [2.x.421] (in the program it is nonzero but very small) then the hottest part of thefluid should be advected along with the flow with its temperatureconstant. That's what we can see in the graphs with the smallest  [2.x.422] : Oncethe maximum temperature is reached, it hardly changes any more. On the otherhand, the larger the artificial diffusion, the more the hot spot isdiffused. Note that for this criterion, the time step size does not play asignificant role.
* So to sum up, likely the best choice would appear to be  [2.x.423] and  [2.x.424] . The curve isa bit wobbly, but overall pictures looks pretty reasonable with theexception of some over and undershoots close to the start time due toGibb's phenomenon.
* 

* [1.x.228][1.x.229]
* 

* One can repeat the same sequence of experiments for higher orderelements as well. Here are the graphs for bi-quadratic shape functions( [2.x.425] ) for the temperature, while weretain the  [2.x.426]  stable Taylor-Hood element for the Stokes system:
*  [2.x.427] 
* Again, small values of  [2.x.428]  lead to less diffusion but we have tochoose the time step very small to keep things under control. Toolarge values of  [2.x.429]  make for more diffusion, but again requiresmall time steps. The best value would appear to be  [2.x.430] , asfor the  [2.x.431]  element, and then we have to choose [2.x.432]  &mdash; exactlyhalf the size for the  [2.x.433]  element, a fact that may not be surprisingif we state the CFL condition as the requirement that the time step besmall enough so that the distance transport advects in each time stepis no longer than one [1.x.230] away (which for  [2.x.434]  elementsis  [2.x.435] , but for  [2.x.436]  elements is  [2.x.437] ). It turns out that  [2.x.438] needs to be slightly larger for obtaining stable results also late inthe simulation at times larger than 60, so we actually choose it as [2.x.439]  in the code.
* 

* [1.x.231][1.x.232]
* 

* One can repeat these experiments in 3d and find the optimal time stepfor each value of  [2.x.440]  and find the best value of  [2.x.441] . What onefinds is that for the same  [2.x.442]  already used in 2d, the time stepsneeds to be a bit smaller, by around a factor of 1.2 or so. This iseasily explained: the time step restriction is [2.x.443]  where  [2.x.444]  isthe [1.x.233] of the cell. However, what is really needed is thedistance between mesh points, which is  [2.x.445] . So amore appropriate form would be [2.x.446] .
* The second find is that one needs to choose  [2.x.447]  slightly bigger(about  [2.x.448]  or so). This then again reduces the time step wecan take.
* 

* 
* 

* [1.x.234][1.x.235]
* 

* Concluding, from the simple computations above,  [2.x.449]  appears to be agood choice for the stabilization parameter in 2d, and  [2.x.450]  in 3d. Ina dimension independent way, we can model this as  [2.x.451] . If one doeslonger computations (several thousand time steps) on finer meshes, onerealizes that the time step size is not quite small enough and that forstability one will have to reduce the above values a bit more (by about afactor of  [2.x.452] ).
* As a consequence, a formula that reconciles 2d, 3d, and variable polynomialdegree and takes all factors in account reads as follows:[1.x.236]
* In the first form (in the center of the equation),  [2.x.453]  is a universal constant,  [2.x.454] is the factor that accounts for the difference between cell diameterand grid point separation, [2.x.455]  accounts for the increase in  [2.x.456]  with space dimension, [2.x.457]  accounts for the distance between grid points forhigher order elements, and  [2.x.458] for the local speed of transport relative to the cell size. This isthe formula that we use in the program.
* As for the question of whether to use  [2.x.459]  or  [2.x.460]  elements for thetemperature, the following considerations may be useful: First,solving the temperature equation is hardly a factor in the overallscheme since almost the entire compute time goes into solving theStokes system in each time step. Higher order elements for thetemperature equation are therefore not a significant drawback. On theother hand, if one compares the size of the over- and undershoots thesolution produces due to the discontinuous source description, onenotices that for the choice of  [2.x.461]  and  [2.x.462]  as above, the  [2.x.463] solution dips down to around  [2.x.464] , whereas the  [2.x.465]  solution onlygoes to  [2.x.466]  (remember that the exact solution should never becomenegative at all. This means that the  [2.x.467]  solution is significantlymore accurate; the program therefore uses these higher order elements,despite the penalty we pay in terms of smaller time steps.
* 

* [1.x.237][1.x.238]
* 

* There are various ways to extend the current program. Of particular interestis, of course, to make it faster and/or increase the resolution of theprogram, in particular in 3d. This is the topic of the  [2.x.468] tutorial program which will implement strategies to solve this problem in%parallel on a cluster. It is also the basis of the much larger opensource code ASPECT (see https://aspect.geodynamics.org/ ) that can solve realisticproblems and that constitutes the further development of  [2.x.469] .
* Another direction would be to make the fluid flow more realistic. The programwas initially written to simulate various cases simulating the convection ofmaterial in the earth's mantle, i.e. the zone between the outer earth core andthe solid earth crust: there, material is heated from below and cooled fromabove, leading to thermal convection. The physics of this fluid are much morecomplicated than shown in this program, however: The viscosity of mantlematerial is strongly dependent on the temperature, i.e.  [2.x.470] , withthe dependency frequently modeled as a viscosity that is reduced exponentiallywith rising temperature. Secondly, much of the dynamics of the mantle isdetermined by chemical reactions, primarily phase changes of the variouscrystals that make up the mantle; the buoyancy term on the right hand side ofthe Stokes equations then depends not only on the temperature, but also on thechemical composition at a given location which is advected by the flow fieldbut also changes as a function of pressure and temperature. We willinvestigate some of these effects in later tutorial programs as well.
* 

* [1.x.239][1.x.240] [2.x.471] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-32_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] ,  [2.x.2] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34][1.x.35][1.x.36][1.x.37][1.x.38][1.x.39][1.x.40][1.x.41][1.x.42][1.x.43][1.x.44][1.x.45]
*  [2.x.3] 
* [1.x.46][1.x.47][1.x.48][1.x.49][1.x.50]
* 

* [1.x.51][1.x.52][1.x.53]
* 

* This program does pretty much exactly what  [2.x.4]  already does: itsolves the Boussinesq equations that describe the motion of a fluidwhose temperature is not in equilibrium. As such, all the equations wehave described in  [2.x.5]  still hold: we solve the same generalpartial differential equation (with only minor modifications to adjustfor more realism in the problem setting), using the same finiteelement scheme, the same time stepping algorithm, and more or less thesame stabilization method for the temperature advection-diffusionequation. As a consequence, you may first want to understand thatprogram &mdash; and its implementation &mdash; before you work on thecurrent one.
* The difference between  [2.x.6]  and the current program is thathere we want to do things in %parallel, using both the availability of manymachines in a cluster (with parallelization based on MPI) as well as manyprocessor cores within a single machine (with parallelization based onthreads). This program's main job is therefore to introduce the changes that arenecessary to utilize the availability of these %parallel computeresources. In this regard, it builds on the  [2.x.7]  program that firstintroduces the necessary classes for much of the %parallelfunctionality, and on  [2.x.8]  that shows how this is done for avector-valued problem.
* In addition to these changes, we also use a slightly differentpreconditioner, and we will have to make a number of changes that haveto do with the fact that we want to solve a [1.x.54] problemhere, not a model problem. The latter, in particular, will requirethat we think about scaling issues as well as what all thoseparameters and coefficients in the equations under considerationactually mean. We will discuss first the issues that affect changes inthe mathematical formulation and solver structure, then how toparallelize things, and finally the actual testcase we will consider.
* 

* [1.x.55][1.x.56]
* 

* In  [2.x.9] , we used the following Stokes model for thevelocity and pressure field:[1.x.57]
* The right hand side of the first equation appears a wee bitunmotivated. Here's how things should really be. Weneed the external forces that act on the fluid, which we assume aregiven by gravity only. In the current case, we assume that the fluiddoes expand slightly for the purposes of this gravity force, but notenough that we need to modify the incompressibility condition (thesecond equation). What this means is that for the purpose of the righthand side, we can assume that  [2.x.10] . An assumption that maynot be entirely justified is that we can assume that the changes ofdensity as a function of temperature are small, leading to anexpression of the form  [2.x.11] , i.e., the density equals [2.x.12]  at reference temperature and decreases linearly asthe temperature increases (as the material expands). The force balanceequation then looks properly written like this:[1.x.58]
* Now note that the gravity force results from a gravity potential as [2.x.13] , so that we can re-write this as follows:[1.x.59]
* The second term on the right is time independent, and so we couldintroduce a new "dynamic" pressure  [2.x.14] with which the Stokes equations would read:[1.x.60]
* This is exactly the form we used in  [2.x.15] , and it wasappropriate to do so because all changes in the fluid flow are onlydriven by the dynamic pressure that results from temperaturedifferences. (In other words: Any contribution to the right hand sidethat results from taking the gradient of a scalar field have no effecton the velocity field.)
* On the other hand, we will here use the form of the Stokes equationsthat considers the total pressure instead:[1.x.61]
* There are several advantages to this:
* 
*  - This way we can plot the pressure in our program in such a way that it  actually shows the total pressure that includes the effects of  temperature differences as well as the static pressure of the  overlying rocks. Since the pressure does not appear any further in any  of the other equations, whether to use one or the other is more a  matter of taste than of correctness. The flow field is exactly the  same, but we get a pressure that we can now compare with values that  are given in geophysical books as those that hold at the bottom of the  earth mantle, for example.
* 
*  - If we wanted to make the model even more realistic, we would have to take  into account that many of the material parameters (e.g. the viscosity, the  density, etc) not only depend on the temperature but also the  [1.x.62] pressure.
* 
*  - The model above assumed a linear dependence  [2.x.16]  and assumed that  [2.x.17]  is small. In  practice, this may not be so. In fact, realistic models are  certainly not linear, and  [2.x.18]  may also not be small for at least  part of the temperature range because the density's behavior is  substantially dependent not only on thermal expansion but by phase  changes.
* 
*  - A final reason to do this is discussed in the results section and  concerns possible extensions to the model we use here. It has to do  with the fact that the temperature equation (see below) we use here does not  include a term that contains the pressure. It should, however:  rock, like gas, heats up as you compress it. Consequently,  material that rises up cools adiabatically, and cold material that  sinks down heats adiabatically. We discuss this further below.
*  [2.x.19]  There is, however, a downside to this procedure. In the earth,the dynamic pressure is several orders of magnitude smaller than thetotal pressure. If we use the equations above and solve all variablesto, say, 4 digits of accuracy, then we may be able to get the velocityand the total pressure right, but we will have no accuracy at all ifwe compute the dynamic pressure by subtracting from the total pressurethe static part  [2.x.20] . If, for example, the dynamicpressure is six orders of magnitude smaller than the static pressure,then we need to solve the overall pressure to at least seven digits ofaccuracy to get anything remotely accurate. That said, in practicethis turns out not to be a limiting factor.
* 

* 
* [1.x.63][1.x.64]
* 

* Remember that we want to solve the following set of equations:[1.x.65]
* augmented by appropriate boundary and initial conditions. As discussedin  [2.x.21] , we will solve this set of equations bysolving for a Stokes problem first in each time step, and then movingthe temperature equation forward by one time interval.
* The problem under consideration in this current section is with theStokes problem: if we discretize it as usual, we get a linear system[1.x.66]
* which in this program we will solve with a FGMRES solver. This solveriterates until the residual of these linear equations is below acertain tolerance, i.e., until[1.x.67]This does not make any sense from the viewpoint of physical units: thequantities involved here have physical units so that the first part ofthe residual has units  [2.x.22]  (most easily established by considering theterm  [2.x.23]  and considering that thepressure has units  [2.x.24]  andthe integration yields a factor of  [2.x.25] ), whereasthe second part of the residual has units [2.x.26] . Taking the normof this residual vector would yield a quantity with units [2.x.27] . This,quite obviously, does not make sense, and we should not be surprisedthat doing so is eventually going to come back hurting us.
* So why is this an issue here, but not in  [2.x.28] ? Thereason back there is that everything was nicely balanced: velocitieswere on the order of one, the pressure likewise, the viscosity wasone, and the domain had a diameter of  [2.x.29] . As a result, whilenonsensical, nothing bad happened. On the other hand, as we will explainbelow, things here will not be that simply scaled:  [2.x.30]  will be around [2.x.31] , velocities on the order of  [2.x.32] , pressure around  [2.x.33] , andthe diameter of the domain is  [2.x.34] . In other words, the order of magnitudefor the first equation is going to be [2.x.35] , whereas the second equation will be around [2.x.36] . Well, sowhat this will lead to is this: if the solver wants to make the residual small,it will almost entirely focus on the first set of equations because they areso much bigger, and ignore the divergence equation that describes massconservation. That's exactly what happens: unless we set the tolerance toextremely small values, the resulting flow field is definitely not divergencefree. As an auxiliary problem, it turns out that it is difficult to find atolerance that always works; in practice, one often ends up with a tolerancethat requires 30 or 40 iterations for most time steps, and 10,000 for someothers.
* So what's a numerical analyst to do in a case like this? The answer is tostart at the root and first make sure that everything is mathematicallyconsistent first. In our case, this means that if we want to solve the systemof Stokes equations jointly, we have to scale them so that they all have thesame physical dimensions. In our case, this means multiplying the secondequation by something that has units  [2.x.37] ; onechoice is to multiply with  [2.x.38]  where  [2.x.39]  is a typical lengthscalein our domain (which experiments show is best chosen to be the diameter ofplumes &mdash; around 10 km &mdash; rather than the diameter of thedomain). Using these %numbers for  [2.x.40]  and  [2.x.41] , this factor is around [2.x.42] . So, we now get this for the Stokes system:[1.x.68]
* The trouble with this is that the result is not symmetric any more (we have [2.x.43]  at the bottom left, but not its transposeoperator at the top right). This, however, can be cured by introducing ascaled pressure  [2.x.44] , and we get the scaled equations[1.x.69]
* This is now symmetric. Obviously, we can easily recover the original pressure [2.x.45]  from the scaled pressure  [2.x.46]  that we compute as a result of thisprocedure.
* In the program below, we will introduce a factor [2.x.47]  that corresponds to [2.x.48] , and we will use this factor in the assembly of the systemmatrix and preconditioner. Because it is annoying and error prone, we willrecover the unscaled pressure immediately following the solution of the linearsystem, i.e., the solution vector's pressure component will immediately beunscaled to retrieve the physical pressure. Since the solver uses the fact thatwe can use a good initial guess by extrapolating the previous solutions, wealso have to scale the pressure immediately [1.x.70] solving.
* 

* 
* [1.x.71][1.x.72]
* 

* In this tutorial program, we apply a variant of the preconditioner used in [2.x.49] . That preconditioner was built to operate on thesystem matrix  [2.x.50]  in block form such that the product matrix[1.x.73]
* is of a form that Krylov-based iterative solvers like GMRES can solve in afew iterations. We then replaced the exact inverse of  [2.x.51]  by the actionof an AMG preconditioner  [2.x.52]  based on a vector Laplace matrix,approximated the Schur complement  [2.x.53]  by a mass matrix  [2.x.54] on the pressure space and wrote an <tt>InverseMatrix</tt> class forimplementing the action of  [2.x.55]  on vectors. In theInverseMatrix class, we used a CG solve with an incomplete Cholesky (IC)preconditioner for performing the inner solves.
* An observation one can make is that we use just the action of apreconditioner for approximating the velocity inverse  [2.x.56]  (and theouter GMRES iteration takes care of the approximate character of theinverse), whereas we use a more or less [1.x.74] inverse for  [2.x.57] ,realized by a fully converged CG solve. This appears unbalanced, but there'ssystem to this madness: almost all the effort goes into the upper left blockto which we apply the AMG preconditioner, whereas even an exact inversion ofthe pressure mass matrix costs basically nothing. Consequently, if it helps usreduce the overall number of iterations somewhat, then this effort is wellspent.
* That said, even though the solver worked well for  [2.x.58] , we have a problemhere that is a bit more complicated (cells are deformed, the pressure variesby orders of magnitude, and we want to plan ahead for more complicatedphysics), and so we'll change a few things slightly:
* 
*  - For more complex problems, it turns out that using just a single AMG V-cycle  as preconditioner is not always sufficient. The outer solver converges just  fine most of the time in a reasonable number of iterations (say, less than  50) but there are the occasional time step where it suddenly takes 700 or  so. What exactly is going on there is hard to determine, but the problem can  be avoided by using a more accurate solver for the top left  block. Consequently, we'll want to use a CG iteration to invert the top left  block of the preconditioner matrix, and use the AMG as a preconditioner for  the CG solver.
* 
*  - The downside of this is that, of course, the Stokes preconditioner becomes  much more expensive (approximately 10 times more expensive than when we just  use a single V-cycle). Our strategy then is this: let's do up to 30 GMRES  iterations with just the V-cycle as a preconditioner and if that doesn't  yield convergence, then take the best approximation of the Stokes solution  obtained after this first round of iterations and use that as the starting  guess for iterations where we use the full inner solver with a rather  lenient tolerance as preconditioner. In all our experiments this leads to  convergence in only a few additional iterations.
* 
*  - One thing we need to pay attention to is that when using a CG with a lenient  tolerance in the preconditioner, then  [2.x.59]  is no longer a  linear function of  [2.x.60]  (it is, of course, if we have a very stringent  tolerance in our solver, or if we only apply a single V-cycle). This is a  problem since now our preconditioner is no longer a linear operator; in  other words, every time GMRES uses it the preconditioner looks  different. The standard GMRES solver can't deal with this, leading to slow  convergence or even breakdown, but the F-GMRES variant is designed to deal  with exactly this kind of situation and we consequently use it.
* 
*  - On the other hand, once we have settled on using F-GMRES we can relax the  tolerance used in inverting the preconditioner for  [2.x.61] . In  [2.x.62] , we ran a  preconditioned CG method on  [2.x.63]  until the residual had been reduced  by 7 orders of magnitude. Here, we can again be more lenient because we know  that the outer preconditioner doesn't suffer.
* 
*  - In  [2.x.64] , we used a left preconditioner in which we first invert the top  left block of the preconditioner matrix, then apply the bottom left  (divergence) one, and then invert the bottom right. In other words, the  application of the preconditioner acts as a lower left block triangular  matrix. Another option is to use a right preconditioner that here would be  upper right block triangulation, i.e., we first invert the bottom right  Schur complement, apply the top right (gradient) operator and then invert  the elliptic top left block. To a degree, which one to choose is a matter of  taste. That said, there is one significant advantage to a right  preconditioner in GMRES-type solvers: the residual with which we determine  whether we should stop the iteration is the true residual, not the norm of  the preconditioned equations. Consequently, it is much simpler to compare it  to the stopping criterion we typically use, namely the norm of the right  hand side vector. In writing this code we found that the scaling issues we  discussed above also made it difficult to determine suitable stopping  criteria for left-preconditioned linear systems, and consequently this  program uses a right preconditioner.
* 
*  - In  [2.x.65] , we used an IC (incomplete Cholesky) preconditioner for the  pressure mass matrix in the Schur complement preconditioner and for the  solution of the temperature system. Here, we could in principle do the same,  but we do choose an even simpler preconditioner, namely a Jacobi  preconditioner for both systems. This is because here we target at massively  %parallel computations, where the decompositions for IC/ILU would have to be  performed block-wise for the locally owned degrees of freedom on each  processor. This means, that the preconditioner gets more like a Jacobi  preconditioner anyway, so we rather start from that variant straight  away. Note that we only use the Jacobi preconditioners for CG solvers with  mass matrices, where they give optimal ([1.x.75]-independent) convergence  anyway, even though they usually require about twice as many iterations as  an IC preconditioner.
* As a final note, let us remark that in  [2.x.66]  we computed theSchur complement  [2.x.67]  by approximating [2.x.68] . Now,however, we have re-scaled the  [2.x.69]  and  [2.x.70]  operators. So  [2.x.71]  should nowapproximate [2.x.72] .We use the discrete form of the right hand side of this as our approximation [2.x.73]  to  [2.x.74] .
* 

* [1.x.76][1.x.77]
* 

* Similarly to  [2.x.75] , we will use an artificial viscosity for stabilizationbased on a residual of the equation.  As a difference to  [2.x.76] , we willprovide two slightly different definitions of the stabilization parameter. For [2.x.77] , we use the same definition as in  [2.x.78] :[1.x.78]
* where we compute the viscosity from a residual  [2.x.79]  ofthe equation, limited by a diffusion proportional to the mesh size  [2.x.80]  inregions where the residual is large (around steep gradients). This definitionhas been shown to work well for the given case,  [2.x.81]  in  [2.x.82] , butit is usually less effective as the diffusion for  [2.x.83] . For that case, wechoose a slightly more readable definition of the viscosity,[1.x.79]
* where the first term gives again the maximum dissipation (similarly to a firstorder upwind scheme),[1.x.80]
* and the entropy viscosity is defined as[1.x.81]
* 
* This formula is described in the article [1.x.82] Compared to the case  [2.x.84] , theresidual is computed from the temperature entropy,  [2.x.85] with  [2.x.86]  an average temperature (we choose the mean between the maximum andminimum temperature in the computation), which gives the following formula[1.x.83]
* The denominator in the formula for  [2.x.87]  is computed as theglobal deviation of the entropy from the space-averaged entropy  [2.x.88] . As in  [2.x.89] , weevaluate the artificial viscosity from the temperature and velocity at twoprevious time levels, in order to avoid a nonlinearity in its definition.
* The above definitions of the viscosity are simple, but depend on twoparameters, namely  [2.x.90]  and  [2.x.91] .  For the current program, we want to goabout this issue a bit more systematically for both parameters in the case [2.x.92] , using the same line of reasoning with which we chose two otherparameters in our discretization,  [2.x.93]  and  [2.x.94] , in the results section of [2.x.95] . In particular, remember that we would like to make the artificialviscosity as small as possible while keeping it as large as necessary. In thefollowing, let us describe the general strategy one may follow. Thecomputations shown here were done with an earlier version of the program andso the actual numerical values you get when running the program may no longermatch those shown here; that said, the general approach remains valid and hasbeen used to find the values of the parameters actually used in the program.
* To see what is happening, note that below we will imposeboundary conditions for the temperature between 973 and 4273 Kelvin,and initial conditions are also chosen in this range; for theseconsiderations, we run the program without %internal heat sources or sinks,and consequently the temperature shouldalways be in this range, barring any %internaloscillations. If the minimal temperature drops below 973 Kelvin, thenwe need to add stabilization by either increasing  [2.x.96]  ordecreasing  [2.x.97] .
* As we did in  [2.x.98] , we first determine an optimal value of  [2.x.99] by using the "traditional" formula[1.x.84]
* which we know to be stable if only  [2.x.100]  is large enough. Doing acouple hundred time steps (on a coarser mesh than the one shown in theprogram, and with a different viscosity that affects transportvelocities and therefore time step sizes) in 2d will produce thefollowing graph:
*  [2.x.101] 
* As can be seen, values  [2.x.102]  are too small whereas [2.x.103]  appears to work, at least to the time horizon shownhere. As a remark on the side, there are at least two questions onemay wonder here: First, what happens at the time when the solutionbecomes unstable? Looking at the graphical output, we can see thatwith the unreasonably coarse mesh chosen for these experiments, aroundtime  [2.x.104]  seconds the plumes of hot material that have beenrising towards the cold outer boundary and have then spread sidewaysare starting to get close to each other, squeezing out the coldmaterial in-between. This creates a layer of cells into which fluidsflows from two opposite sides and flows out toward a third, apparentlya scenario that then produce these instabilities without sufficientstabilization. Second: In  [2.x.105] , we used [2.x.106] ; why does this not work here? The answerto this is not entirely clear
* 
*  -  stabilization parameters arecertainly known to depend on things like the shape of cells, for whichwe had squares in  [2.x.107]  but have trapezoids in the currentprogram. Whatever the exact cause, we at least have a value of [2.x.108] , namely 0.052 for 2d, that works for the current program.A similar set of experiments can be made in 3d where we find that [2.x.109]  is a good choice &mdash; neatly leading to the formula [2.x.110] .
* With this value fixed, we can go back to the original formula for theviscosity  [2.x.111]  and play with the constant  [2.x.112] , making it as largeas possible in order to make  [2.x.113]  as small as possible. This gives usa picture like this:
*  [2.x.114] 
* Consequently,  [2.x.115]  would appear to be the right value here. While thisgraph has been obtained for an exponent  [2.x.116] , in the program we use [2.x.117]  instead, and in that case one has to re-tune the parameter (andobserve that  [2.x.118]  appears in the numerator and not in the denominator). Itturns out that  [2.x.119]  works with  [2.x.120] .
* 

* [1.x.85][1.x.86]
* 

* The standard Taylor-Hood discretization for Stokes, using the  [2.x.121]  element, is globally conservative, i.e.  [2.x.122] . This can easily be seen: the weak form ofthe divergence equation reads  [2.x.123] . Because the pressure space does contain the function  [2.x.124] , weget
* [1.x.87]
* by the divergence theorem. This property is important: if we want to use thevelocity field  [2.x.125]  to transport along other quantities (such as thetemperature in the current equations, but it could also be concentrations ofchemical substances or entirely artificial tracer quantities) then theconservation property guarantees that the amount of the quantity advectedremains constant.
* That said, there are applications where this [1.x.88] property is notenough. Rather, we would like that it holds [1.x.89], on everycell. This can be achieved by using the space [2.x.126]  for discretization, where we have replaced the[1.x.90] space of tensor product polynomials of degree  [2.x.127]  for thepressure by the [1.x.91] space of the complete polynomials of thesame degree. (Note that tensor product polynomials in 2d contain the functions [2.x.128] , whereas the complete polynomials only have the functions  [2.x.129] .)This space turns out to be stable for the Stokes equation.
* Because the space is discontinuous, we can now in particular choose the testfunction  [2.x.130] , i.e. the characteristic functionof cell  [2.x.131] . We then get in a similar fashion as above
* [1.x.92]
* showing the conservation property for cell  [2.x.132] . This clearly holds for eachcell individually.
* There are good reasons to use this discretization. As mentioned above, thiselement guarantees conservation of advected quantities on each cellindividually. A second advantage is that the pressure mass matrix we use as apreconditioner in place of the Schur complement becomes block diagonal andconsequently very easy to invert. However, there are also downsides. For one,there are now more pressure variables, increasing the overall size of theproblem, although this doesn't seem to cause much harm in practice. Moreimportantly, though, the fact that now the divergence integrated over eachcell is zero when it wasn't before does not guarantee that the divergence ispointwise smaller. In fact, as one can easily verify, the  [2.x.133]  norm of thedivergence is [1.x.93] for this than for the standard Taylor-Hooddiscretization. (However, both converge at the same rate to zero, since it iseasy to see that [2.x.134] .) It is therefore not a priori clearthat the error is indeed smaller just because we now have more degrees offreedom.
* Given these considerations, it remains unclear which discretization one shouldprefer. Consequently, we leave that up to the user and make it a parameter inthe input file which one to use.
* 

* [1.x.94][1.x.95]
* 

* In the program, we will use a spherical shell as domain. This meansthat the inner and outer boundary of the domain are no longer"straight" (by which we usually mean that they are bilinear surfacesthat can be represented by the FlatManifold class). Rather, theyare curved and it seems prudent to use a curved approximation in theprogram if we are already using higher order finite elements for thevelocity. Consequently, we will introduce a member variable of typeMappingQ thatdenotes such a mapping ( [2.x.135]  and  [2.x.136]  introduce such mappingsfor the first time) and that we will use in all computations on cellsthat are adjacent to the boundary. Since this only affects arelatively small fraction of cells, the additional effort is not verylarge and we will take the luxury of using a quartic mapping for thesecells.
* 

* [1.x.96][1.x.97]
* 

* Running convection codes in 3d with significant Rayleigh numbers requires a lotof computations &mdash; in the case of whole earth simulations on the order ofone or several hundred million unknowns. This can obviously not be done with asingle machine any more (at least not in 2010 when we started writing thiscode). Consequently, we need to parallelize it.Parallelization of scientific codes across multiple machines in a cluster ofcomputers is almost always done using the Message Passing Interface(MPI). This program is no exception to that, and it follows the general spiritof the  [2.x.137]  and  [2.x.138]  programs in this though in practice it borrows morefrom  [2.x.139]  in which we first introduced the classes and strategies we usewhen we want to [1.x.98] distribute all computations, and [2.x.140]  that shows how to do that for [2.x.141]  "vector-valued problems": including, forexample, splitting the mesh up into a number of parts so that each processoronly stores its own share plus some ghost cells, and using strategies where noprocessor potentially has enough memory to hold the entries of the combinedsolution vector locally. The goal is to run this code on hundreds or maybeeven thousands of processors, at reasonable scalability.
*  [2.x.142]  Even though it has a larger number,  [2.x.143]  comes logically before thecurrent program. The same is true for  [2.x.144] . You will probably wantto look at these programs before you try to understand what we do here.
* MPI is a rather awkward interface to program with. It is a semi-objectoriented set of functions, and while one uses it to send data around anetwork, one needs to explicitly describe the data types because the MPIfunctions insist on getting the address of the data as  [2.x.145] objects rather than deducing the data type automatically through overloadingor templates. We've already seen in  [2.x.146]  and  [2.x.147]  how to avoid almostall of MPI by putting all the communication necessary into either the deal.IIlibrary or, in those programs, into PETSc. We'll do something similar here:like in  [2.x.148]  and  [2.x.149] , deal.II and the underlying p4est library are responsible forall the communication necessary for distributing the mesh, and we will let theTrilinos library (along with the wrappers in namespace TrilinosWrappers) dealwith parallelizing the linear algebra components. We have already usedTrilinos in  [2.x.150] , and will do so again here, with the difference that wewill use its %parallel capabilities.
* Trilinos consists of a significant number of packages, implementing basic%parallel linear algebra operations (the Epetra package), different solver andpreconditioner packages, and on to things that are of less importance todeal.II (e.g., optimization, uncertainty quantification, etc).deal.II's Trilinos interfaces encapsulate many of the things Trilinos offersthat are of relevance to PDE solvers, andprovides wrapper classes (in namespace TrilinosWrappers) that make theTrilinos matrix, vector, solver and preconditioner classes look very much thesame as deal.II's own implementations of this functionality. However, asopposed to deal.II's classes, they can be used in %parallel if we give them thenecessary information. As a consequence, there are two Trilinos classes thatwe have to deal with directly (rather than through wrappers), both of whichare part of Trilinos' Epetra library of basic linear algebra and tool classes: [2.x.151]  [2.x.152]  The Epetra_Comm class is an abstraction of an MPI "communicator", i.e.,  it describes how many and which machines can communicate with each other.  Each distributed object, such as a sparse matrix or a vector for which we  may want to store parts on different machines, needs to have a communicator  object to know how many parts there are, where they can be found, and how  they can be accessed.
*   In this program, we only really use one communicator object
* 
*  -  based on the  MPI variable  [2.x.153] 
* 
*  -  that encompasses [1.x.99]  processes that work together. It would be perfectly legitimate to start a  process on  [2.x.154]  machines but only store vectors on a subset of these by  producing a communicator object that only encompasses this subset of  machines; there is really no compelling reason to do so here, however.
*  [2.x.155]  The IndexSet class is used to describe which elements of a vector or which  rows of a matrix should reside on the current machine that is part of a  communicator. To create such an object, you need to know (i) the total  number of elements or rows, (ii) the indices of the elements you want to  store locally. We will set up these  [2.x.156]  in the   [2.x.157]  function below and then hand  it to every %parallel object we create.
*   Unlike PETSc, Trilinos makes no assumption that the elements of a vector  need to be partitioned into contiguous chunks. At least in principle, we  could store all elements with even indices on one processor and all odd ones  on another. That's not very efficient, of course, but it's  possible. Furthermore, the elements of these partitionings do not  necessarily be mutually exclusive. This is important because when  postprocessing solutions, we need access to all locally relevant or at least  the locally active degrees of freedom (see the module on  [2.x.158]   for a definition, as well as the discussion in  [2.x.159] ). Which elements the  Trilinos vector considers as locally owned is not important to us then. All  we care about is that it stores those elements locally that we need. [2.x.160] 
* There are a number of other concepts relevant to distributing the meshto a number of processors; you may want to take a look at the  [2.x.161]  module and  [2.x.162]  or  [2.x.163]  before trying to understand thisprogram.  The rest of the program is almost completely agnostic aboutthe fact that we don't store all objects completely locally. Therewill be a few points where we have to limit loops over all cells tothose that are locally owned, or where we need to distinguish betweenvectors that store only locally owned elements and those that storeeverything that is locally relevant (see  [2.x.164] "this glossary entry"), but by and large the amount of heavy liftingnecessary to make the program run in %parallel is well hidden in thelibraries upon which this program builds. In any case, we will commenton these locations as we get to them in the program code.
* 

* [1.x.100][1.x.101]
* 

* The second strategy to parallelize a program is to make use of the fact thatmost computers today have more than one processor that all have access to thesame memory. In other words, in this model, we don't explicitly have to saywhich pieces of data reside where
* 
*  -  all of the data we need is directlyaccessible and all we have to do is split [1.x.102] this data betweenthe available processors. We will then couple this with the MPIparallelization outlined above, i.e., we will have all the processors on amachine work together to, for example, assemble the local contributions to theglobal matrix for the cells that this machine actually "owns" but not forthose cells that are owned by other machines. We will use this strategy forfour kinds of operations we frequently do in this program: assembly of theStokes and temperature matrices, assembly of the matrix that forms the Stokespreconditioner, and assembly of the right hand side of the temperature system.
* All of these operations essentially look as follows: we need to loop over allcells for which  [2.x.165]  equals the index ourmachine has within the communicator object used for all communication(i.e.,  [2.x.166] , as explained above). The test we areactually going to use for this, and which describes in a concise way why wetest this condition, is  [2.x.167] . On eachsuch cell we need to assemble the local contributions to the global matrix orvector, and then we have to copy each cell's contribution into the globalmatrix or vector. Note that the first part of this (the loop) defines a rangeof iterators on which something has to happen. The second part, assembly oflocal contributions is something that takes the majority of CPU time in thissequence of steps, and is a typical example of things that can be done in%parallel: each cell's contribution is entirely independent of all other cells'contributions. The third part, copying into the global matrix, must not happenin %parallel since we are modifying one object and so several threads can notat the same time read an existing matrix element, add their contribution, andwrite the sum back into memory without danger of producing a [1.x.103].
* deal.II has a class that is made for exactly this workflow: WorkStream, firstdiscussed in  [2.x.168]  and  [2.x.169] . Itsuse is also extensively documented in the module on  [2.x.170]  (in the sectionon  [2.x.171]  "the WorkStream class") and we won't repeat here therationale and detailed instructions laid out there, though you will want toread through this module to understand the distinction between scratch spaceand per-cell data. Suffice it to mention that we need the following:
* 
*  - An iterator range for those cells on which we are supposed to work. This is  provided by the FilteredIterator class which acts just like every other cell  iterator in deal.II with the exception that it skips all cells that do not  satisfy a particular predicate (i.e., a criterion that evaluates to true or  false). In our case, the predicate is whether a cell is locally owned.
* 
*  - A function that does the work on each cell for each of the tasks identified  above, i.e., functions that assemble the local contributions to Stokes matrix  and preconditioner, temperature matrix, and temperature right hand  side. These are the   [2.x.172] ,   [2.x.173] ,   [2.x.174] , and   [2.x.175]  functions in  the code below. These four functions can all have several instances  running in %parallel at the same time.
* 
*  - %Functions that copy the result of the previous ones into the global object  and that run sequentially to avoid race conditions. These are the   [2.x.176] ,   [2.x.177] ,   [2.x.178] , and   [2.x.179]   functions.
* We will comment on a few more points in the actual code, but in generaltheir structure should be clear from the discussion in  [2.x.180] .
* The underlying technology for WorkStream identifies "tasks" that need to beworked on (e.g. assembling local contributions on a cell) and schedulesthese tasks automatically to available processors. WorkStream creates thesetasks automatically, by splitting the iterator range into suitable chunks.
*  [2.x.181]  Using multiple threads within each MPI process only makes sense if youhave fewer MPI processes running on each node of your cluster than there areprocessor cores on this machine. Otherwise, MPI will already keep yourprocessors busy and you won't get any additional speedup from usingthreads. For example, if your cluster nodes have 8 cores as they often have atthe time of writing this, and if your batch scheduler puts 8 MPI processes oneach node, then using threads doesn't make the program anyfaster. Consequently, you probably want to either configure your deal.II withoutthreads, or set the number of threads in  [2.x.182]  to 1(third argument), or "export DEAL_II_NUM_THREADS=1" before running. That said, atthe time of writing this, we only use the WorkStream class for assembling(parts of) linear systems, while 75% or more of the run time of the program isspent in the linear solvers that are not parallelized &mdash; in other words,the best we could hope is to parallelize the remaining 25%.
* 

* [1.x.104][1.x.105]
* 

* The setup for this program is mildly reminiscent of the problem we wanted tosolve in the first place (see the introduction of  [2.x.183] ):convection in the earth mantle. As a consequence, we choose the followingdata, all of which appears in the program in units of meters and seconds (theSI system) even if we list them here in other units. We do note,however, that these choices are essentially still only exemplary, andnot meant to result in a completely realistic description ofconvection in the earth mantle: for that, more and more difficultphysics would have to be implemented, and several other aspects arecurrently missing from this program as well. We will come back to thisissue in the results section again, but state for now that providing arealistic description is a goal of the [1.x.106] code indevelopment at the time of writing this.
* As a reminder, let us again state the equations we want to solve are these:[1.x.107]
* augmented by boundary and initial conditions. We then have to choose data forthe following quantities: [2.x.184]    [2.x.185] The domain is an annulus (in 2d) or a spherical shell (in 3d) with inner  and outer radii that match that of the earth: the total radius of the earth  is 6371km, with the mantle starting at a depth of around 35km (just under  the solid earth [1.x.108] composed of  [1.x.109] and [1.x.110]) to a depth of 2890km (where the  [1.x.111] starts). The radii are therefore  [2.x.186] . This domain is conveniently generated using the   [2.x.187]  function.
*    [2.x.188] At the interface between crust and mantle, the temperature is between  500 and 900 degrees Celsius, whereas at its bottom it is around 4000 degrees  Celsius (see, for example, [1.x.112]). In Kelvin, we therefore choose  [2.x.189] ,   [2.x.190]  as boundary conditions at the inner and outer edge.
*   In addition to this, we also have to specify some initial conditions for  the temperature field. The real temperature field of the earth is quite  complicated as a consequence of the convection that has been going on for  more than four billion years
* 
*  -  in fact, it is the properties of this  temperature distribution that we want to explore with programs like  this. As a consequence, we  don't really have anything useful to offer here, but we can hope that if we  start with something and let things run for a while that the exact initial  conditions don't matter that much any more &mdash; as is in fact suggested  by looking at the pictures shown in the [1.x.113]. The initial temperature field we use here is given in terms of  the radius by 
* [1.x.114]
*   where 
* [1.x.115]
*   This complicated function is essentially a perturbation of a linear profile  between the inner and outer temperatures. In 2d, the function   [2.x.191]  looks like this (I got the picture from  [1.x.116]):
*    [2.x.192] 
*   The point of this profile is that if we had used  [2.x.193]  instead of  [2.x.194]  in  the definition of  [2.x.195]  then it would simply be a linear  interpolation.  [2.x.196]  has the same function values as  [2.x.197]  on the inner and  outer boundaries (zero and one, respectively), but it stretches the  temperature profile a bit depending on the angle and the  [2.x.198]  value in 3d,  producing an angle-dependent perturbation of the linearly interpolating  field. We will see in the results section that this is an  entirely unphysical temperature field (though it will make for  interesting images) as the equilibrium state for the temperature  will be an almost constant temperature with boundary layers at the  inner and outer boundary.
*    [2.x.199] The right hand side of the temperature equation contains the rate of  %internal heating  [2.x.200] . The earth does heat naturally through several mechanisms:  radioactive decay, chemical separation (heavier elements sink to the bottom,  lighter ones rise to the top; the countercurrents dissipate energy equal to  the loss of potential energy by this separation process); heat release  by crystallization of liquid metal as the solid inner core of the earth  grows; and heat dissipation from viscous friction as the fluid moves.
*   Chemical separation is difficult to model since it requires modeling mantle  material as multiple phases; it is also a relatively small  effect. Crystallization heat is even more difficult since it is confined to  areas where temperature and pressure allow for phase changes, i.e., a  discontinuous process. Given the difficulties in modeling these two  phenomena, we will neglect them.
*   The other two are readily handled and, given the way we scaled the  temperature equation, lead to the equation  [1.x.117]  where  [2.x.201]  is the radiogenic heating in  [2.x.202] , and the second  term in the enumerator is viscous friction heating.  [2.x.203]  is the density  and  [2.x.204]  is the specific heat. The literature provides the following  approximate values:  [2.x.205] .  The other parameters are discussed elsewhere in this section.
*   We neglect one internal heat source, namely adiabatic heating here,  which will lead to a surprising temperature field. This point is  commented on in detail in the results section below.
*    [2.x.206] For the velocity we choose as boundary conditions  [2.x.207]  at the  inner radius (i.e., the fluid sticks to the earth core) and   [2.x.208]  at the outer radius (i.e., the fluid flows  tangentially along the bottom of the earth crust). Neither of these is  physically overly correct: certainly, on both boundaries, fluids can flow  tangentially, but they will incur a shear stress through friction against  the medium at the other side of the interface (the metallic core and the  crust, respectively). Such a situation could be modeled by a Robin-type  boundary condition for the tangential velocity; in either case, the normal (vertical)  velocity would be zero, although even that is not entirely correct since  continental plates also have vertical motion (see, for example, the  phenomenon of [1.x.118]). But to already make things worse for the tangential velocity,  the medium on the other side is in motion as well, so the shear stress  would, in the simplest case, be proportional to the [1.x.119], leading to a boundary condition of the form 
* [1.x.120]
*   with a proportionality constant  [2.x.209] . Rather than going down this route,  however, we go with the choice of zero (stick) and tangential  flow boundary conditions.
*   As a side note of interest, we may also have chosen tangential flow  conditions on both inner and outer boundary. That has a significant  drawback, however: it leaves the velocity not uniquely defined. The reason  is that all velocity fields  [2.x.210]  that correspond to a solid  body rotation around the center of the domain satisfy  [2.x.211] , and   [2.x.212] . As a consequence, if  [2.x.213]   satisfies equations and boundary conditions, then so does  [2.x.214] . That's certainly not a good situation that we would like  to avoid. The traditional way to work around this is to pick an arbitrary  point on the boundary and call this your fixed point by choosing the  velocity to be zero in all components there. (In 3d one has to choose two  points.) Since this program isn't meant to be too realistic to begin with,  we avoid this complication by simply fixing the velocity along the entire  interior boundary.
*    [2.x.215] To first order, the gravity vector always points downward. The question for  a body as big as the earth is just: where is "up". The naive answer of course is  "radially inward, towards the center of the earth". So at the surface of the  earth, we have  [1.x.121]  where  [2.x.216]  happens to be the average gravity  acceleration at the earth surface. But in the earth interior, the question  becomes a bit more complicated: at the (bary-)center of the earth, for  example, you have matter pulling equally hard in all directions, and so   [2.x.217] . In between, the net force is described as follows: let us  define the [1.x.122] by  [1.x.123]  then  [2.x.218] . If we assume that  the density  [2.x.219]  is constant throughout the earth, we can produce an  analytical expression for the gravity vector (don't try to integrate above  equation somehow
* 
*  -  it leads to elliptic integrals; a simpler way is to  notice that  [2.x.220]  and solving this  partial differential equation in all of  [2.x.221]  exploiting the  radial symmetry):  [1.x.124]  The factor  [2.x.222]  is the unit vector pointing  radially inward. Of course, within this problem, we are only interested in  the branch that pertains to within the earth, i.e.,  [2.x.223] . We would therefore only consider the expression  [1.x.125]  where we can infer the last expression because we know Earth's gravity at  the surface (where  [2.x.224] ).
*   One can derive a more general expression by integrating the  differential equation for  [2.x.225]  in the case that the density  distribution is radially symmetric, i.e.,  [2.x.226] . In that case, one would get  [1.x.126]
* 

*   There are two problems with this, however: (i) The Earth is not homogeneous,  i.e., the density  [2.x.227]  depends on  [2.x.228] ; in fact it is not even a  function that only depends on the radius  [2.x.229] . In reality, gravity therefore  does not always decrease as we get deeper: because the earth core is so much  denser than the mantle, gravity actually peaks at around  [2.x.230]  at the core mantle boundary (see [1.x.127]). (ii) The density, and by  consequence the gravity vector, is not even constant in time: after all, the  problem we want to solve is the time dependent upwelling of hot, less dense  material and the downwelling of cold dense material. This leads to a gravity  vector that varies with space and time, and does not always point straight  down.
*   In order to not make the situation more complicated than necessary, we could  use the approximation that at the inner boundary of the mantle,  gravity is  [2.x.231]  and at the outer  boundary it is  [2.x.232] , in each case  pointing radially inward, and that in between gravity varies  linearly with the radial distance from the earth center. That said, it isn't  that hard to actually be slightly more realistic and assume (as we do below)  that the earth mantle has constant density. In that case, the equation above  can be integrated and we get an expression for  [2.x.233]  where we  can fit constants to match the gravity at the top and bottom of the earth  mantle to obtain  [1.x.128]
*    [2.x.234] The density of the earth mantle varies spatially, but not by very  much.  [2.x.235]  is a relatively good average  value for the density at reference temperature  [2.x.236]  Kelvin.
*    [2.x.237] The thermal expansion coefficient  [2.x.238]  also varies with depth  (through its dependence on temperature and pressure). Close to the surface,  it appears to be on the order of  [2.x.239] ,  whereas at the core mantle boundary, it may be closer to  [2.x.240] . As a reasonable value, let us choose   [2.x.241] . The density as a function  of temperature is then   [2.x.242] .
*    [2.x.243] The second to last parameter we need to specify is the viscosity   [2.x.244] . This is a tough one, because rocks at the temperatures and pressure  typical for the earth mantle flow so slowly that the viscosity can not be  determined accurately in the laboratory. So how do we know about the  viscosity of the mantle? The most commonly used route is to consider that  during and after ice ages, ice shields form and disappear on time scales  that are shorter than the time scale of flow in the mantle. As a  consequence, continents slowly sink into the earth mantle under the added  weight of an ice shield, and they rise up again slowly after the ice shield  has disappeared again (this is called [1.x.129][1.x.130]). By measuring the speed of this rebound, we can infer the  viscosity of the material that flows into the area vacated under the  rebounding continental plates.
*   Using this technique, values around  [2.x.245]  have been found as the most  likely, though the error bar on this is at least one order of magnitude.
*   While we will use this value, we again have to caution that there are many  physical reasons to assume that this is not the correct value. First, it  should really be made dependent on temperature: hotter material is most  likely to be less viscous than colder material. In reality, however, the  situation is even more complex. Most rocks in the mantle undergo phase  changes as temperature and pressure change: depending on temperature and  pressure, different crystal configurations are thermodynamically favored  over others, even if the chemical composition of the mantle were  homogeneous. For example, the common mantle material MgSiO<sub>3</sub> exists  in its [1.x.131] throughout most of the mantle, but in the lower mantle the  same substance is stable only as [1.x.132]. Clearly,  to compute realistic viscosities, we would not only need to know the exact  chemical composition of the mantle and the viscosities of all materials, but  we would also have to compute the thermodynamically most stable  configurations for all materials at each quadrature point. This is at the  time of writing this program not a feasible suggestion.
*    [2.x.246] Our last material parameter is the thermal diffusivity  [2.x.247] , which  is defined as  [2.x.248]  where  [2.x.249]  is the thermal  conductivity,  [2.x.250]  the density, and  [2.x.251]  the specific heat. For  this, the literature indicates that it increases from around  [2.x.252]  in the  upper mantle to around  [2.x.253]  in the lower  mantle, though the exact value  is not really all that important: heat transport through convection is  several orders of magnitude more important than through thermal  conduction. It may be of interest to know that perovskite, the most abundant  material in the earth mantle, appears to become transparent at pressures  above around 120 GPa (see, for example, J. Badro et al., Science 305,  383-386 (2004)); in the lower mantle, it may therefore be that heat  transport through radiative transfer is more efficient than through thermal  conduction.
*   In view of these considerations, let us choose   [2.x.254]   for the purpose of this program. [2.x.255] 
* All of these pieces of equation data are defined in the program in the [2.x.256]  namespace. When run, the program produceslong-term maximal velocities around 10-40 centimeters per year (seethe results section below), approximately the physically correct orderof magnitude. We will set the end time to 1 billion years.
*  [2.x.257]  The choice of the constants and material parameters above follows inlarge part the comprehensive book "Mantle Convection in the Earth and Planets,Part 1" by G. Schubert and D. L. Turcotte and P. Olson (Cambridge, 2001). Itcontains extensive discussion of ways to make the program more realistic.
* 

* [1.x.133][1.x.134]
* 

* Compared to  [2.x.258] , this program has a number of noteworthy differences:
* 
*  - The  [2.x.259]  namespace is significantly larger, reflecting  the fact that we now have much more physics to deal with. That said, most of  this additional physical detail is rather self-contained in functions in  this one namespace, and does not proliferate throughout the rest of the  program.
* 
*  - Of more obvious visibility is the fact that we have put a good number of  parameters into an input file handled by the ParameterHandler class (see,  for example,  [2.x.260] , for ways to set up run-time parameter files with this  class). This often makes sense when one wants to avoid re-compiling the  program just because one wants to play with a single parameter (think, for  example, of parameter studies determining the best values of the  stabilization constants discussed above), in particular given that it takes  a nontrivial amount of time to re-compile programs of the current size. To  just give an overview of the kinds of parameters we have moved from fixed  values into the input file, here is a listing of a typical   [2.x.261]  file: 
* [1.x.135]
* 
* 
*  - There are, obviously, a good number of changes that have to do with the fact  that we want to run our program on a possibly very large number of  machines. Although one may suspect that this requires us to completely  re-structure our code, that isn't in fact the case (although the classes  that implement much of this functionality in deal.II certainly look very  different from an implementation viewpoint, but this doesn't reflect in  their public interface). Rather, the changes are mostly subtle, and the  overall structure of the main class is pretty much unchanged. That said, the  devil is in the detail: getting %parallel computing right, without  deadlocks, ensuring that the right data is available at the right place  (see, for example, the discussion on fully distributed vectors vs. vectors  with ghost elements), and avoiding bottlenecks is difficult and discussions  on this topic will appear in a good number of places in this program.
* 

* [1.x.136][1.x.137]
* 

* This is a tutorial program. That means that at least most of its focus needsto lie on demonstrating ways of using deal.II and associated libraries, andnot diluting this teaching lesson by focusing overly much on physicaldetails. Despite the lengthy section above on the choice of physicalparameters, the part of the program devoted to this is actually quite shortand self contained.
* That said, both  [2.x.262]  and the current  [2.x.263]  have not come about by chancebut are certainly meant as wayposts along the path to a more comprehensiveprogram that will simulate convection in the earth mantle. We call this code[1.x.138] (short for [1.x.139]); its development is funded bythe [1.x.140] initiative with support from the National ScienceFoundation. More information on [1.x.141] is available atits [1.x.142].
* 

*  [1.x.143] [1.x.144]
*   [1.x.145]  [1.x.146]
* 

* 
*  The first task as usual is to include the functionality of these well-known deal.II library files and some C++ header files.
* 

* 
* [1.x.147]
* 
*  This is the only include file that is new: It introduces the  [2.x.264]  equivalent of the  [2.x.265]  class to take a solution from on mesh to the next one upon mesh refinement, but in the case of parallel distributed triangulations:
* 

* 
* [1.x.148]
* 
*  The following classes are used in parallel distributed computations and have all already been introduced in  [2.x.266] :
* 

* 
* [1.x.149]
* 
*  The next step is like in all previous tutorial programs: We put everything into a namespace of its own and then import the deal.II classes and functions into it:
* 

* 
* [1.x.150]
* 
*   [1.x.151]  [1.x.152]
* 

* 
*  In the following namespace, we define the various pieces of equation data that describe the problem. This corresponds to the various aspects of making the problem at least slightly realistic and that were exhaustively discussed in the description of the testcase in the introduction.   
*   We start with a few coefficients that have constant values (the comment after the value indicates its physical units):
* 

* 
* [1.x.153]
* 
*  The next set of definitions are for functions that encode the density as a function of temperature, the gravity vector, and the initial values for the temperature. Again, all of these (along with the values they compute) are discussed in the introduction:
* 

* 
* [1.x.154]
* 
*  As mentioned in the introduction we need to rescale the pressure to avoid the relative ill-conditioning of the momentum and mass conservation equations. The scaling factor is  [2.x.267]  where  [2.x.268]  was a typical length scale. By experimenting it turns out that a good length scale is the diameter of plumes, which is around 10 km:
* 

* 
* [1.x.155]
* 
*  The final number in this namespace is a constant that denotes the number of seconds per (average, tropical) year. We use this only when generating screen output: internally, all computations of this program happen in SI units (kilogram, meter, seconds) but writing geological times in seconds yields numbers that one can't relate to reality, and so we convert to years using the factor defined here:
* 

* 
* [1.x.156]
* 
*   [1.x.157]  [1.x.158]
* 

* 
*  This namespace implements the preconditioner. As discussed in the introduction, this preconditioner differs in a number of key portions from the one used in  [2.x.269] . Specifically, it is a right preconditioner, implementing the matrix

* 
* [1.x.159]
*  where the two inverse matrix operations are approximated by linear solvers or, if the right flag is given to the constructor of this class, by a single AMG V-cycle for the velocity block. The three code blocks of the  [2.x.270]  function implement the multiplications with the three blocks of this preconditioner matrix and should be self explanatory if you have read through  [2.x.271]  or the discussion of composing solvers in  [2.x.272] .
* 

* 
* [1.x.160]
* 
*   [1.x.161]  [1.x.162]   
*   As described in the introduction, we will use the WorkStream mechanism discussed in the  [2.x.273]  module to parallelize operations among the processors of a single machine. The WorkStream class requires that data is passed around in two kinds of data structures, one for scratch data and one to pass data from the assembly function to the function that copies local contributions into global objects.   
*   The following namespace (and the two sub-namespaces) contains a collection of data structures that serve this purpose, one pair for each of the four operations discussed in the introduction that we will want to parallelize. Each assembly routine gets two sets of data: a Scratch array that collects all the classes and arrays that are used for the calculation of the cell contribution, and a CopyData array that keeps local matrices and vectors which will be written into the global matrix. Whereas CopyData is a container for the final data that is written into the global matrices and vector (and, thus, absolutely necessary), the Scratch arrays are merely there for performance reasons &mdash; it would be much more expensive to set up a FEValues object on each cell, than creating it only once and updating some derivative data.   
*    [2.x.274]  had four assembly routines: One for the preconditioner matrix of the Stokes system, one for the Stokes matrix and right hand side, one for the temperature matrices and one for the right hand side of the temperature equation. We here organize the scratch arrays and CopyData objects for each of those four assembly components using a  [2.x.275]  environment (since we consider these as temporary objects we pass around, rather than classes that implement functionality of their own, though this is a more subjective point of view to distinguish between  [2.x.276] es).   
*   Regarding the Scratch objects, each struct is equipped with a constructor that creates an  [2.x.277]  object using the  [2.x.278] , Quadrature,  [2.x.279]  (which describes the interpolation of curved boundaries), and  [2.x.280]  instances. Moreover, we manually implement a copy constructor (since the FEValues class is not copyable by itself), and provide some additional vector fields that are used to hold intermediate data during the computation of local contributions.   
*   Let us start with the scratch arrays and, specifically, the one used for assembly of the Stokes preconditioner:
* 

* 
* [1.x.163]
* 
*  The next one is the scratch object used for the assembly of the full Stokes system. Observe that we derive the StokesSystem scratch class from the StokesPreconditioner class above. We do this because all the objects that are necessary for the assembly of the preconditioner are also needed for the actual matrix system and right hand side, plus some extra data. This makes the program more compact. Note also that the assembly of the Stokes system and the temperature right hand side further down requires data from temperature and velocity, respectively, so we actually need two FEValues objects for those two cases.
* 

* 
* [1.x.164]
* 
*  After defining the objects used in the assembly of the Stokes system, we do the same for the assembly of the matrices necessary for the temperature system. The general structure is very similar:
* 

* 
* [1.x.165]
* 
*  The final scratch object is used in the assembly of the right hand side of the temperature system. This object is significantly larger than the ones above because a lot more quantities enter the computation of the right hand side of the temperature equation. In particular, the temperature values and gradients of the previous two time steps need to be evaluated at the quadrature points, as well as the velocities and the strain rates (i.e. the symmetric gradients of the velocity) that enter the right hand side as friction heating terms. Despite the number of terms, the following should be rather self explanatory:
* 

* 
* [1.x.166]
* 
*  The CopyData objects are even simpler than the Scratch objects as all they have to do is to store the results of local computations until they can be copied into the global matrix or vector objects. These structures therefore only need to provide a constructor, a copy operation, and some arrays for local matrix, local vectors and the relation between local and global degrees of freedom (a.k.a.  [2.x.281] ). Again, we have one such structure for each of the four operations we will parallelize using the WorkStream class:
* 

* 
* [1.x.167]
* 
*   [1.x.168]  [1.x.169]   
*   This is the declaration of the main class. It is very similar to  [2.x.282]  but there are a number differences we will comment on below.   
*   The top of the class is essentially the same as in  [2.x.283] , listing the public methods and a set of private functions that do the heavy lifting. Compared to  [2.x.284]  there are only two additions to this section: the function  [2.x.285]  that computes the maximum CFL number over all cells which we then compute the global time step from, and the function  [2.x.286]  that is used in the computation of the entropy stabilization. It is akin to the  [2.x.287]  we have used in  [2.x.288]  for this purpose, but works on the entropy instead of the temperature instead.
* 

* 
* [1.x.170]
* 
*  The first significant new component is the definition of a struct for the parameters according to the discussion in the introduction. This structure is initialized by reading from a parameter file during construction of this object.
* 

* 
* [1.x.171]
* 
*  The  [2.x.289]  (for [1.x.172]) object is used to simplify writing output: each MPI process can use this to generate output as usual, but since each of these processes will (hopefully) produce the same output it will just be replicated many times over; with the ConditionalOStream class, only the output generated by one MPI process will actually be printed to screen, whereas the output by all the other threads will simply be forgotten.
* 

* 
* [1.x.173]
* 
*  The following member variables will then again be similar to those in  [2.x.290]  (and to other tutorial programs). As mentioned in the introduction, we fully distribute computations, so we will have to use the  [2.x.291]  class (see  [2.x.292] ) but the remainder of these variables is rather standard with two exceptions:
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - The  [2.x.293]  variable is used to denote a higher-order polynomial mapping. As mentioned in the introduction, we use this mapping when forming integrals through quadrature for all cells that are adjacent to either the inner or outer boundaries of our domain where the boundary is curved.
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - In a bit of naming confusion, you will notice below that some of the variables from namespace TrilinosWrappers are taken from namespace  [2.x.294]  (such as the right hand side vectors) whereas others are not (such as the various matrices). This is due to legacy reasons. We will frequently have to query velocities and temperatures at arbitrary quadrature points; consequently, rather than importing ghost information of a vector whenever we need access to degrees of freedom that are relevant locally but owned by another processor, we solve linear systems in %parallel but then immediately initialize a vector including ghost entries of the solution for further processing. The various  [2.x.295]  vectors are therefore filled immediately after solving their respective linear system in %parallel and will always contain values for all  [2.x.296]  "locally relevant degrees of freedom"; the fully distributed vectors that we obtain from the solution process and that only ever contain the  [2.x.297]  "locally owned degrees of freedom" are destroyed immediately after the solution process and after we have copied the relevant values into the member variable vectors.
* 

* 
* [1.x.174]
* 
*  The next member variable,  [2.x.298]  is used to conveniently account for compute time spent in certain "sections" of the code that are repeatedly entered. For example, we will enter (and leave) sections for Stokes matrix assembly and would like to accumulate the run time spent in this section over all time steps. Every so many time steps as well as at the end of the program (through the destructor of the TimerOutput class) we will then produce a nice summary of the times spent in the different sections into which we categorize the run-time of this program.
* 

* 
* [1.x.175]
* 
*  After these member variables we have a number of auxiliary functions that have been broken out of the ones listed above. Specifically, there are first three functions that we call from  [2.x.299]  and then the ones that do the assembling of linear systems:
* 

* 
* [1.x.176]
* 
*  Following the  [2.x.300]  "task-based parallelization" paradigm, we split all the assembly routines into two parts: a first part that can do all the calculations on a certain cell without taking care of other threads, and a second part (which is writing the local data into the global matrices and vectors) which can be entered by only one thread at a time. In order to implement that, we provide functions for each of those two steps for all the four assembly routines that we use in this program. The following eight functions do exactly this:
* 

* 
* [1.x.177]
* 
*  Finally, we forward declare a member class that we will define later on and that will be used to compute a number of quantities from our solution vectors that we'd like to put into the output files for visualization.
* 

* 
* [1.x.178]
* 
*   [1.x.179]  [1.x.180]
* 

* 
*   [1.x.181]  [1.x.182]   
*   Here comes the definition of the parameters for the Stokes problem. We allow to set the end time for the simulation, the level of refinements (both global and adaptive, which in the sum specify what maximum level the cells are allowed to have), and the interval between refinements in the time stepping.   
*   Then, we let the user specify constants for the stabilization parameters (as discussed in the introduction), the polynomial degree for the Stokes velocity space, whether to use the locally conservative discretization based on FE_DGP elements for the pressure or not (FE_Q elements for pressure), and the polynomial degree for the temperature interpolation.   
*   The constructor checks for a valid input file (if not, a file with default parameters for the quantities is written), and eventually parses the parameters.
* 

* 
* [1.x.183]
* 
*  Next we have a function that declares the parameters that we expect in the input file, together with their data types, default values and a description:
* 

* 
* [1.x.184]
* 
*  And then we need a function that reads the contents of the ParameterHandler object we get by reading the input file and puts the results into variables that store the values of the parameters we have previously declared:
* 

* 
* [1.x.185]
* 
*   [1.x.186]  [1.x.187]   
*   The constructor of the problem is very similar to the constructor in  [2.x.301] . What is different is the %parallel communication: Trilinos uses a message passing interface (MPI) for data distribution. When entering the BoussinesqFlowProblem class, we have to decide how the parallelization is to be done. We choose a rather simple strategy and let all processors that are running the program work together, specified by the communicator  [2.x.302] . Next, we create the output stream (as we already did in  [2.x.303] ) that only generates output on the first MPI process and is completely forgetful on all others. The implementation of this idea is to check the process number when  [2.x.304]  gets a true argument, and it uses the  [2.x.305]  stream for output. If we are one processor five, for instance, then we will give a  [2.x.306] , which means that the output of that processor will not be printed. With the exception of the mapping object (for which we use polynomials of degree 4) all but the final member variable are exactly the same as in  [2.x.307] .   
*   This final object, the TimerOutput object, is then told to restrict output to the  [2.x.308]  stream (processor 0), and then we specify that we want to get a summary table at the end of the program which shows us wallclock times (as opposed to CPU times). We will manually also request intermediate summaries every so many time steps in the  [2.x.309]  function below.
* 

* 
* [1.x.188]
* 
*   [1.x.189]  [1.x.190]
*  [1.x.191]  [1.x.192]
* 

* 
*  Except for two small details, the function to compute the global maximum of the velocity is the same as in  [2.x.310] . The first detail is actually common to all functions that implement loops over all cells in the triangulation: When operating in %parallel, each processor can only work on a chunk of cells since each processor only has a certain part of the entire triangulation. This chunk of cells that we want to work on is identified via a so-called  [2.x.311] , as we also did in  [2.x.312] . All we need to change is hence to perform the cell-related operations only on cells that are owned by the current process (as opposed to ghost or artificial cells), i.e. for which the subdomain id equals the number of the process ID. Since this is a commonly used operation, there is a shortcut for this operation: we can ask whether the cell is owned by the current processor using  [2.x.313] .   
*   The second difference is the way we calculate the maximum value. Before, we could simply have a  [2.x.314]  variable that we checked against on each quadrature point for each cell. Now, we have to be a bit more careful since each processor only operates on a subset of cells. What we do is to first let each processor calculate the maximum among its cells, and then do a global communication operation  [2.x.315]  that computes the maximum value among all the maximum values of the individual processors. MPI provides such a call, but it's even simpler to use the respective function in namespace  [2.x.316]  using the MPI communicator object since that will do the right thing even if we work without MPI and on a single machine only. The call to  [2.x.317]  needs two arguments, namely the local maximum (input) and the MPI communicator, which is MPI_COMM_WORLD in this example.
* 

* 
* [1.x.193]
* 
*   [1.x.194]  [1.x.195]
* 

* 
*  The next function does something similar, but we now compute the CFL number, i.e., maximal velocity on a cell divided by the cell diameter. This number is necessary to determine the time step size, as we use a semi-explicit time stepping scheme for the temperature equation (see  [2.x.318]  for a discussion). We compute it in the same way as above: Compute the local maximum over all locally owned cells, then exchange it via MPI to find the global maximum.
* 

* 
* [1.x.196]
* 
*   [1.x.197]  [1.x.198]
* 

* 
*  Next comes the computation of the global entropy variation  [2.x.319]  where the entropy  [2.x.320]  is defined as discussed in the introduction.  This is needed for the evaluation of the stabilization in the temperature equation as explained in the introduction. The entropy variation is actually only needed if we use  [2.x.321]  as a power in the residual computation. The infinity norm is computed by the maxima over quadrature points, as usual in discrete computations.   
*   In order to compute this quantity, we first have to find the space-average  [2.x.322]  and then evaluate the maximum. However, that means that we would need to perform two loops. We can avoid the overhead by noting that  [2.x.323] , i.e., the maximum out of the deviation from the average entropy in positive and negative directions. The four quantities we need for the latter formula (maximum entropy, minimum entropy, average entropy, area) can all be evaluated in the same loop over all cells, so we choose this simpler variant.
* 

* 
* [1.x.199]
* 
*  In the two functions above we computed the maximum of numbers that were all non-negative, so we knew that zero was certainly a lower bound. On the other hand, here we need to find the maximum deviation from the average value, i.e., we will need to know the maximal and minimal values of the entropy for which we don't a priori know the sign.     
*   To compute it, we can therefore start with the largest and smallest possible values we can store in a double precision number: The minimum is initialized with a bigger and the maximum with a smaller number than any one that is going to appear. We are then guaranteed that these numbers will be overwritten in the loop on the first cell or, if this processor does not own any cells, in the communication step at the latest. The following loop then computes the minimum and maximum local entropy as well as keeps track of the area/volume of the part of the domain we locally own and the integral over the entropy on it:
* 

* 
* [1.x.200]
* 
*  Now we only need to exchange data between processors: we need to sum the two integrals ( [2.x.324] ), and get the extrema for maximum and minimum. We could do this through four different data exchanges, but we can it with two:  [2.x.325]  also exists in a variant that takes an array of values that are all to be summed up. And we can also utilize the  [2.x.326]  function by realizing that forming the minimum over the minimal entropies equals forming the negative of the maximum over the negative of the minimal entropies; this maximum can then be combined with forming the maximum over the maximal entropies.
* 

* 
* [1.x.201]
* 
*  Having computed everything this way, we can then compute the average entropy and find the  [2.x.327]  norm by taking the larger of the deviation of the maximum or minimum from the average:
* 

* 
* [1.x.202]
* 
*   [1.x.203]  [1.x.204]
* 

* 
*  The next function computes the minimal and maximal value of the extrapolated temperature over the entire domain. Again, this is only a slightly modified version of the respective function in  [2.x.328] . As in the function above, we collect local minima and maxima and then compute the global extrema using the same trick as above.   
*   As already discussed in  [2.x.329] , the function needs to distinguish between the first and all following time steps because it uses a higher order temperature extrapolation scheme when at least two previous time steps are available.
* 

* 
* [1.x.205]
* 
*   [1.x.206]  [1.x.207]
* 

* 
*  The function that calculates the viscosity is purely local and so needs no communication at all. It is mostly the same as in  [2.x.330]  but with an updated formulation of the viscosity if  [2.x.331]  is chosen:
* 

* 
* [1.x.208]
* 
*   [1.x.209]  [1.x.210]
* 

* 
*  The following three functions set up the Stokes matrix, the matrix used for the Stokes preconditioner, and the temperature matrix. The code is mostly the same as in  [2.x.332] , but it has been broken out into three functions of their own for simplicity.   
*   The main functional difference between the code here and that in  [2.x.333]  is that the matrices we want to set up are distributed across multiple processors. Since we still want to build up the sparsity pattern first for efficiency reasons, we could continue to build the [1.x.211] sparsity pattern as a BlockDynamicSparsityPattern, as we did in  [2.x.334] . However, that would be inefficient: every processor would build the same sparsity pattern, but only initialize a small part of the matrix using it. It also violates the principle that every processor should only work on those cells it owns (and, if necessary the layer of ghost cells around it).   
*   Rather, we use an object of type  [2.x.335]  which is (obviously) a wrapper around a sparsity pattern object provided by Trilinos. The advantage is that the Trilinos sparsity pattern class can communicate across multiple processors: if this processor fills in all the nonzero entries that result from the cells it owns, and every other processor does so as well, then at the end after some MPI communication initiated by the  [2.x.336]  call, we will have the globally assembled sparsity pattern available with which the global matrix can be initialized.   
*   There is one important aspect when initializing Trilinos sparsity patterns in parallel: In addition to specifying the locally owned rows and columns of the matrices via the  [2.x.337]  index set, we also supply information about all the rows we are possibly going to write into when assembling on a certain processor. The set of locally relevant rows contains all such rows (possibly also a few unnecessary ones, but it is difficult to find the exact row indices before actually getting indices on all cells and resolving constraints). This additional information allows to exactly determine the structure for the off-processor data found during assembly. While Trilinos matrices are able to collect this information on the fly as well (when initializing them from some other reinit method), it is less efficient and leads to problems when assembling matrices with multiple threads. In this program, we pessimistically assume that only one processor at a time can write into the matrix while assembly (whereas the computation is parallel), which is fine for Trilinos matrices. In practice, one can do better by hinting WorkStream at cells that do not share vertices, allowing for parallelism among those cells (see the graph coloring algorithms and WorkStream with colored iterators argument). However, that only works when only one MPI processor is present because Trilinos' internal data structures for accumulating off-processor data on the fly are not thread safe. With the initialization presented here, there is no such problem and one could safely introduce graph coloring for this algorithm.   
*   The only other change we need to make is to tell the  [2.x.338]  function that it is only supposed to work on a subset of cells, namely the ones whose  [2.x.339]  equals the number of the current processor, and to ignore all other cells.   
*   This strategy is replicated across all three of the following functions.   
*   Note that Trilinos matrices store the information contained in the sparsity patterns, so we can safely release the  [2.x.340]  variable once the matrix has been given the sparsity structure.
* 

* 
* [1.x.212]
* 
*  The remainder of the setup function (after splitting out the three functions above) mostly has to deal with the things we need to do for parallelization across processors. Because setting all of this up is a significant compute time expense of the program, we put everything we do here into a timer group so that we can get summary information about the fraction of time spent in this part of the program at its end.   
*   At the top as usual we enumerate degrees of freedom and sort them by component/block, followed by writing their numbers to the screen from processor zero. The  [2.x.341]  function, when applied to a  [2.x.342]  object, sorts degrees of freedom in such a way that all degrees of freedom associated with subdomain zero come before all those associated with subdomain one, etc. For the Stokes part, this entails, however, that velocities and pressures become intermixed, but this is trivially solved by sorting again by blocks; it is worth noting that this latter operation leaves the relative ordering of all velocities and pressures alone, i.e. within the velocity block we will still have all those associated with subdomain zero before all velocities associated with subdomain one, etc. This is important since we store each of the blocks of this matrix distributed across all processors and want this to be done in such a way that each processor stores that part of the matrix that is roughly equal to the degrees of freedom located on those cells that it will actually work on.   
*   When printing the numbers of degrees of freedom, note that these numbers are going to be large if we use many processors. Consequently, we let the stream put a comma separator in between every three digits. The state of the stream, using the locale, is saved from before to after this operation. While slightly opaque, the code works because the default locale (which we get using the constructor call  [2.x.343] ) implies printing numbers with a comma separator for every third digit (i.e., thousands, millions, billions).   
*   In this function as well as many below, we measure how much time we spend here and collect that in a section called "Setup dof systems" across function invocations. This is done using an  [2.x.344]  object that gets a timer going in the section with above name of the `computing_timer` object upon construction of the local variable; the timer is stopped again when the destructor of the `timing_section` variable is called.  This, of course, happens either at the end of the function, or if we leave the function through a `return` statement or when an exception is thrown somewhere
* 
*  -  in other words, whenever we leave this function in any way. The use of such "scope" objects therefore makes sure that we do not have to manually add code that tells the timer to stop at every location where this function may be left.
* 

* 
* [1.x.213]
* 
*  After this, we have to set up the various partitioners (of type  [2.x.345] , see the introduction) that describe which parts of each matrix or vector will be stored where, then call the functions that actually set up the matrices, and at the end also resize the various vectors we keep around in this program.
* 

* 
* [1.x.214]
* 
*  Following this, we can compute constraints for the solution vectors, including hanging node constraints and homogeneous and inhomogeneous boundary values for the Stokes and temperature fields. Note that as for everything else, the constraint objects can not hold [1.x.215] constraints on every processor. Rather, each processor needs to store only those that are actually necessary for correctness given that it only assembles linear systems on cells it owns. As discussed in the  [2.x.346]  "this paper", the set of constraints we need to know about is exactly the set of constraints on all locally relevant degrees of freedom, so this is what we use to initialize the constraint objects.
* 

* 
* [1.x.216]
* 
*  All this done, we can then initialize the various matrix and vector objects to their proper sizes. At the end, we also record that all matrices and preconditioners have to be re-computed at the beginning of the next time step. Note how we initialize the vectors for the Stokes and temperature right hand sides: These are writable vectors (last boolean argument set to  [2.x.347]  that have the correct one-to-one partitioning of locally owned elements but are still given the relevant partitioning for means of figuring out the vector entries that are going to be set right away. As for matrices, this allows for writing local contributions into the vector with multiple threads (always assuming that the same vector entry is not accessed by multiple threads at the same time). The other vectors only allow for read access of individual elements, including ghosts, but are not suitable for solvers.
* 

* 
* [1.x.217]
* 
*   [1.x.218]  [1.x.219]   
*   Following the discussion in the introduction and in the  [2.x.348]  module, we split the assembly functions into different parts:   
*    [2.x.349]   [2.x.350]  The local calculations of matrices and right hand sides, given a certain cell as input (these functions are named  [2.x.351]  below). The resulting function is, in other words, essentially the body of the loop over all cells in  [2.x.352] . Note, however, that these functions store the result from the local calculations in variables of classes from the CopyData namespace.   
*    [2.x.353] These objects are then given to the second step which writes the local data into the global data structures (these functions are named  [2.x.354]  below). These functions are pretty trivial.   
*    [2.x.355] These two subfunctions are then used in the respective assembly routine (called  [2.x.356]  below), where a WorkStream object is set up and runs over all the cells that belong to the processor's subdomain.   [2.x.357] 
* 

* 
*   [1.x.220]  [1.x.221]   
*   Let us start with the functions that builds the Stokes preconditioner. The first two of these are pretty trivial, given the discussion above. Note in particular that the main point in using the scratch data object is that we want to avoid allocating any objects on the free space each time we visit a new cell. As a consequence, the assembly function below only has automatic local variables, and everything else is accessed through the scratch data object, which is allocated only once before we start the loop over all cells:
* 

* 
* [1.x.222]
* 
*  Now for the function that actually puts things together, using the WorkStream functions.   [2.x.358]  needs a start and end iterator to enumerate the cells it is supposed to work on. Typically, one would use  [2.x.359]  and  [2.x.360]  for that but here we actually only want the subset of cells that in fact are owned by the current processor. This is where the FilteredIterator class comes into play: you give it a range of cells and it provides an iterator that only iterates over that subset of cells that satisfy a certain predicate (a predicate is a function of one argument that either returns true or false). The predicate we use here is  [2.x.361]  i.e., it returns true exactly if the cell is owned by the current processor. The resulting iterator range is then exactly what we need.   
*   With this obstacle out of the way, we call the  [2.x.362]  function with this set of cells, scratch and copy objects, and with pointers to two functions: the local assembly and copy-local-to-global function. These functions need to have very specific signatures: three arguments in the first and one argument in the latter case (see the documentation of the  [2.x.363]  function for the meaning of these arguments). Note how we use a lambda functions to create a function object that satisfies this requirement. It uses function arguments for the local assembly function that specify cell, scratch data, and copy data, as well as function argument for the copy function that expects the data to be written into the global matrix (also see the discussion in  [2.x.364] 's  [2.x.365]  function). On the other hand, the implicit zeroth argument of member functions (namely the  [2.x.366]  pointer of the object on which that member function is to operate on) is [1.x.223] to the  [2.x.367]  pointer of the current function and is captured. The  [2.x.368]  function, as a consequence, does not need to know anything about the object these functions work on.   
*   When the WorkStream is executed, it will create several local assembly routines of the first kind for several cells and let some available processors work on them. The function that needs to be synchronized, i.e., the write operation into the global matrix, however, is executed by only one thread at a time in the prescribed order. Of course, this only holds for the parallelization on a single MPI process. Different MPI processes will have their own WorkStream objects and do that work completely independently (and in different memory spaces). In a distributed calculation, some data will accumulate at degrees of freedom that are not owned by the respective processor. It would be inefficient to send data around every time we encounter such a dof. What happens instead is that the Trilinos sparse matrix will keep that data and send it to the owner at the end of assembly, by calling the  [2.x.369]  command.
* 

* 
* [1.x.224]
* 
*  The final function in this block initiates assembly of the Stokes preconditioner matrix and then in fact builds the Stokes preconditioner. It is mostly the same as in the serial case. The only difference to  [2.x.370]  is that we use a Jacobi preconditioner for the pressure mass matrix instead of IC, as discussed in the introduction.
* 

* 
* [1.x.225]
* 
*   [1.x.226]  [1.x.227]
* 

* 
*  The next three functions implement the assembly of the Stokes system, again split up into a part performing local calculations, one for writing the local data into the global matrix and vector, and one for actually running the loop over all cells with the help of the WorkStream class. Note that the assembly of the Stokes matrix needs only to be done in case we have changed the mesh. Otherwise, just the (temperature-dependent) right hand side needs to be calculated here. Since we are working with distributed matrices and vectors, we have to call the respective  [2.x.371]  functions in the end of the assembly in order to send non-local data to the owner process.
* 

* 
* [1.x.228]
* 
*   [1.x.229]  [1.x.230]
* 

* 
*  The task to be performed by the next three functions is to calculate a mass matrix and a Laplace matrix on the temperature system. These will be combined in order to yield the semi-implicit time stepping matrix that consists of the mass matrix plus a time  [2.x.372] dependent weight factor times the Laplace matrix. This function is again essentially the body of the loop over all cells from  [2.x.373] .   
*   The two following functions perform similar services as the ones above.
* 

* 
* [1.x.231]
* 
*   [1.x.232]  [1.x.233]
* 

* 
*  This is the last assembly function. It calculates the right hand side of the temperature system, which includes the convection and the stabilization terms. It includes a lot of evaluations of old solutions at the quadrature points (which are necessary for calculating the artificial viscosity of stabilization), but is otherwise similar to the other assembly functions. Notice, once again, how we resolve the dilemma of having inhomogeneous boundary conditions, by just making a right hand side at this point (compare the comments for the  [2.x.374]  function above): We create some matrix columns with exactly the values that would be entered for the temperature stiffness matrix, in case we have inhomogeneously constrained dofs. That will account for the correct balance of the right hand side vector with the matrix system of temperature.
* 

* 
* [1.x.234]
* 
*  In the function that runs the WorkStream for actually calculating the right hand side, we also generate the final matrix. As mentioned above, it is a sum of the mass matrix and the Laplace matrix, times some time  [2.x.375] dependent weight. This weight is specified by the BDF-2 time integration scheme, see the introduction in  [2.x.376] . What is new in this tutorial program (in addition to the use of MPI parallelization and the WorkStream class), is that we now precompute the temperature preconditioner as well. The reason is that the setup of the Jacobi preconditioner takes a noticeable time compared to the solver because we usually only need between 10 and 20 iterations for solving the temperature system (this might sound strange, as Jacobi really only consists of a diagonal, but in Trilinos it is derived from more general framework for point relaxation preconditioners which is a bit inefficient). Hence, it is more efficient to precompute the preconditioner, even though the matrix entries may slightly change because the time step might change. This is not too big a problem because we remesh every few time steps (and regenerate the preconditioner then).
* 

* 
* [1.x.235]
* 
*  The next part is computing the right hand side vectors.  To do so, we first compute the average temperature  [2.x.377]  that we use for evaluating the artificial viscosity stabilization through the residual  [2.x.378] . We do this by defining the midpoint between maximum and minimum temperature as average temperature in the definition of the entropy viscosity. An alternative would be to use the integral average, but the results are not very sensitive to this choice. The rest then only requires calling  [2.x.379]  again, binding the arguments to the  [2.x.380]  function that are the same in every call to the correct values:
* 

* 
* [1.x.236]
* 
*   [1.x.237]  [1.x.238]
* 

* 
*  This function solves the linear systems in each time step of the Boussinesq problem. First, we work on the Stokes system and then on the temperature system. In essence, it does the same things as the respective function in  [2.x.381] . However, there are a few changes here.   
*   The first change is related to the way we store our solution: we keep the vectors with locally owned degrees of freedom plus ghost nodes on each MPI node. When we enter a solver which is supposed to perform matrix-vector products with a distributed matrix, this is not the appropriate form, though. There, we will want to have the solution vector to be distributed in the same way as the matrix, i.e. without any ghosts. So what we do first is to generate a distributed vector called  [2.x.382]  and put only the locally owned dofs into that, which is neatly done by the  [2.x.383]  of the Trilinos vector.   
*   Next, we scale the pressure solution (or rather, the initial guess) for the solver so that it matches with the length scales in the matrices, as discussed in the introduction. We also immediately scale the pressure solution back to the correct units after the solution is completed.  We also need to set the pressure values at hanging nodes to zero. This we also did in  [2.x.384]  in order not to disturb the Schur complement by some vector entries that actually are irrelevant during the solve stage. As a difference to  [2.x.385] , here we do it only for the locally owned pressure dofs. After solving for the Stokes solution, each processor copies the distributed solution back into the solution vector that also includes ghost elements.   
*   The third and most obvious change is that we have two variants for the Stokes solver: A fast solver that sometimes breaks down, and a robust solver that is slower. This is what we already discussed in the introduction. Here is how we realize it: First, we perform 30 iterations with the fast solver based on the simple preconditioner based on the AMG V-cycle instead of an approximate solve (this is indicated by the  [2.x.386]  argument to the  [2.x.387]  object). If we converge, everything is fine. If we do not converge, the solver control object will throw an exception  [2.x.388]  Usually, this would abort the program because we don't catch them in our usual  [2.x.389]  functions. This is certainly not what we want to happen here. Rather, we want to switch to the strong solver and continue the solution process with whatever vector we got so far. Hence, we catch the exception with the C++ try/catch mechanism. We then simply go through the same solver sequence again in the  [2.x.390]  clause, this time passing the  [2.x.391]  flag to the preconditioner for the strong solver, signaling an approximate CG solve.
* 

* 
* [1.x.239]
* 
*  Now let's turn to the temperature part: First, we compute the time step size. We found that we need smaller time steps for 3D than for 2D for the shell geometry. This is because the cells are more distorted in that case (it is the smallest edge length that determines the CFL number). Instead of computing the time step from maximum velocity and minimal mesh size as in  [2.x.392] , we compute local CFL numbers, i.e., on each cell we compute the maximum velocity times the mesh size, and compute the maximum of them. Hence, we need to choose the factor in front of the time step slightly smaller.     
*   After temperature right hand side assembly, we solve the linear system for temperature (with fully distributed vectors without any ghosts), apply constraints and copy the vector back to one with ghosts.     
*   In the end, we extract the temperature range similarly to  [2.x.393]  to produce some output (for example in order to help us choose the stabilization constants, as discussed in the introduction). The only difference is that we need to exchange maxima over all processors.
* 

* 
* [1.x.240]
* 
*   [1.x.241]  [1.x.242]
* 

* 
*  Next comes the function that generates the output. The quantities to output could be introduced manually like we did in  [2.x.394] . An alternative is to hand this task over to a class PostProcessor that inherits from the class DataPostprocessor, which can be attached to DataOut. This allows us to output derived quantities from the solution, like the friction heating included in this example. It overloads the virtual function  [2.x.395]  which is then internally called from  [2.x.396]  We have to give it values of the numerical solution, its derivatives, normals to the cell, the actual evaluation points and any additional quantities. This follows the same procedure as discussed in  [2.x.397]  and other programs.
* 

* 
* [1.x.243]
* 
*  Here we define the names for the variables we want to output. These are the actual solution values for velocity, pressure, and temperature, as well as the friction heating and to each cell the number of the processor that owns it. This allows us to visualize the partitioning of the domain among the processors. Except for the velocity, which is vector-valued, all other quantities are scalar.
* 

* 
* [1.x.244]
* 
*  Now we implement the function that computes the derived quantities. As we also did for the output, we rescale the velocity from its SI units to something more readable, namely cm/year. Next, the pressure is scaled to be between 0 and the maximum pressure. This makes it more easily comparable
* 
*  -  in essence making all pressure variables positive or zero. Temperature is taken as is, and the friction heating is computed as  [2.x.398] .   
*   The quantities we output here are more for illustration, rather than for actual scientific value. We come back to this briefly in the results section of this program and explain what one may in fact be interested in.
* 

* 
* [1.x.245]
* 
*  The  [2.x.399]  function has a similar task to the one in  [2.x.400] . However, here we are going to demonstrate a different technique on how to merge output from different DoFHandler objects. The way we're going to achieve this recombination is to create a joint DoFHandler that collects both components, the Stokes solution and the temperature solution. This can be nicely done by combining the finite elements from the two systems to form one FESystem, and let this collective system define a new DoFHandler object. To be sure that everything was done correctly, we perform a sanity check that ensures that we got all the dofs from both Stokes and temperature even in the combined system. We then combine the data vectors. Unfortunately, there is no straight-forward relation that tells us how to sort Stokes and temperature vector into the joint vector. The way we can get around this trouble is to rely on the information collected in the FESystem. For each dof on a cell, the joint finite element knows to which equation component (velocity component, pressure, or temperature) it belongs – that's the information we need! So we step through all cells (with iterators into all three DoFHandlers moving in sync), and for each joint cell dof, we read out that component using the  [2.x.401]  function (see there for a description of what the various parts of its return value contain). We also need to keep track whether we're on a Stokes dof or a temperature dof, which is contained in joint_fe.system_to_base_index(i).first.first. Eventually, the dof_indices data structures on either of the three systems tell us how the relation between global vector and local dofs looks like on the present cell, which concludes this tedious work. We make sure that each processor only works on the subdomain it owns locally (and not on ghost or artificial cells) when building the joint solution vector. The same will then have to be done in  [2.x.402]  but that function does so automatically.   
*   What we end up with is a set of patches that we can write using the functions in DataOutBase in a variety of output formats. Here, we then have to pay attention that what each processor writes is really only its own part of the domain, i.e. we will want to write each processor's contribution into a separate file. This we do by adding an additional number to the filename when we write the solution. This is not really new, we did it similarly in  [2.x.403] . Note that we write in the compressed format  [2.x.404]  instead of plain vtk files, which saves quite some storage.   
*   All the rest of the work is done in the PostProcessor class.
* 

* 
* [1.x.246]
* 
*   [1.x.247]  [1.x.248]
* 

* 
*  This function isn't really new either. Since the  [2.x.405]  function that we call in the middle has its own timer section, we split timing this function into two sections. It will also allow us to easily identify which of the two is more expensive.   
*   One thing of note, however, is that we only want to compute error indicators on the locally owned subdomain. In order to achieve this, we pass one additional argument to the  [2.x.406]  function. Note that the vector for error estimates is resized to the number of active cells present on the current process, which is less than the total number of active cells on all processors (but more than the number of locally owned active cells); each processor only has a few coarse cells around the locally owned ones, as also explained in  [2.x.407] .   
*   The local error estimates are then handed to a %parallel version of GridRefinement (in namespace  [2.x.408]  see also  [2.x.409] ) which looks at the errors and finds the cells that need refinement by comparing the error values across processors. As in  [2.x.410] , we want to limit the maximum grid level. So in case some cells have been marked that are already at the finest level, we simply clear the refine flags.
* 

* 
* [1.x.249]
* 
*  With all flags marked as necessary, we can then tell the  [2.x.411]  objects to get ready to transfer data from one mesh to the next, which they will do when notified by Triangulation as part of the  [2.x.412]  call. The syntax is similar to the non-%parallel solution transfer (with the exception that here a pointer to the vector entries is enough). The remainder of the function further down below is then concerned with setting up the data structures again after mesh refinement and restoring the solution vectors on the new mesh.
* 

* 
* [1.x.250]
* 
*  enforce constraints to make the interpolated solution conforming on the new mesh:
* 

* 
* [1.x.251]
* 
*  enforce constraints to make the interpolated solution conforming on the new mesh:
* 

* 
* [1.x.252]
* 
*   [1.x.253]  [1.x.254]
* 

* 
*  This is the final and controlling function in this class. It, in fact, runs the entire rest of the program and is, once more, very similar to  [2.x.413] . The only substantial difference is that we use a different mesh now (a  [2.x.414]  instead of a simple cube geometry).
* 

* 
* [1.x.255]
* 
*   [2.x.415]  supports parallel vector classes with most standard finite elements via deal.II's own native MatrixFree framework: since we use standard Lagrange elements of moderate order this function works well here.
* 

* 
* [1.x.256]
* 
*  Having so computed the current temperature field, let us set the member variable that holds the temperature nodes. Strictly speaking, we really only need to set  [2.x.416]  since the first thing we will do is to compute the Stokes solution that only requires the previous time step's temperature field. That said, nothing good can come from not initializing the other vectors as well (especially since it's a relatively cheap operation and we only have to do it once at the beginning of the program) if we ever want to extend our numerical method or physical model, and so we initialize  [2.x.417]  and  [2.x.418]  as well. The assignment makes sure that the vectors on the left hand side (which where initialized to contain ghost elements as well) also get the correct ghost elements. In other words, the assignment here requires communication between processors:
* 

* 
* [1.x.257]
* 
*  In order to speed up linear solvers, we extrapolate the solutions from the old time levels to the new one. This gives a very good initial guess, cutting the number of iterations needed in solvers by more than one half. We do not need to extrapolate in the last iteration, so if we reached the final time, we stop here.         
*   As the last thing during a time step (before actually bumping up the number of the time step), we check whether the current time step number is divisible by 100, and if so we let the computing timer print a summary of CPU times spent so far.
* 

* 
* [1.x.258]
* 
*  Trilinos sadd does not like ghost vectors even as input. Copy into distributed vectors for now:
* 

* 
* [1.x.259]
* 
*  If we are generating graphical output, do so also for the last time step unless we had just done so before we left the do-while loop
* 

* 
* [1.x.260]
* 
*   [1.x.261]  [1.x.262]
* 

* 
*  The main function is short as usual and very similar to the one in  [2.x.419] . Since we use a parameter file which is specified as an argument in the command line, we have to read it in here and pass it on to the Parameters class for parsing. If no filename is given in the command line, we simply use the  [2.x.420]  file which is distributed together with the program.
* 

* 
*  Because 3d computations are simply very slow unless you throw a lot of processors at them, the program defaults to 2d. You can get the 3d version by changing the constant dimension below to 3.
* 

* 
* [1.x.263]
* [1.x.264][1.x.265]
* 

* When run, the program simulates convection in 3d in much the same wayas  [2.x.421]  did, though with an entirely different testcase.
* 

* [1.x.266][1.x.267]
* 

* Before we go to this testcase, however, let us show a few results from aslightly earlier version of this program that was solving exactly thetestcase we used in  [2.x.422] , just that we now solve it in parallel and withmuch higher resolution. We show these results mainly for comparison.
* Here are two images that show this higher resolution if we choose a 3dcomputation in  [2.x.423]  and if we set [2.x.424]  and [2.x.425] . At the time steps shown, themeshes had around 72,000 and 236,000 cells, for a total of 2,680,000and 8,250,000 degrees of freedom, respectively, more than an order ofmagnitude more than we had available in  [2.x.426] :
*  [2.x.427] 
* The computation was done on a subset of 50 processors of the Brazoscluster at Texas A&amp;M University.
* 

* [1.x.268][1.x.269]
* 

* Next, we will run  [2.x.428]  with the parameter file in the directory with onechange: we increase the final time to 1e9. Here we are using 16 processors. Thecommand to launch is (note that  [2.x.429] .prm is the default):
* <code><pre>\ [2.x.430]  mpirun
* 
*  - p 16 ./ [2.x.431] Number of active cells: 12,288 (on 6 levels)Number of degrees of freedom: 186,624 (99,840+36,864+49,920)
* Timestep 0:  t=0 years
*    Rebuilding Stokes preconditioner...   Solving Stokes system... 41 iterations.   Maximal velocity: 60.4935 cm/year   Time step: 18166.9 years   17 CG iterations for temperature   Temperature range: 973 4273.16
* Number of active cells: 15,921 (on 7 levels)Number of degrees of freedom: 252,723 (136,640+47,763+68,320)
* Timestep 0:  t=0 years
*    Rebuilding Stokes preconditioner...   Solving Stokes system... 50 iterations.   Maximal velocity: 60.3223 cm/year   Time step: 10557.6 years   19 CG iterations for temperature   Temperature range: 973 4273.16
* Number of active cells: 19,926 (on 8 levels)Number of degrees of freedom: 321,246 (174,312+59,778+87,156)
* Timestep 0:  t=0 years
*    Rebuilding Stokes preconditioner...   Solving Stokes system... 50 iterations.   Maximal velocity: 57.8396 cm/year   Time step: 5453.78 years   18 CG iterations for temperature   Temperature range: 973 4273.16
* Timestep 1:  t=5453.78 years
*    Solving Stokes system... 49 iterations.   Maximal velocity: 59.0231 cm/year   Time step: 5345.86 years   18 CG iterations for temperature   Temperature range: 973 4273.16
* Timestep 2:  t=10799.6 years
*    Solving Stokes system... 24 iterations.   Maximal velocity: 60.2139 cm/year   Time step: 5241.51 years   17 CG iterations for temperature   Temperature range: 973 4273.16
* [...]
* Timestep 100:  t=272151 years
*    Solving Stokes system... 21 iterations.   Maximal velocity: 161.546 cm/year   Time step: 1672.96 years   17 CG iterations for temperature   Temperature range: 973 4282.57
* Number of active cells: 56,085 (on 8 levels)Number of degrees of freedom: 903,408 (490,102+168,255+245,051)
* 

* 
* +---------------------------------------------+------------+------------+| Total wallclock time elapsed since start    |       115s |            ||                                             |            |            || Section                         | no. calls |  wall time | % of total |+---------------------------------+-----------+------------+------------+| Assemble Stokes system          |       103 |      2.82s |       2.5% || Assemble temperature matrices   |        12 |     0.452s |      0.39% || Assemble temperature rhs        |       103 |      11.5s |        10% || Build Stokes preconditioner     |        12 |      2.09s |       1.8% || Solve Stokes system             |       103 |      90.4s |        79% || Solve temperature system        |       103 |      1.53s |       1.3% || Postprocessing                  |         3 |     0.532s |      0.46% || Refine mesh structure, part 1   |        12 |      0.93s |      0.81% || Refine mesh structure, part 2   |        12 |     0.384s |      0.33% || Setup dof systems               |        13 |      2.96s |       2.6% |+---------------------------------+-----------+------------+------------+
* [...]
* +---------------------------------------------+------------+------------+| Total wallclock time elapsed since start    |  9.14e+04s |            ||                                             |            |            || Section                         | no. calls |  wall time | % of total |+---------------------------------+-----------+------------+------------+| Assemble Stokes system          |     47045 |  2.05e+03s |       2.2% || Assemble temperature matrices   |      4707 |       310s |      0.34% || Assemble temperature rhs        |     47045 |   8.7e+03s |       9.5% || Build Stokes preconditioner     |      4707 |  1.48e+03s |       1.6% || Solve Stokes system             |     47045 |  7.34e+04s |        80% || Solve temperature system        |     47045 |  1.46e+03s |       1.6% || Postprocessing                  |      1883 |       222s |      0.24% || Refine mesh structure, part 1   |      4706 |       641s |       0.7% || Refine mesh structure, part 2   |      4706 |       259s |      0.28% || Setup dof systems               |      4707 |  1.86e+03s |         2% |+---------------------------------+-----------+------------+------------+</pre></code>
* The simulation terminates when the time reaches the 1 billion yearsselected in the input file.  You can extrapolate from this how long asimulation would take for a different final time (the time step sizeultimately settles on somewhere around 20,000 years, so computing fortwo billion years will take 100,000 time steps, give or take 20%).  Ascan be seen here, we spend most of the compute time in assemblinglinear systems and &mdash; above all &mdash; in solving Stokessystems.
* 

* To demonstrate the output we show the output from every 1250th time step here: [2.x.432] 
* The last two images show the grid as well as the partitioning of the mesh forthe same computation with 16 subdomains and 16 processors. The full dynamics ofthis simulation are really only visible by looking at an animation, for examplethe one [1.x.270]. This image is well worth watching due to its artistic qualityand entrancing depiction of the evolution of the magma plumes.
* If you watch the movie, you'll see that the convection pattern goesthrough several stages: First, it gets rid of the instable temperaturelayering with the hot material overlain by the dense coldmaterial. After this great driver is removed and we have a sort ofstable situation, a few blobs start to separate from the hot boundarylayer at the inner ring and rise up, with a few cold fingers alsodropping down from the outer boundary layer. During this phase, the solutionremains mostly symmetric, reflecting the 12-fold symmetry of theoriginal mesh. In a final phase, the fluid enters vigorous chaoticstirring in which all symmetries are lost. This is a pattern that thencontinues to dominate flow.
* These different phases can also be identified if we look at themaximal velocity as a function of time in the simulation:
*  [2.x.433] 
* Here, the velocity (shown in centimeters per year) becomes very large,to the order of several meters per year) at the beginning when thetemperature layering is instable. It then calms down to relativelysmall values before picking up again in the chaotic stirringregime. There, it remains in the range of 10-40 centimeters per year,quite within the physically expected region.
* 

* [1.x.271][1.x.272]
* 

* 3d computations are very expensive computationally. Furthermore, asseen above, interesting behavior only starts after quite a long timerequiring more CPU hours than is available on a typicalcluster. Consequently, rather than showing a complete simulation here,let us simply show a couple of pictures we have obtained using thesuccessor to this program, called [1.x.273] (short for [1.x.274]), that is beingdeveloped independently of deal.II and that already incorporates someof the extensions discussed below. The following two pictures showisocontours of the temperature and the partition of the domain (alongwith the mesh) onto 512 processors:
*  [2.x.434] 
* 

* [1.x.275][1.x.276][1.x.277]
* 

* There are many directions in which this program could be extended. Asmentioned at the end of the introduction, most of these are under activedevelopment in the [1.x.278] (short for [1.x.279]) code at the time this tutorial program is beingfinished. Specifically, the following are certainly topics that one shouldaddress to make the program more useful:
*  [2.x.435]    [2.x.436]  [1.x.280]  The temperature field we get in our simulations after a while  is mostly constant with boundary layers at the inner and outer  boundary, and streamers of cold and hot material mixing  everything. Yet, this doesn't match our expectation that things  closer to the earth core should be hotter than closer to the  surface. The reason is that the energy equation we have used does  not include a term that describes adiabatic cooling and heating:  rock, like gas, heats up as you compress it. Consequently, material  that rises up cools adiabatically, and cold material that sinks down  heats adiabatically. The correct temperature equation would  therefore look somewhat like this:  [1.x.281]
*   or, expanding the advected derivative  [2.x.437] :  [1.x.282]
*   In other words, as pressure increases in a rock volume  ( [2.x.438] ) we get an additional heat source, and vice  versa.
*   The time derivative of the pressure is a bit awkward to  implement. If necessary, one could approximate using the fact  outlined in the introduction that the pressure can be decomposed  into a dynamic component due to temperature differences and the  resulting flow, and a static component that results solely from the  static pressure of the overlying rock. Since the latter is much  bigger, one may approximate  [2.x.439] , and consequently   [2.x.440] .  In other words, if the fluid is moving in the direction of gravity  (downward) it will be compressed and because in that case  [2.x.441]  we get a positive heat source. Conversely, the  fluid will cool down if it moves against the direction of gravity.
*  [2.x.442]  [1.x.283]  As already hinted at in the temperature model above,  mantle rocks are not incompressible. Rather, given the enormous pressures in  the earth mantle (at the core-mantle boundary, the pressure is approximately  140 GPa, equivalent to 1,400,000 times atmospheric pressure), rock actually  does compress to something around 1.5 times the density it would have  at surface pressure. Modeling this presents any number of  difficulties. Primarily, the mass conservation equation is no longer   [2.x.443]  but should read   [2.x.444]  where the density  [2.x.445]  is now no longer  spatially constant but depends on temperature and pressure. A consequence is  that the model is now no longer linear; a linearized version of the Stokes  equation is also no longer symmetric requiring us to rethink preconditioners  and, possibly, even the discretization. We won't go into detail here as to  how this can be resolved.
*  [2.x.446]  [1.x.284] As already hinted at in various places,  material parameters such as the density, the viscosity, and the various  thermal parameters are not constant throughout the earth mantle. Rather,  they nonlinearly depend on the pressure and temperature, and in the case of  the viscosity on the strain rate  [2.x.447] . For complicated  models, the only way to solve such models accurately may be to actually  iterate this dependence out in each time step, rather than simply freezing  coefficients at values extrapolated from the previous time step(s).
*  [2.x.448]  [1.x.285] Running this program in 2d on a number of  processors allows solving realistic models in a day or two. However, in 3d,  compute times are so large that one runs into two typical problems: (i) On  most compute clusters, the queuing system limits run times for individual  jobs are to 2 or 3 days; (ii) losing the results of a computation due to  hardware failures, misconfigurations, or power outages is a shame when  running on hundreds of processors for a couple of days. Both of these  problems can be addressed by periodically saving the state of the program  and, if necessary, restarting the program at this point. This technique is  commonly called [1.x.286] and it requires that the entire  state of the program is written to a permanent storage location (e.g. a hard  drive). Given the complexity of the data structures of this program, this is  not entirely trivial (it may also involve writing gigabytes or more of  data), but it can be made easier by realizing that one can save the state  between two time steps where it essentially only consists of the mesh and  solution vectors; during restart one would then first re-enumerate degrees  of freedom in the same way as done before and then re-assemble  matrices. Nevertheless, given the distributed nature of the data structures  involved here, saving and restoring the state of a program is not  trivial. An additional complexity is introduced by the fact that one may  want to change the number of processors between runs, for example because  one may wish to continue computing on a mesh that is finer than the one used  to precompute a starting temperature field at an intermediate time.
*  [2.x.449]  [1.x.287] The point of computations like this is  not simply to solve the equations. Rather, it is typically the exploration  of different physical models and their comparison with things that we can  measure at the earth surface, in order to find which models are realistic  and which are contradicted by reality. To this end, we need to compute  quantities from our solution vectors that are related to what we can  observe. Among these are, for example, heatfluxes at the surface of the  earth, as well as seismic velocities throughout the mantle as these affect  earthquake waves that are recorded by seismographs.
*  [2.x.450]  [1.x.288] As can be seen above for the3d case, the mesh in 3d is primarily refined along the innerboundary. This is because the boundary layer there is stronger thanany other transition in the domain, leading us to refine there almostexclusively and basically not at all following the plumes. Onecertainly needs better refinement criteria to track the parts of thesolution we are really interested in better than the criterion usedhere, namely the KellyErrorEstimator applied to the temperature, isable to. [2.x.451] 
* 

* There are many other ways to extend the current program. However, rather thandiscussing them here, let us point to the much larger opensource code ASPECT (see https://aspect.geodynamics.org/ ) that constitutes thefurther development of  [2.x.452]  and that already includes many such possibleextensions.
* 

* [1.x.289][1.x.290] [2.x.453] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-33_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] ,  [2.x.2] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34][1.x.35][1.x.36][1.x.37][1.x.38][1.x.39][1.x.40][1.x.41][1.x.42][1.x.43][1.x.44][1.x.45]
*  [2.x.3] 
* [1.x.46]
*  [2.x.4]  The program uses the [1.x.47] linear solvers (these can be foundin Trilinos in the Aztec/Amesos packages) and an automaticdifferentiation package, Sacado, also part of Trilinos. deal.II mustbe configured to use Trilinos. Refer to the [1.x.48] file for instructions how todo this.
*  [2.x.5]  While this program demonstrates the use of automatic differentiationwell, it does not express the state of the art in Euler equation solvers.There are much faster and more accurate method for this equation, andyou should take a look at  [2.x.6]  and  [2.x.7]  to see how this equationcan be solved more efficiently.
* 

* 
* [1.x.49][1.x.50] [1.x.51]
* 

* [1.x.52][1.x.53]
* 

* The equations that describe the movement of a compressible, inviscidgas (the so-called Euler equations of gas dynamics) area basic system of conservation laws. In spatial dimension  [2.x.8]  they read[1.x.54]with the solution  [2.x.9]  consisting of  [2.x.10]  the fluid density,  [2.x.11]  theflow velocity (and thus  [2.x.12]  being the linear momentumdensity), and [2.x.13]  the energy density of the gas. We interpret the equations above as [2.x.14] ,  [2.x.15] .
* For the Euler equations, the flux matrix  [2.x.16]  (or system of flux functions)is defined as (shown here for the case  [2.x.17] )[1.x.55]
* and we will choose as particular right hand side forcing only the effects ofgravity, described by[1.x.56]
* where  [2.x.18]  denotes the gravity vector.With this, the entire system of equations reads:[1.x.57]
* These equations describe, respectively, the conservation of momentum,mass, and energy.The system is closed by a relation that defines the pressure:  [2.x.19] . For the constituentsof air (mainly nitrogen and oxygen) and other diatomic gases, the ratio ofspecific heats is  [2.x.20] .
* This problem obviously falls into the class of vector-valuedproblems. A general overview of how to deal with these problems indeal.II can be found in the  [2.x.21]  module.
* [1.x.58][1.x.59]
* 

* Discretization happens in the usual way, taking into account that thisis a hyperbolic problem in the same style as the simple one discussedin  [2.x.22] :We choose a finite element space  [2.x.23] , and integrate our conservation law againstour (vector-valued) test function  [2.x.24] .  We then integrate by parts and approximate theboundary flux with a [1.x.60] flux  [2.x.25] ,[1.x.61]
* where a superscript  [2.x.26]  denotes the interior trace of a function, and  [2.x.27]  represents the outer trace.The diffusion term  [2.x.28]  is introduced strictly for stability, where  [2.x.29]  is the mesh size and  [2.x.30]  is a parameter prescribing how much diffusion to add.
* On the boundary, we have to say what the outer trace  [2.x.31]  is.Depending on the boundary condition, we prescribe either of the following: [2.x.32]  [2.x.33]  Inflow boundary:  [2.x.34]  is prescribed to be the desired value. [2.x.35]  Supersonic outflow boundary:  [2.x.36]  [2.x.37]  Subsonic outflow boundary:  [2.x.38]  except that the energy variableis modified to support a prescribed pressure  [2.x.39] , i.e. [2.x.40]  [2.x.41]  Reflective boundary: we set  [2.x.42]  so that  [2.x.43]  and [2.x.44] . [2.x.45] 
* More information on these issues can be found, for example, in RalfHartmann's PhD thesis ("Adaptive Finite Element Methods for theCompressible Euler Equations", PhD thesis, University of Heidelberg, 2002).
* We use a time stepping scheme to substitute the time derivative in theabove equations. For simplicity, we define  [2.x.46]  as the spatial residual at time step  [2.x.47]  :
* [1.x.62]
* 
* At each time step, our full discretization is thusthat the residual applied to any testfunction  [2.x.48]  equals zero:[1.x.63]
* where  [2.x.49]  and [2.x.50] . Choosing [2.x.51]  results in the explicit (forward) Euler scheme,  [2.x.52] in the stable implicit (backward) Euler scheme, and  [2.x.53] in the Crank-Nicolson scheme.
* In the implementation below, we choose the Lax-Friedrichs flux for thefunction  [2.x.54] , i.e.   [2.x.55] ,where  [2.x.56]  is either a fixed number specified in the input file, or where [2.x.57]  is a mesh dependent value. In the latter case, it is chosen as [2.x.58]  with  [2.x.59]  the diameter of the face to which the flux isapplied, and  [2.x.60]  the current time step.
* With these choices, equating the residual to zero results in anonlinear system of equations  [2.x.61] . We solve this nonlinear system by aNewton iteration (in the same way as explained in  [2.x.62] ), i.e. by iterating[1.x.64]
* until  [2.x.63]  (the residual) is sufficiently small. Bytesting with the nodal basis of a finite element space instead of all [2.x.64] , we arrive at a linear system for  [2.x.65] :[1.x.65]
* This linear system is, in general, neither symmetric nor has anyparticular definiteness properties. We will either use a direct solveror Trilinos' GMRES implementation to solve it. As will become apparent fromthe [1.x.66], this fully implicit iterationconverges very rapidly (typically in 3 steps) and with the quadraticconvergence order expected from a Newton method.
* 

* [1.x.67][1.x.68]
* 

* Since computing the Jacobian matrix  [2.x.66]  is aterrible beast, we use an automatic differentiation package, Sacado,to do this.  Sacado is a package within the [1.x.69] frameworkand offers a C++ template class  [2.x.67] ( [2.x.68]  standing for "forward automaticdifferentiation") that supports basic arithmetic operators andfunctions such as  [2.x.69]  etc. In order touse this feature, one declares a collection of variables of this typeand then denotes some of this collection as degrees of freedom, the rest ofthe variables being functions of the independent variables.  Thesevariables are used in an algorithm, and as the variables are used,their sensitivities with respect to the degrees of freedom arecontinuously updated.
* One can imagine that for the full Jacobian matrix as a whole,this could be prohibitively expensive: the number of independent variables arethe  [2.x.70] , the dependent variables the elements of the vector  [2.x.71] . Both of these vectors can easily have tens of thousands ofelements or more.  However, it is important to note that not all elements of [2.x.72]  depend on all elements of  [2.x.73] : in fact, an entry in [2.x.74]  only depends on an element of  [2.x.75]  if the twocorresponding shape functions overlap and couple in the weak form.
* Specifically, it is wise to define a minimum set ofindependent AD variables that the residual on the current cell may possiblydepend on: on every element, we define those variables asindependent that correspond to the degrees of freedom defined on thiscell (or, if we have to compute jump terms between cells, thatcorrespond to degrees of freedom defined on either of the two adjacentcells), and the dependent variables are the elements of the localresidual vector. Not doing this, i.e. defining [1.x.70] elements of [2.x.76]  as independent, will result a very expensive computationof a lot of zeros: the elements of the local residual vector areindependent of almost all elements of the solution vector, andconsequently their derivatives are zero; however, trying to computethese zeros can easily take 90% or more of the compute time of theentire program, as shown in an experiment inadvertently made by a student a fewyears after this program was first written.
* 

* Coming back to the question of computing the Jacobian automatically:The author has used this approach side by side with a hand coded Jacobian forthe incompressible Navier-Stokes problem and found the Sacado approach to bejust as fast as using a hand coded Jacobian, but infinitely simpler and lesserror prone: Since using the auto-differentiation requires only that one codethe residual  [2.x.77] , ensuring code correctness and maintaining codebecomes tremendously more simple
* 
*  -  the Jacobian matrix  [2.x.78]  iscomputed by essentially the same code that also computes the residual  [2.x.79] .
* All this said, here's a very simple example showing how Sacado can beused:
* [1.x.71]
* 
* The output are the derivatives  [2.x.80]  of  [2.x.81]  at  [2.x.82] .
* It should be noted that Sacado provides more auto-differentiation capabilities than the small subsetused in this program.  However, understanding the example above isenough to understand the use of Sacado in this Euler flow program.
* [1.x.72][1.x.73]
* The program uses either the Aztec iterative solvers, or the Amesossparse direct solver, both provided bythe Trilinos package.  This package is inherently designed to be used in a parallel program, however,it may be used in serial just as easily, as is done here.  The Epetra package is the basicvector/matrix library upon which the solvers are built.  This very powerful package can be usedto describe the parallel distribution of a vector, and to define sparse matrices that operateon these vectors.  Please view the commented code for more details on how these solvers are usedwithin the example.
* [1.x.74][1.x.75]
* The example uses an ad hoc refinement indicator that shows some usefulness in shock-type problems, andin the downhill flow example included.  We refine according to the squared gradient of the density.Hanging nodes are handled by computing the numerical flux across cells that are of differingrefinement levels, rather than using the AffineConstraints class as inall other tutorial programs so far.  In this way, the example combinesthe continuous and DG methodologies. It also simplifies the generationof the Jacobian because we do not have to track constrained degrees offreedom through the automatic differentiation used to compute it.
*  [2.x.83]  Whereas this program was written in 2008, we were unaware of anypublication that would actually have used this approach. However, amore recent paper by A. Dedner, R. Kl&ouml;fkorn, and M. Kr&auml;nkel("Continuous Finite-Elements on Non-Conforming Grids UsingDiscontinuous Galerkin Stabilization", Proceedings of Finite Volumesfor Complex Applications VII
* 
*  - Methods and Theoretical Aspects,Springer, 2014) comes close.
* Further, we enforce a maximum number of refinement levels to keep refinement under check.  It is theauthor's experience that for adaptivity for a time dependent problem, refinement can easily lead the simulation toa screeching halt, because of time step restrictions if the meshbecomes too fine in any part of the domain, if care is not taken.  The amount of refinement islimited in the example by letting the user specify themaximum level of refinement that will be present anywhere in the mesh.  In this way, refinementtends not to slow the simulation to a halt.  This, of course, is purely a heuristic strategy, andif the author's advisor heard about it, the author would likely be exiled forever from the finite element error estimation community.
* [1.x.76][1.x.77]
* 

* We use an input file deck to drive the simulation.  In this way, we can alter the boundary conditionsand other important properties of the simulation without having to recompile.  For more information onthe format, look at the [1.x.78], where wedescribe an example input file in more detail.
* In previous example programs, we have usually hard-coded the initialand boundary conditions. In this program, we instead use theexpression parser class FunctionParser so that we can specify ageneric expression in the input file and have it parsed at run time &mdash;this way, we can change initial conditions without the need torecompile the program. Consequently, no classes namedInitialConditions or BoundaryConditions will be declared in theprogram below.
* 

* [1.x.79][1.x.80]
* 

* The implementation of this program is split into three essential parts: [2.x.84]    [2.x.85] The  [2.x.86]  class that encapsulates everything that  completely describes the specifics of the Euler equations. This includes the  flux matrix  [2.x.87] , the numerical flux  [2.x.88] , the right hand side  [2.x.89] ,  boundary conditions, refinement indicators, postprocessing the output, and  similar things that require knowledge of the meaning of the individual  components of the solution vectors and the equations.
*    [2.x.90] A namespace that deals with everything that has to do with run-time  parameters.
*    [2.x.91] The  [2.x.92]  class that deals with time stepping,  outer nonlinear and inner linear solves, assembling the linear systems, and  the top-level logic that drives all this. [2.x.93] 
* The reason for this approach is that it separates the various concerns in aprogram: the  [2.x.94]  is written in such a way that itwould be relatively straightforward to adapt it to a different set ofequations: One would simply re-implement the members of the [2.x.95]  class for some other hyperbolic equation, oraugment the existing equations by additional ones (for example by advectingadditional variables, or by adding chemistry, etc). Such modifications,however, would not affect the time stepping, or the nonlinear solvers ifcorrectly done, and consequently nothing in the  [2.x.96] would have to be modified.
* Similarly, if we wanted to improve on the linear or nonlinear solvers, or onthe time stepping scheme (as hinted at the end of the [1.x.81]), then this would not require changes inthe  [2.x.97]  at all.
* 

*  [1.x.82] [1.x.83]
*   [1.x.84]  [1.x.85]
* 

* 
*  First a standard set of deal.II includes. Nothing special to comment on here:
* 

* 
* [1.x.86]
* 
*  Then, as mentioned in the introduction, we use various Trilinos packages as linear solvers as well as for automatic differentiation. These are in the following include files.
* 

* 
*  Since deal.II provides interfaces to the basic Trilinos matrices, preconditioners and solvers, we include them similarly as deal.II linear algebra structures.
* 

* 
* [1.x.87]
* 
*  Sacado is the automatic differentiation package within Trilinos, which is used to find the Jacobian for a fully implicit Newton iteration:
* 

* 
* [1.x.88]
* 
*  And this again is C++:
* 

* 
* [1.x.89]
* 
*  To end this section, introduce everything in the dealii library into the namespace into which the contents of this program will go:
* 

* 
* [1.x.90]
* 
*   [1.x.91]  [1.x.92]
* 

* 
*  Here we define the flux function for this particular system of conservation laws, as well as pretty much everything else that's specific to the Euler equations for gas dynamics, for reasons discussed in the introduction. We group all this into a structure that defines everything that has to do with the flux. All members of this structure are static, i.e. the structure has no actual state specified by instance member variables. The better way to do this, rather than a structure with all static members would be to use a namespace
* 
*  -  but namespaces can't be templatized and we want some of the member variables of the structure to depend on the space dimension, which we in our usual way introduce using a template parameter.
* 

* 
* [1.x.93]
* 
*   [1.x.94]  [1.x.95]
* 

* 
*  First a few variables that describe the various components of our solution vector in a generic way. This includes the number of components in the system (Euler's equations have one entry for momenta in each spatial direction, plus the energy and density components, for a total of  [2.x.98]  components), as well as functions that describe the index within the solution vector of the first momentum component, the density component, and the energy density component. Note that all these %numbers depend on the space dimension; defining them in a generic way (rather than by implicit convention) makes our code more flexible and makes it easier to later extend it, for example by adding more components to the equations.
* 

* 
* [1.x.96]
* 
*  When generating graphical output way down in this program, we need to specify the names of the solution variables as well as how the various components group into vector and scalar fields. We could describe this there, but in order to keep things that have to do with the Euler equation localized here and the rest of the program as generic as possible, we provide this sort of information in the following two functions:
* 

* 
* [1.x.97]
* 
*   [1.x.98]  [1.x.99]
* 

* 
*  Next, we define the gas constant. We will set it to 1.4 in its definition immediately following the declaration of this class (unlike integer variables, like the ones above, static const floating point member variables cannot be initialized within the class declaration in C++). This value of 1.4 is representative of a gas that consists of molecules composed of two atoms, such as air which consists up to small traces almost entirely of  [2.x.99]  and  [2.x.100] .
* 

* 
* [1.x.100]
* 
*  In the following, we will need to compute the kinetic energy and the pressure from a vector of conserved variables. This we can do based on the energy density and the kinetic energy  [2.x.101]  (note that the independent variables contain the momentum components  [2.x.102] , not the velocities  [2.x.103] ).
* 

* 
* [1.x.101]
* 
*   [1.x.102]  [1.x.103]
* 

* 
*  We define the flux function  [2.x.104]  as one large matrix.  Each row of this matrix represents a scalar conservation law for the component in that row.  The exact form of this matrix is given in the introduction. Note that we know the size of the matrix: it has as many rows as the system has components, and  [2.x.105]  columns; rather than using a FullMatrix object for such a matrix (which has a variable number of rows and columns and must therefore allocate memory on the heap each time such a matrix is created), we use a rectangular array of numbers right away.     
*   We templatize the numerical type of the flux function so that we may use the automatic differentiation type here.  Similarly, we will call the function with different input vector data types, so we templatize on it as well:
* 

* 
* [1.x.104]
* 
*  First compute the pressure that appears in the flux matrix, and then compute the first  [2.x.106]  columns of the matrix that correspond to the momentum terms:
* 

* 
* [1.x.105]
* 
*  Then the terms for the density (i.e. mass conservation), and, lastly, conservation of energy:
* 

* 
* [1.x.106]
* 
*   [1.x.107]  [1.x.108]
* 

* 
*  On the boundaries of the domain and across hanging nodes we use a numerical flux function to enforce boundary conditions.  This routine is the basic Lax-Friedrich's flux with a stabilization parameter  [2.x.107] . It's form has also been given already in the introduction:
* 

* 
* [1.x.109]
* 
*   [1.x.110]  [1.x.111]
* 

* 
*  In the same way as describing the flux function  [2.x.108] , we also need to have a way to describe the right hand side forcing term. As mentioned in the introduction, we consider only gravity here, which leads to the specific form  [2.x.109] , shown here for the 3d case. More specifically, we will consider only  [2.x.110]  in 3d, or  [2.x.111]  in 2d. This naturally leads to the following function:
* 

* 
* [1.x.112]
* 
*   [1.x.113]  [1.x.114]
* 

* 
*  Another thing we have to deal with is boundary conditions. To this end, let us first define the kinds of boundary conditions we currently know how to deal with:
* 

* 
* [1.x.115]
* 
*  The next part is to actually decide what to do at each kind of boundary. To this end, remember from the introduction that boundary conditions are specified by choosing a value  [2.x.112]  on the outside of a boundary given an inhomogeneity  [2.x.113]  and possibly the solution's value  [2.x.114]  on the inside. Both are then passed to the numerical flux  [2.x.115]  to define boundary contributions to the bilinear form.     
*   Boundary conditions can in some cases be specified for each component of the solution vector independently. For example, if component  [2.x.116]  is marked for inflow, then  [2.x.117] . If it is an outflow, then  [2.x.118] . These two simple cases are handled first in the function below.     
*   There is a little snag that makes this function unpleasant from a C++ language viewpoint: The output vector  [2.x.119]  will of course be modified, so it shouldn't be a  [2.x.120]  argument. Yet it is in the implementation below, and needs to be in order to allow the code to compile. The reason is that we call this function at a place where  [2.x.121]  is of type  [2.x.122] , this being 2d table with indices representing the quadrature point and the vector component, respectively. We call this function with  [2.x.123]  as last argument; subscripting a 2d table yields a temporary accessor object representing a 1d vector, just what we want here. The problem is that a temporary accessor object can't be bound to a non-const reference argument of a function, as we would like here, according to the C++ 1998 and 2003 standards (something that will be fixed with the next standard in the form of rvalue references).  We get away with making the output argument here a constant because it is the [1.x.116] object that's constant, not the table it points to: that one can still be written to. The hack is unpleasant nevertheless because it restricts the kind of data types that may be used as template argument to this function: a regular vector isn't going to do because that one can not be written to when marked  [2.x.124] . With no good solution around at the moment, we'll go with the pragmatic, even if not pretty, solution shown here:
* 

* 
* [1.x.117]
* 
*  Prescribed pressure boundary conditions are a bit more complicated by the fact that even though the pressure is prescribed, we really are setting the energy component here, which will depend on velocity and pressure. So even though this seems like a Dirichlet type boundary condition, we get sensitivities of energy to velocity and density (unless these are also prescribed):
* 

* 
* [1.x.118]
* 
*  We prescribe the velocity (we are dealing with a particular component here so that the average of the velocities is orthogonal to the surface normal.  This creates sensitivities of across the velocity components.
* 

* 
* [1.x.119]
* 
*   [1.x.120]  [1.x.121]
* 

* 
*  In this class, we also want to specify how to refine the mesh. The class  [2.x.125]  that will use all the information we provide here in the  [2.x.126]  class is pretty agnostic about the particular conservation law it solves: as doesn't even really care how many components a solution vector has. Consequently, it can't know what a reasonable refinement indicator would be. On the other hand, here we do, or at least we can come up with a reasonable choice: we simply look at the gradient of the density, and compute  [2.x.127] , where  [2.x.128]  is the center of cell  [2.x.129] .     
*   There are certainly a number of equally reasonable refinement indicators, but this one does, and it is easy to compute:
* 

* 
* [1.x.122]
* 
*   [1.x.123]  [1.x.124]
* 

* 
*  Finally, we declare a class that implements a postprocessing of data components. The problem this class solves is that the variables in the formulation of the Euler equations we use are in conservative rather than physical form: they are momentum densities  [2.x.130] , density  [2.x.131] , and energy density  [2.x.132] . What we would like to also put into our output file are velocities  [2.x.133]  and pressure  [2.x.134] .     
*   In addition, we would like to add the possibility to generate schlieren plots. Schlieren plots are a way to visualize shocks and other sharp interfaces. The word "schlieren" is a German word that may be translated as "striae"
* 
*  -  it may be simpler to explain it by an example, however: schlieren is what you see when you, for example, pour highly concentrated alcohol, or a transparent saline solution, into water; the two have the same color, but they have different refractive indices and so before they are fully mixed light goes through the mixture along bent rays that lead to brightness variations if you look at it. That's "schlieren". A similar effect happens in compressible flow because the refractive index depends on the pressure (and therefore the density) of the gas.     
*   The origin of the word refers to two-dimensional projections of a three-dimensional volume (we see a 2d picture of the 3d fluid). In computational fluid dynamics, we can get an idea of this effect by considering what causes it: density variations. Schlieren plots are therefore produced by plotting  [2.x.135] ; obviously,  [2.x.136]  is large in shocks and at other highly dynamic places. If so desired by the user (by specifying this in the input file), we would like to generate these schlieren plots in addition to the other derived quantities listed above.     
*   The implementation of the algorithms to compute derived quantities from the ones that solve our problem, and to output them into data file, rests on the DataPostprocessor class. It has extensive documentation, and other uses of the class can also be found in  [2.x.137] . We therefore refrain from extensive comments.
* 

* 
* [1.x.125]
* 
*  This is the only function worth commenting on. When generating graphical output, the DataOut and related classes will call this function on each cell, with access to values, gradients, Hessians, and normal vectors (in case we're working on faces) at each quadrature point. Note that the data at each quadrature point is itself vector-valued, namely the conserved variables. What we're going to do here is to compute the quantities we're interested in at each quadrature point. Note that for this we can ignore the Hessians ("inputs.solution_hessians") and normal vectors ("inputs.normals").
* 

* 
* [1.x.126]
* 
*  At the beginning of the function, let us make sure that all variables have the correct sizes, so that we can access individual vector elements without having to wonder whether we might read or write invalid elements; we also check that the  [2.x.138]  vector only contains data if we really need it (the system knows about this because we say so in the  [2.x.139]  function below). For the inner vectors, we check that at least the first element of the outer vector has the correct inner size:
* 

* 
* [1.x.127]
* 
*  Then loop over all quadrature points and do our work there. The code should be pretty self-explanatory. The order of output variables is first  [2.x.140]  velocities, then the pressure, and if so desired the schlieren plot. Note that we try to be generic about the order of variables in the input vector, using the  [2.x.141]  and  [2.x.142]  information:
* 

* 
* [1.x.128]
* 
*   [1.x.129]  [1.x.130]
* 

* 
*  Our next job is to define a few classes that will contain run-time parameters (for example solver tolerances, number of iterations, stabilization parameter, and the like). One could do this in the main class, but we separate it from that one to make the program more modular and easier to read: Everything that has to do with run-time parameters will be in the following namespace, whereas the program logic is in the main class.   
*   We will split the run-time parameters into a few separate structures, which we will all put into a namespace  [2.x.143] . Of these classes, there are a few that group the parameters for individual groups, such as for solvers, mesh refinement, or output. Each of these classes have functions  [2.x.144]  and  [2.x.145]  that declare parameter subsections and entries in a ParameterHandler object, and retrieve actual parameter values from such an object, respectively. These classes declare all their parameters in subsections of the ParameterHandler.   
*   The final class of the following namespace combines all the previous classes by deriving from them and taking care of a few more entries at the top level of the input file, as well as a few odd other entries in subsections that are too short to warrant a structure by themselves.   
*   It is worth pointing out one thing here: None of the classes below have a constructor that would initialize the various member variables. This isn't a problem, however, since we will read all variables declared in these classes from the input file (or indirectly: a ParameterHandler object will read it from there, and we will get the values from this object), and they will be initialized this way. In case a certain variable is not specified at all in the input file, this isn't a problem either: The ParameterHandler class will in this case simply take the default value that was specified when declaring an entry in the  [2.x.146]  functions of the classes below.
* 

* 
* [1.x.131]
* 
*   [1.x.132]  [1.x.133]     
*   The first of these classes deals with parameters for the linear inner solver. It offers parameters that indicate which solver to use (GMRES as a solver for general non-symmetric indefinite systems, or a sparse direct solver), the amount of output to be produced, as well as various parameters that tweak the thresholded incomplete LU decomposition (ILUT) that we use as a preconditioner for GMRES.     
*   In particular, the ILUT takes the following parameters:
* 

* 
* 
*  - ilut_fill: the number of extra entries to add when forming the ILU decomposition
* 

* 
* 
*  - ilut_atol, ilut_rtol: When forming the preconditioner, for certain problems bad conditioning (or just bad luck) can cause the preconditioner to be very poorly conditioned.  Hence it can help to add diagonal perturbations to the original matrix and form the preconditioner for this slightly better matrix.  ATOL is an absolute perturbation that is added to the diagonal before forming the prec, and RTOL is a scaling factor  [2.x.147] .
* 

* 
* 
*  - ilut_drop: The ILUT will drop any values that have magnitude less than this value.  This is a way to manage the amount of memory used by this preconditioner.     
*   The meaning of each parameter is also briefly described in the third argument of the  [2.x.148]  call in  [2.x.149] .
* 

* 
* [1.x.134]
* 
*   [1.x.135]  [1.x.136]     
*   Similarly, here are a few parameters that determine how the mesh is to be refined (and if it is to be refined at all). For what exactly the shock parameters do, see the mesh refinement functions further down.
* 

* 
* [1.x.137]
* 
*   [1.x.138]  [1.x.139]     
*   Next a section on flux modifications to make it more stable. In particular, two options are offered to stabilize the Lax-Friedrichs flux: either choose  [2.x.150]  where  [2.x.151]  is either a fixed number specified in the input file, or where  [2.x.152]  is a mesh dependent value. In the latter case, it is chosen as  [2.x.153]  with  [2.x.154]  the diameter of the face to which the flux is applied, and  [2.x.155]  the current time step.
* 

* 
* [1.x.140]
* 
*   [1.x.141]  [1.x.142]     
*   Then a section on output parameters. We offer to produce Schlieren plots (the squared gradient of the density, a tool to visualize shock fronts), and a time interval between graphical output in case we don't want an output file every time step.
* 

* 
* [1.x.143]
* 
*   [1.x.144]  [1.x.145]     
*   Finally the class that brings it all together. It declares a number of parameters itself, mostly ones at the top level of the parameter file as well as several in section too small to warrant their own classes. It also contains everything that is actually space dimension dependent, like initial or boundary conditions.     
*   Since this class is derived from all the ones above, the  [2.x.156]  functions call the respective functions of the base classes as well.     
*   Note that this class also handles the declaration of initial and boundary conditions specified in the input file. To this end, in both cases, there are entries like "w_0 value" which represent an expression in terms of  [2.x.157]  that describe the initial or boundary condition as a formula that will later be parsed by the FunctionParser class. Similar expressions exist for "w_1", "w_2", etc, denoting the  [2.x.158]  conserved variables of the Euler system. Similarly, we allow up to  [2.x.159]  boundary indicators to be used in the input file, and each of these boundary indicators can be associated with an inflow, outflow, or pressure boundary condition, with homogeneous boundary conditions being specified for each component and each boundary indicator separately.     
*   The data structure used to store the boundary indicators is a bit complicated. It is an array of  [2.x.160]  elements indicating the range of boundary indicators that will be accepted. For each entry in this array, we store a pair of data in the  [2.x.161]  structure: first, an array of size  [2.x.162]  that for each component of the solution vector indicates whether it is an inflow, outflow, or other kind of boundary, and second a FunctionParser object that describes all components of the solution vector for this boundary id at once.     
*   The  [2.x.163]  structure requires a constructor since we need to tell the function parser object at construction time how many vector components it is to describe. This initialization can therefore not wait till we actually set the formulas the FunctionParser object represents later in  [2.x.164]      
*   For the same reason of having to tell Function objects their vector size at construction time, we have to have a constructor of the  [2.x.165]  class that at least initializes the other FunctionParser object, i.e. the one describing initial conditions.
* 

* 
* [1.x.146]
* 
*   [1.x.147]  [1.x.148]
* 

* 
*  Here finally comes the class that actually does something with all the Euler equation and parameter specifics we've defined above. The public interface is pretty much the same as always (the constructor now takes the name of a file from which to read parameters, which is passed on the command line). The private function interface is also pretty similar to the usual arrangement, with the  [2.x.166]  function split into three parts: one that contains the main loop over all cells and that then calls the other two for integrals over cells and faces, respectively.
* 

* 
* [1.x.149]
* 
*  The first few member variables are also rather standard. Note that we define a mapping object to be used throughout the program when assembling terms (we will hand it to every FEValues and FEFaceValues object); the mapping we use is just the standard  [2.x.167]  mapping
* 
*  -  nothing fancy, in other words
* 
*  -  but declaring one here and using it throughout the program will make it simpler later on to change it if that should become necessary. This is, in fact, rather pertinent: it is known that for transsonic simulations with the Euler equations, computations do not converge even as  [2.x.168]  if the boundary approximation is not of sufficiently high order.
* 

* 
* [1.x.150]
* 
*  Next come a number of data vectors that correspond to the solution of the previous time step ( [2.x.169] ), the best guess of the current solution ( [2.x.170] ; we say [1.x.151] because the Newton iteration to compute it may not have converged yet, whereas  [2.x.171]  refers to the fully converged final result of the previous time step), and a predictor for the solution at the next time step, computed by extrapolating the current and previous solution one time step into the future:
* 

* 
* [1.x.152]
* 
*  This final set of member variables (except for the object holding all run-time parameters at the very bottom and a screen output stream that only prints something if verbose output has been requested) deals with the interface we have in this program to the Trilinos library that provides us with linear solvers. Similarly to including PETSc matrices in  [2.x.172]  and  [2.x.173] , all we need to do is to create a Trilinos sparse matrix instead of the standard deal.II class. The system matrix is used for the Jacobian in each Newton step. Since we do not intend to run this program in parallel (which wouldn't be too hard with Trilinos data structures, though), we don't have to think about anything else like distributing the degrees of freedom.
* 

* 
* [1.x.153]
* 
*   [1.x.154]  [1.x.155]   
*   There is nothing much to say about the constructor. Essentially, it reads the input file and fills the parameter object with the parsed values:
* 

* 
* [1.x.156]
* 
*   [1.x.157]  [1.x.158]   
*   The following (easy) function is called each time the mesh is changed. All it does is to resize the Trilinos matrix according to a sparsity pattern that we generate as in all the previous tutorial programs.
* 

* 
* [1.x.159]
* 
*   [1.x.160]  [1.x.161]   
*   This and the following two functions are the meat of this program: They assemble the linear system that results from applying Newton's method to the nonlinear system of conservation equations.   
*   This first function puts all of the assembly pieces together in a routine that dispatches the correct piece for each cell/face.  The actual implementation of the assembly on these objects is done in the following functions.   
*   At the top of the function we do the usual housekeeping: allocate FEValues, FEFaceValues, and FESubfaceValues objects necessary to do the integrations on cells, faces, and subfaces (in case of adjoining cells on different refinement levels). Note that we don't need all information (like values, gradients, or real locations of quadrature points) for all of these objects, so we only let the FEValues classes whatever is actually necessary by specifying the minimal set of UpdateFlags. For example, when using a FEFaceValues object for the neighboring cell we only need the shape values: Given a specific face, the quadrature points and  [2.x.174]  values are the same as for the current cells, and the normal vectors are known to be the negative of the normal vectors of the current cell.
* 

* 
* [1.x.162]
* 
*  Then loop over all cells, initialize the FEValues object for the current cell and call the function that assembles the problem on this cell.
* 

* 
* [1.x.163]
* 
*  Then loop over all the faces of this cell.  If a face is part of the external boundary, then assemble boundary conditions there (the fifth argument to  [2.x.175]  indicates whether we are working on an external or internal face; if it is an external face, the fourth argument denoting the degrees of freedom indices of the neighbor is ignored, so we pass an empty vector):
* 

* 
* [1.x.164]
* 
*  The alternative is that we are dealing with an internal face. There are two cases that we need to distinguish: that this is a normal face between two cells at the same refinement level, and that it is a face between two cells of the different refinement levels.           
*   In the first case, there is nothing we need to do: we are using a continuous finite element, and face terms do not appear in the bilinear form in this case. The second case usually does not lead to face terms either if we enforce hanging node constraints strongly (as in all previous tutorial programs so far whenever we used continuous finite elements
* 
*  -  this enforcement is done by the AffineConstraints class together with  [2.x.176]  In the current program, however, we opt to enforce continuity weakly at faces between cells of different refinement level, for two reasons: (i) because we can, and more importantly (ii) because we would have to thread the automatic differentiation we use to compute the elements of the Newton matrix from the residual through the operations of the AffineConstraints class. This would be possible, but is not trivial, and so we choose this alternative approach.           
*   What needs to be decided is which side of an interface between two cells of different refinement level we are sitting on.           
*   Let's take the case where the neighbor is more refined first. We then have to loop over the children of the face of the current cell and integrate on each of them. We sprinkle a couple of assertions into the code to ensure that our reasoning trying to figure out which of the neighbor's children's faces coincides with a given subface of the current cell's faces is correct
* 
*  -  a bit of defensive programming never hurts.           
*   We then call the function that integrates over faces; since this is an internal face, the fifth argument is false, and the sixth one is ignored so we pass an invalid value again:
* 

* 
* [1.x.165]
* 
*  The other possibility we have to care for is if the neighbor is coarser than the current cell (in particular, because of the usual restriction of only one hanging node per face, the neighbor must be exactly one level coarser than the current cell, something that we check with an assertion). Again, we then integrate over this interface:
* 

* 
* [1.x.166]
* 
*   [1.x.167]  [1.x.168]   
*   This function assembles the cell term by computing the cell part of the residual, adding its negative to the right hand side vector, and adding its derivative with respect to the local variables to the Jacobian (i.e. the Newton matrix). Recall that the cell contributions to the residual read  [2.x.177]   [2.x.178]   [2.x.179]  where  [2.x.180]   [2.x.181]   [2.x.182]  for both  [2.x.183]  and  [2.x.184]  ,  [2.x.185]  is the  [2.x.186] th vector valued test function. Furthermore, the scalar product  [2.x.187]  is understood as  [2.x.188]  where  [2.x.189]  is the  [2.x.190] th component of the  [2.x.191] th test function.   
*     
*   At the top of this function, we do the usual housekeeping in terms of allocating some local variables that we will need later. In particular, we will allocate variables that will hold the values of the current solution  [2.x.192]  after the  [2.x.193] th Newton iteration (variable  [2.x.194] ) and the previous time step's solution  [2.x.195]  (variable  [2.x.196] ).   
*   In addition to these, we need the gradients of the current variables.  It is a bit of a shame that we have to compute these; we almost don't.  The nice thing about a simple conservation law is that the flux doesn't generally involve any gradients.  We do need these, however, for the diffusion stabilization.   
*   The actual format in which we store these variables requires some explanation. First, we need values at each quadrature point for each of the  [2.x.197]  components of the solution vector. This makes for a two-dimensional table for which we use deal.II's Table class (this is more efficient than  [2.x.198]  because it only needs to allocate memory once, rather than once for each element of the outer vector). Similarly, the gradient is a three-dimensional table, which the Table class also supports.   
*   Secondly, we want to use automatic differentiation. To this end, we use the  [2.x.199]  template for everything that is computed from the variables with respect to which we would like to compute derivatives. This includes the current solution and gradient at the quadrature points (which are linear combinations of the degrees of freedom) as well as everything that is computed from them such as the residual, but not the previous time step's solution. These variables are all found in the first part of the function, along with a variable that we will use to store the derivatives of a single component of the residual:
* 

* 
* [1.x.169]
* 
*  Next, we have to define the independent variables that we will try to determine by solving a Newton step. These independent variables are the values of the local degrees of freedom which we extract here:
* 

* 
* [1.x.170]
* 
*  The next step incorporates all the magic: we declare a subset of the autodifferentiation variables as independent degrees of freedom, whereas all the other ones remain dependent functions. These are precisely the local degrees of freedom just extracted. All calculations that reference them (either directly or indirectly) will accumulate sensitivities with respect to these variables.     
*   In order to mark the variables as independent, the following does the trick, marking  [2.x.200]  as the  [2.x.201] th independent variable out of a total of  [2.x.202] :
* 

* 
* [1.x.171]
* 
*  After all these declarations, let us actually compute something. First, the values of  [2.x.203]  and  [2.x.204] , which we can compute from the local DoF values by using the formula  [2.x.205] , where  [2.x.206]  is the  [2.x.207] th entry of the (local part of the) solution vector, and  [2.x.208]  the value of the  [2.x.209] th vector-valued shape function evaluated at quadrature point  [2.x.210] . The gradient can be computed in a similar way.     
*   Ideally, we could compute this information using a call into something like  [2.x.211]  and  [2.x.212]  but since (i) we would have to extend the FEValues class for this, and (ii) we don't want to make the entire  [2.x.213]  vector fad types, only the local cell variables, we explicitly code the loop above. Before this, we add another loop that initializes all the fad variables to zero:
* 

* 
* [1.x.172]
* 
*  Next, in order to compute the cell contributions, we need to evaluate  [2.x.214] ,  [2.x.215]  and  [2.x.216] ,  [2.x.217]  at all quadrature points. To store these, we also need to allocate a bit of memory. Note that we compute the flux matrices and right hand sides in terms of autodifferentiation variables, so that the Jacobian contributions can later easily be computed from it:
* 

* 
*  

* 
* [1.x.173]
* 
*  We now have all of the pieces in place, so perform the assembly.  We have an outer loop through the components of the system, and an inner loop over the quadrature points, where we accumulate contributions to the  [2.x.218] th residual  [2.x.219] . The general formula for this residual is given in the introduction and at the top of this function. We can, however, simplify it a bit taking into account that the  [2.x.220] th (vector-valued) test function  [2.x.221]  has in reality only a single nonzero component (more on this topic can be found in the  [2.x.222]  vector_valued module). It will be represented by the variable  [2.x.223]  below. With this, the residual term can be re-written as [1.x.174]
*  where integrals are understood to be evaluated through summation over quadrature points.     
*   We initially sum all contributions of the residual in the positive sense, so that we don't need to negative the Jacobian entries.  Then, when we sum into the  [2.x.224]  vector, we negate this residual.
* 

* 
* [1.x.175]
* 
*  The residual for each row (i) will be accumulating into this fad variable.  At the end of the assembly for this row, we will query for the sensitivities to this variable and add them into the Jacobian.
* 

* 
*  

* 
* [1.x.176]
* 
*  At the end of the loop, we have to add the sensitivities to the matrix and subtract the residual from the right hand side. Trilinos FAD data type gives us access to the derivatives using  [2.x.225] , so we store the data in a temporary array. This information about the whole row of local dofs is then added to the Trilinos matrix at once (which supports the data types we have chosen).
* 

* 
* [1.x.177]
* 
*   [1.x.178]  [1.x.179]   
*   Here, we do essentially the same as in the previous function. At the top, we introduce the independent variables. Because the current function is also used if we are working on an internal face between two cells, the independent variables are not only the degrees of freedom on the current cell but in the case of an interior face also the ones on the neighbor.
* 

* 
* [1.x.180]
* 
*  Next, we need to define the values of the conservative variables  [2.x.226]  on this side of the face ( [2.x.227] ) and on the opposite side ( [2.x.228] ), for both  [2.x.229]  and   [2.x.230] . The "this side" values can be computed in exactly the same way as in the previous function, but note that the  [2.x.231]  variable now is of type FEFaceValues or FESubfaceValues:
* 

* 
* [1.x.181]
* 
*  Computing "opposite side" is a bit more complicated. If this is an internal face, we can compute it as above by simply using the independent variables from the neighbor:
* 

* 
* [1.x.182]
* 
*  On the other hand, if this is an external boundary face, then the values of  [2.x.232]  will be either functions of  [2.x.233] , or they will be prescribed, depending on the kind of boundary condition imposed here.     
*   To start the evaluation, let us ensure that the boundary id specified for this boundary is one for which we actually have data in the parameters object. Next, we evaluate the function object for the inhomogeneity.  This is a bit tricky: a given boundary might have both prescribed and implicit values.  If a particular component is not prescribed, the values evaluate to zero and are ignored below.     
*   The rest is done by a function that actually knows the specifics of Euler equation boundary conditions. Note that since we are using fad variables here, sensitivities will be updated appropriately, a process that would otherwise be tremendously complicated.
* 

* 
* [1.x.183]
* 
*  Here we assume that boundary type, boundary normal vector and boundary data values maintain the same during time advancing.
* 

* 
* [1.x.184]
* 
*  Now that we have  [2.x.234]  and  [2.x.235] , we can go about computing the numerical flux function  [2.x.236]  for each quadrature point. Before calling the function that does so, we also need to determine the Lax-Friedrich's stability parameter:
* 

* 
*  

* 
* [1.x.185]
* 
*  Now assemble the face term in exactly the same way as for the cell contributions in the previous function. The only difference is that if this is an internal face, we also have to take into account the sensitivities of the residual contributions to the degrees of freedom on the neighboring cell:
* 

* 
* [1.x.186]
* 
*   [1.x.187]  [1.x.188]   
*   Here, we actually solve the linear system, using either of Trilinos' Aztec or Amesos linear solvers. The result of the computation will be written into the argument vector passed to this function. The result is a pair of number of iterations and the final linear residual.
* 

* 
*  

* 
* [1.x.189]
* 
*  If the parameter file specified that a direct solver shall be used, then we'll get here. The process is straightforward, since deal.II provides a wrapper class to the Amesos direct solver within Trilinos. All we have to do is to create a solver control object (which is just a dummy object here, since we won't perform any iterations), and then create the direct solver object. When actually doing the solve, note that we don't pass a preconditioner. That wouldn't make much sense for a direct solver anyway.  At the end we return the solver control statistics &mdash; which will tell that no iterations have been performed and that the final linear residual is zero, absent any better information that may be provided here:
* 

* 
* [1.x.190]
* 
*  Likewise, if we are to use an iterative solver, we use Aztec's GMRES solver. We could use the Trilinos wrapper classes for iterative solvers and preconditioners here as well, but we choose to use an Aztec solver directly. For the given problem, Aztec's internal preconditioner implementations are superior over the ones deal.II has wrapper classes to, so we use ILU-T preconditioning within the AztecOO solver and set a bunch of options that can be changed from the parameter file.         
*   There are two more practicalities: Since we have built our right hand side and solution vector as deal.II Vector objects (as opposed to the matrix, which is a Trilinos object), we must hand the solvers Trilinos Epetra vectors.  Luckily, they support the concept of a 'view', so we just send in a pointer to our deal.II vectors. We have to provide an Epetra_Map for the vector that sets the parallel distribution, which is just a dummy object in serial. The easiest way is to ask the matrix for its map, and we're going to be ready for matrix-vector products with it.         
*   Secondly, the Aztec solver wants us to pass a Trilinos Epetra_CrsMatrix in, not the deal.II wrapper class itself. So we access to the actual Trilinos matrix in the Trilinos wrapper class by the command trilinos_matrix(). Trilinos wants the matrix to be non-constant, so we have to manually remove the constantness using a const_cast.
* 

* 
* [1.x.191]
* 
*   [1.x.192]  [1.x.193]
* 

* 
*  This function is real simple: We don't pretend that we know here what a good refinement indicator would be. Rather, we assume that the  [2.x.237]  class would know about this, and so we simply defer to the respective function we've implemented there:
* 

* 
* [1.x.194]
* 
*   [1.x.195]  [1.x.196]
* 

* 
*  Here, we use the refinement indicators computed before and refine the mesh. At the beginning, we loop over all cells and mark those that we think should be refined:
* 

* 
* [1.x.197]
* 
*  Then we need to transfer the various solution vectors from the old to the new grid while we do the refinement. The SolutionTransfer class is our friend here; it has a fairly extensive documentation, including examples, so we won't comment much on the following code. The last three lines simply re-set the sizes of some other vectors to the now correct size:
* 

* 
* [1.x.198]
* 
*   [1.x.199]  [1.x.200]
* 

* 
*  This function now is rather straightforward. All the magic, including transforming data from conservative variables to physical ones has been abstracted and moved into the EulerEquations class so that it can be replaced in case we want to solve some other hyperbolic conservation law.   
*   Note that the number of the output file is determined by keeping a counter in the form of a static variable that is set to zero the first time we come to this function and is incremented by one at the end of each invocation.
* 

* 
* [1.x.201]
* 
*   [1.x.202]  [1.x.203]
* 

* 
*  This function contains the top-level logic of this program: initialization, the time loop, and the inner Newton iteration.   
*   At the beginning, we read the mesh file specified by the parameter file, setup the DoFHandler and various vectors, and then interpolate the given initial conditions on this mesh. We then perform a number of mesh refinements, based on the initial conditions, to obtain a mesh that is already well adapted to the starting solution. At the end of this process, we output the initial solution.
* 

* 
* [1.x.204]
* 
*  Size all of the fields.
* 

* 
* [1.x.205]
* 
*  We then enter into the main time stepping loop. At the top we simply output some status information so one can keep track of where a computation is, as well as the header for a table that indicates progress of the nonlinear inner iteration:
* 

* 
* [1.x.206]
* 
*  Then comes the inner Newton iteration to solve the nonlinear problem in each time step. The way it works is to reset matrix and right hand side to zero, then assemble the linear system. If the norm of the right hand side is small enough, then we declare that the Newton iteration has converged. Otherwise, we solve the linear system, update the current solution with the Newton increment, and output convergence information. At the end, we check that the number of Newton iterations is not beyond a limit of 10
* 
*  -  if it is, it appears likely that iterations are diverging and further iterations would do no good. If that happens, we throw an exception that will be caught in  [2.x.238]  with status information being displayed before the program aborts.         
*   Note that the way we write the AssertThrow macro below is by and large equivalent to writing something like <code>if (!(nonlin_iter  [2.x.239]  10)) throw ExcMessage ("No convergence in nonlinear solver");</code>. The only significant difference is that AssertThrow also makes sure that the exception being thrown carries with it information about the location (file name and line number) where it was generated. This is not overly critical here, because there is only a single place where this sort of exception can happen; however, it is generally a very useful tool when one wants to find out where an error occurred.
* 

* 
* [1.x.207]
* 
*  We only get to this point if the Newton iteration has converged, so do various post convergence tasks here:         
*   First, we update the time and produce graphical output if so desired. Then we update a predictor for the solution at the next time step by approximating  [2.x.240]  to try and make adaptivity work better.  The idea is to try and refine ahead of a front, rather than stepping into a coarse set of elements and smearing the old_solution.  This simple time extrapolator does the job. With this, we then refine the mesh if so desired by the user, and finally continue on with the next time step:
* 

* 
* [1.x.208]
* 
*   [1.x.209]  [1.x.210]
* 

* 
*  The following ``main'' function is similar to previous examples and need not to be commented on. Note that the program aborts if no input file name is given on the command line.
* 

* 
* [1.x.211]
* [1.x.212][1.x.213][1.x.214]
* 

* We run the problem with the mesh  [2.x.241]  (this file is in thesame directory as the source code for this program) and the following inputdeck (available as  [2.x.242]  in the same directory):
* [1.x.215]
* 
* When we run the program, we get the following kind of output:
* [1.x.216]
* 
* This output reports the progress of the Newton iterations and the timestepping. Note that our implementation of the Newton iteration indeed showsthe expected quadratic convergence order: the norm of the nonlinear residualin each step is roughly the norm of the previous step squared. This leads tothe very rapid convergence we can see here. This holds untiltimes up to  [2.x.243]  at which time the nonlinear iteration reports alack of convergence:
* [1.x.217]
* 
* We may find out the cause and possible remedies by looking at the animation of the solution.
* The result of running these computations is a bunch of output files that wecan pass to our visualization program of choice. When we collate them into amovie, the results of last several time steps looks like this:
*  [2.x.244] 
* As we see, when the heavy mass of fluid hits the left bottom corner,some oscillation occurs and lead to the divergence of the iteration. A lazy solution tothis issue is add more viscosity. If we set the diffusion power  [2.x.245]  instead of  [2.x.246] ,the simulation would be able to survive this crisis. Then, the result looks like this:
* 

*  [2.x.247] 
* The heavy mass of fluid is drawn down the slope by gravity, whereit collides with the ski lodge and is flung into the air!  Hopefully everyoneescapes! And also, we can see the boundary between heavy mass and light mass blur quicklydue to the artificial viscosity.
* We can also visualize the evolution of the adaptively refined grid:
*  [2.x.248] 
* The adaptivity follows and precedes the flow pattern, based on the heuristicrefinement scheme discussed above.
* 

* 
* 

* [1.x.218][1.x.219][1.x.220]
* 

* [1.x.221][1.x.222]
* 

* The numerical scheme we have chosen is not particularlystable when the artificial viscosity is small while is too diffusive whenthe artificial viscosity is large. Furthermore, it is known there are moreadvanced techniques to stabilize the solution, for example streamlinediffusion, least-squares stabilization terms, entropy viscosity.
* 

* 
* [1.x.223][1.x.224]
* 

* While the Newton method as a nonlinear solver appears to work verywell if the time step is small enough, the linear solver can beimproved. For example, in the current scheme whenever we use aniterative solver, an ILU is computed anew for each Newton step;likewise, for the direct solver, an LU decomposition of the Newtonmatrix is computed in each step. This is obviously wasteful: from oneNewton step to another, and probably also between time steps, theNewton matrix does not radically change: an ILU or a sparse LUdecomposition for one Newton step is probably still a very goodpreconditioner for the next Newton or time step. Avoiding therecomputation would therefore be a good way to reduce the amount ofcompute time.
* One could drive this a step further: since close to convergence theNewton matrix changes only a little bit, one may be able to define aquasi-Newton scheme where we only re-compute the residual (i.e. theright hand side vector) in each Newton iteration, and re-use theNewton matrix. The resulting scheme will likely not be of quadraticconvergence order, and we have to expect to do a few more nonlineariterations; however, given that we don't have to spend the time tobuild the Newton matrix each time, the resulting scheme may well befaster.
* 

* [1.x.225][1.x.226]
* 

* The residual calculated in  [2.x.249]  functionreads    [2.x.250] This means that we calculate the spatial residual twice at one Newtoniteration step: once respect to the current solution  [2.x.251] and once more respect to the last time step solution  [2.x.252]  whichremains the same during all Newton iterations through one timestep.Cache up the explicit part of residual  [2.x.253] during Newton iteration will save lots of labor.
* 

* [1.x.227][1.x.228]
* 

* Finally, as a direction beyond the immediate solution of the Eulerequations, this program tries very hard to separate the implementationof everything that is specific to the Euler equations into one class(the  [2.x.254]  class), and everything that isspecific to assembling the matrices and vectors, nonlinear and linearsolvers, and the general top-level logic into another (the [2.x.255]  class).
* By replacing the definitions of flux matrices and numerical fluxes inthis class, as well as the various other parts defined there, itshould be possible to apply the  [2.x.256]  class toother hyperbolic conservation laws as well.
* 

* [1.x.229][1.x.230] [2.x.257] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-34_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25]
*  [2.x.2] 
* [1.x.26]
*  [2.x.3] 
* [1.x.27]
* [1.x.28][1.x.29]
* 

* [1.x.30][1.x.31]
* The incompressible motion of an inviscid fluid past a body (forexample air past an airplane wing, or air or water past a propeller) isusually modeled by the Euler equations of fluid dynamics:
* [1.x.32]where the fluid density  [2.x.4]  and the acceleration  [2.x.5]  dueto external forces are given and the velocity  [2.x.6]  and thepressure  [2.x.7]  are the unknowns. Here  [2.x.8]  is a closed boundedregion representing the body around which the fluid moves.
* The above equations can be derived from Navier-Stokes equationsassuming that the effects due to viscosity are negligible compared tothose due to the pressure gradient, inertial forces and the externalforces. This is the opposite case of the Stokes equations discussed in [2.x.9]  which are the limit case of dominant viscosity,i.e. where the velocity is so small that inertia forces can beneglected. On the other hand, owing to the assumed incompressibility,the equations are not suited for very high speed gas flows wherecompressibility and the equation of state of the gas have to be takeninto account, leading to the Euler equations of gas dynamics, ahyperbolic system.
* For the purpose of this tutorial program, we will consider only stationaryflow without external forces:[1.x.33]
* 

* Uniqueness of the solution of the Euler equations is ensured by adding theboundary conditions[1.x.34]
* which is to say that the body is at rest in our coordinate systems andis not permeable, and that the fluid has (constant) velocity [2.x.10]  at infinity. An alternative viewpoint is that ourcoordinate system moves along with the body whereas the backgroundfluid is at rest at infinity. Notice that we define the normal [2.x.11]  as the [1.x.35] normal to the domain  [2.x.12] , whichis the opposite of the outer normal to the integration domain.
* For both stationary and non stationary flow, the solution processstarts by solving for the velocity in the second equation andsubstituting in the first equation in order to find the pressure.The solution of the stationary Euler equations is typically performedin order to understand the behavior of the given (possibly complex)geometry when a prescribed motion is enforced on the system.
* The first step in this process is to change the frame of reference from acoordinate system moving along with the body to one in which the body movesthrough a fluid that is at rest at infinity. This can be expressed byintroducing a new velocity  [2.x.13]  forwhich we find that the same equations hold (because  [2.x.14] ) and we have boundary conditions[1.x.36]
* If we assume that the fluid is irrotational, i.e.,  [2.x.15]  in  [2.x.16] , we can represent thevelocity, and consequently also the perturbation velocity, as thegradient of a scalar function:[1.x.37]and so the second part of Euler equations above can be rewrittenas the homogeneous Laplace equation for the unknown  [2.x.17] :[1.x.38]while the momentum equation reduces to Bernoulli's equation that expresses thepressure  [2.x.18]  as a function of the potential  [2.x.19] :[1.x.39]
* So we can solve the problem by solving the Laplace equation for thepotential.  We recall that the following functions, called fundamentalsolutions of the Laplace equation,
* [1.x.40]
* satisfy in a distributional sense the equation:
* [1.x.41]
* where the derivative is done in the variable  [2.x.20] . By usingthe usual Green identities, our problem can be written on the boundary [2.x.21]  only. We recall the general definition ofthe second Green %identity:
* [1.x.42]
* where  [2.x.22]  is the normal to the surface of  [2.x.23]  pointingoutwards from the domain of integration  [2.x.24] .
* In our case the domain of integration is the domain [2.x.25] , whose boundary is  [2.x.26] , where the "boundary" at infinity is defined as
* [1.x.43]
* In our program the normals are defined as [1.x.44] to the domain [2.x.27] , that is, they are in fact [1.x.45] to the integrationdomain, and some care is required in defining the various integralswith the correct signs for the normals, i.e. replacing  [2.x.28] by  [2.x.29] .
* If we substitute  [2.x.30]  and  [2.x.31]  in the Green %identity with the solution [2.x.32]  and with the fundamental solution of the Laplace equationrespectively, as long as  [2.x.33]  is chosen in the region [2.x.34] , we obtain:[1.x.46]
* where the normals are now pointing [1.x.47] the domain ofintegration.
* Notice that in the above equation, we also have the integrals on theportion of the boundary at  [2.x.35] . Using the boundaryconditions of our problem, we have that  [2.x.36]  is zero atinfinity (which simplifies the integral on  [2.x.37]  on theright hand side).
* The integral on  [2.x.38]  that appears on the left hand side canbe treated by observing that  [2.x.39]  implies that  [2.x.40]  atinfinity is necessarily constant. We define its value to be [2.x.41] .  It is an easy exercise to prove that
* [1.x.48]
* Using this result, we can reduce the above equation only on theboundary  [2.x.42]  using the so-called Single and Double LayerPotential operators:
* [1.x.49]
* (The name of these operators comes from the fact that they describe theelectric potential in  [2.x.43]  due to a single thin sheet of chargesalong a surface, and due to a double sheet of charges and anti-charges alongthe surface, respectively.)
* In our case, we know the Neumann values of  [2.x.44]  on the boundary: [2.x.45] .Consequently,[1.x.50]If we take the limit for  [2.x.46]  tending to  [2.x.47]  ofthe above equation, using well known properties of the single and double layeroperators, we obtain an equation for  [2.x.48]  just on the boundary  [2.x.49]  of [2.x.50] :
* [1.x.51]
* which is the Boundary Integral Equation (BIE) we were looking for,where the quantity  [2.x.51]  is the fraction of angle orsolid angle by which the point  [2.x.52]  sees the domain ofintegration  [2.x.53] .
* In particular, at points  [2.x.54]  where the boundary [2.x.55]  is differentiable (i.e. smooth) we have [2.x.56] , but the value may be smaller or largerat points where the boundary has a corner or an edge.
* Substituting the single and double layer operators we get:[1.x.52]for two dimensional flows and[1.x.53]for three dimensional flows, where the normal derivatives of the fundamentalsolutions have been written in a form that makes computation easier. In eithercase,  [2.x.57]  is the solution of an integral equation posed entirely on theboundary since both  [2.x.58] .
* Notice that the fraction of angle (in 2d) or solid angle (in 3d) [2.x.59]  by which the point  [2.x.60]  sees the domain [2.x.61]  can be defined using the double layer potential itself:[1.x.54]
* The reason why this is possible can be understood if we consider thefact that the solution of a pure Neumann problem is known up to anarbitrary constant  [2.x.62] , which means that, if we set the Neumann datato be zero, then any constant  [2.x.63]  will be a solution.Inserting the constant solution and the Neumann boundary condition in theboundary integral equation, we have
* [1.x.55]
* The integral on  [2.x.64]  is unity, see above, so division by the constant  [2.x.65]  gives us the explicitexpression above for  [2.x.66] .
* While this example program is really only focused on the solution of theboundary integral equation, in a realistic setup one would still need to solvefor the velocities. To this end, note that we have just computed [2.x.67]  for all  [2.x.68] . In the next step, wecan compute (analytically, if we want) the solution  [2.x.69]  in allof  [2.x.70] . To this end, recall that we had[1.x.56]where now we have everything that is on the right hand side ( [2.x.71]  and  [2.x.72]  areintegrals we can evaluate, the normal velocity on the boundary is given, and [2.x.73]  on the boundary we have just computed). Finally, we can then recoverthe velocity as  [2.x.74] .
* Notice that the evaluation of the above formula for  [2.x.75]  should yield zero as a result, since the integration of theDirac delta  [2.x.76]  in the domain [2.x.77]  is always zero by definition.
* As a final test, let us verify that this velocity indeed satisfies themomentum balance equation for a stationary flow field, i.e., whether [2.x.78]  where [2.x.79]  for some (unknown) pressure [2.x.80]  and a given constant  [2.x.81] . In other words, we would like to verify thatBernoulli's law as stated above indeed holds. To show this, we use thatthe left hand side of this equation equates to
* [1.x.57]
* where we have used that  [2.x.82]  is constant. We would like towrite this expression as the gradient of something (remember that  [2.x.83]  is aconstant). The next step is moreconvenient if we consider the components of the equation individually(summation over indices that appear twice is implied):
* [1.x.58]
* because  [2.x.84]  and  [2.x.85] . Next,
* [1.x.59]
* Again, the last term disappears because  [2.x.86]  is constant and wecan merge the first and third term into one:
* [1.x.60]
* 
* We now only need to massage that last term a bit more. Using the product rule,we get
* [1.x.61]
* The first of these terms is zero (because, again, the summation over  [2.x.87]  gives [2.x.88] , which is zero). The last term can be written as  [2.x.89]  which is in the desired gradientform. As a consequence, we can now finally state that
* [1.x.62]
* or in vector form:[1.x.63]or in other words:[1.x.64]Because the pressure is only determined up to a constant (it appears only witha gradient in the equations), an equally valid definition is[1.x.65]This is exactly Bernoulli's law mentioned above.
* 

* [1.x.66][1.x.67]
* 

* Numerical approximations of Boundary Integral Equations (BIE) are commonlyreferred to as the boundary element method or panel method (the latterexpression being used mostly in the computational fluid dynamics community).The goal of the following test problem is to solve the integralformulation of the Laplace equation with Neumann boundary conditions,using a circle and a sphere respectively in two and three spacedimensions, illustrating along the way the features that allow one totreat boundary element problems almost as easily as finite elementproblems using the deal.II library.
* To this end, let  [2.x.90]  be a subdivision of themanifold  [2.x.91]  into  [2.x.92]  line segments if  [2.x.93] , or  [2.x.94] quadrilaterals if  [2.x.95] . We will call each individual segment orquadrilateral an [1.x.68] or [1.x.69], independently of thedimension  [2.x.96]  of the surrounding space  [2.x.97] .We define the finite dimensional space  [2.x.98]  as[1.x.70]with basis functions  [2.x.99]  for which we will use the usual FE_Qfinite element, with the catch that this time it is defined on a manifold ofcodimension one (which we do by using the second template argument that isusually defaulted to equal the first; here, we will create objects [2.x.100] dimensional cells in a  [2.x.101]  dimensional space).An element  [2.x.102]  of  [2.x.103]  is uniquelyidentified by the vector  [2.x.104]  of its coefficients [2.x.105] , that is:[1.x.71]where summation  is implied over repeated indexes. Note that we could usediscontinuous elements here &mdash; in fact, there is no real reason to usecontinuous ones since the integral formulation does notimply any derivatives on our trial functions so continuity is unnecessary,and often in the literature only piecewise constant elements are used.
* [1.x.72][1.x.73]
* 

* By far, the most common approximation of boundary integral equationsis by use of the collocation based boundary element method.
* This method requires the evaluation of the boundary integral equationat a number of collocation points which is equal to the number ofunknowns of the system. The choice of these points is a delicatematter, that requires a careful study. Assume that these points areknown for the moment, and call them  [2.x.106]  with  [2.x.107] .
* The problem then becomes:Given the datum  [2.x.108] , find a function  [2.x.109]  in  [2.x.110] such that the following  [2.x.111]  equations are satisfied:
* [1.x.74]
* where the quantity  [2.x.112]  is the fraction of (solid)angle by which the point  [2.x.113]  sees the domain  [2.x.114] , asexplained above, and we set  [2.x.115]  to be zero.  If the supportpoints  [2.x.116]  are chosen appropriately, then the problem canbe written as the following linear system:
* [1.x.75]
* where
* [1.x.76]
* From a linear algebra point of view, the best possible choice of thecollocation points is the one that renders the matrix [2.x.117]  the most diagonally dominant. A natural choiceis then to select the  [2.x.118]  collocation points to be thesupport points of the nodal basis functions  [2.x.119] . In thatcase,  [2.x.120] , and as a consequence the matrix [2.x.121]  is diagonal with entries[1.x.77]where we have used that  [2.x.122]  for the usual Lagrangeelements.With this choice of collocation points, the computation of the entriesof the matrices  [2.x.123] ,  [2.x.124]  and of the right hand side [2.x.125]  requires the evaluation of singular integrals on theelements  [2.x.126]  of the triangulation  [2.x.127] .As usual in these cases, all integrations are performed on a referencesimple domain, i.e., we assume that each element  [2.x.128]  of [2.x.129]  can be expressed as a linear (in two dimensions) orbi-linear (in three dimensions) transformation of the referenceboundary element  [2.x.130] , and we perform the integrations after achange of variables from the real element  [2.x.131]  to the referenceelement  [2.x.132] .
* [1.x.78][1.x.79]
* 

* In two dimensions it is not necessary to compute the diagonal elements [2.x.133]  of the system matrix, since, even if the denominatorgoes to zero when  [2.x.134] , the numerator is alwayszero because  [2.x.135]  and  [2.x.136]  areorthogonal (on our polygonal approximation of the boundary of  [2.x.137] ), andthe only singular integral arises in the computationof  [2.x.138]  on the i-th element of  [2.x.139] :[1.x.80]
* This can be easily treated by the QGaussLogR quadratureformula.
* Similarly, it is possible to use the QGaussOneOverR quadrature formulato perform the singular integrations in three dimensions. Theinterested reader will find detailed explanations on how thesequadrature rules work in their documentation.
* The resulting matrix  [2.x.140]  is full. Depending on itssize, it might be convenient to use a direct solver or an iterativeone. For the purpose of this example code, we chose to use only aniterative solver, without providing any preconditioner.
* If this were a production code rather than a demonstration of principles,there are techniques that are available to not store full matrices but insteadstore only those entries that are large and/or relevant. In the literature onboundary element methods, a plethora of methods is available that allows todetermine which elements are important and which are not, leading to asignificantly sparser representation of these matrices that also facilitatesrapid evaluations of the scalar product between vectors and matrices. This notbeing the goal of this program, we leave this for more sophisticatedimplementations.
* 

* [1.x.81][1.x.82]
* 

* The implementation is rather straight forward. The main point that hasn't beenused in any of the previous tutorial programs is that most classes in deal.IIare not only templated on the dimension, but in fact on the dimension of themanifold on which we pose the differential equation as well as the dimensionof the space into which this manifold is embedded. By default, the secondtemplate argument equals the first, meaning for example that we want to solveon a two-dimensional region of two-dimensional space. The triangulation classto use in this case would be  [2.x.141] , which is anequivalent way of writing  [2.x.142] .
* However, this doesn't have to be so: in the current example, we will forexample want to solve on the surface of a sphere, which is a two-dimensionalmanifold embedded in a three-dimensional space. Consequently, the right classwill be  [2.x.143] , and correspondingly we will use [2.x.144]  as the DoF handler class and [2.x.145]  for finite elements.
* Some further details on what one can do with things that live oncurved manifolds can be found in the report[1.x.83][1.x.84]. In addition, the [2.x.146]  tutorial program extends what we show here to cases where the equationposed on the manifold is not an integral operator but in fact involvesderivatives.
* 

* [1.x.85][1.x.86]
* 

* The testcase we will be solving is for a circular (in 2d) or spherical(in 3d) obstacle. Meshes for these geometries will be read in fromfiles in the current directory and an object of type SphericalManifoldwill then be attached to the triangulation to allow mesh refinementthat respects the continuous geometry behind the discrete initialmesh.
* For a sphere of radius  [2.x.147]  translating at a velocity of  [2.x.148]  in the  [2.x.149]  direction, the potential reads
* [1.x.87]
* see, e.g. J. N. Newman, [1.x.88], 1977,pp. 127. For unit speed and radius, and restricting  [2.x.150]  to lieon the surface of the sphere, [2.x.151] . In the test problem,the flow is  [2.x.152] , so the appropriate exact solution on thesurface of the sphere is the superposition of the above solution withthe analogous solution along the  [2.x.153]  and  [2.x.154]  axes, or  [2.x.155] .
* 

*  [1.x.89] [1.x.90]
*   [1.x.91]  [1.x.92]
* 

* 
*  The program starts with including a bunch of include files that we will use in the various parts of the program. Most of them have been discussed in previous tutorials already:
* 

* 
* [1.x.93]
* 
*  And here are a few C++ standard header files that we will need:
* 

* 
* [1.x.94]
* 
*  The last part of this preamble is to import everything in the dealii namespace into the one into which everything in this program will go:
* 

* 
* [1.x.95]
* 
*   [1.x.96]  [1.x.97]
* 

* 
*  First, let us define a bit of the boundary integral equation machinery.
* 

* 
*  The following two functions are the actual calculations of the single and double layer potential kernels, that is  [2.x.156]  and  [2.x.157] . They are well defined only if the vector  [2.x.158]  is different from zero.
* 

* 
* [1.x.98]
* 
*   [1.x.99]  [1.x.100]
* 

* 
*  The structure of a boundary element method code is very similar to the structure of a finite element code, and so the member functions of this class are like those of most of the other tutorial programs. In particular, by now you should be familiar with reading parameters from an external file, and with the splitting of the different tasks into different modules. The same applies to boundary element methods, and we won't comment too much on them, except on the differences.
* 

* 
* [1.x.101]
* 
*  The only really different function that we find here is the assembly routine. We wrote this function in the most possible general way, in order to allow for easy generalization to higher order methods and to different fundamental solutions (e.g., Stokes or Maxwell).     
*   The most noticeable difference is the fact that the final matrix is full, and that we have a nested loop inside the usual loop on cells that visits all support points of the degrees of freedom.  Moreover, when the support point lies inside the cell which we are visiting, then the integral we perform becomes singular.     
*   The practical consequence is that we have two sets of quadrature formulas, finite element values and temporary storage, one for standard integration and one for the singular integration, which are used where necessary.
* 

* 
* [1.x.102]
* 
*  There are two options for the solution of this problem. The first is to use a direct solver, and the second is to use an iterative solver. We opt for the second option.     
*   The matrix that we assemble is not symmetric, and we opt to use the GMRES method; however the construction of an efficient preconditioner for boundary element methods is not a trivial issue. Here we use a non preconditioned GMRES solver. The options for the iterative solver, such as the tolerance, the maximum number of iterations, are selected through the parameter file.
* 

* 
* [1.x.103]
* 
*  Once we obtained the solution, we compute the  [2.x.159]  error of the computed potential as well as the  [2.x.160]  error of the approximation of the solid angle. The mesh we are using is an approximation of a smooth curve, therefore the computed diagonal matrix of fraction of angles or solid angles  [2.x.161]  should be constantly equal to  [2.x.162] . In this routine we output the error on the potential and the error in the approximation of the computed angle. Notice that the latter error is actually not the error in the computation of the angle, but a measure of how well we are approximating the sphere and the circle.     
*   Experimenting a little with the computation of the angles gives very accurate results for simpler geometries. To verify this you can comment out, in the read_domain() method, the tria.set_manifold(1, manifold) line, and check the alpha that is generated by the program. By removing this call, whenever the mesh is refined new nodes will be placed along the straight lines that made up the coarse mesh, rather than be pulled onto the surface that we really want to approximate. In the three dimensional case, the coarse grid of the sphere is obtained starting from a cube, and the obtained values of alphas are exactly  [2.x.163]  on the nodes of the faces,  [2.x.164]  on the nodes of the edges and  [2.x.165]  on the 8 nodes of the vertices.
* 

* 
* [1.x.104]
* 
*  Once we obtained a solution on the codimension one domain, we want to interpolate it to the rest of the space. This is done by performing again the convolution of the solution with the kernel in the compute_exterior_solution() function.     
*   We would like to plot the velocity variable which is the gradient of the potential solution. The potential solution is only known on the boundary, but we use the convolution with the fundamental solution to interpolate it on a standard dim dimensional continuous finite element space. The plot of the gradient of the extrapolated solution will give us the velocity we want.     
*   In addition to the solution on the exterior domain, we also output the solution on the domain's boundary in the output_results() function, of course.
* 

* 
* [1.x.105]
* 
*  To allow for dimension independent programming, we specialize this single function to extract the singular quadrature formula needed to integrate the singular kernels in the interior of the cells.
* 

* 
* [1.x.106]
* 
*  The usual deal.II classes can be used for boundary element methods by specifying the "codimension" of the problem. This is done by setting the optional second template arguments to Triangulation, FiniteElement and DoFHandler to the dimension of the embedding space. In our case we generate either 1 or 2 dimensional meshes embedded in 2 or 3 dimensional spaces.     
*   The optional argument by default is equal to the first argument, and produces the usual finite element classes that we saw in all previous examples.     
*   The class is constructed in a way to allow for arbitrary order of approximation of both the domain (through high order mapping) and the finite element space. The order of the finite element space and of the mapping can be selected in the constructor of the class.
* 

* 
*  

* 
* [1.x.107]
* 
*  In BEM methods, the matrix that is generated is dense. Depending on the size of the problem, the final system might be solved by direct LU decomposition, or by iterative methods. In this example we use an unpreconditioned GMRES method. Building a preconditioner for BEM method is non trivial, and we don't treat this subject here.
* 

* 
*  

* 
* [1.x.108]
* 
*  The next two variables will denote the solution  [2.x.166]  as well as a vector that will hold the values of  [2.x.167]  (the fraction of  [2.x.168]  visible from a point  [2.x.169] ) at the support points of our shape functions.
* 

* 
*  

* 
* [1.x.109]
* 
*  The convergence table is used to output errors in the exact solution and in the computed alphas.
* 

* 
*  

* 
* [1.x.110]
* 
*  The following variables are the ones that we fill through a parameter file.  The new objects that we use in this example are the  [2.x.170]  object and the QuadratureSelector object.     
*   The  [2.x.171]  class allows us to easily and quickly define new function objects via parameter files, with custom definitions which can be very complex (see the documentation of that class for all the available options).     
*   We will allocate the quadrature object using the QuadratureSelector class that allows us to generate quadrature formulas based on an identifying string and on the possible degree of the formula itself. We used this to allow custom selection of the quadrature formulas for the standard integration, and to define the order of the singular quadrature rule.     
*   We also define a couple of parameters which are used in case we wanted to extend the solution to the entire domain.
* 

* 
*  

* 
* [1.x.111]
* 
*   [1.x.112]  [1.x.113]
* 

* 
*  The constructor initializes the various object in much the same way as done in the finite element programs such as  [2.x.172]  or  [2.x.173] . The only new ingredient here is the ParsedFunction object, which needs, at construction time, the specification of the number of components.   
*   For the exact solution the number of vector components is one, and no action is required since one is the default value for a ParsedFunction object. The wind, however, requires dim components to be specified. Notice that when declaring entries in a parameter file for the expression of the  [2.x.174]  we need to specify the number of components explicitly, since the function  [2.x.175]  is static, and has no knowledge of the number of components.
* 

* 
* [1.x.114]
* 
*  For both two and three dimensions, we set the default input data to be such that the solution is  [2.x.176]  or  [2.x.177] . The actually computed solution will have value zero at infinity. In this case, this coincide with the exact solution, and no additional corrections are needed, but you should be aware of the fact that we arbitrarily set  [2.x.178] , and the exact solution we pass to the program needs to have the same value at infinity for the error to be computed correctly.     
*   The use of the  [2.x.179]  object is pretty straight forward. The  [2.x.180]  function takes an additional integer argument that specifies the number of components of the given function. Its default value is one. When the corresponding  [2.x.181]  method is called, the calling object has to have the same number of components defined here, otherwise an exception is thrown.     
*   When declaring entries, we declare both 2 and three dimensional functions. However only the dim-dimensional one is ultimately parsed. This allows us to have only one parameter file for both 2 and 3 dimensional problems.     
*   Notice that from a mathematical point of view, the wind function on the boundary should satisfy the condition  [2.x.182] , for the problem to have a solution. If this condition is not satisfied, then no solution can be found, and the solver will not converge.
* 

* 
* [1.x.115]
* 
*  In the solver section, we set all SolverControl parameters. The object will then be fed to the GMRES solver in the solve_system() function.
* 

* 
* [1.x.116]
* 
*  After declaring all these parameters to the ParameterHandler object, let's read an input file that will give the parameters their values. We then proceed to extract these values from the ParameterHandler object:
* 

* 
* [1.x.117]
* 
*  Finally, here's another example of how to use parameter files in dimension independent programming.  If we wanted to switch off one of the two simulations, we could do this by setting the corresponding "Run 2d simulation" or "Run 3d simulation" flag to false:
* 

* 
* [1.x.118]
* 
*   [1.x.119]  [1.x.120]
* 

* 
*  A boundary element method triangulation is basically the same as a (dim-1) dimensional triangulation, with the difference that the vertices belong to a (dim) dimensional space.   
*   Some of the mesh formats supported in deal.II use by default three dimensional points to describe meshes. These are the formats which are compatible with the boundary element method capabilities of deal.II. In particular we can use either UCD or GMSH formats. In both cases, we have to be particularly careful with the orientation of the mesh, because, unlike in the standard finite element case, no reordering or compatibility check is performed here.  All meshes are considered as oriented, because they are embedded in a higher dimensional space. (See the documentation of the GridIn and of the Triangulation for further details on orientation of cells in a triangulation.) In our case, the normals to the mesh are external to both the circle in 2d or the sphere in 3d.   
*   The other detail that is required for appropriate refinement of the boundary element mesh is an accurate description of the manifold that the mesh approximates. We already saw this several times for the boundary of standard finite element meshes (for example in  [2.x.183]  and  [2.x.184] ), and here the principle and usage is the same, except that the SphericalManifold class takes an additional template parameter that specifies the embedding space dimension.
* 

* 
*  

* 
* [1.x.121]
* 
*  The call to  [2.x.185]  copies the manifold (via  [2.x.186]  so we do not need to worry about invalid pointers to  [2.x.187] :
* 

* 
* [1.x.122]
* 
*   [1.x.123]  [1.x.124]
* 

* 
*  This function globally refines the mesh, distributes degrees of freedom, and resizes matrices and vectors.
* 

* 
*  

* 
* [1.x.125]
* 
*   [1.x.126]  [1.x.127]
* 

* 
*  The following is the main function of this program, assembling the matrix that corresponds to the boundary integral equation.
* 

* 
* [1.x.128]
* 
*  First we initialize an FEValues object with the quadrature formula for the integration of the kernel in non singular cells. This quadrature is selected with the parameter file, and needs to be quite precise, since the functions we are integrating are not polynomial functions.
* 

* 
* [1.x.129]
* 
*  Unlike in finite element methods, if we use a collocation boundary element method, then in each assembly loop we only assemble the information that refers to the coupling between one degree of freedom (the degree associated with support point  [2.x.188] ) and the current cell. This is done using a vector of fe.dofs_per_cell elements, which will then be distributed to the matrix in the global row  [2.x.189] . The following object will hold this information:
* 

* 
* [1.x.130]
* 
*  The index  [2.x.190]  runs on the collocation points, which are the support points of the  [2.x.191] th basis function, while  [2.x.192]  runs on inner integration points.
* 

* 
*  We construct a vector of support points which will be used in the local integrations:
* 

* 
* [1.x.131]
* 
*  After doing so, we can start the integration loop over all cells, where we first initialize the FEValues object and get the values of  [2.x.193]  at the quadrature points (this vector field should be constant, but it doesn't hurt to be more general):
* 

* 
* [1.x.132]
* 
*  We then form the integral over the current cell for all degrees of freedom (note that this includes degrees of freedom not located on the current cell, a deviation from the usual finite element integrals). The integral that we need to perform is singular if one of the local degrees of freedom is the same as the support point  [2.x.194] . A the beginning of the loop we therefore check whether this is the case, and we store which one is the singular index:
* 

* 
* [1.x.133]
* 
*  We then perform the integral. If the index  [2.x.195]  is not one of the local degrees of freedom, we simply have to add the single layer terms to the right hand side, and the double layer terms to the matrix:
* 

* 
* [1.x.134]
* 
*  Now we treat the more delicate case. If we are here, this means that the cell that runs on the  [2.x.196]  index contains support_point[i]. In this case both the single and the double layer potential are singular, and they require special treatment.                 
*   Whenever the integration is performed with the singularity inside the given cell, then a special quadrature formula is used that allows one to integrate arbitrary functions against a singular weight on the reference cell.                 
*   The correct quadrature formula is selected by the get_singular_quadrature function, which is explained in detail below.
* 

* 
* [1.x.135]
* 
*  Finally, we need to add the contributions of the current cell to the global matrix.
* 

* 
* [1.x.136]
* 
*  The second part of the integral operator is the term  [2.x.197] . Since we use a collocation scheme,  [2.x.198]  and the corresponding matrix is a diagonal one with entries equal to  [2.x.199] .
* 

* 
*  One quick way to compute this diagonal matrix of the solid angles, is to use the Neumann matrix itself. It is enough to multiply the matrix with a vector of elements all equal to
* 
*  - , to get the diagonal matrix of the alpha angles, or solid angles (see the formula in the introduction for this). The result is then added back onto the system matrix object to yield the final form of the matrix:
* 

* 
* [1.x.137]
* 
*   [1.x.138]  [1.x.139]
* 

* 
*  The next function simply solves the linear system.
* 

* 
* [1.x.140]
* 
*   [1.x.141]  [1.x.142]
* 

* 
*  The computation of the errors is exactly the same in all other example programs, and we won't comment too much. Notice how the same methods that are used in the finite element methods can be used here.
* 

* 
* [1.x.143]
* 
*  The error in the alpha vector can be computed directly using the  [2.x.200]  function, since on each node, the value should be  [2.x.201] . All errors are then output and appended to our ConvergenceTable object for later computation of convergence rates:
* 

* 
* [1.x.144]
* 
*  Singular integration requires a careful selection of the quadrature rules. In particular the deal.II library provides quadrature rules which are tailored for logarithmic singularities (QGaussLog, QGaussLogR), as well as for 1/R singularities (QGaussOneOverR).   
*   Singular integration is typically obtained by constructing weighted quadrature formulas with singular weights, so that it is possible to write   
*   [1.x.145]   
*   where  [2.x.202]  is a given singularity, and the weights and quadrature points  [2.x.203]  are carefully selected to make the formula above an equality for a certain class of functions  [2.x.204] .   
*   In all the finite element examples we have seen so far, the weight of the quadrature itself (namely, the function  [2.x.205] ), was always constantly equal to 1.  For singular integration, we have two choices: we can use the definition above, factoring out the singularity from the integrand (i.e., integrating  [2.x.206]  with the special quadrature rule), or we can ask the quadrature rule to "normalize" the weights  [2.x.207]  with  [2.x.208] :   
*   [1.x.146]   
*   We use this second option, through the  [2.x.209]  parameter of both QGaussLogR and QGaussOneOverR.   
*   These integrals are somewhat delicate, especially in two dimensions, due to the transformation from the real to the reference cell, where the variable of integration is scaled with the determinant of the transformation.   
*   In two dimensions this process does not result only in a factor appearing as a constant factor on the entire integral, but also on an additional integral altogether that needs to be evaluated:   
*   [1.x.147]   
*   This process is taken care of by the constructor of the QGaussLogR class, which adds additional quadrature points and weights to take into consideration also the second part of the integral.   
*   A similar reasoning should be done in the three dimensional case, since the singular quadrature is tailored on the inverse of the radius  [2.x.210]  in the reference cell, while our singular function lives in real space, however in the three dimensional case everything is simpler because the singularity scales linearly with the determinant of the transformation. This allows us to build the singular two dimensional quadrature rules only once and, reuse them over all cells.   
*   In the one dimensional singular integration this is not possible, since we need to know the scaling parameter for the quadrature, which is not known a priori. Here, the quadrature rule itself depends also on the size of the current cell. For this reason, it is necessary to create a new quadrature for each singular integration.   
*   The different quadrature rules are built inside the get_singular_quadrature, which is specialized for dim=2 and dim=3, and they are retrieved inside the assemble_system function. The index given as an argument is the index of the unit support point where the singularity is located.
* 

* 
*  

* 
* [1.x.148]
* 
*   [1.x.149]  [1.x.150]
* 

* 
*  We'd like to also know something about the value of the potential  [2.x.211]  in the exterior domain: after all our motivation to consider the boundary integral problem was that we wanted to know the velocity in the exterior domain!   
*   To this end, let us assume here that the boundary element domain is contained in the box  [2.x.212] , and we extrapolate the actual solution inside this box using the convolution with the fundamental solution. The formula for this is given in the introduction.   
*   The reconstruction of the solution in the entire space is done on a continuous finite element grid of dimension dim. These are the usual ones, and we don't comment any further on them. At the end of the function, we output this exterior solution in, again, much the usual way.
* 

* 
* [1.x.151]
* 
*   [1.x.152]  [1.x.153]
* 

* 
*  Outputting the results of our computations is a rather mechanical tasks. All the components of this function have been discussed before.
* 

* 
* [1.x.154]
* 
*   [1.x.155]  [1.x.156]
* 

* 
*  This is the main function. It should be self explanatory in its briefness:
* 

* 
* [1.x.157]
* 
*   [1.x.158]  [1.x.159]
* 

* 
*  This is the main function of this program. It is exactly like all previous tutorial programs:
* 

* 
* [1.x.160]
* [1.x.161][1.x.162]
* 

* We ran the program using the following  [2.x.213]  file (whichcan also be found in the directory in which all the other source files are):
* [1.x.163]
* 
* When we run the program, the following is printed on screen:
* [1.x.164]
* 
* As we can see from the convergence table in 2d, if we choosequadrature formulas which are accurate enough, then the error weobtain for  [2.x.214]  should be exactly the inverse of thenumber of elements. The approximation of the circle with N segments ofequal size generates a regular polygon with N faces, whose angles areexactly  [2.x.215] , therefore the error we commit should beexactly  [2.x.216] . In fact this isa very good indicator that we are performing the singular integrals inan appropriate manner.
* The error in the approximation of the potential  [2.x.217]  is largely dueto approximation of the domain. A much better approximation could beobtained by using higher order mappings.
* If we modify the main() function, setting fe_degree and mapping_degreeto two, and raise the order of the quadrature formulas  inthe parameter file, we obtain the following convergence table for thetwo dimensional simulation
* [1.x.165]
* 
* and
* [1.x.166]
* 
* for the three dimensional case. As we can see, convergence results aremuch better with higher order mapping, mainly due to a betterresolution of the curved geometry. Notice that, given the same numberof degrees of freedom, for example in step 3 of the Q1 case and step 2of Q2 case in the three dimensional simulation, the error is roughlythree orders of magnitude lower.
* The result of running these computations is a bunch of output files that wecan pass to our visualization program of choice.The output files are of two kind: the potential on the boundaryelement surface, and the potential extended to the outer and innerdomain. The combination of the two for the two dimensional case lookslike
*  [2.x.218] 
* while in three dimensions we show first the potential on the surface,together with a contour plot,
*  [2.x.219] 
* and then the external contour plot of the potential, with opacity set to 25%:
*  [2.x.220] 
* 

* [1.x.167][1.x.168][1.x.169]
* 

* This is the first tutorial program that considers solving equations defined onsurfaces embedded in higher dimensional spaces. But the equation discussedhere was relatively simple because it only involved an integral operator, notderivatives which are more difficult to define on the surface. The  [2.x.221] tutorial program considers such problems and provides the necessary tools.
* From a practical perspective, the Boundary Element Method (BEM) usedhere suffers from two bottlenecks. The first is that assembling thematrix has a cost that isquadratic* in the number of unknowns, thatis  [2.x.222]  where  [2.x.223]  is the total number of unknowns. This canbe seen by looking at the `assemble_system()` function, which has thisstructure:
* [1.x.170]
* Here, the first loop walks over all cells (one factor of  [2.x.224] ) whereasthe inner loop contributes another factor of  [2.x.225] .
* This has to be contrasted with the finite element method forlocal*
differential operators: There, we loop over all cells (one factor of [2.x.226] ) and on each cell do an amount of work that is independent of howmany cells or unknowns there are. This clearly presents abottleneck.
* The second bottleneck is that the system matrix is dense (i.e., is oftype FullMatrix) because every degree of freedom couples with everyother degree of freedom. As pointed out above, justcomputing* thismatrix with its  [2.x.227]  nonzero entries necessarily requires at least [2.x.228]  operations, but it's worth pointing out that it alsocosts this many operations to just do one matrix-vector product. Ifthe GMRES method used to solve the linear system requires a number ofiterations that grows with the size of the problem, as is typicallythe case, then solving the linear system will require a number ofoperations that grows even faster than just  [2.x.229] .
* "Real" boundary element methods address these issues by strategiesthat determine which entries of the matrix will be small and canconsequently be neglected (at the cost of introducing an additionalerror, of course). This is possible by recognizing that the matrixentries decay with the (physical) distance between the locations wheredegrees of freedom  [2.x.230]  and  [2.x.231]  are defined. This can be exploited inmethods such as the Fast Multipole Method (FMM) that control whichmatrix entries must be stored and computed to achieve a certainaccuracy, and
* 
*  -  if done right
* 
*  -  result in methods in which bothassembly and solution of the linear system requires less than [2.x.232]  operations.
* Implementing these methods clearly presents opportunities to extendthe current program.
* 

* [1.x.171][1.x.172] [2.x.233] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-35_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28]
*  [2.x.2] 
* [1.x.29]
* [1.x.30][1.x.31][1.x.32]
* 

* [1.x.33][1.x.34][1.x.35]
* The purpose of this program is to show how to effectively solve the incompressible time-dependentNavier-Stokes equations. These equations describe the flow of a viscous incompressible fluid and read
* [1.x.36]
* where  [2.x.3]  represents the velocity of the flow and  [2.x.4]  the pressure. This system of equations is supplemented bythe initial condition[1.x.37]with  [2.x.5]  sufficiently smooth and solenoidal, and suitable boundary conditions. For instance, an admissible boundarycondition, is[1.x.38]It is possible to prescribe other boundary conditions as well. In the test case that we solve here the boundaryis partitioned into two disjoint subsets  [2.x.6]  and we have[1.x.39]and[1.x.40]where  [2.x.7]  is the outer unit normal. The boundary conditions on  [2.x.8]  are oftenused to model outflow conditions.
* In previous tutorial programs (see for instance  [2.x.9]  and [2.x.10] ) we have seenhow to solve the time-independent Stokes equations using a Schur complement approach. For thetime-dependent case, after time discretization, we would arrive at a system like
* [1.x.41]
* where  [2.x.11]  is the time-step. Although the structure of this system is similar to the Stokes system and thusit could be solved using a Schur complement approach, it turns out that the condition number of theSchur complement is proportional to  [2.x.12] . This makes the system verydifficult to solve, and means that for the Navier-Stokes equations, this isnot a useful avenue to the solution.
* [1.x.42][1.x.43][1.x.44]
* 

* Rather, we need to come up with a different approach to solve the time-dependent Navier-Stokesequations. The difficulty in their solution comes from the fact that the velocity and the pressure are coupledthrough the constraint[1.x.45]for which the pressure is the Lagrange multiplier.Projection methods aim at decoupling this constraint from the diffusion (Laplace) operator.
* Let us shortly describe how the projection methods look like in a semi-discrete setting. The objective is toobtain a sequence of velocities  [2.x.13]  and pressures  [2.x.14] . We willalso obtain a sequence  [2.x.15]  of auxiliary variables.Suppose that from the initial conditions, and an application of a first order method we have found [2.x.16]  and  [2.x.17] . Then the projection method consists of the following steps: [2.x.18]    [2.x.19]  [1.x.46]: Extrapolation. Define:  [1.x.47]   [2.x.20]  [1.x.48]: Diffusion step. We find  [2.x.21]  that solves the single  linear equation  [1.x.49]
*    [2.x.22]  [1.x.50]: Projection. Find  [2.x.23]  that solves  [1.x.51]   [2.x.24]  [1.x.52]: Pressure correction. Here we have two options:     [2.x.25]        [2.x.26]  [1.x.53]. The pressure is updated by:      [1.x.54]       [2.x.27]  [1.x.55]. In this case      [1.x.56]     [2.x.28]  [2.x.29] 
* Without going into details, let us remark a few things about the projection methods that we have just described: [2.x.30]    [2.x.31]  The advection term  [2.x.32]  is replaced by its [1.x.57]  [1.x.58]  This is consistent with the continuous equation (because  [2.x.33] ,  though this is not true pointwise for the discrete solution) and it is needed to  guarantee unconditional stability of the  time-stepping scheme. Moreover, to linearize the term we use the second order extrapolation  [2.x.34]  of   [2.x.35] .   [2.x.36]  The projection step is a realization of the Helmholtz decomposition  [1.x.59]  where  [1.x.60]  and  [1.x.61]  Indeed, if we use this decomposition on  [2.x.37]  we obtain  [1.x.62]  with  [2.x.38] . Taking the divergence of this equation we arrive at the projection equation.   [2.x.39]  The more accurate of the two variants outlined above is the rotational  one. However, the program below implements both variants. Moreover, in the author's experience,  the standard form is the one that should be used if, for instance, the viscosity  [2.x.40]  is variable. [2.x.41] 
* 

*  [2.x.42] The standard incremental scheme and the rotational incremental scheme were first considered by van Kan in [2.x.43]    [2.x.44]  J. van Kan, "A second-order accurate pressure-correction scheme for viscous incompressible flow",       SIAM Journal on Scientific and Statistical Computing, vol. 7, no. 3, pp. 870–891, 1986 [2.x.45] and is analyzed by Guermond in [2.x.46]    [2.x.47]  J.-L. Guermond, "Un résultat de convergence d’ordre deux en temps pour                        l’approximation des équations de Navier–Stokes par une technique de projection incrémentale",       ESAIM: Mathematical Modelling and Numerical Analysis, vol. 33, no. 1, pp. 169–189, 1999 [2.x.48] for the case  [2.x.49] .It turns out that this technique suffers from unphysical boundary conditions for the kinematic pressure thatlead to reduced rates of convergence. To prevent this, Timmermans et al. proposed in [2.x.50]    [2.x.51]  L. Timmermans, P. Minev, and F. Van De Vosse,       "An approximate projection scheme for incompressible flow using spectral elements",       International Journal for Numerical Methods in Fluids, vol. 22, no. 7, pp. 673–688, 1996 [2.x.52] the rotational pressure-correction projection method that uses a divergence correction for the kinematic pressure.A thorough analysis for scheme has first been performed in [2.x.53]    [2.x.54]  J.-L. Guermond and J. Shen, "On the error estimates for the rotational pressure-correction projection methods",       Mathematics of Computation, vol. 73, no. 248, pp. 1719–1737, 2004 [2.x.55] for the Stokes problem. [2.x.56] 
* [1.x.63][1.x.64][1.x.65]
* To obtain a fully discrete setting of the method we, as always, need a variational formulation. There is onesubtle issue here given the nature of the boundary conditions. When we multiply the equation by a suitable testfunction one of the term that arises is[1.x.66]If we, say, had Dirichlet boundary conditions on the whole boundary then after integration by parts we wouldobtain[1.x.67]One of the advantages of this formulation is that it fully decouples the components of the velocity. Moreover,they all share the same system matrix. This can be exploited in the program.
* However, given the nonstandard boundary conditions, to be able to take them into account we need to usethe following %identity[1.x.68]so that when we integrate by parts and take into account the boundary conditions we obtain[1.x.69]which is the form that we would have to use. Notice that this couples the components of the velocity.Moreover, to enforce the boundary condition on the pressure, we need to rewrite[1.x.70]where the boundary integral in  [2.x.57]  equals zero given the boundary conditions for the velocity,and the one in  [2.x.58]  given the boundary conditions for the pressure.
* In the simplified case where the boundary  [2.x.59]  is %parallel to a coordinate axis, which holds forthe testcase that we carry out below, it can actually be shown that[1.x.71]This issue is not very often addressed in the literature. For more information the reader can consult, forinstance, [2.x.60]    [2.x.61]  J.-L. GUERMOND, L. QUARTAPELLE, On the approximation of the unsteady Navier-Stokes equations by  finite element projection methods, Numer. Math., 80  (1998) 207-238   [2.x.62]  J.-L. GUERMOND, P. MINEV, J. SHEN, Error analysis of pressure-correction schemes for the  Navier-Stokes equations with open boundary conditions, SIAM J. Numer. Anal., 43  1 (2005) 239--258. [2.x.63] 
* 

* 
* [1.x.72][1.x.73][1.x.74]
* 

* Our implementation of the projection methods follows [1.x.75] the description given above. We must note,however, that as opposed to most other problems that have several solution components, we do not usevector-valued finite elements. Instead, we use separate finite elements the components of the velocityand the pressure, respectively, and use different  [2.x.64] 's for those as well. The mainreason for doing this is that, as we see from the description of the scheme, the  [2.x.65]  componentsof the velocity and the pressure are decoupled. As a consequence, the equations for all the velocity componentslook all the same, have the same system matrix, and can be solved in %parallel. Obviously, this approachhas also its disadvantages. For instance, we need to keep several  [2.x.66] s and iteratorssynchronized when assembling matrices and right hand sides; obtaining quantities that are inherent tovector-valued functions (e.g. divergences) becomes a little awkward, and others.
* [1.x.76][1.x.77][1.x.78]
* 

* The testcase that we use for this program consists of the flow around a square obstacle. The geometry isas follows:
*  [2.x.67] 
* with  [2.x.68] , making the geometry slightly non-symmetric.
* We impose no-slip boundary conditions on both the top and bottom walls and the obstacle. On the left side wehave the inflow boundary condition[1.x.79]with  [2.x.69] , i.e. the inflow boundary conditions correspond to Poiseuille flow for this configuration.Finally, on the right vertical wall we impose the condition that the vertical component of the velocityand the pressure should both be zero.The final time  [2.x.70] .
* 

*  [1.x.80] [1.x.81]
*   [1.x.82]  [1.x.83]
* 

* 
*  We start by including all the necessary deal.II header files and some C++ related ones. Each one of them has been discussed in previous tutorial programs, so we will not get into details here.
* 

* 
* [1.x.84]
* 
*  Finally this is as in all previous programs:
* 

* 
* [1.x.85]
* 
*   [1.x.86]  [1.x.87]   
*   Since our method has several parameters that can be fine-tuned we put them into an external file, so that they can be determined at run-time.   
*   This includes, in particular, the formulation of the equation for the auxiliary variable  [2.x.71] , for which we declare an  [2.x.72] . Next, we declare a class that is going to read and store all the parameters that our program needs to run.
* 

* 
* [1.x.88]
* 
*  In the constructor of this class we declare all the parameters. The details of how this works have been discussed elsewhere, for example in  [2.x.73] .
* 

* 
* [1.x.89]
* 
*   [1.x.90]  [1.x.91]
* 

* 
*  In the next namespace, we declare the initial and boundary conditions:
* 

* 
* [1.x.92]
* 
*  As we have chosen a completely decoupled formulation, we will not take advantage of deal.II's capabilities to handle vector valued problems. We do, however, want to use an interface for the equation data that is somehow dimension independent. To be able to do that, our functions should be able to know on which spatial component we are currently working, and we should be able to have a common interface to do that. The following class is an attempt in that direction.
* 

* 
* [1.x.93]
* 
*  With this class defined, we declare classes that describe the boundary conditions for velocity and pressure:
* 

* 
* [1.x.94]
* 
*   [1.x.95]  [1.x.96]
* 

* 
*  Now for the main class of the program. It implements the various versions of the projection method for Navier-Stokes equations. The names for all the methods and member variables should be self-explanatory, taking into account the implementation details given in the introduction.
* 

* 
* [1.x.97]
* 
*  The next few structures and functions are for doing various things in parallel. They follow the scheme laid out in  [2.x.74] , using the WorkStream class. As explained there, this requires us to declare two structures for each of the assemblers, a per-task data and a scratch data structure. These are then handed over to functions that assemble local contributions and that copy these local contributions to the global objects.     
*   One of the things that are specific to this program is that we don't just have a single DoFHandler object that represents both the velocities and the pressure, but we use individual DoFHandler objects for these two kinds of variables. We pay for this optimization when we want to assemble terms that involve both variables, such as the divergence of the velocity and the gradient of the pressure, times the respective test functions. When doing so, we can't just anymore use a single FEValues object, but rather we need two, and they need to be initialized with cell iterators that point to the same cell in the triangulation but different DoFHandlers.     
*   To do this in practice, we declare a "synchronous" iterator
* 
*  -  an object that internally consists of several (in our case two) iterators, and each time the synchronous iteration is moved forward one step, each of the iterators stored internally is moved forward one step as well, thereby always staying in sync. As it so happens, there is a deal.II class that facilitates this sort of thing. (What is important here is to know that two DoFHandler objects built on the same triangulation will walk over the cells of the triangulation in the same order.)
* 

* 
* [1.x.98]
* 
*  The same general layout also applies to the following classes and functions implementing the assembly of the advection term:
* 

* 
* [1.x.99]
* 
*  The final few functions implement the diffusion solve as well as postprocessing the output, including computing the curl of the velocity:
* 

* 
* [1.x.100]
* 
*   [1.x.101]  [1.x.102]
* 

* 
*  In the constructor, we just read all the data from the  [2.x.75]  object that is passed as an argument, verify that the data we read is reasonable and, finally, create the triangulation and load the initial data.
* 

* 
* [1.x.103]
* 
*   [1.x.104]  [1.x.105]
* 

* 
*  The method that creates the triangulation and refines it the needed number of times. After creating the triangulation, it creates the mesh dependent data, i.e. it distributes degrees of freedom and renumbers them, and initializes the matrices and vectors that we will use.
* 

* 
* [1.x.106]
* 
*   [1.x.107]  [1.x.108]
* 

* 
*  This method creates the constant matrices and loads the initial data
* 

* 
* [1.x.109]
* 
*   [1.x.110]  [1.x.111]
* 

* 
*  In this set of methods we initialize the sparsity patterns, the constraints (if any) and assemble the matrices that do not depend on the timestep  [2.x.76] . Note that for the Laplace and mass matrices, we can use functions in the library that do this. Because the expensive operations of this function
* 
*  -  creating the two matrices
* 
*  -  are entirely independent, we could in principle mark them as tasks that can be worked on in %parallel using the  [2.x.77]  functions. We won't do that here since these functions internally already are parallelized, and in particular because the current function is only called once per program run and so does not incur a cost in each time step. The necessary modifications would be quite straightforward, however.
* 

* 
* [1.x.112]
* 
*  The initialization of the matrices that act on the pressure space is similar to the ones that act on the velocity space.
* 

* 
* [1.x.113]
* 
*  For the gradient operator, we start by initializing the sparsity pattern and compressing it. It is important to notice here that the gradient operator acts from the pressure space into the velocity space, so we have to deal with two different finite element spaces. To keep the loops synchronized, we use the alias that we have defined before, namely  [2.x.78] .
* 

* 
* [1.x.114]
* 
*   [1.x.115]  [1.x.116]
* 

* 
*  This is the time marching function, which starting at  [2.x.79]  advances in time using the projection method with time step  [2.x.80]  until  [2.x.81] .   
*   Its second parameter,  [2.x.82]  indicates whether the function should output information what it is doing at any given moment: for example, it will say whether we are working on the diffusion, projection substep; updating preconditioners etc. Rather than implementing this output using code like  [2.x.83]  we use the ConditionalOStream class to do that for us. That class takes an output stream and a condition that indicates whether the things you pass to it should be passed through to the given output stream, or should just be ignored. This way, above code simply becomes  [2.x.84]  and does the right thing in either case.
* 

* 
* [1.x.119]
* 
*   [1.x.120]  [1.x.121]
* 

* 
*  The implementation of a diffusion step. Note that the expensive operation is the diffusion solve at the end of the function, which we have to do once for each velocity component. To accelerate things a bit, we allow to do this in %parallel, using the  [2.x.85]  function which makes sure that the  [2.x.86]  solves are all taken care of and are scheduled to available processors: if your machine has more than one processor core and no other parts of this program are using resources currently, then the diffusion solves will run in %parallel. On the other hand, if your system has only one processor core then running things in %parallel would be inefficient (since it leads, for example, to cache congestion) and things will be executed sequentially.
* 

* 
* [1.x.122]
* 
*   [1.x.123]  [1.x.124]
* 

* 
*  The following few functions deal with assembling the advection terms, which is the part of the system matrix for the diffusion step that changes at every time step. As mentioned above, we will run the assembly loop over all cells in %parallel, using the WorkStream class and other facilities as described in the documentation module on  [2.x.87] .
* 

* 
* [1.x.125]
* 
*   [1.x.126]  [1.x.127]
* 

* 
*  This implements the projection step:
* 

* 
* [1.x.128]
* 
*   [1.x.129]  [1.x.130]
* 

* 
*  This is the pressure update step of the projection method. It implements the standard formulation of the method, that is [1.x.131] or the rotational form, which is [1.x.132]
* 

* 
* [1.x.133]
* 
*   [1.x.134]  [1.x.135]
* 

* 
*  This method plots the current solution. The main difficulty is that we want to create a single output file that contains the data for all velocity components, the pressure, and also the vorticity of the flow. On the other hand, velocities and the pressure live on separate DoFHandler objects, and so can't be written to the same file using a single DataOut object. As a consequence, we have to work a bit harder to get the various pieces of data into a single DoFHandler object, and then use that to drive graphical output.   
*   We will not elaborate on this process here, but rather refer to  [2.x.88] , where a similar procedure is used (and is documented) to create a joint DoFHandler object for all variables.   
*   Let us also note that we here compute the vorticity as a scalar quantity in a separate function, using the  [2.x.89]  projection of the quantity  [2.x.90]  onto the finite element space used for the components of the velocity. In principle, however, we could also have computed as a pointwise quantity from the velocity, and do so through the DataPostprocessor mechanism discussed in  [2.x.91]  and  [2.x.92] .
* 

* 
* [1.x.136]
* 
*  Following is the helper function that computes the vorticity by projecting the term  [2.x.93]  onto the finite element space used for the components of the velocity. The function is only called whenever we generate graphical output, so not very often, and as a consequence we didn't bother parallelizing it using the WorkStream concept as we do for the other assembly functions. That should not be overly complicated, however, if needed. Moreover, the implementation that we have here only works for 2d, so we bail if that is not the case.
* 

* 
* [1.x.137]
* 
*   [1.x.138]  [1.x.139]
* 

* 
*  The main function looks very much like in all the other tutorial programs, so there is little to comment on here:
* 

* 
* [1.x.140]
* [1.x.141][1.x.142][1.x.143]
* 

* [1.x.144][1.x.145][1.x.146]
* 

* We run the code with the following  [2.x.94] , which can be found in thesame directory as the source:
* [1.x.147]
* 
* Since the  [2.x.95] ,we do not get any kind of output besides the number of the time stepthe program is currently working on.If we were to set it to  [2.x.96]  we would get information on what the program is doing andhow many steps each iterative process had to make to converge, etc.
* Let us plot the obtained results for  [2.x.97]  (i.e. time steps200, 1000, 2400, 4000, and 5000), where in the left column we show thevorticity and in the right the velocity field:
*  [2.x.98] 
* The images show nicely the development and extension of a vortex chainbehind the obstacles, with the sign of the vorticity indicatingwhether this is a left or right turning vortex.
* 

* [1.x.148][1.x.149][1.x.150]
* 

* We can change the Reynolds number,  [2.x.99] , in the parameter file to avalue of  [2.x.100] . Doing so, and reducing the time step somewhat as well,yields the following images at times  [2.x.101] :
*  [2.x.102] 
* For this larger Reynolds number, we observe unphysical oscillations, especiallyfor the vorticity. The discretization scheme has now difficulties in correctlyresolving the flow, which should still be laminar and well-organized.These phenomena are typical of discretization schemes that lack robustnessin under-resolved scenarios, where under-resolved means that the Reynoldsnumber computed with the mesh size instead of the physical dimensions ofthe geometry is large. We look at a zoom at the region behind the obstacle, andthe mesh size we have there:
* 

*  [2.x.103] 
* We can easily test our hypothesis by re-running the simulation with one moremesh refinement set in the parameter file:
*  [2.x.104] 
* Indeed, the vorticity field now looks much smoother. While we can expect thatfurther refining the mesh will suppress the remaining oscillations as well,one should take measures to obtain a robust scheme in the limit of coarseresolutions, as described below.
* 

* [1.x.151][1.x.152][1.x.153]
* 

* This program can be extended in the following directions: [2.x.105]    [2.x.106]  Adaptive mesh refinement: As we have seen, we computed everything on a single fixed mesh.  Using adaptive mesh refinement can lead to increased accuracy while not significantly increasing the  computational time.
*    [2.x.107]  Adaptive time-stepping: Although there apparently is currently no theory about  projection methods with variable time step,  practice shows that they perform very well.
*    [2.x.108]  High Reynolds %numbers: As we can see from the results, increasing the Reynolds number changes significantly  the behavior of the discretization scheme. Using well-known stabilization techniques we could be able to  compute the flow in this, or many other problems, when the Reynolds number is very large and where computational  costs demand spatial resolutions for which the flow is only marginally resolved, especially for 3D turbulent  flows.
*    [2.x.109]  Variable density incompressible flows: There are projection-like methods for the case of incompressible  flows with variable density. Such flows play a role if fluids of different  density mix, for example fresh water and salt water, or alcohol and water.
*    [2.x.110]  Compressible Navier-Stokes equations: These equations are relevant for  cases where  velocities are high enough so that the fluid becomes compressible, but not  fast enough that we get into a regime where viscosity becomes negligible  and the Navier-Stokes equations need to be replaced by the hyperbolic Euler  equations of gas dynamics. Compressibility starts to become a factor if the  velocity becomes greater than about one third of the speed of sound, so it  is not a factor for almost all terrestrial vehicles. On the other hand,  commercial jetliners fly at about 85 per cent of the speed of sound, and  flow over the wings becomes significantly supersonic, a regime in which the  compressible Navier-Stokes equations are not applicable any more  either. There are significant applications for the range in between,  however, such as for small aircraft or the fast trains in many European and  East Asian countries. [2.x.111] 
* 

* [1.x.154][1.x.155] [2.x.112] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-36_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19]
*  [2.x.2] 
* [1.x.20]
* [1.x.21][1.x.22][1.x.23]
* 

* The problem we want to solve in this example is an eigenspectrumproblem. Eigenvalue problems appear in a wide context of problems, forexample in the computation of electromagnetic standing waves incavities, vibration modes of drum membranes, or oscillations of lakesand estuaries. One of the most enigmatic applications is probably thecomputation of stationary or quasi-static wave functions in quantummechanics. The latter application is what we would like to investigatehere, though the general techniques outlined in this program are ofcourse equally applicable to the other applications above.
* Eigenspectrum problems have the general form
* [1.x.24]
* where the Dirichlet boundary condition on  [2.x.3]  could also bereplaced by Neumann or Robin conditions;  [2.x.4]  is an operator that generallyalso contains differential operators.
* Under suitable conditions, the above equations have a set of solutions [2.x.5] ,  [2.x.6] , where  [2.x.7]  canbe a finite or infinite set (and in the latter case it may be a discrete orsometimes at least in part a continuous set). In either case, let us note thatthere isno longer just a single solution, but a set of solutions (the variouseigenfunctions and corresponding eigenvalues) that we want tocompute. The problem of numerically finding all eigenvalues(eigenfunctions) of such eigenvalue problems is a formidablechallenge. In fact, if the set  [2.x.8]  is infinite, the challenge isof course intractable.  Most of the time however we are really onlyinterested in a small subset of these values (functions); andfortunately, the interface to the SLEPc library that we will use forthis tutorial program allows us to select which portion of theeigenspectrum and how many solutions we want to solve for.
* In this program, the eigenspectrum solvers we use are classes providedby deal.II that wrap around the linear algebra implementation of the[1.x.25]library; SLEPc itself builds on the [1.x.26] libraryfor linear algebra contents.
* [1.x.27][1.x.28][1.x.29]
* 

* The basic equation of stationary quantum mechanics is theSchrödinger equation which models the motion of particles in anexternal potential  [2.x.9] . The particle is described by a wavefunction  [2.x.10]  that satisfies a relation of the(nondimensionalized) form
* [1.x.30]
* As a consequence, this particle can only exist in a certain number ofeigenstates that correspond to the energy eigenvalues [2.x.11]  admitted as solutions of this equation. Theorthodox (Copenhagen) interpretation of quantum mechanics posits that, if aparticle has energy  [2.x.12]  then the probability of findingit at location  [2.x.13]  is proportional to  [2.x.14]  where  [2.x.15]  is the eigenfunction that corresponds to thiseigenvalue.
* In order to numerically find solutions to this equation, i.e. a set ofpairs of eigenvalues/eigenfunctions, we use the usual finite elementapproach of multiplying the equation from the left with test functions,integrating by parts, and searching for solutions in finitedimensional spaces by approximating  [2.x.16] ,where  [2.x.17]  is a vector of expansion coefficients. We thenimmediately arrive at the following equation that discretizes thecontinuous eigenvalue problem: [1.x.31] Inmatrix and vector notation, this equation then reads: [1.x.32] where  [2.x.18]  isthe stiffness matrix arising from the differential operator  [2.x.19] , and [2.x.20]  is the mass matrix. The solution to the eigenvalue problem is aneigenspectrum  [2.x.21] , with associated eigenfunctions [2.x.22] .
* 

* [1.x.33][1.x.34]
* 

* In this program, we use Dirichlet boundary conditions for the wavefunction  [2.x.23] . What this means, from the perspective of a finiteelement code, is that only the interior degrees of freedom are realdegrees of [1.x.35]: the ones on the boundary are not free butare forced to have a zero value, after all. On the other hand, thefinite element method gains much of its power and simplicity fromthe fact that we just do the same thing on every cell, withouthaving to think too much about where a cell is, whether it boundson a less refined cell and consequently has a hanging node, or isadjacent to the boundary. All such checks would make the assemblyof finite element linear systems unbearably difficult to write andeven more so to read.
* Consequently, of course, when you distribute degrees of freedom withyour DoFHandler object, you don't care whether some of the degreesof freedom you enumerate are at a Dirichlet boundary. They all getnumbers. We just have to take care of these degrees of freedom at alater time when we apply boundary values. There are two basic waysof doing this (either using  [2.x.24] [1.x.36] assembling the linear system, or using [2.x.25]  [1.x.37] assembly;see the  [2.x.26]  "constraints module" for more information),but both result in the same: a linear system that has a totalnumber of rows equal to the number of [1.x.38] degrees of freedom,including those that lie on the boundary. However, degrees offreedom that are constrained by Dirichlet conditions are separatedfrom the rest of the linear system by zeroing out the correspondingrow and column, putting a single positive entry on the diagonal,and the corresponding Dirichlet value on the right hand side.
* If you assume for a moment that we had renumbered degrees of freedomin such a way that all of those on the Dirichlet boundary come last,then the linear system we would get when solving a regular PDE witha right hand side would look like this:
* [1.x.39]
* Here, subscripts  [2.x.27]  and  [2.x.28]  correspond to interior and boundarydegrees of freedom, respectively. The interior degrees of freedomsatisfy the linear system  [2.x.29]  which yields the correctsolution in the interior, and boundary values are determined by [2.x.30]  where  [2.x.31]  is a diagonal matrix that resultsfrom the process of eliminating boundary degrees of freedom, and [2.x.32]  is chosen in such a way that  [2.x.33] has the correct boundary values for every boundary degree of freedom [2.x.34] . (For the curious, the entries of thematrix  [2.x.35]  result from adding modified local contributions to theglobal matrix where for the local matrices the diagonal elements, if non-zero,are set to their absolute value; otherwise, they are set to the average ofabsolute values of the diagonal. This process guarantees that the entriesof  [2.x.36]  are positive and of a size comparable to the rest of the diagonalentries, ensuring that the resulting matrix does not incur unreasonablelosses of accuracy due to roundoff involving matrix entries of drasticallydifferent size. The actual values that end up on the diagonal are difficultto predict and you should treat them as arbitrary and unpredictable, butpositive.)
* For "regular" linear systems, this all leads to the correct solution.On the other hand, for eigenvalue problems, this is not so trivial.There, eliminating boundary values affects both matrices [2.x.37]  and  [2.x.38]  that we will solve with in the current tutorial program.After elimination of boundary values, we then receive an eigenvalueproblem that can be partitioned like this:
* [1.x.40]
* This form makes it clear that there are two sets of eigenvalues:the ones we care about, and spurious eigenvalues from theseparated problem[1.x.41]These eigenvalues are spurious since they result from an eigenvaluesystem that operates only on boundary nodes
* 
*  -  nodes that are notreal degrees of [1.x.42].Of course, since the two matrices  [2.x.39]  are diagonal, we canexactly quantify these spurious eigenvalues: they are [2.x.40]  (where the indices [2.x.41]  corresponds exactly to the degrees of freedom that are constrainedby Dirichlet boundary values).
* So how does one deal with them? The fist part is to recognize when oureigenvalue solver finds one of them. To this end, the program computesand prints an interval within which these eigenvalues lie, by computingthe minimum and maximum of the expression  [2.x.42] over all constrained degrees of freedom. In the program below, thisalready suffices: we find that this interval lies outside the set ofsmallest eigenvalues and corresponding eigenfunctions we are interestedin and compute, so there is nothing we need to do here.
* On the other hand, it may happen that we find that one of the eigenvalueswe compute in this program happens to be in this interval, and in thatcase we would not know immediately whether it is a spurious or a trueeigenvalue. In that case, one could simply scale the diagonal elements ofeither matrix after computing the two matrices,thus shifting them away from the frequency of interest in the eigen-spectrum.This can be done by using the following code, making sure that all spuriouseigenvalues are exactly equal to  [2.x.43] :
* [1.x.43]
* However, this strategy is not pursued here as the spurious eigenvalueswe get from our program as-is happen to be greater than the lowestfive that we will calculate and are interested in.
* 

* [1.x.44][1.x.45]
* 

* The program below is essentially just a slightly modified version of [2.x.44] . The things that are different are the following:
*  [2.x.45] 
*  [2.x.46] The main class (named  [2.x.47] ) now nolonger has a single solution vector, but a whole set of vectors forthe various eigenfunctions we want to compute. Moreover, the [2.x.48]  function, which has the top-level control overeverything here, initializes and finalizes the interface to SLEPc andPETSc simultaneously via  [2.x.49]  and [2.x.50] . [2.x.51] 
*  [2.x.52] We use PETSc matrices and vectors as in  [2.x.53]  and [2.x.54]  since that is what the SLEPc eigenvalue solversrequire. [2.x.55] 
*  [2.x.56] The function  [2.x.57]  is entirelydifferent from anything seen so far in the tutorial, as it does notjust solve a linear system but actually solves the eigenvalue problem.It is built on the SLEPc library, and more immediately on the deal.IISLEPc wrappers in the class  [2.x.58] 
*  [2.x.59] We use the ParameterHandler class to describe a few inputparameters, such as the exact form of the potential  [2.x.60] , the number of global refinement steps of the mesh,or the number of eigenvalues we want to solve for. We could go muchfurther with this but stop at making only a few of the things that onecould select at run time actual input file parameters. In order to seewhat could be done in this regard, take a look at  [2.x.61] " [2.x.62] " and  [2.x.63] . [2.x.64] 
*  [2.x.65] We use the FunctionParser class to make the potential  [2.x.66]  a run-time parameter that can be specified in the input file as aformula. [2.x.67] 
*  [2.x.68] 
* The rest of the program follows in a pretty straightforward way from [2.x.69] .
* 

*  [1.x.46] [1.x.47]
*   [1.x.48]  [1.x.49]
* 

* 
*  As mentioned in the introduction, this program is essentially only a slightly revised version of  [2.x.70] . As a consequence, most of the following include files are as used there, or at least as used already in previous tutorial programs:
* 

* 
* [1.x.50]
* 
*  IndexSet is used to set the size of each  [2.x.71] 
* 

* 
* [1.x.51]
* 
*  PETSc appears here because SLEPc depends on this library:
* 

* 
* [1.x.52]
* 
*  And then we need to actually import the interfaces for solvers that SLEPc provides:
* 

* 
* [1.x.53]
* 
*  We also need some standard C++:
* 

* 
* [1.x.54]
* 
*  Finally, as in previous programs, we import all the deal.II class and function names into the namespace into which everything in this program will go:
* 

* 
* [1.x.55]
* 
*   [1.x.56]  [1.x.57]
* 

* 
*  Following is the class declaration for the main class template. It looks pretty much exactly like what has already been shown in  [2.x.72] :
* 

* 
* [1.x.58]
* 
*  With these exceptions: For our eigenvalue problem, we need both a stiffness matrix for the left hand side as well as a mass matrix for the right hand side. We also need not just one solution function, but a whole set of these for the eigenfunctions we want to compute, along with the corresponding eigenvalues:
* 

* 
* [1.x.59]
* 
*  And then we need an object that will store several run-time parameters that we will specify in an input file:
* 

* 
* [1.x.60]
* 
*  Finally, we will have an object that contains "constraints" on our degrees of freedom. This could include hanging node constraints if we had adaptively refined meshes (which we don't have in the current program). Here, we will store the constraints for boundary nodes  [2.x.73] .
* 

* 
* [1.x.61]
* 
*   [1.x.62]  [1.x.63]
* 

* 
*   [1.x.64]  [1.x.65]
* 

* 
*  First up, the constructor. The main new part is handling the run-time input parameters. We need to declare their existence first, and then read their values from the input file whose name is specified as an argument to this function:
* 

* 
* [1.x.66]
* 
*  TODO investigate why the minimum number of refinement steps required to obtain the correct eigenvalue degeneracies is 6
* 

* 
* [1.x.67]
* 
*   [1.x.68]  [1.x.69]
* 

* 
*  The next function creates a mesh on the domain  [2.x.74] , refines it as many times as the input file calls for, and then attaches a DoFHandler to it and initializes the matrices and vectors to their correct sizes. We also build the constraints that correspond to the boundary values  [2.x.75] .   
*   For the matrices, we use the PETSc wrappers. These have the ability to allocate memory as necessary as non-zero entries are added. This seems inefficient: we could as well first compute the sparsity pattern, initialize the matrices with it, and as we then insert entries we can be sure that we do not need to re-allocate memory and free the one used previously. One way to do that would be to use code like this:  [2.x.76]  instead of the two  [2.x.77]  calls for the stiffness and mass matrices below.   
*   This doesn't quite work, unfortunately. The code above may lead to a few entries in the non-zero pattern to which we only ever write zero entries; most notably, this holds true for off-diagonal entries for those rows and columns that belong to boundary nodes. This shouldn't be a problem, but for whatever reason, PETSc's ILU preconditioner, which we use to solve linear systems in the eigenvalue solver, doesn't like these extra entries and aborts with an error message.   
*   In the absence of any obvious way to avoid this, we simply settle for the second best option, which is have PETSc allocate memory as necessary. That said, since this is not a time critical part, this whole affair is of no further importance.
* 

* 
* [1.x.71]
* 
*  The next step is to take care of the eigenspectrum. In this case, the outputs are eigenvalues and eigenfunctions, so we set the size of the list of eigenfunctions and eigenvalues to be as large as we asked for in the input file. When using a  [2.x.78]  the Vector is initialized using an IndexSet. IndexSet is used not only to resize the  [2.x.79]  but it also associates an index in the  [2.x.80]  with a degree of freedom (see  [2.x.81]  for a more detailed explanation). The function complete_index_set() creates an IndexSet where every valid index is part of the set. Note that this program can only be run sequentially and will throw an exception if used in parallel.
* 

* 
* [1.x.72]
* 
*   [1.x.73]  [1.x.74]
* 

* 
*  Here, we assemble the global stiffness and mass matrices from local contributions  [2.x.82]  and  [2.x.83]  respectively. This function should be immediately familiar if you've seen previous tutorial programs. The only thing new would be setting up an object that described the potential  [2.x.84]  using the expression that we got from the input file. We then need to evaluate this object at the quadrature points on each cell. If you've seen how to evaluate function objects (see, for example the coefficient in  [2.x.85] ), the code here will also look rather familiar.
* 

* 
* [1.x.75]
* 
*  Now that we have the local matrix contributions, we transfer them into the global objects and take care of zero boundary constraints:
* 

* 
* [1.x.76]
* 
*  At the end of the function, we tell PETSc that the matrices have now been fully assembled and that the sparse matrix representation can now be compressed as no more entries will be added:
* 

* 
* [1.x.77]
* 
*  Before leaving the function, we calculate spurious eigenvalues, introduced to the system by zero Dirichlet constraints. As discussed in the introduction, the use of Dirichlet boundary conditions coupled with the fact that the degrees of freedom located at the boundary of the domain remain part of the linear system we solve, introduces a number of spurious eigenvalues. Below, we output the interval within which they all lie to ensure that we can ignore them should they show up in our computations.
* 

* 
* [1.x.78]
* 
*   [1.x.79]  [1.x.80]
* 

* 
*  This is the key new functionality of the program. Now that the system is set up, here is a good time to actually solve the problem: As with other examples this is done using a "solve" routine. Essentially, it works as in other programs: you set up a SolverControl object that describes the accuracy to which we want to solve the linear systems, and then we select the kind of solver we want. Here we choose the Krylov-Schur solver of SLEPc, a pretty fast and robust choice for this kind of problem:
* 

* 
* [1.x.81]
* 
*  We start here, as we normally do, by assigning convergence control we want:
* 

* 
* [1.x.82]
* 
*  Before we actually solve for the eigenfunctions and
* 
*  - alues, we have to also select which set of eigenvalues to solve for. Lets select those eigenvalues and corresponding eigenfunctions with the smallest real part (in fact, the problem we solve here is symmetric and so the eigenvalues are purely real). After that, we can actually let SLEPc do its work:
* 

* 
* [1.x.83]
* 
*  The output of the call above is a set of vectors and values. In eigenvalue problems, the eigenfunctions are only determined up to a constant that can be fixed pretty arbitrarily. Knowing nothing about the origin of the eigenvalue problem, SLEPc has no other choice than to normalize the eigenvectors to one in the  [2.x.86]  (vector) norm. Unfortunately this norm has little to do with any norm we may be interested from a eigenfunction perspective: the  [2.x.87]  norm, or maybe the  [2.x.88]  norm.     
*   Let us choose the latter and rescale eigenfunctions so that they have  [2.x.89]  instead of  [2.x.90]  (where  [2.x.91]  is the  [2.x.92] th eigen[1.x.84] and  [2.x.93]  the corresponding vector of nodal values). For the  [2.x.94]  elements chosen here, we know that the maximum of the function  [2.x.95]  is attained at one of the nodes, so  [2.x.96] , making the normalization in the  [2.x.97]  norm trivial. Note that this doesn't work as easily if we had chosen  [2.x.98]  elements with  [2.x.99] : there, the maximum of a function does not necessarily have to be attained at a node, and so  [2.x.100]  (although the equality is usually nearly true).
* 

* 
* [1.x.85]
* 
*  Finally return the number of iterations it took to converge:
* 

* 
* [1.x.86]
* 
*   [1.x.87]  [1.x.88]
* 

* 
*  This is the last significant function of this program. It uses the DataOut class to generate graphical output from the eigenfunctions for later visualization. It works as in many of the other tutorial programs.   
*   The whole collection of functions is then output as a single VTK file.
* 

* 
* [1.x.89]
* 
*  The only thing worth discussing may be that because the potential is specified as a function expression in the input file, it would be nice to also have it as a graphical representation along with the eigenfunctions. The process to achieve this is relatively straightforward: we build an object that represents  [2.x.101]  and then we interpolate this continuous function onto the finite element space. The result we also attach to the DataOut object for visualization.
* 

* 
* [1.x.90]
* 
*   [1.x.91]  [1.x.92]
* 

* 
*  This is the function which has the top-level control over everything. It is almost exactly the same as in  [2.x.102] :
* 

* 
* [1.x.93]
* 
*   [1.x.94]  [1.x.95]
* 

* 
* [1.x.96]
* 
*  This program can only be run in serial. Otherwise, throw an exception.
* 

* 
* [1.x.97]
* 
*  All the while, we are watching out if any exceptions should have been generated. If that is so, we panic...
* 

* 
* [1.x.98]
* 
*  If no exceptions are thrown, then we tell the program to stop monkeying around and exit nicely:
* 

* 
* [1.x.99]
* [1.x.100][1.x.101]
* 

* [1.x.102][1.x.103]
* 

* The problem's input is parameterized by an input file  [2.x.103] which could, for example, contain the following text:
* [1.x.104]
* 
* Here, the potential is zero inside the domain, and we know that theeigenvalues are given by  [2.x.104]  where [2.x.105] . Eigenfunctions are sines and cosines with  [2.x.106]  and  [2.x.107] periods in  [2.x.108]  and  [2.x.109]  directions. This matches the output our programgenerates:
* [1.x.105] These eigenvalues are exactly the ones thatcorrespond to pairs  [2.x.110] ,  [2.x.111]  and  [2.x.112] ,  [2.x.113] , and [2.x.114] . A visualization of the corresponding eigenfunctions wouldlook like this:
*  [2.x.115] 
* [1.x.106][1.x.107]
* 

* It is always worth playing a few games in the playground! So here goeswith a few suggestions:
*  [2.x.116] 
*  [2.x.117]  The potential used above (called the [1.x.108] becauseit is a flat potential surrounded by infinitely high walls) isinteresting because it allows for analytically known solutions. Apartfrom that, it is rather boring, however. That said, it is trivial toplay around with the potential by just setting it to somethingdifferent in the input file. For example, let us assume that we wantedto work with the following potential in2d:[1.x.109]In other words, the potential is
* 
*  - 00 in two sectors of a circle of radius0.75,
* 
*  -  in the other two sectors, and zero outside the circle. We can achievethis by using the following in the input file:
* [1.x.110]
* If in addition we also increase the mesh refinement by one level, we get thefollowing results:
* [1.x.111]
* 
* The output file also contains an interpolated version of the potential, whichlooks like this (note that as expected the lowest few eigenmodes haveprobability densities  [2.x.118]  that are significant only where thepotential is the lowest, i.e. in the top right and bottom left sector of innercircle of the potential):
*  [2.x.119] 
* The first five eigenfunctions are now like this:
*  [2.x.120] 
*  [2.x.121]  In our derivation of the problem we have assumed that theparticle is confined to a domain  [2.x.122]  and that at the boundary ofthis domain its probability  [2.x.123]  of being is zero. This isequivalent to solving the eigenvalue problem on all of  [2.x.124] and assuming that the energy potential is finite only inside a region [2.x.125]  and infinite outside. It is relatively easy to show that [2.x.126]  at all locations  [2.x.127]  where  [2.x.128] . So the question is what happens if our potential is not ofthis form, i.e. there is no bounded domain outside of which thepotential is infinite? In that case, it may be worth to just considera very large domain at the boundary of which  [2.x.129]  is atleast very large, if not infinite. Play around with a few cases likethis and explore how the spectrum and eigenfunctions change as we makethe computational region larger and larger.
*  [2.x.130]  What happens if we investigate the simple harmonic oscillatorproblem  [2.x.131] ? This potential is exactly ofthe form discussed in the previous paragraph and has hyper sphericalsymmetry. One may want to use a large spherical domain with a largeouter radius, to approximate the whole-space problem (say, by invoking [2.x.132] 
*  [2.x.133]  The plots above show the wave function  [2.x.134] , but thephysical quantity of interest is actually the probability density [2.x.135]  for the particle to be at location  [2.x.136] .Some visualization programs can compute derived quantities from the data inan input file, but we can also do so right away when creating the outputfile. The facility to do that is the DataPostprocessor class that canbe used in conjunction with the DataOut class. Examples of how thiscan be done can be found in  [2.x.137]  and [2.x.138] .
*  [2.x.139]  What happens if the particle in the box has %internal degrees offreedom? For example, if the particle were a spin- [2.x.140]  particle? Inthat case, we may want to start solving a vector-valued probleminstead.
*  [2.x.141]  Our implementation of the deal.II library here uses thePETScWrappers and SLEPcWrappers and is suitable for running on serialmachine architecture. However, for larger grids and with a largernumber of degrees-of-freedom, we may want to run our application onparallel architectures. A parallel implementation of the above codecan be particularly useful here since the generalized eigenspectrumproblem is somewhat more expensive to solve than the standard problemsconsidered in most of the earlier tutorials. Fortunately, modifying the aboveprogram to be MPI compliant is a relatively straightforwardprocedure. A sketch of how this can be done can be found in  [2.x.142]  " [2.x.143] ".
*  [2.x.144]  Finally, there are alternatives to using the SLEPc eigenvaluesolvers. deal.II has interfaces to one of them, ARPACK (see [1.x.112] forsetup instructions), implemented in the ArpackSolver class. Here is a short andquick overview of what one would need to change to use it, provided you have aworking installation of ARPACK and deal.II has been configured properly for it(see the deal.II [1.x.113] file):
* First, in order to use the ARPACK interfaces, we can go back to using standarddeal.II matrices and vectors, so we start by replacing the PETSc and SLEPcheaders
* [1.x.114]
* with these:
* [1.x.115]
* ARPACK allows complex eigenvalues, so we will also need
* [1.x.116]
* 
* Secondly, we switch back to the deal.II matrix and vector definitions in themain class:
* [1.x.117]
* and initialize them as usual in  [2.x.145] :
* [1.x.118]
* 
* For solving the eigenvalue problem with ARPACK, we finally need to modify [2.x.146] :
* [1.x.119]
* Note how we have used an exact decomposition (using SparseDirectUMFPACK) as apreconditioner to ARPACK. [2.x.147] 
* 

* [1.x.120][1.x.121] [2.x.148] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-37_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] ,  [2.x.2] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31]
*  [2.x.3] 
* [1.x.32][1.x.33]
* [1.x.34][1.x.35][1.x.36]
* 

* This example shows how to implement a matrix-free method, that is, a methodthat does not explicitly store the matrix elements, for a second-order Poissonequation with variable coefficients on a hypercube. The linear system will besolved with a multigrid method and uses large-scale parallelism with MPI.
* The major motivation for matrix-free methods is the fact that on today'sprocessors access to main memory (i.e., for objects that do not fit in thecaches) has become the bottleneck in many solvers for partial differential equations: To perform amatrix-vector product based on matrices, modern CPUs spend far more timewaiting for data to arrive from memory than on actually doing the floatingpoint multiplications and additions. Thus, if we could substitute looking upmatrix elements in memory by re-computing them &mdash; or rather, the operatorrepresented by these entries &mdash;, we may win in terms of overall run-timeeven if this requires a significant number of additional floating pointoperations. That said, to realize this with a trivial implementation is notenough and one needs to really look at the details to gain inperformance. This tutorial program and the papers referenced above show howone can implement such a scheme and demonstrates the speedup that can beobtained.
* 

* [1.x.37][1.x.38]
* 

* In this example, we consider the Poisson problem [1.x.39] where  [2.x.4]  is a variable coefficient.Below, we explain how to implement a matrix-vector product for thisproblem without explicitly forming the matrix. The construction can,of course, be done in a similar way for other equations as well.
* We choose as domain  [2.x.5]  and  [2.x.6] . Since the coefficient is symmetric around theorigin but the domain is not, we will end up with a non-symmetricsolution.
* 

* [1.x.40][1.x.41]
* 

* In order to find out how we can write a code that performs a matrix-vectorproduct, but does not need to store the matrix elements, let us start atlooking how a finite element matrix [1.x.42] is assembled:[1.x.43]
* In this formula, the matrix [1.x.44]<sub>cell,loc-glob</sub> is a rectangularmatrix that defines the index mapping from local degrees of freedom in thecurrent cell to the global degrees of freedom. The information from which thisoperator can be built is usually encoded in the  [2.x.7] variable and is used in the assembly calls filling matrices in deal.II. Here,[1.x.45]<sub>cell</sub> denotes the cell matrix associated with [1.x.46].
* If we are to perform a matrix-vector product, we can hence use that[1.x.47]
* where [1.x.48]<sub>cell</sub> are the values of [1.x.49] at the degrees of freedomof the respective cell, and[1.x.50]<sub>cell</sub>=[1.x.51]<sub>cell</sub>[1.x.52]<sub>cell</sub>correspondingly for the result.A naive attempt to implement the local action of the Laplacian would hence beto use the following code:
* [1.x.53]
* 
* Here we neglected boundary conditions as well as any hanging nodes we mayhave, though neither would be very difficult to include using theAffineConstraints class. Note how we first generate the local matrix in theusual way as a sum over all quadrature points for each local matrix entry.To form the actual product as expressed in the above formula, weextract the values of  [2.x.8]  of the cell-related degrees of freedom(the action of [1.x.54]<sub>cell,loc-glob</sub>), multiply by the local matrix(the action of [1.x.55]<sub>cell</sub>), and finally add the result to thedestination vector  [2.x.9]  (the action of[1.x.56]<sub>cell,loc-glob</sub><sup>T</sup>, added over all the elements). Itis not more difficult than that, in principle.
* While this code is completely correct, it is very slow. For every cell, wegenerate a local matrix, which takes three nested loops with loop length equalto the number of local degrees of freedom to compute. Themultiplication itself is then done by two nested loops, which means that itis much cheaper.
* One way to improve this is to realize that conceptually the localmatrix can be thought of as the product of three matrices,[1.x.57]
* where for the example of the Laplace operator the ([1.x.58]*dim+[1.x.59])-thelement of [1.x.60]<sub>cell</sub> is given by [2.x.10] . This matrix consists of [2.x.11]  rows and  [2.x.12]  columns. The matrix[1.x.61]<sub>cell</sub> is diagonal and contains the values [2.x.13]  (or, rather,  [2.x.14] dim copies of each of these values). This kind of representation offinite element matrices can often be found in the engineering literature.
* When the cell matrix is applied to a vector,[1.x.62]
* one would then not form the matrix-matrix products, but rather multiply onematrix at a time with a vector from right to left so that only threesuccessive matrix-vector products are formed. This approach removes the threenested loops in the calculation of the local matrix, which reduces thecomplexity of the work on one cell from something like  [2.x.15]  to  [2.x.16] . An interpretation of this algorithm is thatwe first transform the vector of values on the local DoFs to a vector ofgradients on the quadrature points. In the second loop, we multiply thesegradients by the integration weight and the coefficient. The third loop appliesthe second gradient (in transposed form), so that we get back to a vector of(Laplacian) values on the cell dofs.
* The bottleneck in the above code is the operations done by the call to [2.x.17]  for every  [2.x.18] , which take about as much time asthe other steps together (at least if the mesh is unstructured; deal.II canrecognize that the gradients are often unchanged on structured meshes). Thatis certainly not ideal and we would like to do better than this. What thereinit function does is to calculate the gradient in real space bytransforming the gradient on the reference cell using the Jacobian of thetransformation from real to reference cell. This is done for each basisfunction on the cell, for each quadrature point. The Jacobian does not dependon the basis function, but it is different on different quadrature points ingeneral. If you only build the matrix once as we've done in all previoustutorial programs, there is nothing to be optimized since  [2.x.19] needs to be called on every cell. In this process, the transformation isapplied while computing the local matrix elements.
* In a matrix-free implementation, however, we will compute those integrals veryoften because iterative solvers will apply the matrix many times during thesolution process. Therefore, we need to think about whether we may be able tocache some data that gets reused in the operator applications, i.e., integralcomputations. On the other hand, we realize that we must not cache too muchdata since otherwise we get back to the situation where memory access becomesthe dominating factor. Therefore, we will not store the transformed gradientsin the matrix [1.x.63], as they would in general be different for each basisfunction and each quadrature point on every element for curved meshes.
* The trick is to factor out the Jacobian transformation and first apply thegradient on the reference cell only. This operation interpolates the vector ofvalues on the local dofs to a vector of (unit-coordinate) gradients on thequadrature points. There, we first apply the Jacobian that we factored outfrom the gradient, then apply the weights of the quadrature, and finally applythe transposed Jacobian for preparing the third loop which tests by thegradients on the unit cell and sums over quadrature points.
* Let us again write this in terms of matrices. Let the matrix[1.x.64]<sub>cell</sub> denote the cell-related gradient matrix, with each rowcontaining the values on the quadrature points. It is constructed by amatrix-matrix product as [1.x.65] where[1.x.66]<sub>ref_cell</sub> denotes the gradient on the reference cell and[1.x.67]<sup>-T</sup><sub>cell</sub> denotes the inverse transpose Jacobian ofthe transformation from unit to real cell (in the language of transformations,the operation represented by [1.x.68]<sup>-T</sup><sub>cell</sub> represents acovariant transformation). [1.x.69]<sup>-T</sup><sub>cell</sub> isblock-diagonal, and the blocks size is equal to the dimension of theproblem. Each diagonal block is the Jacobian transformation that goes from thereference cell to the real cell.
* Putting things together, we find that[1.x.70]
* so we calculate the product (starting the local product from the right)[1.x.71]
* 
* [1.x.72]
* 
* Note how we create an additional FEValues object for the reference cellgradients and how we initialize it to the reference cell. The actualderivative data is then applied by the inverse, transposed Jacobians (deal.IIcalls the Jacobian matrix from real to unit cell inverse_jacobian, as theforward transformation is from unit to real cell). The factor [2.x.20]  isblock-diagonal over quadrature. In this form, one realizes that variablecoefficients (possibly expressed through a tensor) and general grid topologieswith Jacobian transformations have a similar effect on the coefficienttransforming the unit-cell derivatives.
* At this point, one might wonder why we store the matrix [2.x.21]  and the coefficient separately, rather thanonly the complete factor  [2.x.22] . The latter would use less memory because thetensor is symmetric with six independent values in 3D, whereas for the formerwe would need nine entries for the inverse transposed Jacobian, one for thequadrature weight and Jacobian determinant, and one for the coefficient,totaling to 11 doubles. The reason is that the former approach allows forimplementing generic differential operators through a common framework ofcached data, whereas the latter specifically stores the coefficient for theLaplacian. In case applications demand for it, this specialization could payoff and would be worthwhile to consider. Note that the implementation indeal.II is smart enough to detect Cartesian or affine geometries where theJacobian is constant throughout the cell and needs not be stored for everycell (and indeed often is the same over different cells as well).
* The final optimization that is most crucial from an operation count point ofview is to make use of the tensor product structure in the basisfunctions. This is possible because we have factored out the gradient from thereference cell operation described by [1.x.73]<sub>ref_cell</sub>, i.e., aninterpolation operation over the completely regular data fields of thereference cell. We illustrate the process of complexity reduction in two spacedimensions, but the same technique can be used in higher dimensions. On thereference cell, the basis functions are of the tensor product form [2.x.23] . The part of the matrix[1.x.74]<sub>ref_cell</sub> that computes the first component has the form [2.x.24] , where[1.x.75]<sub>grad,x</sub> and [1.x.76]<sub>val,y</sub> contain the evaluationof all the 1D basis functions on all the 1D quadrature points. Forming amatrix [1.x.77] with [1.x.78] containing the coefficient belonging tobasis function  [2.x.25] , we get  [2.x.26] . Thisreduces the complexity for computing this product from  [2.x.27]  to  [2.x.28] , where[1.x.79]-1 is the degree of the finite element (i.e., equivalently, [1.x.80]is the number of shape functions in each coordinate direction), or  [2.x.29]  to [2.x.30]  in general. The reason why we look at the complexity in terms ofthe polynomial degree is since we want to be able to go to high degrees andpossibly increase the polynomial degree [1.x.81] instead of the gridresolution. Good algorithms for moderate degrees like the ones used here arelinear in the polynomial degree independent on the dimension, as opposed tomatrix-based schemes or naive evaluation through FEValues. The techniques usedin the implementations of deal.II have been established in the spectralelement community since the 1980s.
* Implementing a matrix-free and cell-based finite element operator requires asomewhat different program design as compared to the usual matrix assemblycodes shown in previous tutorial programs. The data structures for doing thisare the MatrixFree class that collects all data and issues a (parallel) loopover all cells and the FEEvaluation class that evaluates finite element basisfunctions by making use of the tensor product structure.
* The implementation of the matrix-free matrix-vector product shown in thistutorial is slower than a matrix-vector product using a sparse matrix forlinear elements, but faster for all higher order elements thanks to thereduced complexity due to the tensor product structure and due to less memorytransfer during computations. The impact of reduced memory transfer isparticularly beneficial when working on a multicore processor where severalprocessing units share access to memory. In that case, an algorithm which iscomputation bound will show almost perfect parallel speedup (apart frompossible changes of the processor's clock frequency through turbo modesdepending on how many cores are active), whereas an algorithm that is bound bymemory transfer might not achieve similar speedup (even when the work isperfectly parallel and one could expect perfect scaling like in sparsematrix-vector products). An additional gain with this implementation is thatwe do not have to build the sparse matrix itself, which can also be quiteexpensive depending on the underlying differential equation. Moreover, theabove framework is simple to generalize to nonlinear operations, as wedemonstrate in  [2.x.31] .
* 

* [1.x.82][1.x.83]
* 

* Above, we have gone to significant lengths to implement a matrix-vectorproduct that does not actually store the matrix elements. In many user codes,however, one wants more than just doing a few matrix-vector products &mdash;one wants to do as few of these operations as possible when solving linearsystems. In theory, we could use the CG method without preconditioning;however, that would not be very efficient for the Laplacian. Rather,preconditioners are used for increasing the speed ofconvergence. Unfortunately, most of the more frequently used preconditionerssuch as SSOR, ILU or algebraic multigrid (AMG) cannot be used here becausetheir implementation requires knowledge of the elements of the system matrix.
* One solution is to use geometric multigrid methods as shown in  [2.x.32] . Theyare known to be very fast, and they are suitable for our purpose since allingredients, including the transfer between different grid levels, can beexpressed in terms of matrix-vector products related to a collection ofcells. All one needs to do is to find a smoother that is based onmatrix-vector products rather than all the matrix entries. One such candidatewould be a damped Jacobi iteration that requires access to the matrixdiagonal, but it is often not sufficiently good in damping all high-frequencyerrors. The properties of the Jacobi method can be improved by iterating it afew times with the so-called Chebyshev iteration. The Chebyshev iteration isdescribed by a polynomial expression of the matrix-vector product where thecoefficients can be chosen to achieve certain properties, in this case tosmooth the high-frequency components of the error which are associated to theeigenvalues of the Jacobi-preconditioned matrix. At degree zero, the Jacobimethod with optimal damping parameter is retrieved, whereas higher ordercorrections are used to improve the smoothing properties. The effectiveness ofChebyshev smoothing in multigrid has been demonstrated, e.g., in the article[1.x.84][1.x.85]. This publication also identifies one more advantage ofChebyshev smoothers that we exploit here, namely that they are easy toparallelize, whereas SOR/Gauss&ndash;Seidel smoothing relies on substitutions,for which a naive parallelization works on diagonal sub-blocks of the matrix,thereby decreases efficiency (for more detail see e.g. Y. Saad, IterativeMethods for Sparse Linear Systems, SIAM, 2nd edition, 2003, chapters 11 & 12).
* The implementation into the multigrid framework is then straightforward. Themultigrid implementation in this program is similar to  [2.x.33]  and includesadaptivity.
* 

* [1.x.86][1.x.87]
* 

* The computational kernels for evaluation in FEEvaluation are written in a wayto optimally use computational resources. To achieve this, they do not operateon double data types, but something we call VectorizedArray (check e.g. thereturn type of  [2.x.34]  which is VectorizedArray for ascalar element and a Tensor of VectorizedArray for a vector finiteelement). VectorizedArray is a short array of doubles or float whose lengthdepends on the particular computer system in use. For example, systems basedon x86-64 support the streaming SIMD extensions (SSE), where the processor'svector units can process two doubles (or four single-precision floats) by oneCPU instruction. Newer processors (from about 2012 and onwards) support theso-called advanced vector extensions (AVX) with 256 bit operands, which canuse four doubles and eight floats, respectively. Vectorization is asingle-instruction/multiple-data (SIMD) concept, that is, one CPU instructionis used to process multiple data values at once. Often, finite elementprograms do not use vectorization explicitly as the benefits of this conceptare only in arithmetic intensive operations. The bulk of typical finiteelement workloads are memory bandwidth limited (operations on sparse matricesand vectors) where the additional computational power is useless.
* Behind the scenes, optimized BLAS packages might heavily rely onvectorization, though. Also, optimizing compilers might automaticallytransform loops involving standard code into more efficient vectorized form(deal.II uses OpenMP SIMD pragmas inside the regular loops of vectorupdates). However, the data flow must be very regular in order for compilersto produce efficient code. For example, already the automatic vectorization ofthe prototype operation that benefits from vectorization, matrix-matrixproducts, fails on most compilers (as of writing this tutorial in early 2012and updating in late 2016, neither gcc nor the Intel compiler manage toproduce useful vectorized code for the  [2.x.35]  function, and noteven on the simpler case where the matrix bounds are compile-time constantsinstead of run-time constants as in  [2.x.36]  The main reason forthis is that the information to be processed at the innermost loop (that iswhere vectorization is applied) is not necessarily a multiple of the vectorlength, leaving parts of the resources unused. Moreover, the data that canpotentially be processed together might not be laid out in a contiguous way inmemory or not with the necessary alignment to address boundaries that areneeded by the processor. Or the compiler might not be able to prove that dataarrays do not overlap when loading several elements at once.
* In the matrix-free implementation in deal.II, we have therefore chosen toapply vectorization at the level which is most appropriate for finite elementcomputations: The cell-wise computations are typically exactly the same forall cells (except for indices in the indirect addressing used while readingfrom and writing to vectors), and hence SIMD can be used to process severalcells at once. In all what follows, you can think of a VectorizedArray to holddata from several cells. Remember that it is not related to the spatialdimension and the number of elements e.g. in a Tensor or Point.
* Note that vectorization depends on the CPU the code is running on and forwhich the code is compiled. In order to generate the fastest kernels ofFEEvaluation for your computer, you should compile deal.II with the so-called[1.x.88] processor variant. When using the gcc compiler, it can beenabled by setting the variable <tt>CMAKE_CXX_FLAGS</tt> to<tt>"-march=native"</tt> in the cmake build settings (on the command line,specify <tt>-DCMAKE_CXX_FLAGS="-march=native"</tt>, see the deal.II README formore information). Similar options exist for other compilers. We outputthe current vectorization length in the run() function of this example.
* 

* [1.x.89][1.x.90]
* 

* As mentioned above, all components in the matrix-free framework can easily beparallelized with MPI using domain decomposition. Thanks to the easy access tolarge-scale parallel meshes through p4est (see  [2.x.37]  for details) indeal.II, and the fact that cell-based loops with matrix-free evaluation[1.x.91] need a decomposition of the mesh into chunks of roughly equal sizeon each processor, there is relatively little to do to write a parallelprogram working with distributed memory. While other tutorial programs usingMPI have relied on either PETSc or Trilinos, this program uses deal.II's ownparallel vector facilities.
* The deal.II parallel vector class,  [2.x.38]  holdsthe processor-local part of the solution as well as data fields for ghostedDoFs, i.e. DoFs that are owned by a remote processor but accessed by cellsthat are owned by the present processor. In the  [2.x.39] "glossary" these degrees of freedom are referred to as locally active degreesof freedom. The function  [2.x.40]  provides a methodthat sets this design. Note that hanging nodes can relate to additionalghosted degrees of freedom that must be included in the distributed vector butare not part of the locally active DoFs in the sense of the  [2.x.41]  "glossary". Moreover, the distributed vector holds theMPI metadata for DoFs that are owned locally but needed by otherprocessors. A benefit of the design of this vector class is the way ghostedentries are accessed. In the storage scheme of the vector, the data arrayextends beyond the processor-local part of the solution with further vectorentries available for the ghosted degrees of freedom. This gives a contiguousindex range for all locally active degrees of freedom. (Note that the indexrange depends on the exact configuration of the mesh.) Since matrix-freeoperations can be thought of doing linear algebra that is performancecritical, and performance-critical code cannot waste time on doing MPI-globalto MPI-local index translations, the availability of an index spaces local toone MPI rank is fundamental. The way things are accessed here is a directarray access. This is provided through [2.x.42]  but it is actually rarelyneeded because all of this happens internally in FEEvaluation.
* The design of  [2.x.43]  is similar to the [2.x.44]  and  [2.x.45]  data types wehave used in  [2.x.46]  and  [2.x.47]  before, but since we do not need any otherparallel functionality of these libraries, we use the [2.x.48]  class of deal.II instead of linking in anotherlarge library in this tutorial program. Also note that the PETSc and Trilinosvectors do not provide the fine-grained control over ghost entries with directarray access because they abstract away the necessary implementation details.
* 

*  [1.x.92] [1.x.93]
*  First include the necessary files from the deal.II library.
* 

* 
* [1.x.94]
* 
*  This includes the data structures for the efficient implementation of matrix-free methods or more generic finite element operators with the class MatrixFree.
* 

* 
* [1.x.95]
* 
*  To be efficient, the operations performed in the matrix-free implementation require knowledge of loop lengths at compile time, which are given by the degree of the finite element. Hence, we collect the values of the two template parameters that can be changed at one place in the code. Of course, one could make the degree of the finite element a run-time parameter by compiling the computational kernels for all degrees that are likely (say, between 1 and 6) and selecting the appropriate kernel at run time. Here, we simply choose second order  [2.x.49]  elements and choose dimension 3 as standard.
* 

* 
* [1.x.96]
* 
*   [1.x.97]  [1.x.98]
* 

* 
*  We define a variable coefficient function for the Poisson problem. It is similar to the function in  [2.x.50]  but we use the form  [2.x.51]  instead of a discontinuous one. It is merely to demonstrate the possibilities of this implementation, rather than making much sense physically. We define the coefficient in the same way as functions in earlier tutorial programs. There is one new function, namely a  [2.x.52]  method with template argument  [2.x.53] 
* 

* 
* [1.x.99]
* 
*  This is the new function mentioned above: Evaluate the coefficient for abstract type  [2.x.54]  It might be just a usual double, but it can also be a somewhat more complicated type that we call VectorizedArray. This data type is essentially a short array of doubles as discussed in the introduction that holds data from several cells. For example, we evaluate the coefficient shown here not on a simple point as usually done, but we hand it a Point<dim,VectorizedArray<double> > point, which is actually a collection of four points in the case of AVX. Do not confuse the entries in VectorizedArray with the different coordinates of the point. Indeed, the data is laid out such that  [2.x.55]  returns a VectorizedArray, which in turn contains the x-coordinate for the first point and the second point. You may access the coordinates individually using e.g.  [2.x.56] , j=0,1,2,3, but it is recommended to define operations on a VectorizedArray as much as possible in order to make use of vectorized operations.   
*   In the function implementation, we assume that the number type overloads basic arithmetic operations, so we just write the code as usual. The base class function  [2.x.57]  is then computed from the templated function with double type, in order to avoid duplicating code.
* 

* 
* [1.x.100]
* 
*   [1.x.101]  [1.x.102]
* 

* 
*  The following class, called  [2.x.58] , implements the differential operator. For all practical purposes, it is a matrix, i.e., you can ask it for its size (member functions  [2.x.59] ) and you can apply it to a vector (the  [2.x.60]  function). The difference to a real matrix of course lies in the fact that this class does not actually store the [1.x.103] of the matrix, but only knows how to compute the action of the operator when applied to a vector.   
*   The infrastructure describing the matrix size, the initialization from a MatrixFree object, and the various interfaces to matrix-vector products through vmult() and Tvmult() methods, is provided by the class  [2.x.61]  from which this class derives. The LaplaceOperator class defined here only has to provide a few interfaces, namely the actual action of the operator through the apply_add() method that gets used in the vmult() functions, and a method to compute the diagonal entries of the underlying matrix. We need the diagonal for the definition of the multigrid smoother. Since we consider a problem with variable coefficient, we further implement a method that can fill the coefficient values.   
*   Note that the file  [2.x.62]  already contains an implementation of the Laplacian through the class  [2.x.63]  For educational purposes, the operator is re-implemented in this tutorial program, explaining the ingredients and concepts used there.   
*   This program makes use of the data cache for finite element operator application that is integrated in deal.II. This data cache class is called MatrixFree. It contains mapping information (Jacobians) and index relations between local and global degrees of freedom. It also contains constraints like the ones from hanging nodes or Dirichlet boundary conditions. Moreover, it can issue a loop over all cells in %parallel, making sure that only cells are worked on that do not share any degree of freedom (this makes the loop thread-safe when writing into destination vectors). This is a more advanced strategy compared to the WorkStream class described in the  [2.x.64]  module. Of course, to not destroy thread-safety, we have to be careful when writing into class-global structures.   
*   The class implementing the Laplace operator has three template arguments, one for the dimension (as many deal.II classes carry), one for the degree of the finite element (which we need to enable efficient computations through the FEEvaluation class), and one for the underlying scalar type. We want to use  [2.x.65]  numbers (i.e., double precision, 64-bit floating point) for the final matrix, but floats (single precision, 32-bit floating point numbers) for the multigrid level matrices (as that is only a preconditioner, and floats can be processed twice as fast). The class FEEvaluation also takes a template argument for the number of quadrature points in one dimension. In the code below, we hard-code it to  [2.x.66] . If we wanted to change it independently of the polynomial degree, we would need to add a template parameter as is done in the  [2.x.67]  class.   
*   As a sidenote, if we implemented several different operations on the same grid and degrees of freedom (like a mass matrix and a Laplace matrix), we would define two classes like the current one for each of the operators (derived from the  [2.x.68]  class), and let both of them refer to the same MatrixFree data cache from the general problem class. The interface through  [2.x.69]  requires us to only provide a minimal set of functions. This concept allows for writing complex application codes with many matrix-free operations.   
*  

* 
*  [2.x.70]  Storing values of type  [2.x.71]  requires care: Here, we use the deal.II table class which is prepared to hold the data with correct alignment. However, storing e.g. an  [2.x.72]  is not possible with vectorization: A certain alignment of the data with the memory address boundaries is required (essentially, a VectorizedArray that is 32 bytes long in case of AVX needs to start at a memory address that is divisible by 32). The table class (as well as the AlignedVector class it is based on) makes sure that this alignment is respected, whereas  [2.x.73]  does not in general, which may lead to segmentation faults at strange places for some systems or suboptimal performance for other systems.
* 

* 
* [1.x.104]
* 
*  This is the constructor of the  [2.x.74]  class. All it does is to call the default constructor of the base class  [2.x.75]  which in turn is based on the Subscriptor class that asserts that this class is not accessed after going out of scope e.g. in a preconditioner.
* 

* 
* [1.x.105]
* 
*   [1.x.106]  [1.x.107]
* 

* 
*  To initialize the coefficient, we directly give it the Coefficient class defined above and then select the method  [2.x.76]  with vectorized number (which the compiler can deduce from the point data type). The use of the FEEvaluation class (and its template arguments) will be explained below.
* 

* 
* [1.x.108]
* 
*   [1.x.109]  [1.x.110]
* 

* 
*  Here comes the main function of this class, the evaluation of the matrix-vector product (or, in general, a finite element operator evaluation). This is done in a function that takes exactly four arguments, the MatrixFree object, the destination and source vectors, and a range of cells that are to be worked on. The method  [2.x.77]  in the MatrixFree class will internally call this function with some range of cells that is obtained by checking which cells are possible to work on simultaneously so that write operations do not cause any race condition. Note that the cell range used in the loop is not directly the number of (active) cells in the current mesh, but rather a collection of batches of cells.  In other word, "cell" may be the wrong term to begin with, since FEEvaluation groups data from several cells together. This means that in the loop over quadrature points we are actually seeing a group of quadrature points of several cells as one block. This is done to enable a higher degree of vectorization.  The number of such "cells" or "cell batches" is stored in MatrixFree and can be queried through  [2.x.78]  Compared to the deal.II cell iterators, in this class all cells are laid out in a plain array with no direct knowledge of level or neighborship relations, which makes it possible to index the cells by unsigned integers.   
*   The implementation of the Laplace operator is quite simple: First, we need to create an object FEEvaluation that contains the computational kernels and has data fields to store temporary results (e.g. gradients evaluated on all quadrature points on a collection of a few cells). Note that temporary results do not use a lot of memory, and since we specify template arguments with the element order, the data is stored on the stack (without expensive memory allocation). Usually, one only needs to set two template arguments, the dimension as a first argument and the degree of the finite element as the second argument (this is equal to the number of degrees of freedom per dimension minus one for FE_Q elements). However, here we also want to be able to use float numbers for the multigrid preconditioner, which is the last (fifth) template argument. Therefore, we cannot rely on the default template arguments and must also fill the third and fourth field, consequently. The third argument specifies the number of quadrature points per direction and has a default value equal to the degree of the element plus one. The fourth argument sets the number of components (one can also evaluate vector-valued functions in systems of PDEs, but the default is a scalar element), and finally the last argument sets the number type.   
*   Next, we loop over the given cell range and then we continue with the actual implementation:  [2.x.79]   [2.x.80] Tell the FEEvaluation object the (macro) cell we want to work on.   [2.x.81] Read in the values of the source vectors ( [2.x.82]  including the resolution of constraints. This stores  [2.x.83]  as described in the introduction.   [2.x.84] Compute the unit-cell gradient (the evaluation of finite element functions). Since FEEvaluation can combine value computations with gradient computations, it uses a unified interface to all kinds of derivatives of order between zero and two. We only want gradients, no values and no second derivatives, so we set the function arguments to true in the gradient slot (second slot), and to false in the values slot (first slot). There is also a third slot for the Hessian which is false by default, so it needs not be given. Note that the FEEvaluation class internally evaluates shape functions in an efficient way where one dimension is worked on at a time (using the tensor product form of shape functions and quadrature points as mentioned in the introduction). This gives complexity equal to  [2.x.85]  for polynomial degree  [2.x.86]  in  [2.x.87]  dimensions, compared to the naive approach with loops over all local degrees of freedom and quadrature points that is used in FEValues and costs  [2.x.88] .   [2.x.89] Next comes the application of the Jacobian transformation, the multiplication by the variable coefficient and the quadrature weight. FEEvaluation has an access function  [2.x.90]  that applies the Jacobian and returns the gradient in real space. Then, we just need to multiply by the (scalar) coefficient, and let the function  [2.x.91]  apply the second Jacobian (for the test function) and the quadrature weight and Jacobian determinant (JxW). Note that the submitted gradient is stored in the same data field as where it is read from in  [2.x.92]  Therefore, you need to make sure to not read from the same quadrature point again after having called  [2.x.93]  on that particular quadrature point. In general, it is a good idea to copy the result of  [2.x.94]  when it is used more often than once.   [2.x.95] Next follows the summation over quadrature points for all test functions that corresponds to the actual integration step. For the Laplace operator, we just multiply by the gradient, so we call the integrate function with the respective argument set. If you have an equation where you test by both the values of the test functions and the gradients, both template arguments need to be set to true. Calling first the integrate function for values and then gradients in a separate call leads to wrong results, since the second call will internally overwrite the results from the first call. Note that there is no function argument for the second derivative for integrate step.   [2.x.96] Eventually, the local contributions in the vector  [2.x.97]  as mentioned in the introduction need to be added into the result vector (and constraints are applied). This is done with a call to  [2.x.98]  the same name as the corresponding function in the AffineConstraints (only that we now store the local vector in the FEEvaluation object, as are the indices between local and global degrees of freedom).   [2.x.99] 
* 

* 
* [1.x.111]
* 
*  This function implements the loop over all cells for the  [2.x.100]  interface. This is done with the  [2.x.101]  of the MatrixFree class, which takes the operator() of this class with arguments MatrixFree, OutVector, InVector, cell_range. When working with MPI parallelization (but no threading) as is done in this tutorial program, the cell loop corresponds to the following three lines of code:   
*    [2.x.102]    
*   Here, the two calls update_ghost_values() and compress() perform the data exchange on processor boundaries for MPI, once for the source vector where we need to read from entries owned by remote processors, and once for the destination vector where we have accumulated parts of the residuals that need to be added to the respective entry of the owner processor. However,  [2.x.103]  does not only abstract away those two calls, but also performs some additional optimizations. On the one hand, it will split the update_ghost_values() and compress() calls in a way to allow for overlapping communication and computation. The local_apply function is then called with three cell ranges representing partitions of the cell range from 0 to  [2.x.104]  On the other hand, cell_loop also supports thread parallelism in which case the cell ranges are split into smaller chunks and scheduled in an advanced way that avoids access to the same vector entry by several threads. That feature is explained in  [2.x.105] .   
*   Note that after the cell loop, the constrained degrees of freedom need to be touched once more for sensible vmult() operators: Since the assembly loop automatically resolves constraints (just as the  [2.x.106]  call does), it does not compute any contribution for constrained degrees of freedom, leaving the respective entries zero. This would represent a matrix that had empty rows and columns for constrained degrees of freedom. However, iterative solvers like CG only work for non-singular matrices. The easiest way to do that is to set the sub-block of the matrix that corresponds to constrained DoFs to an identity matrix, in which case application of the matrix would simply copy the elements of the right hand side vector into the left hand side. Fortunately, the vmult() implementations  [2.x.107]  do this automatically for us outside the apply_add() function, so we do not need to take further action here.   
*   When using the combination of MatrixFree and FEEvaluation in parallel with MPI, there is one aspect to be careful about &mdash; the indexing used for accessing the vector. For performance reasons, MatrixFree and FEEvaluation are designed to access vectors in MPI-local index space also when working with multiple processors. Working in local index space means that no index translation needs to be performed at the place the vector access happens, apart from the unavoidable indirect addressing. However, local index spaces are ambiguous: While it is standard convention to access the locally owned range of a vector with indices between 0 and the local size, the numbering is not so clear for the ghosted entries and somewhat arbitrary. For the matrix-vector product, only the indices appearing on locally owned cells (plus those referenced via hanging node constraints) are necessary. However, in deal.II we often set all the degrees of freedom on ghosted elements as ghosted vector entries, called the  [2.x.108]  "locally relevant DoFs described in the glossary". In that case, the MPI-local index of a ghosted vector entry can in general be different in the two possible ghost sets, despite referring to the same global index. To avoid problems, FEEvaluation checks that the partitioning of the vector used for the matrix-vector product does indeed match with the partitioning of the indices in MatrixFree by a check called  [2.x.109]  To facilitate things, the  [2.x.110]  class includes a mechanism to fit the ghost set to the correct layout. This happens in the ghost region of the vector, so keep in mind that the ghost region might be modified in both the destination and source vector after a call to a vmult() method. This is legitimate because the ghost region of a distributed deal.II vector is a mutable section and filled on demand. Vectors used in matrix-vector products must not be ghosted upon entry of vmult() functions, so no information gets lost.
* 

* 
* [1.x.113]
* 
*  The following function implements the computation of the diagonal of the operator. Computing matrix entries of a matrix-free operator evaluation turns out to be more complicated than evaluating the operator. Fundamentally, we could obtain a matrix representation of the operator by applying the operator on [1.x.114] unit vectors. Of course, that would be very inefficient since we would need to perform [1.x.115] operator evaluations to retrieve the whole matrix. Furthermore, this approach would completely ignore the matrix sparsity. On an individual cell, however, this is the way to go and actually not that inefficient as there usually is a coupling between all degrees of freedom inside the cell.   
*   We first initialize the diagonal vector to the correct parallel layout. This vector is encapsulated in a member called inverse_diagonal_entries of type DiagonalMatrix in the base class  [2.x.111]  This member is a shared pointer that we first need to initialize and then get the vector representing the diagonal entries in the matrix. As to the actual diagonal computation, we again use the cell_loop infrastructure of MatrixFree to invoke a local worker routine called local_compute_diagonal(). Since we will only write into a vector but not have any source vector, we put a dummy argument of type <tt>unsigned int</tt> in place of the source vector to confirm with the cell_loop interface. After the loop, we need to set the vector entries subject to Dirichlet boundary conditions to one (either those on the boundary described by the AffineConstraints object inside MatrixFree or the indices at the interface between different grid levels in adaptive multigrid). This is done through the function  [2.x.112]  and matches with the setting in the matrix-vector product provided by the Base operator. Finally, we need to invert the diagonal entries which is the form required by the Chebyshev smoother based on the Jacobi iteration. In the loop, we assert that all entries are non-zero, because they should either have obtained a positive contribution from integrals or be constrained and treated by  [2.x.113]  following cell_loop.
* 

* 
* [1.x.116]
* 
*  In the local compute loop, we compute the diagonal by a loop over all columns in the local matrix and putting the entry 1 in the [1.x.117]th slot and a zero entry in all other slots, i.e., we apply the cell-wise differential operator on one unit vector at a time. The inner part invoking  [2.x.114]  the loop over quadrature points, and  [2.x.115]  is exactly the same as in the local_apply function. Afterwards, we pick out the [1.x.118]th entry of the local result and put it to a temporary storage (as we overwrite all entries in the array behind  [2.x.116]  with the next loop iteration). Finally, the temporary storage is written to the destination vector. Note how we use  [2.x.117]  and  [2.x.118]  to read and write to the data field that FEEvaluation uses for the integration on the one hand and writes into the global vector on the other hand.   
*   Given that we are only interested in the matrix diagonal, we simply throw away all other entries of the local matrix that have been computed along the way. While it might seem wasteful to compute the complete cell matrix and then throw away everything but the diagonal, the integration are so efficient that the computation does not take too much time. Note that the complexity of operator evaluation per element is  [2.x.119]  for polynomial degree  [2.x.120] , so computing the whole matrix costs us  [2.x.121]  operations, not too far away from  [2.x.122]  complexity for computing the diagonal with FEValues. Since FEEvaluation is also considerably faster due to vectorization and other optimizations, the diagonal computation with this function is actually the fastest (simple) variant. (It would be possible to compute the diagonal with sum factorization techniques in  [2.x.123]  operations involving specifically adapted kernels&mdash;but since such kernels are only useful in that particular context and the diagonal computation is typically not on the critical path, they have not been implemented in deal.II.)   
*   Note that the code that calls distribute_local_to_global on the vector to accumulate the diagonal entries into the global matrix has some limitations. For operators with hanging node constraints that distribute an integral contribution of a constrained DoF to several other entries inside the distribute_local_to_global call, the vector interface used here does not exactly compute the diagonal entries, but lumps some contributions located on the diagonal of the local matrix that would end up in a off-diagonal position of the global matrix to the diagonal. The result is correct up to discretization accuracy as explained in [1.x.119], but not mathematically equal. In this tutorial program, no harm can happen because the diagonal is only used for the multigrid level matrices where no hanging node constraints appear.
* 

* 
* [1.x.120]
* 
*   [1.x.121]  [1.x.122]
* 

* 
*  This class is based on the one in  [2.x.124] . However, we replaced the SparseMatrix<double> class by our matrix-free implementation, which means that we can also skip the sparsity patterns. Notice that we define the LaplaceOperator class with the degree of finite element as template argument (the value is defined at the top of the file), and that we use float numbers for the multigrid level matrices.   
*   The class also has a member variable to keep track of all the detailed timings for setting up the entire chain of data before we actually go about solving the problem. In addition, there is an output stream (that is disabled by default) that can be used to output details for the individual setup operations instead of the summary only that is printed out by default.   
*   Since this program is designed to be used with MPI, we also provide the usual  [2.x.125]  output stream that only prints the information of the processor with MPI rank 0. The grid used for this programs can either be a distributed triangulation based on p4est (in case deal.II is configured to use p4est), otherwise it is a serial grid that only runs without MPI.
* 

* 
* [1.x.123]
* 
*  When we initialize the finite element, we of course have to use the degree specified at the top of the file as well (otherwise, an exception will be thrown at some point, since the computational kernel defined in the templated LaplaceOperator class and the information from the finite element read out by MatrixFree will not match). The constructor of the triangulation needs to set an additional flag that tells the grid to conform to the 2:1 cell balance over vertices, which is needed for the convergence of the geometric multigrid routines. For the distributed grid, we also need to specifically enable the multigrid hierarchy.
* 

* 
* [1.x.124]
* 
*  The LaplaceProblem class holds an additional output stream that collects detailed timings about the setup phase. This stream, called time_details, is disabled by default through the  [2.x.126]  argument specified here. For detailed timings, removing the  [2.x.127]  argument prints all the details.
* 

* 
* [1.x.125]
* 
*   [1.x.126]  [1.x.127]
* 

* 
*  The setup stage is in analogy to  [2.x.128]  with relevant changes due to the LaplaceOperator class. The first thing to do is to set up the DoFHandler, including the degrees of freedom for the multigrid levels, and to initialize constraints from hanging nodes and homogeneous Dirichlet conditions. Since we intend to use this programs in %parallel with MPI, we need to make sure that the constraints get to know the locally relevant degrees of freedom, otherwise the storage would explode when using more than a few hundred millions of degrees of freedom, see  [2.x.129] .
* 

* 
*  Once we have created the multigrid dof_handler and the constraints, we can call the reinit function for the global matrix operator as well as each level of the multigrid scheme. The main action is to set up the  [2.x.130]  instance for the problem. The base class of the  [2.x.131]  class,  [2.x.132]  is initialized with a shared pointer to MatrixFree object. This way, we can simply create it here and then pass it on to the system matrix and level matrices, respectively. For setting up MatrixFree, we need to activate the update flag in the AdditionalData field of MatrixFree that enables the storage of quadrature point coordinates in real space (by default, it only caches data for gradients (inverse transposed Jacobians) and JxW values). Note that if we call the reinit function without specifying the level (i.e., giving  [2.x.133] ), MatrixFree constructs a loop over the active cells. In this tutorial, we do not use threads in addition to MPI, which is why we explicitly disable it by setting the  [2.x.134]  to  [2.x.135]  Finally, the coefficient is evaluated and vectors are initialized as explained above.
* 

* 
* [1.x.128]
* 
*  Next, initialize the matrices for the multigrid method on all the levels. The data structure MGConstrainedDoFs keeps information about the indices subject to boundary conditions as well as the indices on edges between different refinement levels as described in the  [2.x.136]  tutorial program. We then go through the levels of the mesh and construct the constraints and matrices on each level. These follow closely the construction of the system matrix on the original mesh, except the slight difference in naming when accessing information on the levels rather than the active cells.
* 

* 
* [1.x.129]
* 
*   [1.x.130]  [1.x.131]
* 

* 
*  The assemble function is very simple since all we have to do is to assemble the right hand side. Thanks to FEEvaluation and all the data cached in the MatrixFree class, which we query from  [2.x.137]  this can be done in a few lines. Since this call is not wrapped into a  [2.x.138]  (which would be an alternative), we must not forget to call compress() at the end of the assembly to send all the contributions of the right hand side to the owner of the respective degree of freedom.
* 

* 
* [1.x.132]
* 
*   [1.x.133]  [1.x.134]
* 

* 
*  The solution process is similar as in  [2.x.139] . We start with the setup of the transfer. For  [2.x.140]  there is a very fast transfer class called MGTransferMatrixFree that does the interpolation between the grid levels with the same fast sum factorization kernels that get also used in FEEvaluation.
* 

* 
* [1.x.135]
* 
*  As a smoother, this tutorial program uses a Chebyshev iteration instead of SOR in  [2.x.141] . (SOR would be very difficult to implement because we do not have the matrix elements available explicitly, and it is difficult to make it work efficiently in %parallel.)  The smoother is initialized with our level matrices and the mandatory additional data for the Chebyshev smoother. We use a relatively high degree here (5), since matrix-vector products are comparably cheap. We choose to smooth out a range of  [2.x.142]  in the smoother where  [2.x.143]  is an estimate of the largest eigenvalue (the factor 1.2 is applied inside PreconditionChebyshev). In order to compute that eigenvalue, the Chebyshev initialization performs a few steps of a CG algorithm without preconditioner. Since the highest eigenvalue is usually the easiest one to find and a rough estimate is enough, we choose 10 iterations. Finally, we also set the inner preconditioner type in the Chebyshev method which is a Jacobi iteration. This is represented by the DiagonalMatrix class that gets the inverse diagonal entry provided by our LaplaceOperator class.     
*   On level zero, we initialize the smoother differently because we want to use the Chebyshev iteration as a solver. PreconditionChebyshev allows the user to switch to solver mode where the number of iterations is internally chosen to the correct value. In the additional data object, this setting is activated by choosing the polynomial degree to  [2.x.144]  The algorithm will then attack all eigenvalues between the smallest and largest one in the coarse level matrix. The number of steps in the Chebyshev smoother are chosen such that the Chebyshev convergence estimates guarantee to reduce the residual by the number specified in the variable  [2.x.145]  smoothing_range. Note that for solving,  [2.x.146]  is a relative tolerance and chosen smaller than one, in this case, we select three orders of magnitude, whereas it is a number larger than 1 when only selected eigenvalues are smoothed.     
*   From a computational point of view, the Chebyshev iteration is a very attractive coarse grid solver as long as the coarse size is moderate. This is because the Chebyshev method performs only matrix-vector products and vector updates, which typically parallelize better to the largest cluster size with more than a few tens of thousands of cores than inner product involved in other iterative methods. The former involves only local communication between neighbors in the (coarse) mesh, whereas the latter requires global communication over all processors.
* 

* 
* [1.x.136]
* 
*  The next step is to set up the interface matrices that are needed for the case with hanging nodes. The adaptive multigrid realization in deal.II implements an approach called local smoothing. This means that the smoothing on the finest level only covers the local part of the mesh defined by the fixed (finest) grid level and ignores parts of the computational domain where the terminal cells are coarser than this level. As the method progresses to coarser levels, more and more of the global mesh will be covered. At some coarser level, the whole mesh will be covered. Since all level matrices in the multigrid method cover a single level in the mesh, no hanging nodes appear on the level matrices. At the interface between multigrid levels, homogeneous Dirichlet boundary conditions are set while smoothing. When the residual is transferred to the next coarser level, however, the coupling over the multigrid interface needs to be taken into account. This is done by the so-called interface (or edge) matrices that compute the part of the residual that is missed by the level matrix with homogeneous Dirichlet conditions. We refer to the  [2.x.147]  "Multigrid paper by Janssen and Kanschat" for more details.     
*   For the implementation of those interface matrices, there is already a pre-defined class  [2.x.148]  that wraps the routines  [2.x.149]  and  [2.x.150]  in a new class with  [2.x.151]  vmult() and  [2.x.152]  operations (that were originally written for matrices, hence expecting those names). Note that vmult_interface_down is used during the restriction phase of the multigrid V-cycle, whereas vmult_interface_up is used during the prolongation phase.     
*   Once the interface matrix is created, we set up the remaining Multigrid preconditioner infrastructure in complete analogy to  [2.x.153]  to obtain a  [2.x.154]  object that can be applied to a matrix.
* 

* 
* [1.x.137]
* 
*  The setup of the multigrid routines is quite easy and one cannot see any difference in the solve process as compared to  [2.x.155] . All the magic is hidden behind the implementation of the  [2.x.156]  operation. Note that we print out the solve time and the accumulated setup time through standard out, i.e., in any case, whereas detailed times for the setup operations are only printed in case the flag for detail_times in the constructor is changed.
* 

* 
*  

* 
* [1.x.138]
* 
*   [1.x.139]  [1.x.140]
* 

* 
*  Here is the data output, which is a simplified version of  [2.x.157] . We use the standard VTU (= compressed VTK) output for each grid produced in the refinement process. In addition, we use a compression algorithm that is optimized for speed rather than disk usage. The default setting (which optimizes for disk usage) makes saving the output take about 4 times as long as running the linear solver, while setting  [2.x.158]  to  [2.x.159]  lowers this to only one fourth the time of the linear solve.   
*   We disable the output when the mesh gets too large. A variant of this program has been run on hundreds of thousands MPI ranks with as many as 100 billion grid cells, which is not directly accessible to classical visualization tools.
* 

* 
* [1.x.141]
* 
*   [1.x.142]  [1.x.143]
* 

* 
*  The function that runs the program is very similar to the one in  [2.x.160] . We do few refinement steps in 3D compared to 2D, but that's it.   
*   Before we run the program, we output some information about the detected vectorization level as discussed in the introduction.
* 

* 
* [1.x.144]
* 
*   [1.x.145]  [1.x.146]
* 

* 
*  Apart from the fact that we set up the MPI framework according to  [2.x.161] , there are no surprises in the main function.
* 

* 
* [1.x.147]
* [1.x.148][1.x.149]
* 

* [1.x.150][1.x.151]
* 

* Since this example solves the same problem as  [2.x.162]  (except fora different coefficient), there is little to say about thesolution. We show a picture anyway, illustrating the size of thesolution through both isocontours and volume rendering:
*  [2.x.163] 
* Of more interest is to evaluate some aspects of the multigrid solver.When we run this program in 2D for quadratic ( [2.x.164] ) elements, we get thefollowing output (when run on one core in release mode):
* [1.x.152]
* 
* As in  [2.x.165] , we see that the number of CG iterations remains constant withincreasing number of degrees of freedom. A constant number of iterations(together with optimal computational properties) means that the computing timeapproximately quadruples as the problem size quadruples from one cycle to thenext. The code is also very efficient in terms of storage. Around 2-4 milliondegrees of freedom fit into 1 GB of memory, see also the MPI results below. Aninteresting fact is that solving one linear system is cheaper than the setup,despite not building a matrix (approximately half of which is spent in the [2.x.166]  and  [2.x.167] calls). This shows the high efficiency of this approach, but also that thedeal.II data structures are quite expensive to set up and the setup cost mustbe amortized over several system solves.
* Not much changes if we run the program in three spatial dimensions. Since weuse uniform mesh refinement, we get eight times as many elements andapproximately eight times as many degrees of freedom with each cycle:
* [1.x.153]
* 
* Since it is so easy, we look at what happens if we increase the polynomialdegree. When selecting the degree as four in 3D, i.e., on  [2.x.168] elements, by changing the line <code>const unsigned intdegree_finite_element=4;</code> at the top of the program, we get thefollowing program output:
* [1.x.154]
* 
* Since  [2.x.169]  elements on a certain mesh correspond to  [2.x.170] elements on half the mesh size, we can compare the run time at cycle 4 withfourth degree polynomials with cycle 5 using quadratic polynomials, both at2.1 million degrees of freedom. The surprising effect is that the solver for [2.x.171]  element is actually slightly faster than for the quadraticcase, despite using one more linear iteration. The effect that higher-degreepolynomials are similarly fast or even faster than lower degree ones is one ofthe main strengths of matrix-free operator evaluation through sumfactorization, see the [1.x.155]. This is fundamentally different to matrix-based methods that getmore expensive per unknown as the polynomial degree increases and the couplinggets denser.
* In addition, also the setup gets a bit cheaper for higher order, which isbecause fewer elements need to be set up.
* Finally, let us look at the timings with degree 8, which corresponds toanother round of mesh refinement in the lower order methods:
* [1.x.156]
* 
* Here, the initialization seems considerably slower than before, which ismainly due to the computation of the diagonal of the matrix, which actuallycomputes a 729 x 729 matrix on each cell and throws away everything but thediagonal. The solver times, however, are again very close to the quartic case,showing that the linear increase with the polynomial degree that istheoretically expected is almost completely offset by better computationalcharacteristics and the fact that higher order methods have a smaller share ofdegrees of freedom living on several cells that add to the evaluationcomplexity.
* [1.x.157][1.x.158]
* 

* In order to understand the capabilities of the matrix-free implementation, wecompare the performance of the 3d example above with a sparse matriximplementation based on  [2.x.172]  by measuring both thecomputation times for the initialization of the problem (distribute DoFs,setup and assemble matrices, setup multigrid structures) and the actualsolution for the matrix-free variant and the variant based on sparsematrices. We base the preconditioner on float numbers and the actual matrixand vectors on double numbers, as shown above. Tests are run on an Intel Corei7-5500U notebook processor (two cores and [1.x.159]support, i.e., four operations on doubles can be done with one CPUinstruction, which is heavily used in FEEvaluation), optimized mode, and twoMPI ranks.
*  [2.x.173] 
* The table clearly shows that the matrix-free implementation is more than twiceas fast for the solver, and more than six times as fast when it comes toinitialization costs. As the problem size is made a factor 8 larger, we notethat the times usually go up by a factor eight, too (as the solver iterationsare constant at six). The main deviation is in the sparse matrix between 5kand 36k degrees of freedom, where the time increases by a factor 12. This isthe threshold where the (L3) cache in the processor can no longer hold alldata necessary for the matrix-vector products and all matrix elements must befetched from main memory.
* Of course, this picture does not necessarily translate to all cases, as thereare problems where knowledge of matrix entries enables much better solvers (ashappens when the coefficient is varying more strongly than in the aboveexample). Moreover, it also depends on the computer system. The present systemhas good memory performance, so sparse matrices perform comparablywell. Nonetheless, the matrix-free implementation gives a nice speedup alreadyfor the [1.x.160]<sub>2</sub> elements used in this example. This becomesparticularly apparent for time-dependent or nonlinear problems where sparsematrices would need to be reassembled over and over again, which becomes mucheasier with this class. And of course, thanks to the better complexity of theproducts, the method gains increasingly larger advantages when the order of theelements increases (the matrix-free implementation has costs4[1.x.161]<sup>2</sup>[1.x.162] per degree of freedom, compared to2[1.x.163] for the sparse matrix, so it will win anyway for order 4and higher in 3d).
* [1.x.164][1.x.165]
* 

* As explained in the introduction and the in-code comments, this program can berun in parallel with MPI. It turns out that geometric multigrid schemes workreally well and can scale to very large machines. To the authors' knowledge,the geometric multigrid results shown here are the largest computations donewith deal.II as of late 2016, run on up to 147,456 cores of the [1.x.166]. The ingredients for scalability beyond 1000 cores arethat no data structure that depends on the global problem size is held in itsentirety on a single processor and that the communication is not too frequentin order not to run into latency issues of the network.  For PDEs solved withiterative solvers, the communication latency is often the limiting factor,rather than the throughput of the network. For the example of the SuperMUCsystem, the point-to-point latency between two processors is between 1e-6 and1e-5 seconds, depending on the proximity in the MPI network. The matrix-vectorproducts with  [2.x.174]  from this class involves severalpoint-to-point communication steps, interleaved with computations on eachcore. The resulting latency of a matrix-vector product is around 1e-4seconds. Global communication, for example an  [2.x.175]  operation thataccumulates the sum of a single number per rank over all ranks in the MPInetwork, has a latency of 1e-4 seconds. The multigrid V-cycle used in thisprogram is also a form of global communication. Think about the coarse gridsolve that happens on a single processor: It accumulates the contributionsfrom all processors before it starts. When completed, the coarse grid solutionis transferred to finer levels, where more and more processors help insmoothing until the fine grid. Essentially, this is a tree-like pattern overthe processors in the network and controlled by the mesh. As opposed to the [2.x.176]  operations where the tree in the reduction is optimized to theactual links in the MPI network, the multigrid V-cycle does this according tothe partitioning of the mesh. Thus, we cannot expect the sameoptimality. Furthermore, the multigrid cycle is not simply a walk up and downthe refinement tree, but also communication on each level when doing thesmoothing. In other words, the global communication in multigrid is morechallenging and related to the mesh that provides less optimizationopportunities. The measured latency of the V-cycle is between 6e-3 and 2e-2seconds, i.e., the same as 60 to 200 MPI_Allreduce operations.
* The following figure shows a scaling experiments on  [2.x.177] elements. Along the lines, the problem size is held constant as the number ofcores is increasing. When doubling the number of cores, one expects a halvingof the computational time, indicated by the dotted gray lines. The resultsshow that the implementation shows almost ideal behavior until an absolutetime of around 0.1 seconds is reached. The solver tolerances have been setsuch that the solver performs five iterations. This way of plotting data isthe [1.x.167] of the algorithm. As we go to very large corecounts, the curves flatten out a bit earlier, which is because of thecommunication network in SuperMUC where communication between processorsfarther away is slightly slower.
*  [2.x.178] 
* In addition, the plot also contains results for [1.x.168] that listshow the algorithm behaves as both the number of processor cores and elementsis increased at the same pace. In this situation, we expect that the computetime remains constant. Algorithmically, the number of CG iterations isconstant at 5, so we are good from that end. The lines in the plot arearranged such that the top left point in each data series represents the samesize per processor, namely 131,072 elements (or approximately 3.5 milliondegrees of freedom per core). The gray lines indicating ideal strong scalingare by the same factor of 8 apart. The results show again that the scaling isalmost ideal. The parallel efficiency when going from 288 cores to 147,456cores is at around 75% for a local problem size of 750,000 degrees of freedomper core which takes 1.0s on 288 cores, 1.03s on 2304 cores, 1.19s on 18kcores, and 1.35s on 147k cores. The algorithms also reach a very highutilization of the processor. The largest computation on 147k cores reachesaround 1.7 PFLOPs/s on SuperMUC out of an arithmetic peak of 3.2 PFLOPs/s. Foran iterative PDE solver, this is a very high number and significantly more isoften only reached for dense linear algebra. Sparse linear algebra is limitedto a tenth of this value.
* As mentioned in the introduction, the matrix-free method reduces the memoryconsumption of the data structures. Besides the higher performance due to lessmemory transfer, the algorithms also allow for very large problems to fit intomemory. The figure below shows the computational time as we increase theproblem size until an upper limit where the computation exhausts memory. We dothis for 1k cores, 8k cores, and 65k cores and see that the problem size canbe varied over almost two orders of magnitude with ideal scaling. The largestcomputation shown in this picture involves 292 billion ( [2.x.179] )degrees of freedom. On a DG computation of 147k cores, the above algorithmswere also run involving up to 549 billion (2^39) DoFs.
*  [2.x.180] 
* Finally, we note that while performing the tests on the large-scale systemshown above, improvements of the multigrid algorithms in deal.II have beendeveloped. The original version contained the sub-optimal code based onMGSmootherPrecondition where some MPI_Allreduce commands (checking whetherall vector entries are zero) were done on each smoothingoperation on each level, which only became apparent on 65k cores andmore. However, the following picture shows that the improvement already payoff on a smaller scale, here shown on computations on up to 14,336 cores for [2.x.181]  elements:
*  [2.x.182] 
* 

* [1.x.169][1.x.170]
* 

* As explained in the code, the algorithm presented here is prepared to run onadaptively refined meshes. If only part of the mesh is refined, the multigridcycle will run with local smoothing and impose Dirichlet conditions along theinterfaces which differ in refinement level for smoothing through the [2.x.183]  class. Due to the way the degrees of freedom aredistributed over levels, relating the owner of the level cells to the owner ofthe first descendant active cell, there can be an imbalance between differentprocessors in MPI, which limits scalability by a factor of around two to five.
* [1.x.171][1.x.172]
* 

* [1.x.173][1.x.174]
* 

* As mentioned above the code is ready for locally adaptive h-refinement.For the Poisson equation one can employ the Kelly error indicator,implemented in the KellyErrorEstimator class. However one needs to be carefulwith the ghost indices of parallel vectors.In order to evaluate the jump terms in the error indicator, each MPI processneeds to know locally relevant DoFs.However  [2.x.184]  function initializes the vector only withsome locally relevant DoFs.The ghost indices made available in the vector are a tight set of only those indicesthat are touched in the cell integrals (including constraint resolution).This choice has performance reasons, because sending all locally relevant degreesof freedom would be too expensive compared to the matrix-vector product.Consequently the solution vector as-is isnot suitable for the KellyErrorEstimator class.The trick is to change the ghost part of the partition, for example using atemporary vector and  [2.x.185] as shown below.
* [1.x.175]
* 
* [1.x.176][1.x.177]
* 

* This program is parallelized with MPI only. As an alternative, the MatrixFreeloop can also be issued in hybrid mode, for example by using MPI parallelizingover the nodes of a cluster and with threads through Intel TBB within theshared memory region of one node. To use this, one would need to both set thenumber of threads in the MPI_InitFinalize data structure in the main function,and set the  [2.x.186]  topartition_color to actually do the loop in parallel. This use case isdiscussed in  [2.x.187] .
* [1.x.178][1.x.179]
* 

* The presented program assumes homogeneous Dirichlet boundary conditions. Whengoing to non-homogeneous conditions, the situation is a bit more intricate. Tounderstand how to implement such a setting, let us first recall how thesearise in the mathematical formulation and how they are implemented in amatrix-based variant. In essence, an inhomogeneous Dirichlet condition setssome of the nodal values in the solution to given values rather thandetermining them through the variational principles,[1.x.180]
* where  [2.x.188]  denotes the nodal values of the solution and  [2.x.189]  denotesthe set of all nodes. The set  [2.x.190]  is the subsetof the nodes that are subject to Dirichlet boundary conditions where thesolution is forced to equal  [2.x.191]  as the interpolationof boundary values on the Dirichlet-constrained node points  [2.x.192] . We then insert this solutionrepresentation into the weak form, e.g. the Laplacian shown above, and movethe known quantities to the right hand side:[1.x.181]
* In this formula, the equations are tested for all basis functions  [2.x.193] with  [2.x.194]  that are not related to the nodesconstrained by Dirichlet conditions.
* In the implementation in deal.II, the integrals  [2.x.195] on the right hand side are already contained in the local matrix contributionswe assemble on each cell. When using [2.x.196]  as first described in the [2.x.197]  and  [2.x.198]  tutorial programs, we can account for the contribution ofinhomogeneous constraints [1.x.182] by multiplying the columns [1.x.183] androws [1.x.184] of the local matrix according to the integrals  [2.x.199]  by the inhomogeneities and subtracting the resulting fromthe position [1.x.185] in the global right-hand-side vector, see also the  [2.x.200]  module. In essence, we use some of the integrals that geteliminated from the left hand side of the equation to finalize the right handside contribution. Similar mathematics are also involved when first writingall entries into a left hand side matrix and then eliminating matrix rows andcolumns by  [2.x.201] 
* In principle, the components that belong to the constrained degrees of freedomcould be eliminated from the linear system because they do not carry anyinformation. In practice, in deal.II we always keep the size of the linearsystem the same to avoid handling two different numbering systems and avoidconfusion about the two different index sets. In order to ensure that thelinear system does not get singular when not adding anything to constrainedrows, we then add dummy entries to the matrix diagonal that are otherwiseunrelated to the real entries.
* In a matrix-free method, we need to take a different approach, since the  [2.x.202] LaplaceOperator class represents the matrix-vector product of a[1.x.186] operator (the left-hand side of the last formula).  It doesnot matter whether the AffineConstraints object passed to the [2.x.203]  contains inhomogeneous constraints or not, the [2.x.204]  call will only resolve the homogeneous part of theconstraints as long as it represents a [1.x.187] operator.
* In our matrix-free code, the right hand side computation where thecontribution of inhomogeneous conditions ends up is completely decoupled fromthe matrix operator and handled by a different function above. Thus, we needto explicitly generate the data that enters the right hand side rather thanusing a byproduct of the matrix assembly. Since we already know how to applythe operator on a vector, we could try to use those facilities for a vectorwhere we only set the Dirichlet values:
* [1.x.188]
* or, equivalently, if we already had filled the inhomogeneous constraints intoan AffineConstraints object,
* [1.x.189]
* 
* We could then pass the vector  [2.x.205]  to the  [2.x.206]  [2.x.207]  function and add the new contribution to the  [2.x.208] system_rhs vector that gets filled in the  [2.x.209] function. However, this idea does not work because the [2.x.210]  call used inside the vmult() functions assumeshomogeneous values on all constraints (otherwise the operator would not be alinear operator but an affine one). To also retrieve the values of theinhomogeneities, we could select one of two following strategies.
* [1.x.190][1.x.191]
* 

* The class FEEvaluation has a facility that addresses precisely thisrequirement: For non-homogeneous Dirichlet values, we do want to skip theimplicit imposition of homogeneous (Dirichlet) constraints upon reading thedata from the vector  [2.x.211]  For example, we could extend the  [2.x.212]  [2.x.213]  function to deal with inhomogeneous Dirichletvalues as follows, assuming the Dirichlet values have been interpolated intothe object  [2.x.214] 
* [1.x.192]
* 
* In this code, we replaced the  [2.x.215]  function for thetentative solution vector by  [2.x.216]  thatignores all constraints. Due to this setup, we must make sure that otherconstraints, e.g. by hanging nodes, are correctly distributed to the inputvector already as they are not resolved as in [2.x.217]  Inside the loop, we then evaluate theLaplacian and repeat the second derivative call with [2.x.218]  from the  [2.x.219]  class, but with thesign switched since we wanted to subtract the contribution of Dirichletconditions on the right hand side vector according to the formula above. Whenwe invoke the  [2.x.220]  call, we then set both argumentsregarding the value slot and first derivative slot to true to account for bothterms added in the loop over quadrature points. Once the right hand side isassembled, we then go on to solving the linear system for the homogeneousproblem, say involving a variable  [2.x.221]  After solving, we canadd  [2.x.222]  to the  [2.x.223]  vector that contains the final(inhomogeneous) solution.
* Note that the negative sign for the Laplacian alongside with a positive signfor the forcing that we needed to build the right hand side is a more generalconcept: We have implemented nothing else than Newton's method for nonlinearequations, but applied to a linear system. We have used an initial guess forthe variable  [2.x.224]  in terms of the Dirichlet boundary conditions andcomputed a residual  [2.x.225] . The linear system was then solved as [2.x.226]  and we finally computed  [2.x.227] . For alinear system, we obviously reach the exact solution after a singleiteration. If we wanted to extend the code to a nonlinear problem, we wouldrename the  [2.x.228]  function into a more descriptive name like  [2.x.229] assemble_residual() that computes the (weak) form of the residual, whereas the [2.x.230]  function would get the linearization of theresidual with respect to the solution variable.
* [1.x.193][1.x.194]
* 

* A second alternative to get the right hand side that re-uses the  [2.x.231]  [2.x.232]  function is to instead add a second LaplaceOperatorthat skips Dirichlet constraints. To do this, we initialize a second MatrixFreeobject which does not have any boundary value constraints. This  [2.x.233] object is then passed to a  [2.x.234]  class instance  [2.x.235] inhomogeneous_operator that is only used to create the right hand side:
* [1.x.195]
* 
* A more sophisticated implementation of this technique could reuse the originalMatrixFree object. This can be done by initializing the MatrixFree object withmultiple blocks, where each block corresponds to a different AffineConstraintsobject. Doing this would require making substantial modifications to theLaplaceOperator class, but the  [2.x.236]  class thatcomes with the library can do this. See the discussion on blocks in [2.x.237]  for more information on how to set up blocks.
* 

* [1.x.196][1.x.197] [2.x.238] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-38_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19]
*  [2.x.2] 
* [1.x.20]
* [1.x.21]
* [1.x.22][1.x.23]
* 

* In this example, we show how to solve a partial differential equation (PDE)on a codimension one surface  [2.x.3] made of quadrilaterals, i.e. on a surface in 3d or a line in 2d.We focus on the following elliptic second order PDE
* [1.x.24]
* which generalized the Laplace equation we have previously solved in several ofthe early tutorial programs. Our implementation is based on  [2.x.4] .  [2.x.5] also solves problems on lower dimensional surfaces; however, there we onlyconsider integral equations that do not involve derivatives on the solutionvariable, while here we actually have to investigate what it means to takederivatives of a function only defined on a (possibly curved) surface.
* In order to define the above operator, we start by introducing some notations.Let  [2.x.6]  be a parameterization ofa surface  [2.x.7]  from a reference element  [2.x.8] ,i.e. each point  [2.x.9]  induces a point  [2.x.10] . Then let[1.x.25]denotes the corresponding first fundamental form, where  [2.x.11]  is thederivative (Jacobian) of the mapping.In the following,  [2.x.12]  will be either the entire surface  [2.x.13]  or,more convenient for the finite element method, any face  [2.x.14] , where  [2.x.15]  is a partition (triangulation) of  [2.x.16] constituted of quadrilaterals.We are now in position to define the tangential gradient of a function  [2.x.17]  by[1.x.26]The surface Laplacian (also called the Laplace-Beltrami operator) is thendefined as   [2.x.18] .Note that an alternate way to compute the surface gradient on smooth surfaces  [2.x.19]  is[1.x.27]where  [2.x.20]  is a "smooth" extension of  [2.x.21]  in a tubular neighborhood of  [2.x.22]  and [2.x.23]  is the normal of  [2.x.24] .Since  [2.x.25] , we deduce[1.x.28]Worth mentioning, the term  [2.x.26]  appearing in the above expression is the total curvature of the surface (sum of principal curvatures).
* As usual, we are only interested in weak solutions for which we can use  [2.x.27] finite elements (rather than requiring  [2.x.28]  continuity as for strongsolutions). We therefore resort to the weak formulation[1.x.29]and take advantage of the partition  [2.x.29]  to further write[1.x.30]Moreover, each integral in the above expression is computed in the referenceelement  [2.x.30] so that
* [1.x.31]
* and[1.x.32]Finally, we use a quadrature formula defined by points  [2.x.31]  and weights  [2.x.32]  toevaluate the above integrals andobtain[1.x.33]and[1.x.34]
* 

* Fortunately, deal.II has already all the tools to compute the aboveexpressions.In fact, they barely differ from the ways in which we solve the usualLaplacian, only requiring the surface coordinate mapping to be provided in theconstructor of the FEValues class.This surface description given, in the codimension one surface case, the tworoutines  [2.x.33]  and  [2.x.34] return
* [1.x.35]
* This provides exactly the terms we need for our computations.
* On a more general note, details for the finite element approximation onsurfaces can be found for instance in[Dziuk, in Partial differential equations and calculus ofvariations 1357, Lecture Notes in Math., 1988],[Demlow, SIAM J. Numer. Anal.  47(2), 2009]and[Bonito, Nochetto, and Pauletti, SIAM J. Numer. Anal. 48(5), 2010].
* 

* 
* [1.x.36][1.x.37]
* 

* In general when you want to test numerically the accuracy and/or order ofconvergence of an algorithm you need to provide an exact solution. The usualtrick is to pick a function that we want to be the solution, then apply thedifferential operator to it that defines a forcing term for the right handside. This is what we do in this example. In the current case, the form of thedomain is obviously also essential.
* We produce one test case for a 2d problem and another one for 3d:
*  [2.x.35]  [2.x.36]   In 2d, let's choose as domain a half circle. On this domain, we choose the  function  [2.x.37]  as the solution. To compute the right hand  side, we have to compute the surface Laplacian of the  solution function. There are (at least) two ways to do that. The first one  is to project away the normal derivative as described above using the natural extension of  [2.x.38]  (still denoted by  [2.x.39] ) over  [2.x.40] , i.e. to compute  [1.x.38]  where  [2.x.41]  is the total curvature of  [2.x.42] .  Since we are on the unit circle,  [2.x.43]  and  [2.x.44]  so that  [1.x.39]
*   A somewhat simpler way, at least for the current case of a curve in  two-dimensional space, is to note that we can map the interval  [2.x.45]  onto the domain  [2.x.46]  using the transformation   [2.x.47] .  At position  [2.x.48] , the value of the solution is then   [2.x.49] .  Taking into account that the transformation is length preserving, i.e. a  segment of length  [2.x.50]  is mapped onto a piece of curve of exactly the same  length, the tangential Laplacian then satisfies 
* [1.x.40]
*   which is of course the same result as we had above. [2.x.51]  [2.x.52]   In 3d, the domain is again half of the surface of the unit ball, i.e. a half  sphere or dome. We choose  [2.x.53]  as  the solution. We can compute the right hand side of the  equation,  [2.x.54] , in the same way as the method above (with  [2.x.55] ), yielding an  awkward and lengthy expression. You can find the full expression in the  source code. [2.x.56]  [2.x.57] 
* In the program, we will also compute the  [2.x.58]  seminorm error of thesolution. Since the solution function and its numerical approximation are onlydefined on the manifold, the obvious definition of this error functional is [2.x.59] . This requires us to provide the[1.x.41] gradient  [2.x.60]  to the function  [2.x.61] (first introduced in  [2.x.62] ), which wewill do by implementing the function  [2.x.63]  in theprogram below.
* 

* [1.x.42][1.x.43]
* 

* If you've read through  [2.x.64]  and understand the discussion above of howsolution and right hand side correspond to each other, you will be immediatelyfamiliar with this program as well. In fact, there are only two things thatare of significance:
* 
*  - The way we generate the mesh that triangulates the computational domain.
* 
*  - The way we use Mapping objects to describe that the domain on which we solve  the partial differential equation is not planar but in fact curved.
* Mapping objects were already introduced in  [2.x.65]  and  [2.x.66]  and asexplained there, there is usually not a whole lot you have to know about howthey work as long as you have a working description of how the boundarylooks. In essence, we will simply declare an appropriate object of typeMappingQ that will automatically obtain the boundary description from theTriangulation. The mapping object will then be passed to the appropriatefunctions, and we will get a boundary description for half circles or halfspheres that is predefined in the library.
* The rest of the program follows closely  [2.x.67]  and, as far as computing theerror,  [2.x.68] . Some aspects of this program, in particular the use of twotemplate arguments on the classes Triangulation, DoFHandler, and similar, arealready described in detail in  [2.x.69] ; you may wish to read through thistutorial program as well.
* 

*  [1.x.44] [1.x.45]
*   [1.x.46]  [1.x.47]
* 

* 
*  If you've read through  [2.x.70]  and  [2.x.71] , you will recognize that we have used all of the following include files there already. Consequently, we will not explain their meaning here again.
* 

* 
* [1.x.48]
* 
*   [1.x.49]  [1.x.50]
* 

* 
*  This class is almost exactly similar to the  [2.x.72]  class in  [2.x.73] .
* 

* 
*  The essential differences are these:
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - The template parameter now denotes the dimensionality of the embedding space, which is no longer the same as the dimensionality of the domain and the triangulation on which we compute. We indicate this by calling the parameter  [2.x.74]  and introducing a constant  [2.x.75]  equal to the dimensionality of the domain
* 
*  -  here equal to  [2.x.76] .
* 

* 
* 
*  - All member variables that have geometric aspects now need to know about both their own dimensionality as well as that of the embedding space. Consequently, we need to specify both of their template parameters one for the dimension of the mesh  [2.x.77]  and the other for the dimension of the embedding space,  [2.x.78]  This is exactly what we did in  [2.x.79] , take a look there for a deeper explanation.
* 

* 
* 
*  - We need an object that describes which kind of mapping to use from the reference cell to the cells that the triangulation is composed of. The classes derived from the Mapping base class do exactly this. Throughout most of deal.II, if you don't do anything at all, the library assumes that you want an object of kind MappingQ1 that uses a (bi-, tri-)linear mapping. In many cases, this is quite sufficient, which is why the use of these objects is mostly optional: for example, if you have a polygonal two-dimensional domain in two-dimensional space, a bilinear mapping of the reference cell to the cells of the triangulation yields an exact representation of the domain. If you have a curved domain, one may want to use a higher order mapping for those cells that lie at the boundary of the domain
* 
*  -  this is what we did in  [2.x.80] , for example. However, here we have a curved domain, not just a curved boundary, and while we can approximate it with bilinearly mapped cells, it is really only prudent to use a higher order mapping for all cells. Consequently, this class has a member variable of type MappingQ; we will choose the polynomial degree of the mapping equal to the polynomial degree of the finite element used in the computations to ensure optimal approximation, though this iso-parametricity is not required.
* 

* 
* [1.x.51]
* 
*   [1.x.52]  [1.x.53]
* 

* 
*  Next, let us define the classes that describe the exact solution and the right hand sides of the problem. This is in analogy to  [2.x.81]  and  [2.x.82]  where we also defined such objects. Given the discussion in the introduction, the actual formulas should be self-explanatory. A point of interest may be how we define the value and gradient functions for the 2d and 3d cases separately, using explicit specializations of the general template. An alternative to doing it this way might have been to define the general template and have a  [2.x.83]  statement (or a sequence of  [2.x.84] s) for each possible value of the spatial dimension.
* 

* 
* [1.x.54]
* 
*   [1.x.55]  [1.x.56]
* 

* 
*  The rest of the program is actually quite unspectacular if you know  [2.x.85] . Our first step is to define the constructor, setting the polynomial degree of the finite element and mapping, and associating the DoF handler to the triangulation:
* 

* 
* [1.x.57]
* 
*   [1.x.58]  [1.x.59]
* 

* 
*  The next step is to create the mesh, distribute degrees of freedom, and set up the various variables that describe the linear system. All of these steps are standard with the exception of how to create a mesh that describes a surface. We could generate a mesh for the domain we are interested in, generate a triangulation using a mesh generator, and read it in using the GridIn class. Or, as we do here, we generate the mesh using the facilities in the GridGenerator namespace.   
*   In particular, what we're going to do is this (enclosed between the set of braces below): we generate a  [2.x.86]  dimensional mesh for the half disk (in 2d) or half ball (in 3d), using the  [2.x.87]  function. This function sets the boundary indicators of all faces on the outside of the boundary to zero for the ones located on the perimeter of the disk/ball, and one on the straight part that splits the full disk/ball into two halves. The next step is the main point: The  [2.x.88]  function creates a mesh that consists of those cells that are the faces of the previous mesh, i.e. it describes the [1.x.60] cells of the original (volume) mesh. However, we do not want all faces: only those on the perimeter of the disk or ball which carry boundary indicator zero; we can select these cells using a set of boundary indicators that we pass to  [2.x.89]    
*   There is one point that needs to be mentioned. In order to refine a surface mesh appropriately if the manifold is curved (similarly to refining the faces of cells that are adjacent to a curved boundary), the triangulation has to have an object attached to it that describes where new vertices should be located. If you don't attach such a boundary object, they will be located halfway between existing vertices; this is appropriate if you have a domain with straight boundaries (e.g. a polygon) but not when, as here, the manifold has curvature. So for things to work properly, we need to attach a manifold object to our (surface) triangulation, in much the same way as we've already done in 1d for the boundary. We create such an object and attach it to the triangulation.   
*   The final step in creating the mesh is to refine it a number of times. The rest of the function is the same as in previous tutorial programs.
* 

* 
* [1.x.61]
* 
*   [1.x.62]  [1.x.63]
* 

* 
*  The following is the central function of this program, assembling the matrix that corresponds to the surface Laplacian (Laplace-Beltrami operator). Maybe surprisingly, it actually looks exactly the same as for the regular Laplace operator discussed in, for example,  [2.x.90] . The key is that the  [2.x.91]  function does the magic: It returns the surface gradient  [2.x.92]  of the  [2.x.93] th shape function at the  [2.x.94] th quadrature point. The rest then does not need any changes either:
* 

* 
* [1.x.64]
* 
*   [1.x.65]  [1.x.66]
* 

* 
*  The next function is the one that solves the linear system. Here, too, no changes are necessary:
* 

* 
* [1.x.67]
* 
*   [1.x.68]  [1.x.69]
* 

* 
*  This is the function that generates graphical output from the solution. Most of it is boilerplate code, but there are two points worth pointing out:
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - The  [2.x.95]  function can take two kinds of vectors: Either vectors that have one value per degree of freedom defined by the DoFHandler object previously attached via  [2.x.96]  and vectors that have one value for each cell of the triangulation, for example to output estimated errors for each cell. Typically, the DataOut class knows to tell these two kinds of vectors apart: there are almost always more degrees of freedom than cells, so we can differentiate by the two kinds looking at the length of a vector. We could do the same here, but only because we got lucky: we use a half sphere. If we had used the whole sphere as domain and  [2.x.97]  elements, we would have the same number of cells as vertices and consequently the two kinds of vectors would have the same number of elements. To avoid the resulting confusion, we have to tell the  [2.x.98]  function which kind of vector we have: DoF data. This is what the third argument to the function does.
* 

* 
* 
*  - The  [2.x.99]  function can generate output that subdivides each cell so that visualization programs can resolve curved manifolds or higher polynomial degree shape functions better. We here subdivide each element in each coordinate direction as many times as the polynomial degree of the finite element in use.
* 

* 
* [1.x.70]
* 
*   [1.x.71]  [1.x.72]
* 

* 
*  This is the last piece of functionality: we want to compute the error in the numerical solution. It is a verbatim copy of the code previously shown and discussed in  [2.x.100] . As mentioned in the introduction, the  [2.x.101]  class provides the (tangential) gradient of the solution. To avoid evaluating the error only a superconvergence points, we choose a quadrature rule of sufficiently high order.
* 

* 
* [1.x.73]
* 
*   [1.x.74]  [1.x.75]
* 

* 
*  The last function provides the top-level logic. Its contents are self-explanatory:
* 

* 
* [1.x.76]
* 
*   [1.x.77]  [1.x.78]
* 

* 
*  The remainder of the program is taken up by the  [2.x.102]  function. It follows exactly the general layout first introduced in  [2.x.103]  and used in all following tutorial programs:
* 

* 
* [1.x.79]
* [1.x.80][1.x.81]
* 

* When you run the program, the following output should be printed on screen:
* [1.x.82]
* 
* 

* By playing around with the number of global refinements in the [2.x.104]  function you increase or decrease meshrefinement. For example, doing one more refinement and only running the 3d surfaceproblem yields the followingoutput:
* [1.x.83]
* 
* This is what we expect: make the mesh size smaller by a factor of two and theerror goes down by a factor of four (remember that we use bi-quadraticelements). The full sequence of errors from one to five refinements looks likethis, neatly following the theoretically predicted pattern:
* [1.x.84]
* 
* Finally, the program produces graphical output that we can visualize. Here isa plot of the results:
*  [2.x.105] 
* The program also works for 1d curves in 2d, not just 2d surfaces in 3d. Youcan test this by changing the template argument in  [2.x.106]  likeso:
* [1.x.85]
* The domain is a curve in 2d, and we can visualize the solution by using thethird dimension (and color) to denote the value of the function  [2.x.107] . Thisthen looks like so (the white curve is the domain, the colored curve is thesolution extruded into the third dimension, clearly showing the change in signas the curve moves from one quadrant of the domain into the adjacent one):
*  [2.x.108] 
* 

* [1.x.86][1.x.87][1.x.88]
* 

* Computing on surfaces only becomes interesting if the surface is moreinteresting than just a half sphere. To achieve this, deal.II can readmeshes that describe surfaces through the usual GridIn class. Or, in case youhave an analytic description, a simple mesh can sometimes be stretched andbent into a shape we are interested in.
* Let us consider a relatively simple example: we take the half sphere we usedbefore, we stretch it by a factor of 10 in the z-direction, and then we jumblethe x- and y-coordinates a bit. Let's show the computational domain and thesolution first before we go into details of the implementation below:
*  [2.x.109] 
*  [2.x.110] 
* The way to produce such a mesh is by using the  [2.x.111] function. It needs a way to transform each individual mesh point to adifferent position. Let us here use the following, rather simple function(remember: stretch in one direction, jumble in the other two):
* [1.x.89]
* 
* If we followed the  [2.x.112]  function, we wouldextract the half spherical surface mesh as before, warp it into the shape wewant, and refine as often as necessary. This is not quite as simple as we'dlike here, though: refining requires that we have an appropriate manifoldobject attached to the triangulation that describes where new vertices of themesh should be located upon refinement. I'm sure it's possible to describethis manifold in a not-too-complicated way by simply undoing thetransformation above (yielding the spherical surface again), finding thelocation of a new point on the sphere, and then re-warping the result. But I'ma lazy person, and since doing this is not really the point here, let's justmake our lives a bit easier: we'll extract the half sphere, refine it asoften as necessary, get rid of the object that describes the manifold since wenow no longer need it, and then finally warp the mesh. With the functionabove, this would look as follows:
* [1.x.90]
* 
* Note that the only essential addition is the line marked withasterisks. It is worth pointing out one other thing here, though: because wedetach the manifold description from the surface mesh, whenever we use amapping object in the rest of the program, it has no curves boundarydescription to go on any more. Rather, it will have to use the implicit,FlatManifold class that is used on all parts of the domain notexplicitly assigned a different manifold object. Consequently, whether we useMappingQ(2), MappingQ(15) or MappingQ1, each cell of our mesh will be mappedusing a bilinear approximation.
* All these drawbacks aside, the resulting pictures are still pretty. The onlyother differences to what's in  [2.x.113]  is that we changed the right hand sideto  [2.x.114]  and the boundary values (through the [2.x.115]  class) to  [2.x.116] . Ofcourse, we now no longer know the exact solution, so the computation of theerror at the end of  [2.x.117]  will yield a meaninglessnumber.
* 

* [1.x.91][1.x.92] [2.x.118] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-39_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] b.
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9]
* [1.x.10]
* In this program, we use the interior penalty method and Nitsche's weakboundary conditions to solve Poisson's equation. We use multigridmethods on locally refined meshes, which are generated using a bulkcriterion and a standard error estimator based on cell and faceresiduals. All operators are implemented using the MeshWorker interface.
* Like in  [2.x.2] , the discretization relies on finite element spaces,which are polynomial inside the mesh cells  [2.x.3] , buthave no continuity between cells. Since such functions have two valueson each interior face  [2.x.4] , one from each side, wedefine mean value and jump operators as follows: let[1.x.11]<sub>1</sub> and [1.x.12]<sub>2</sub> be the two cells sharinga face, and let the traces of functions [1.x.13] and theouter normal vectors [1.x.14][1.x.15] be labeledaccordingly. Then, on the face, we let[1.x.16]
* Note, that if such an expression contains a normal vector, theaveraging operator turns into a jump. The interior penalty method for the problem[1.x.17]becomes[1.x.18]
* 
* Here,  [2.x.5]  is the penalty parameter, which is chosen as follows:for a face [1.x.19] of a cell [1.x.20], compute the value[1.x.21]where [1.x.22] is the polynomial degree of the finite elementfunctions and  [2.x.6]  and  [2.x.7]  denote the  [2.x.8]  and  [2.x.9] dimensional Hausdorff measure of the correspondingobject. If the face is at the boundary, choose  [2.x.10] .For an interior face, we take the average of the two values at this face.
* In our finite element program, we distinguish three differentintegrals, corresponding to the sums over cells, interior faces andboundary faces above. Since the  [2.x.11]  organizes the sumsfor us, we only need to implement the integrals over each meshelement. The class MatrixIntegrator below has these three functionsfor the left hand side of the formula, the class RHSIntegrator for theright.
* As we will see below, even the error estimate is of the samestructure, since it can be written as
* [1.x.23]
* 
* Thus, the functions for assembling matrices, right hand side and errorestimates below exhibit that these loops are all generic and can beprogrammed in the same way.
* This program is related to  [2.x.12] b, in that it uses MeshWorker anddiscontinuous Galerkin methods. While there, we solved an advectionproblem, here it is a diffusion problem. Here, we also use multigridpreconditioning and a theoretically justified error estimator, seeKarakashian and Pascal (2003). The multilevel scheme was discussed indetail in Kanschat (2004). The adaptive iteration and its convergencehave been discussed (for triangular meshes) in Hoppe, Kanschat, andWarburton (2009).
* 

*  [1.x.24] [1.x.25]
*  The include files for the linear algebra: A regular SparseMatrix, which in turn will include the necessary files for SparsityPattern and Vector classes.
* 

* 
* [1.x.26]
* 
*  Include files for setting up the mesh
* 

* 
* [1.x.27]
* 
*  Include files for FiniteElement classes and DoFHandler.
* 

* 
* [1.x.28]
* 
*  The include files for using the MeshWorker framework
* 

* 
* [1.x.29]
* 
*  The include file for local integrators associated with the Laplacian
* 

* 
* [1.x.30]
* 
*  Support for multigrid methods
* 

* 
* [1.x.31]
* 
*  Finally, we take our exact solution from the library as well as quadrature and additional tools.
* 

* 
* [1.x.32]
* 
*  All classes of the deal.II library are in the namespace dealii. In order to save typing, we tell the compiler to search names in there as well.
* 

* 
* [1.x.33]
* 
*  This is the function we use to set the boundary values and also the exact solution we compare to.
* 

* 
* [1.x.34]
* 
*   [1.x.35]  [1.x.36]
* 

* 
*  MeshWorker separates local integration from the loops over cells and faces. Thus, we have to write local integration classes for generating matrices, the right hand side and the error estimator.
* 

* 
*  All these classes have the same three functions for integrating over cells, boundary faces and interior faces, respectively. All the information needed for the local integration is provided by  [2.x.13]  Note that the signature of the functions cannot be changed, because it is expected by  [2.x.14] 
* 

* 
*  The first class defining local integrators is responsible for computing cell and face matrices. It is used to assemble the global matrix as well as the level matrices.
* 

* 
* [1.x.37]
* 
*  On each cell, we integrate the Dirichlet form. We use the library of ready made integrals in LocalIntegrators to avoid writing these loops ourselves. Similarly, we implement Nitsche boundary conditions and the interior penalty fluxes between cells.   
*   The boundary and flux terms need a penalty parameter, which should be adjusted to the cell size and the polynomial degree. A safe choice of this parameter for constant coefficients can be found in  [2.x.15]  and we use this below.
* 

* 
* [1.x.38]
* 
*  Interior faces use the interior penalty method
* 

* 
* [1.x.39]
* 
*  The second local integrator builds the right hand side. In our example, the right hand side function is zero, such that only the boundary condition is set here in weak form.
* 

* 
* [1.x.40]
* 
*  The third local integrator is responsible for the contributions to the error estimate. This is the standard energy estimator due to Karakashian and Pascal (2003).
* 

* 
* [1.x.41]
* 
*  The cell contribution is the Laplacian of the discrete solution, since the right hand side is zero.
* 

* 
* [1.x.42]
* 
*  At the boundary, we use simply a weighted form of the boundary residual, namely the norm of the difference between the finite element solution and the correct boundary condition.
* 

* 
* [1.x.43]
* 
*  Finally, on interior faces, the estimator consists of the jumps of the solution and its normal derivative, weighted appropriately.
* 

* 
* [1.x.44]
* 
*  Finally we have an integrator for the error. Since the energy norm for discontinuous Galerkin problems not only involves the difference of the gradient inside the cells, but also the jump terms across faces and at the boundary, we cannot just use  [2.x.16]  Instead, we use the MeshWorker interface to compute the error ourselves.
* 

* 
*  There are several different ways to define this energy norm, but all of them are equivalent to each other uniformly with mesh size (some not uniformly with polynomial degree). Here, we choose [1.x.45]
* 

* 
*  

* 
* [1.x.46]
* 
*  Here we have the integration on cells. There is currently no good interface in MeshWorker that would allow us to access values of regular functions in the quadrature points. Thus, we have to create the vectors for the exact function's values and gradients inside the cell integrator. After that, everything is as before and we just add up the squares of the differences.
* 

* 
*  Additionally to computing the error in the energy norm, we use the capability of the mesh worker to compute two functionals at the same time and compute the [1.x.47]-error in the same loop. Obviously, this one does not have any jump terms and only appears in the integration on cells.
* 

* 
* [1.x.48]
* 
*   [1.x.49]  [1.x.50]
* 

* 
*  This class does the main job, like in previous examples. For a description of the functions declared here, please refer to the implementation below.
* 

* 
* [1.x.51]
* 
*  The member objects related to the discretization are here.
* 

* 
* [1.x.52]
* 
*  Then, we have the matrices and vectors related to the global discrete system.
* 

* 
* [1.x.53]
* 
*  Finally, we have a group of sparsity patterns and sparse matrices related to the multilevel preconditioner.  First, we have a level matrix and its sparsity pattern.
* 

* 
* [1.x.54]
* 
*  When we perform multigrid with local smoothing on locally refined meshes, additional matrices are required; see Kanschat (2004). Here is the sparsity pattern for these edge matrices. We only need one, because the pattern of the up matrix is the transpose of that of the down matrix. Actually, we do not care too much about these details, since the MeshWorker is filling these matrices.
* 

* 
* [1.x.55]
* 
*  The flux matrix at the refinement edge, coupling fine level degrees of freedom to coarse level.
* 

* 
* [1.x.56]
* 
*  The transpose of the flux matrix at the refinement edge, coupling coarse level degrees of freedom to fine level.
* 

* 
* [1.x.57]
* 
*  The constructor simply sets up the coarse grid and the DoFHandler. The FiniteElement is provided as a parameter to allow flexibility.
* 

* 
* [1.x.58]
* 
*  In this function, we set up the dimension of the linear system and the sparsity patterns for the global matrix as well as the level matrices.
* 

* 
* [1.x.59]
* 
*  First, we use the finite element to distribute degrees of freedom over the mesh and number them.
* 

* 
* [1.x.60]
* 
*  Then, we already know the size of the vectors representing finite element functions.
* 

* 
* [1.x.61]
* 
*  Next, we set up the sparsity pattern for the global matrix. Since we do not know the row sizes in advance, we first fill a temporary DynamicSparsityPattern object and copy it to the regular SparsityPattern once it is complete.
* 

* 
* [1.x.62]
* 
*  The global system is set up, now we attend to the level matrices. We resize all matrix objects to hold one matrix per level.
* 

* 
* [1.x.63]
* 
*  It is important to update the sparsity patterns after <tt>clear()</tt> was called for the level matrices, since the matrices lock the sparsity pattern through the SmartPointer and Subscriptor mechanism.
* 

* 
* [1.x.64]
* 
*  Now all objects are prepared to hold one sparsity pattern or matrix per level. What's left is setting up the sparsity patterns on each level.
* 

* 
* [1.x.65]
* 
*  These are roughly the same lines as above for the global matrix, now for each level.
* 

* 
* [1.x.66]
* 
*  Additionally, we need to initialize the transfer matrices at the refinement edge between levels. They are stored at the index referring to the finer of the two indices, thus there is no such object on level 0.
* 

* 
* [1.x.67]
* 
*  In this function, we assemble the global system matrix, where by global we indicate that this is the matrix of the discrete system we solve and it is covering the whole mesh.
* 

* 
* [1.x.68]
* 
*  First, we need t set up the object providing the values we integrate. This object contains all FEValues and FEFaceValues objects needed and also maintains them automatically such that they always point to the current cell. To this end, we need to tell it first, where and what to compute. Since we are not doing anything fancy, we can rely on their standard choice for quadrature rules.     
*   Since their default update flags are minimal, we add what we need additionally, namely the values and gradients of shape functions on all objects (cells, boundary and interior faces). Afterwards, we are ready to initialize the container, which will create all necessary FEValuesBase objects for integration.
* 

* 
* [1.x.69]
* 
*  This is the object into which we integrate local data. It is filled by the local integration routines in MatrixIntegrator and then used by the assembler to distribute the information into the global matrix.
* 

* 
* [1.x.70]
* 
*  Furthermore, we need an object that assembles the local matrix into the global matrix. These assembler objects have all the knowledge of the structures of the target object, in this case a SparseMatrix, possible constraints and the mesh structure.
* 

* 
* [1.x.71]
* 
*  Now comes the part we coded ourselves, the local integrator. This is the only part which is problem dependent.
* 

* 
* [1.x.72]
* 
*  Now, we throw everything into a  [2.x.17]  which here traverses all active cells of the mesh, computes cell and face matrices and assembles them into the global matrix. We use the variable <tt>dof_handler</tt> here in order to use the global numbering of degrees of freedom.
* 

* 
* [1.x.73]
* 
*  Now, we do the same for the level matrices. Not too surprisingly, this function looks like a twin of the previous one. Indeed, there are only two minor differences.
* 

* 
* [1.x.74]
* 
*  Obviously, the assembler needs to be replaced by one filling level matrices. Note that it automatically fills the edge matrices as well.
* 

* 
* [1.x.75]
* 
*  Here is the other difference to the previous function: we run over all cells, not only the active ones. And we use functions ending on  [2.x.18]  since we need the degrees of freedom on each level, not the global numbering.
* 

* 
* [1.x.76]
* 
*  Here we have another clone of the assemble function. The difference to assembling the system matrix consists in that we assemble a vector here.
* 

* 
* [1.x.77]
* 
*  Since this assembler allows us to fill several vectors, the interface is a little more complicated as above. The pointers to the vectors have to be stored in an AnyData object. While this seems to cause two extra lines of code here, it actually comes handy in more complex applications.
* 

* 
* [1.x.78]
* 
*  Now that we have coded all functions building the discrete linear system, it is about time that we actually solve it.
* 

* 
* [1.x.79]
* 
*  The solver of choice is conjugate gradient.
* 

* 
* [1.x.80]
* 
*  Now we are setting up the components of the multilevel preconditioner. First, we need transfer between grid levels. The object we are using here generates sparse matrices for these transfers.
* 

* 
* [1.x.81]
* 
*  Then, we need an exact solver for the matrix on the coarsest level.
* 

* 
* [1.x.82]
* 
*  While transfer and coarse grid solver are pretty much generic, more flexibility is offered for the smoother. First, we choose Gauss-Seidel as our smoothing method.
* 

* 
* [1.x.83]
* 
*  Do two smoothing steps on each level.
* 

* 
* [1.x.84]
* 
*  Since the SOR method is not symmetric, but we use conjugate gradient iteration below, here is a trick to make the multilevel preconditioner a symmetric operator even for nonsymmetric smoothers.
* 

* 
* [1.x.85]
* 
*  The smoother class optionally implements the variable V-cycle, which we do not want here.
* 

* 
* [1.x.86]
* 
*  Finally, we must wrap our matrices in an object having the required multiplication functions.
* 

* 
* [1.x.87]
* 
*  Now, we are ready to set up the V-cycle operator and the multilevel preconditioner.
* 

* 
* [1.x.88]
* 
*  Let us not forget the edge matrices needed because of the adaptive refinement.
* 

* 
* [1.x.89]
* 
*  After all preparations, wrap the Multigrid object into another object, which can be used as a regular preconditioner,
* 

* 
* [1.x.90]
* 
*  and use it to solve the system.
* 

* 
* [1.x.91]
* 
*  Another clone of the assemble function. The big difference to the previous ones is here that we also have an input vector.
* 

* 
* [1.x.92]
* 
*  The results of the estimator are stored in a vector with one entry per cell. Since cells in deal.II are not numbered, we have to create our own numbering in order to use this vector. For the assembler used below the information in which component of a vector the result is stored is transmitted by the user_index variable for each cell. We need to set this numbering up here.     
*   On the other hand, somebody might have used the user indices already. So, let's be good citizens and save them before tampering with them.
* 

* 
* [1.x.93]
* 
*  This starts like before,
* 

* 
* [1.x.94]
* 
*  but now we need to notify the info box of the finite element function we want to evaluate in the quadrature points. First, we create an AnyData object with this vector, which is the solution we just computed.
* 

* 
* [1.x.95]
* 
*  Then, we tell the  [2.x.19]  for cells, that we need the second derivatives of this solution (to compute the Laplacian). Therefore, the Boolean arguments selecting function values and first derivatives a false, only the last one selecting second derivatives is true.
* 

* 
* [1.x.96]
* 
*  On interior and boundary faces, we need the function values and the first derivatives, but not second derivatives.
* 

* 
* [1.x.97]
* 
*  And we continue as before, with the exception that the default update flags are already adjusted to the values and derivatives we requested above.
* 

* 
* [1.x.98]
* 
*  The assembler stores one number per cell, but else this is the same as in the computation of the right hand side.
* 

* 
* [1.x.99]
* 
*  Right before we return the result of the error estimate, we restore the old user indices.
* 

* 
* [1.x.100]
* 
*  Here we compare our finite element solution with the (known) exact solution and compute the mean quadratic error of the gradient and the function itself. This function is a clone of the estimation function right above.
* 

* 
*  Since we compute the error in the energy and the [1.x.101]-norm, respectively, our block vector needs two blocks here.
* 

* 
* [1.x.102]
* 
*  Create graphical output. We produce the filename by collating the name from its various components, including the refinement cycle that we output with two digits.
* 

* 
* [1.x.103]
* 
*  And finally the adaptive loop, more or less like in previous examples.
* 

* 
* [1.x.104]
* [1.x.105][1.x.106]
* 

* [1.x.107][1.x.108]
* First, the program produces the usual logfile here stored in <tt>deallog</tt>. It reads (with omission of intermediate steps)
* [1.x.109]
* 
* This log for instance shows that the number of conjugate gradientiteration steps is constant at approximately 15.
* [1.x.110][1.x.111]
* 

*  [2.x.20] Using the perl script <tt>postprocess.pl</tt>, we extract relevantdata into <tt>output.dat</tt>, which can be used to plot graphs with<tt>gnuplot</tt>. The graph above for instance was produced using the gnuplotscript <tt>plot_errors.gpl</tt> via
* [1.x.112]
* 
* Reference data can be found in <tt>output.reference.dat</tt>.
* 

* [1.x.113][1.x.114] [2.x.21] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-40_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18]
*  [2.x.2] 
* [1.x.19]
* 

* 
*  [2.x.3]  As a prerequisite of this program, you need to have both PETSc and thep4est library installed. The installation of deal.IItogether with these two additional libraries is described in the [1.x.20] file. Note also thatto work properly, this program needs access to the Hyprepreconditioner package implementing algebraic multigrid; it can beinstalled as part of PETSc but has to be explicitly enabled duringPETSc configuration; see the page linked to from the installationinstructions for PETSc.
* 

* [1.x.21][1.x.22][1.x.23]
* 

*  [2.x.4] 
* Given today's computers, most finite element computations can be done ona single machine. The majority of previous tutorial programs thereforeshows only this, possibly splitting up work among a number ofprocessors that, however, can all access the same, shared memoryspace. That said, there are problems that are simply too big for asingle machine and in that case the problem has to be split up in asuitable way among multiple machines each of which contributes itspart to the whole. A simple way to do that was shown in  [2.x.5]  and [2.x.6] , where we show how a program can use [1.x.24] to parallelizeassembling the linear system, storing it, solving it, and computingerror estimators. All of these operations scale relatively trivially(for a definition of what it means for an operation to "scale", see [2.x.7]  "this glossary entry"),but there was one significant drawback: for this to be moderatelysimple to implement, each MPI processor had to keep its own copy ofthe entire Triangulation and DoFHandler objects. Consequently, whilewe can suspect (with good reasons) that the operations listed abovecan scale to thousands of computers and problem sizes of billions ofcells and billions of degrees of freedom, building the one big mesh for theentire problem these thousands of computers are solving on every lastprocessor is clearly not going to scale: it is going to take forever,and maybe more importantly no single machine will have enough memoryto store a mesh that has a billion cells (at least not at the time ofwriting this). In reality, programs like  [2.x.8]  and  [2.x.9]  cantherefore not be run on more than maybe 100 or 200 processors and eventhere storing the Triangulation and DoFHandler objects consumes thevast majority of memory on each machine.
* Consequently, we need to approach the problem differently: to scale tovery large problems each processor can only store its own little pieceof the Triangulation and DoFHandler objects. deal.II implements such ascheme in the  [2.x.10]  namespace and the classestherein. It builds on an external library, [1.x.25] (a play on the expression[1.x.26] that describes the parallel storage of ahierarchically constructed mesh as a forest of quad- oroct-trees). You need to [1.x.27]but apart from that all of its workings are hidden under the surfaceof deal.II.
* In essence, what the  [2.x.11]  class andcode inside the DoFHandler class do is to splitthe global mesh so that every processor only stores a small bit it"owns" along with one layer of "ghost" cells that surround the ones itowns. What happens in the rest of the domain on which we want to solvethe partial differential equation is unknown to each processor and canonly be inferred through communication with other machines if suchinformation is needed. This implies that we also have to think aboutproblems in a different way than we did in, for example,  [2.x.12]  and [2.x.13] : no processor can have the entire solution vector forpostprocessing, for example, and every part of a program has to beparallelized because no processor has all the information necessaryfor sequential operations.
* A general overview of how this parallelization happens is described inthe  [2.x.14]  documentation module. You should read it for atop-level overview before reading through the source code of thisprogram. A concise discussion of many terms we will use in the programis also provided in the  [2.x.15]  "Distributed Computing paper".It is probably worthwhile reading it for background information on howthings work internally in this program.
* 

* [1.x.28][1.x.29]
* 

* This program essentially re-solves what we already do in [2.x.16] , i.e. it solves the Laplace equation
* [1.x.30]
* The difference of course is now that we want to do so on a mesh thatmay have a billion cells, with a billion or so degrees offreedom. There is no doubt that doing so is completely silly for sucha simple problem, but the point of a tutorial program is, after all,not to do something useful but to show how useful programs can beimplemented using deal.II. Be that as it may, to make things at leasta tiny bit interesting, we choose the right hand side as adiscontinuous function,
* [1.x.31]
* so that the solution has a singularity along the sinusoidal linesnaking its way through the domain. As a consequence, mesh refinementwill be concentrated along this line. You can see this in the meshpicture shown below in the results section.
* Rather than continuing here and giving a long introduction, let us gostraight to the program code. If you have read through  [2.x.17]  and the [2.x.18]  documentation module, most of things that are goingto happen should be familiar to you already. In fact, comparing the twoprograms you will notice that the additional effort necessary to make thingswork in %parallel is almost insignificant: the two programs have about thesame number of lines of code (though  [2.x.19]  spends more space on dealing withcoefficients and output). In either case, the comments below will only be onthe things that set  [2.x.20]  apart from  [2.x.21]  and that aren't already coveredin the  [2.x.22]  documentation module.
* 

* 
*  [2.x.23]  This program will be able to compute on as many processors as you wantto throw at it, and for as large a problem as you have the memory and patienceto solve. However, there [1.x.32] a limit: the number of unknowns can notexceed the largest number that can be stored with an object of type [2.x.24]  By default, this is an alias for <code>unsignedint</code>, which on most machines today is a 32-bit integer, limiting you tosome 4 billion (in reality, since this program uses PETSc, you will be limitedto half that as PETSc uses signed integers). However, this can be changedduring configuration to use 64-bit integers, see the ReadMe file. This willgive problem sizes you are unlikely to exceed anytime soon.
* 

*  [1.x.33] [1.x.34]
*   [1.x.35]  [1.x.36]
* 

* 
*  Most of the include files we need for this program have already been discussed in previous programs. In particular, all of the following should already be familiar friends:
* 

* 
* [1.x.37]
* 
*  This program can use either PETSc or Trilinos for its parallel algebra needs. By default, if deal.II has been configured with PETSc, it will use PETSc. Otherwise, the following few lines will check that deal.II has been configured with Trilinos and take that.
* 

* 
*  But there may be cases where you want to use Trilinos, even though deal.II hasalso* been configured with PETSc, for example to compare the performance of these two libraries. To do this, add the following \#define to the source code:  [2.x.25] 
* 

* 
*  Using this logic, the following lines will then import either the PETSc or Trilinos wrappers into the namespace `LA` (for "linear algebra). In the former case, we are also defining the macro `USE_PETSC_LA` so that we can detect if we are using PETSc (see solve() for an example where this is necessary).
* 

* 
* [1.x.39]
* 
*  The following, however, will be new or be used in new roles. Let's walk through them. The first of these will provide the tools of the  [2.x.26]  namespace that we will use to query things like the number of processors associated with the current MPI universe, or the number within this universe the processor this job runs on is:
* 

* 
* [1.x.40]
* 
*  The next one provides a class, ConditionOStream that allows us to write code that would output things to a stream (such as  [2.x.27]  on every processor but throws the text away on all but one of them. We could achieve the same by simply putting an  [2.x.28]  statement in front of each place where we may generate output, but this doesn't make the code any prettier. In addition, the condition whether this processor should or should not produce output to the screen is the same every time
* 
*  -  and consequently it should be simple enough to put it into the statements that generate output itself.
* 

* 
* [1.x.41]
* 
*  After these preliminaries, here is where it becomes more interesting. As mentioned in the  [2.x.29]  module, one of the fundamental truths of solving problems on large numbers of processors is that there is no way for any processor to store everything (e.g. information about all cells in the mesh, all degrees of freedom, or the values of all elements of the solution vector). Rather, every processor will [1.x.42] a few of each of these and, if necessary, may [1.x.43] about a few more, for example the ones that are located on cells adjacent to the ones this processor owns itself. We typically call the latter [1.x.44], [1.x.45] or [1.x.46]. The point of this discussion here is that we need to have a way to indicate which elements a particular processor owns or need to know of. This is the realm of the IndexSet class: if there are a total of  [2.x.30]  cells, degrees of freedom, or vector elements, associated with (non-negative) integral indices  [2.x.31] , then both the set of elements the current processor owns as well as the (possibly larger) set of indices it needs to know about are subsets of the set  [2.x.32] . IndexSet is a class that stores subsets of this set in an efficient format:
* 

* 
* [1.x.47]
* 
*  The next header file is necessary for a single function,  [2.x.33]  The role of this function will be explained below.
* 

* 
* [1.x.48]
* 
*  The final two, new header files provide the class  [2.x.34]  that provides meshes distributed across a potentially very large number of processors, while the second provides the namespace  [2.x.35]  that offers functions that can adaptively refine such distributed meshes:
* 

* 
* [1.x.49]
* 
*   [1.x.50]  [1.x.51]
* 

* 
*  Next let's declare the main class of this program. Its structure is almost exactly that of the  [2.x.36]  tutorial program. The only significant differences are:
* 

* 
* 
*  - The  [2.x.37]  variable that describes the set of processors we want this code to run on. In practice, this will be MPI_COMM_WORLD, i.e. all processors the batch scheduling system has assigned to this particular job.
* 

* 
* 
*  - The presence of the  [2.x.38]  variable of type ConditionOStream.
* 

* 
* 
*  - The obvious use of  [2.x.39]  instead of Triangulation.
* 

* 
* 
*  - The presence of two IndexSet objects that denote which sets of degrees of freedom (and associated elements of solution and right hand side vectors) we own on the current processor and which we need (as ghost elements) for the algorithms in this program to work.
* 

* 
* 
*  - The fact that all matrices and vectors are now distributed. We use either the PETSc or Trilinos wrapper classes so that we can use one of the sophisticated preconditioners offered by Hypre (with PETSc) or ML (with Trilinos). Note that as part of this class, we store a solution vector that does not only contain the degrees of freedom the current processor owns, but also (as ghost elements) all those vector elements that correspond to "locally relevant" degrees of freedom (i.e. all those that live on locally owned cells or the layer of ghost cells that surround it).
* 

* 
* [1.x.52]
* 
*   [1.x.53]  [1.x.54]
* 

* 
*   [1.x.55]  [1.x.56]
* 

* 
*  Constructors and destructors are rather trivial. In addition to what we do in  [2.x.40] , we set the set of processors we want to work on to all machines available (MPI_COMM_WORLD); ask the triangulation to ensure that the mesh remains smooth and free to refined islands, for example; and initialize the  [2.x.41]  variable to only allow processor zero to output anything. The final piece is to initialize a timer that we use to determine how much compute time the different parts of the program take:
* 

* 
* [1.x.57]
* 
*   [1.x.58]  [1.x.59]
* 

* 
*  The following function is, arguably, the most interesting one in the entire program since it goes to the heart of what distinguishes %parallel  [2.x.42]  from sequential  [2.x.43] .   
*   At the top we do what we always do: tell the DoFHandler object to distribute degrees of freedom. Since the triangulation we use here is distributed, the DoFHandler object is smart enough to recognize that on each processor it can only distribute degrees of freedom on cells it owns; this is followed by an exchange step in which processors tell each other about degrees of freedom on ghost cell. The result is a DoFHandler that knows about the degrees of freedom on locally owned cells and ghost cells (i.e. cells adjacent to locally owned cells) but nothing about cells that are further away, consistent with the basic philosophy of distributed computing that no processor can know everything.
* 

* 
* [1.x.60]
* 
*  The next two lines extract some information we will need later on, namely two index sets that provide information about which degrees of freedom are owned by the current processor (this information will be used to initialize solution and right hand side vectors, and the system matrix, indicating which elements to store on the current processor and which to expect to be stored somewhere else); and an index set that indicates which degrees of freedom are locally relevant (i.e. live on cells that the current processor owns or on the layer of ghost cells around the locally owned cells; we need all of these degrees of freedom, for example, to estimate the error on the local cells).
* 

* 
* [1.x.61]
* 
*  Next, let us initialize the solution and right hand side vectors. As mentioned above, the solution vector we seek does not only store elements we own, but also ghost entries; on the other hand, the right hand side vector only needs to have the entries the current processor owns since all we will ever do is write into it, never read from it on locally owned cells (of course the linear solvers will read from it, but they do not care about the geometric location of degrees of freedom).
* 

* 
* [1.x.62]
* 
*  The next step is to compute hanging node and boundary value constraints, which we combine into a single object storing all constraints.     
*   As with all other things in %parallel, the mantra must be that no processor can store all information about the entire universe. As a consequence, we need to tell the AffineConstraints object for which degrees of freedom it can store constraints and for which it may not expect any information to store. In our case, as explained in the  [2.x.44]  module, the degrees of freedom we need to care about on each processor are the locally relevant ones, so we pass this to the  [2.x.45]  function. As a side note, if you forget to pass this argument, the AffineConstraints class will allocate an array with length equal to the largest DoF index it has seen so far. For processors with high MPI process number, this may be very large
* 
*  -  maybe on the order of billions. The program would then allocate more memory than for likely all other operations combined for this single array.
* 

* 
* [1.x.63]
* 
*  The last part of this function deals with initializing the matrix with accompanying sparsity pattern. As in previous tutorial programs, we use the DynamicSparsityPattern as an intermediate with which we then initialize the system matrix. To do so we have to tell the sparsity pattern its size but as above there is no way the resulting object will be able to store even a single pointer for each global degree of freedom; the best we can hope for is that it stores information about each locally relevant degree of freedom, i.e. all those that we may ever touch in the process of assembling the matrix (the  [2.x.46]  "distributed computing paper" has a long discussion why one really needs the locally relevant, and not the small set of locally active degrees of freedom in this context).     
*   So we tell the sparsity pattern its size and what DoFs to store anything for and then ask  [2.x.47]  to fill it (this function ignores all cells that are not locally owned, mimicking what we will do below in the assembly process). After this, we call a function that exchanges entries in these sparsity pattern between processors so that in the end each processor really knows about all the entries that will exist in that part of the finite element matrix that it will own. The final step is to initialize the matrix with the sparsity pattern.
* 

* 
* [1.x.64]
* 
*   [1.x.65]  [1.x.66]
* 

* 
*  The function that then assembles the linear system is comparatively boring, being almost exactly what we've seen before. The points to watch out for are:
* 

* 
* 
*  - Assembly must only loop over locally owned cells. There are multiple ways to test that; for example, we could compare a cell's subdomain_id against information from the triangulation as in <code>cell->subdomain_id() == triangulation.locally_owned_subdomain()</code>, or skip all cells for which the condition <code>cell->is_ghost() || cell->is_artificial()</code> is true. The simplest way, however, is to simply ask the cell whether it is owned by the local processor.
* 

* 
* 
*  - Copying local contributions into the global matrix must include distributing constraints and boundary values. In other words, we cannot (as we did in  [2.x.48] ) first copy every local contribution into the global matrix and only in a later step take care of hanging node constraints and boundary values. The reason is, as discussed in  [2.x.49] , that the parallel vector classes do not provide access to arbitrary elements of the matrix once they have been assembled into it
* 
*  -  in parts because they may simply no longer reside on the current processor but have instead been shipped to a different machine.
* 

* 
* 
*  - The way we compute the right hand side (given the formula stated in the introduction) may not be the most elegant but will do for a program whose focus lies somewhere entirely different.
* 

* 
* [1.x.67]
* 
*  Notice that the assembling above is just a local operation. So, to form the "global" linear system, a synchronization between all processors is needed. This could be done by invoking the function compress(). See  [2.x.50]  "Compressing distributed objects" for more information on what is compress() designed to do.
* 

* 
* [1.x.68]
* 
*   [1.x.69]  [1.x.70]
* 

* 
*  Even though solving linear systems on potentially tens of thousands of processors is by far not a trivial job, the function that does this is
* 
*  -  at least at the outside
* 
*  -  relatively simple. Most of the parts you've seen before. There are really only two things worth mentioning:
* 

* 
* 
*  - Solvers and preconditioners are built on the deal.II wrappers of PETSc and Trilinos functionality. It is relatively well known that the primary bottleneck of massively %parallel linear solvers is not actually the communication between processors, but the fact that it is difficult to produce preconditioners that scale well to large numbers of processors. Over the second half of the first decade of the 21st century, it has become clear that algebraic multigrid (AMG) methods turn out to be extremely efficient in this context, and we will use one of them
* 
*  -  either the BoomerAMG implementation of the Hypre package that can be interfaced to through PETSc, or a preconditioner provided by ML, which is part of Trilinos
* 
*  -  for the current program. The rest of the solver itself is boilerplate and has been shown before. Since the linear system is symmetric and positive definite, we can use the CG method as the outer solver.
* 

* 
* 
*  - Ultimately, we want a vector that stores not only the elements of the solution for degrees of freedom the current processor owns, but also all other locally relevant degrees of freedom. On the other hand, the solver itself needs a vector that is uniquely split between processors, without any overlap. We therefore create a vector at the beginning of this function that has these properties, use it to solve the linear system, and only assign it to the vector we want at the very end. This last step ensures that all ghost elements are also copied as necessary.
* 

* 
* [1.x.71]
* 
*   [1.x.72]  [1.x.73]
* 

* 
*  The function that estimates the error and refines the grid is again almost exactly like the one in  [2.x.51] . The only difference is that the function that flags cells to be refined is now in namespace  [2.x.52] 
* 
*  -  a namespace that has functions that can communicate between all involved processors and determine global thresholds to use in deciding which cells to refine and which to coarsen.   
*   Note that we didn't have to do anything special about the KellyErrorEstimator class: we just give it a vector with as many elements as the local triangulation has cells (locally owned cells, ghost cells, and artificial ones), but it only fills those entries that correspond to cells that are locally owned.
* 

* 
* [1.x.74]
* 
*   [1.x.75]  [1.x.76]
* 

* 
*  Compared to the corresponding function in  [2.x.53] , the one here is a tad more complicated. There are two reasons: the first one is that we do not just want to output the solution but also for each cell which processor owns it (i.e. which "subdomain" it is in). Secondly, as discussed at length in  [2.x.54]  and  [2.x.55] , generating graphical data can be a bottleneck in parallelizing. In  [2.x.56] , we have moved this step out of the actual computation but shifted it into a separate program that later combined the output from various processors into a single file. But this doesn't scale: if the number of processors is large, this may mean that the step of combining data on a single processor later becomes the longest running part of the program, or it may produce a file that's so large that it can't be visualized any more. We here follow a more sensible approach, namely creating individual files for each MPI process and leaving it to the visualization program to make sense of that.   
*   To start, the top of the function looks like it usually does. In addition to attaching the solution vector (the one that has entries for all locally relevant, not only the locally owned, elements), we attach a data vector that stores, for each cell, the subdomain the cell belongs to. This is slightly tricky, because of course not every processor knows about every cell. The vector we attach therefore has an entry for every cell that the current processor has in its mesh (locally owned ones, ghost cells, and artificial cells), but the DataOut class will ignore all entries that correspond to cells that are not owned by the current processor. As a consequence, it doesn't actually matter what values we write into these vector entries: we simply fill the entire vector with the number of the current MPI process (i.e. the subdomain_id of the current process); this correctly sets the values we care for, i.e. the entries that correspond to locally owned cells, while providing the wrong value for all other elements
* 
*  -  but these are then ignored anyway.
* 

* 
* [1.x.77]
* 
*  The next step is to write this data to disk. We write up to 8 VTU files in parallel with the help of MPI-IO. Additionally a PVTU record is generated, which groups the written VTU files.
* 

* 
* [1.x.78]
* 
*   [1.x.79]  [1.x.80]
* 

* 
*  The function that controls the overall behavior of the program is again like the one in  [2.x.57] . The minor difference are the use of  [2.x.58]  for output to the console (see also  [2.x.59] ) and that we only generate graphical output if at most 32 processors are involved. Without this limit, it would be just too easy for people carelessly running this program without reading it first to bring down the cluster interconnect and fill any file system available :-)   
*   A functional difference to  [2.x.60]  is the use of a square domain and that we start with a slightly finer mesh (5 global refinement cycles)
* 
*  -  there just isn't much of a point showing a massively %parallel program starting on 4 cells (although admittedly the point is only slightly stronger starting on 1024).
* 

* 
* [1.x.81]
* 
*   [1.x.82]  [1.x.83]
* 

* 
*  The final function,  [2.x.61] , again has the same structure as in all other programs, in particular  [2.x.62] . Like the other programs that use MPI, we have to initialize and finalize MPI, which is done using the helper object  [2.x.63]  The constructor of that class also initializes libraries that depend on MPI, such as p4est, PETSc, SLEPc, and Zoltan (though the last two are not used in this tutorial). The order here is important: we cannot use any of these libraries until they are initialized, so it does not make sense to do anything before creating an instance of  [2.x.64] 
* 

* 
*  After the solver finishes, the LaplaceProblem destructor will run followed by  [2.x.65]  This order is also important:  [2.x.66]  calls  [2.x.67]  (and finalization functions for other libraries), which will delete any in-use PETSc objects. This must be done after we destruct the Laplace solver to avoid double deletion errors. Fortunately, due to the order of destructor call rules of C++, we do not need to worry about any of this: everything happens in the correct order (i.e., the reverse of the order of construction). The last function called by  [2.x.68]  is  [2.x.69] : i.e., once this object is destructed the program should exit since MPI will no longer be available.
* 

* 
* [1.x.84]
* [1.x.85][1.x.86]
* 

* When you run the program, on a single processor or with your local MPIinstallation on a few, you should get output like this:
* [1.x.87]
* 
* The exact numbers differ, depending on how many processors we use;this is due to the fact that the preconditioner depends on thepartitioning of the problem, the solution then differs in the last fewdigits, and consequently the mesh refinement differs slightly.The primary thing to notice here, though, is that the number ofiterations does not increase with the size of the problem. Thisguarantees that we can efficiently solve even the largest problems.
* When run on a sufficiently large number of machines (say a fewthousand), this program can relatively easily solve problems with wellover one billion unknowns in less than a minute. On the other hand,such big problems can no longer be visualized, so we also ran theprogram on only 16 processors. Here are a mesh, along with itspartitioning onto the 16 processors, and the corresponding solution:
*  [2.x.70] 
* The mesh on the left has a mere 7,069 cells. This is of course aproblem we would easily have been able to solve already on a singleprocessor using  [2.x.71] , but the point of the program was to show howto write a program that scales to many more machines. For example,here are two graphs that show how the run time of a large number of partsof the program scales on problems with around 52 and 375 million degrees offreedom if we take more and more processors (these and the next couple ofgraphs are taken from an earlier version of the [2.x.72]  "Distributed Computing paper"; updated graphs showingdata of runs on even larger numbers of processors, and a lotmore interpretation can be found in the final version of the paper):
*  [2.x.73] 
* As can clearly be seen, the program scales nicely to very largenumbers of processors.(For a discussion of what we consider "scalable" programs, see [2.x.74]  "this glossary entry".)The curves, in particular the linear solver, become abit wobbly at the right end of the graphs since each processor has too littleto do to offset the cost of communication (the part of the whole problem eachprocessor has to solve in the above two examples is only 13,000 and 90,000degrees of freedom when 4,096 processors are used; a good rule of thumb is thatparallel programs work well if each processor has at least 100,000 unknowns).
* While the strong scaling graphs above show that we can solve a problem offixed size faster and faster if we take more and more processors, the moreinteresting question may be how big problems can become so that they can stillbe solved within a reasonable time on a machine of a particular size. We showthis in the following two graphs for 256 and 4096 processors:
*  [2.x.75] 
* What these graphs show is that all parts of the program scale linearly withthe number of degrees of freedom. This time, lines are wobbly at the left asthe size of local problems is too small. For more discussions of these resultswe refer to the  [2.x.76]  "Distributed Computing paper".
* So how large are the largest problems one can solve? At the time of writingthis problem, thelimiting factor is that the program uses the BoomerAMG algebraicmultigrid method from the [1.x.88] asa preconditioner, which unfortunately uses signed 32-bit integers toindex the elements of a %distributed matrix. This limits the size ofproblems to  [2.x.77]  degrees of freedom. From the graphsabove it is obvious that the scalability would extend beyond thisnumber, and one could expect that given more than the 4,096 machinesshown above would also further reduce the compute time. That said, onecan certainly expect that this limit will eventually be lifted by thehypre developers.
* On the other hand, this does not mean that deal.II cannot solve biggerproblems. Indeed,  [2.x.78]  shows how one can solve problems that are notjust a little, but very substantially larger than anything we have shownhere.
* 

* 
* [1.x.89][1.x.90][1.x.91]
* 

* In a sense, this program is the ultimate solver for the Laplaceequation: it can essentially solve the equation to whatever accuracyyou want, if only you have enough processors available. Since theLaplace equation by itself is not terribly interesting at this levelof accuracy, the more interesting possibilities for extensiontherefore concern not so much this program but what comes beyondit. For example, several of the other programs in this tutorial havesignificant run times, especially in 3d. It would therefore beinteresting to use the techniques explained here to extend otherprograms to support parallel distributed computations. We have donethis for  [2.x.79]  in the  [2.x.80]  tutorial program, but the same wouldapply to, for example,  [2.x.81]  and  [2.x.82]  for hyperbolic timedependent problems,  [2.x.83]  for gas dynamics, or  [2.x.84]  for theNavier-Stokes equations.
* Maybe equally interesting is the problem of postprocessing. Asmentioned above, we only show pictures of the solution and the meshfor 16 processors because 4,096 processors solving 1 billion unknownswould produce graphical output on the order of several 10gigabyte. Currently, no program is able to visualize this amount ofdata in any reasonable way unless it also runs on at least severalhundred processors. There are, however, approaches where visualizationprograms directly communicate with solvers on each processor with eachvisualization process rendering the part of the scene computed by thesolver on this processor. Implementing such an interface would allowto quickly visualize things that are otherwise not amenable tographical display.
* 

* [1.x.92][1.x.93] [2.x.85] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-4_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18]
* [1.x.19][1.x.20][1.x.21]
* 

*  [2.x.2] 
* deal.II has a unique feature which we call``dimension independent programming''. You may have noticed in theprevious examples that many classes had a number in angle bracketssuffixed to them. This is to indicate that for example thetriangulation in two and three space dimensions are different, butrelated data %types. We could as well have called them [2.x.3]  insteadof  [2.x.4]  and [2.x.5]  to name the two classes, but thishas an important drawback: assume you have a function which doesexactly the same functionality, but on 2d or 3d triangulations,depending on which dimension we would like to solve the equation inpresently (if you don't believe that it is the common case that afunction does something that is the same in all dimensions, just takea look at the code below
* 
*  - there are almost no distinctions between 2dand 3d!). We would have to write the same function twice, onceworking on  [2.x.6]  and once working with a [2.x.7] . This is an unnecessary obstacle inprogramming and leads to a nuisance to keep the two function in sync(at best) or difficult to find errors if the two versions get out ofsync (at worst; this would probably the more common case).
* 

* 
* 

* Such obstacles can be circumvented by using some template magic asprovided by the C++ language: templatized classes and functions arenot really classes or functions but only a pattern depending on anas-yet undefined data type parameter or on a numerical value which isalso unknown at the point of definition. However, the compiler canbuild proper classes or functions from these templates if you provideit with the information that is needed for that. Of course, parts ofthe template can depend on the template parameters, and they will beresolved at the time of compilation for a specific templateparameter. For example, consider the following piece of code:
* [1.x.22]
* 
* 

* 
* At the point where the compiler sees this function, it does not knowanything about the actual value of  [2.x.8] . The only thing the compiler has isa template, i.e. a blueprint, to generatefunctions  [2.x.9]  if given a particular value of [2.x.10]  has an unknown value, there is nocode the compiler can generate for the moment.
* 

* 
* However, if later down the compiler would encounter code that looks, forexample, like this,
* [1.x.23]
* then the compiler will deduce that the function  [2.x.11]  for [2.x.12]  wasrequested and will compile the template above into a function with dim replacedby 2 everywhere, i.e. it will compile the function as if it were definedas
* [1.x.24]
* 
* 

* 
* However, it is worth to note that the function [2.x.13]  depends on the dimension aswell, so in this case, the compiler will call the function [2.x.14]  while if dim were 3,it would call  [2.x.15]  whichmight be (and actually is) a totally unrelated  function.
* 

* 
* The same can be done with member variables. Consider the followingfunction, which might in turn call the above one:
* [1.x.25]
* This function has a member variable of type [2.x.16] . Again, the compiler can'tcompile this function until it knows for which dimension. If you callthis function for a specific dimension as above, the compiler willtake the template, replace all occurrences of dim by the dimension forwhich it was called, and compile it. If you call the function severaltimes for different dimensions, it will compile it several times, eachtime calling the right  [2.x.17]  function and reserving the rightamount of memory for the member variable; note that the size of a [2.x.18]  might, and indeed does, depend on the space dimension.
* 

* 
* The deal.II library is built around this conceptof dimension-independent programming, and therefore allows you to program ina way that will not need todistinguish between the space dimensions. It should be noted that inonly a very few places is it necessary to actually compare thedimension using  [2.x.19] es. However, since the compilerhas to compile each function for each dimension separately, even thereit knows the value of  [2.x.20]  at the time of compilation and willtherefore be able to optimize away the  [2.x.21]  statement along with theunused branch.
* 

* 
* In this example program, we will show how to program dimensionindependently (which in fact is even simpler than if you had to takecare about the dimension) and we will extend the Laplace problem ofthe last example to a program that runs in two and three spacedimensions at the same time. Other extensions are the use of anon-constant right hand side function and of non-zero boundary values.
* 

* 
*  [2.x.22]  When using templates, C++ imposes all sorts of syntax constraints thatmake it sometimes a bit difficult to understand why exactly something has tobe written this way. A typical example is the need to use the keyword [2.x.23]  in so many places. If you are not entirely familiar withthis already, then several of these difficulties are explained in the deal.IIFrequently Asked Questions (FAQ) linked to from the [1.x.26].
* <!--We need a blank line to end the above block properly.-->
* 

*  [1.x.27] [1.x.28]
*   [1.x.29]  [1.x.30]
* 

* 
*  The first few (many?) include files have already been used in the previous example, so we will not explain their meaning here again.
* 

* 
* [1.x.31]
* 
*  This is new, however: in the previous example we got some unwanted output from the linear solvers. If we want to suppress it, we have to include this file and add a single line somewhere to the program (see the main() function below for that):
* 

* 
* [1.x.32]
* 
*  The final step, as in previous programs, is to import all the deal.II class and function names into the global namespace:
* 

* 
* [1.x.33]
* 
*   [1.x.34]  [1.x.35]
* 

* 
*  This is again the same  [2.x.24]  class as in the previous example. The only difference is that we have now declared it as a class with a template parameter, and the template parameter is of course the spatial dimension in which we would like to solve the Laplace equation. Of course, several of the member variables depend on this dimension as well, in particular the Triangulation class, which has to represent quadrilaterals or hexahedra, respectively. Apart from this, everything is as before.
* 

* 
* [1.x.36]
* 
*   [1.x.37]  [1.x.38]
* 

* 
*  In the following, we declare two more classes denoting the right hand side and the non-homogeneous Dirichlet boundary values. Both are functions of a dim-dimensional space variable, so we declare them as templates as well.
* 

* 
*  Each of these classes is derived from a common, abstract base class Function, which declares the common interface which all functions have to follow. In particular, concrete classes have to overload the  [2.x.25]  function, which takes a point in dim-dimensional space as parameters and returns the value at that point as a  [2.x.26]  variable.
* 

* 
*  The  [2.x.27]  function takes a second argument, which we have here named  [2.x.28] : This is only meant for vector-valued functions, where you may want to access a certain component of the vector at the point  [2.x.29] . However, our functions are scalar, so we need not worry about this parameter and we will not use it in the implementation of the functions. Inside the library's header files, the Function base class's declaration of the  [2.x.30]  function has a default value of zero for the component, so we will access the  [2.x.31]  function of the right hand side with only one parameter, namely the point where we want to evaluate the function. A value for the component can then simply be omitted for scalar functions.
* 

* 
*  Function objects are used in lots of places in the library (for example, in  [2.x.32]  we used a  [2.x.33]  instance as an argument to  [2.x.34]  and this is the first tutorial where we define a new class that inherits from Function. Since we only ever call  [2.x.35]  we could get away with just a plain function (and this is what is done in  [2.x.36] ), but since this is a tutorial we inherit from Function for the sake of example.
* 

* 
* [1.x.39]
* 
*  If you are not familiar with what the keywords `virtual` and `override` in the function declarations above mean, you will probably want to take a look at your favorite C++ book or an online tutorial such as http://www.cplusplus.com/doc/tutorial/polymorphism/ . In essence, what is happening here is that Function<dim> is an "abstract" base class that declares a certain "interface"
* 
*  -  a set of functions one can call on objects of this kind. But it does not actuallyimplement* these functions: it just says "this is how Function objects look like", but what kind of function it actually is, is left to derived classes that implement the `value()` function.
* 

* 
*  Deriving one class from another is often called an "is-a" relationship function. Here, the `RightHandSide` class "is a" Function class because it implements the interface described by the Function base class. (The actual implementation of the `value()` function is in the code block below.) The `virtual` keyword then means "Yes, the function here is one that can be overridden by derived classes", and the `override` keyword means "Yes, this is in fact a function we know has been declared as part of the base class". The `override` keyword is not strictly necessary, but is an insurance against typos: If we get the name of the function or the type of one argument wrong, the compiler will warn us by stating "You say that this function overrides one in a base class, but I don't actually know any such function with this name and these arguments."
* 

* 
*  But back to the concrete case here: For this tutorial, we choose as right hand side the function  [2.x.37]  in 2D, or  [2.x.38]  in 3D. We could write this distinction using an if-statement on the space dimension, but here is a simple way that also allows us to use the same function in 1D (or in 4D, if you should desire to do so), by using a short loop.  Fortunately, the compiler knows the size of the loop at compile time (remember that at the time when you define the template, the compiler doesn't know the value of  [2.x.39] , but when it later encounters a statement or declaration  [2.x.40] , it will take the template, replace all occurrences of dim by 2 and compile the resulting function).  In other words, at the time of compiling this function, the number of times the body will be executed is known, and the compiler can minimize the overhead needed for the loop; the result will be as fast as if we had used the formulas above right away.
* 

* 
*  The last thing to note is that a  [2.x.41]  denotes a point in dim-dimensional space, and its individual components (i.e.  [2.x.42] ,  [2.x.43] , ... coordinates) can be accessed using the () operator (in fact, the [] operator will work just as well) with indices starting at zero as usual in C and C++.
* 

* 
* [1.x.40]
* 
*  As boundary values, we choose  [2.x.44]  in 2D, and  [2.x.45]  in 3D. This happens to be equal to the square of the vector from the origin to the point at which we would like to evaluate the function, irrespective of the dimension. So that is what we return:
* 

* 
* [1.x.41]
* 
*   [1.x.42]  [1.x.43]
* 

* 
*  Next for the implementation of the class template that makes use of the functions above. As before, we will write everything as templates that have a formal parameter  [2.x.46]  that we assume unknown at the time we define the template functions. Only later, the compiler will find a declaration of  [2.x.47]  function, actually) and compile the entire class with  [2.x.48]  replaced by 2, a process referred to as `instantiation of a template'. When doing so, it will also replace instances of  [2.x.49]  by  [2.x.50]  and instantiate the latter class from the class template.
* 

* 
*  In fact, the compiler will also find a declaration  [2.x.51]  in  [2.x.52] . This will cause it to again go back to the general  [2.x.53]  template, replace all occurrences of  [2.x.54] , this time by 3, and compile the class a second time. Note that the two instantiations  [2.x.55]  and  [2.x.56]  are completely independent classes; their only common feature is that they are both instantiated from the same general template, but they are not convertible into each other, for example, and share no code (both instantiations are compiled completely independently).
* 

* 
*  
*  
*  [1.x.44]  [1.x.45]
* 

* 
*  After this introduction, here is the constructor of the  [2.x.57]  class. It specifies the desired polynomial degree of the finite elements and associates the DoFHandler to the triangulation just as in the previous example program,  [2.x.58] :
* 

* 
* [1.x.46]
* 
*   [1.x.47]  [1.x.48]
* 

* 
*  Grid creation is something inherently dimension dependent. However, as long as the domains are sufficiently similar in 2D or 3D, the library can abstract for you. In our case, we would like to again solve on the square  [2.x.59]  in 2D, or on the cube  [2.x.60]  in 3D; both can be termed  [2.x.61]  so we may use the same function in whatever dimension we are. Of course, the functions that create a hypercube in two and three dimensions are very much different, but that is something you need not care about. Let the library handle the difficult things.
* 

* 
* [1.x.49]
* 
*   [1.x.50]  [1.x.51]
* 

* 
*  This function looks exactly like in the previous example, although it performs actions that in their details are quite different if  [2.x.62]  happens to be 3. The only significant difference from a user's perspective is the number of cells resulting, which is much higher in three than in two space dimensions!
* 

* 
* [1.x.52]
* 
*   [1.x.53]  [1.x.54]
* 

* 
*  Unlike in the previous example, we would now like to use a non-constant right hand side function and non-zero boundary values. Both are tasks that are readily achieved with only a few new lines of code in the assemblage of the matrix and right hand side.
* 

* 
*  More interesting, though, is the way we assemble matrix and right hand side vector dimension independently: there is simply no difference to the two-dimensional case. Since the important objects used in this function (quadrature formula, FEValues) depend on the dimension by way of a template parameter as well, they can take care of setting up properly everything for the dimension for which this function is compiled. By declaring all classes which might depend on the dimension using a template parameter, the library can make nearly all work for you and you don't have to care about most things.
* 

* 
* [1.x.55]
* 
*  We wanted to have a non-constant right hand side, so we use an object of the class declared above to generate the necessary data. Since this right hand side object is only used locally in the present function, we declare it here as a local variable:
* 

* 
* [1.x.56]
* 
*  Compared to the previous example, in order to evaluate the non-constant right hand side function we now also need the quadrature points on the cell we are presently on (previously, we only required values and gradients of the shape function from the FEValues object, as well as the quadrature weights,  [2.x.63]  ). We can tell the FEValues object to do for us by also giving it the #update_quadrature_points flag:
* 

* 
* [1.x.57]
* 
*  We then again define the same abbreviation as in the previous program. The value of this variable of course depends on the dimension which we are presently using, but the FiniteElement class does all the necessary work for you and you don't have to care about the dimension dependent parts:
* 

* 
* [1.x.58]
* 
*  Next, we again have to loop over all cells and assemble local contributions.  Note, that a cell is a quadrilateral in two space dimensions, but a hexahedron in 3D. In fact, the  [2.x.64]  data type is something different, depending on the dimension we are in, but to the outside world they look alike and you will probably never see a difference. In any case, the real type is hidden by using `auto`:
* 

* 
* [1.x.59]
* 
*  Now we have to assemble the local matrix and right hand side. This is done exactly like in the previous example, but now we revert the order of the loops (which we can safely do since they are independent of each other) and merge the loops for the local matrix and the local vector as far as possible to make things a bit faster.       
*   Assembling the right hand side presents the only significant difference to how we did things in  [2.x.65] : Instead of using a constant right hand side with value 1, we use the object representing the right hand side and evaluate it at the quadrature points:
* 

* 
* [1.x.60]
* 
*  As a final remark to these loops: when we assemble the local contributions into  [2.x.66] , we have to multiply the gradients of shape functions  [2.x.67]  and  [2.x.68]  at point number q_index and multiply it with the scalar weights JxW. This is what actually happens:  [2.x.69]  returns a  [2.x.70]  dimensional vector, represented by a  [2.x.71]  object, and the operator* that multiplies it with the result of  [2.x.72]  makes sure that the  [2.x.73]  components of the two vectors are properly contracted, and the result is a scalar floating point number that then is multiplied with the weights. Internally, this operator* makes sure that this happens correctly for all  [2.x.74]  components of the vectors, whether  [2.x.75]  be 2, 3, or any other space dimension; from a user's perspective, this is not something worth bothering with, however, making things a lot simpler if one wants to write code dimension independently.
* 

* 
*  With the local systems assembled, the transfer into the global matrix and right hand side is done exactly as before, but here we have again merged some loops for efficiency:
* 

* 
* [1.x.61]
* 
*  As the final step in this function, we wanted to have non-homogeneous boundary values in this example, unlike the one before. This is a simple task, we only have to replace the  [2.x.76]  used there by an object of the class which describes the boundary values we would like to use (i.e. the  [2.x.77]  class declared above):   
*   The function  [2.x.78]  will only work on faces that have been marked with boundary indicator 0 (because that's what we say the function should work on with the second argument below). If there are faces with boundary id other than 0, then the function interpolate_boundary_values will do nothing on these faces. For the Laplace equation doing nothing is equivalent to assuming that on those parts of the boundary a zero Neumann boundary condition holds.
* 

* 
* [1.x.62]
* 
*   [1.x.63]  [1.x.64]
* 

* 
*  Solving the linear system of equations is something that looks almost identical in most programs. In particular, it is dimension independent, so this function is copied verbatim from the previous example.
* 

* 
* [1.x.65]
* 
*  We have made one addition, though: since we suppress output from the linear solvers, we have to print the number of iterations by hand.
* 

* 
* [1.x.66]
* 
*   [1.x.67]  [1.x.68]
* 

* 
*  This function also does what the respective one did in  [2.x.79] . No changes here for dimension independence either.
* 

* 
*  Since the program will run both 2d and 3d versions of the Laplace solver, we use the dimension in the filename to generate distinct filenames for each run (in a better program, one would check whether  [2.x.80]  can have other values than 2 or 3, but we neglect this here for the sake of brevity).
* 

* 
* [1.x.69]
* 
*   [1.x.70]  [1.x.71]
* 

* 
*  This is the function which has the top-level control over everything. Apart from one line of additional output, it is the same as for the previous example.
* 

* 
* [1.x.72]
* 
*   [1.x.73]  [1.x.74]
* 

* 
*  And this is the main function. It also looks mostly like in  [2.x.81] , but if you look at the code below, note how we first create a variable of type  [2.x.82]  (forcing the compiler to compile the class template with  [2.x.83] ) and run a 2d simulation, and then we do the whole thing over in 3d.
* 

* 
*  In practice, this is probably not what you would do very frequently (you probably either want to solve a 2d problem, or one in 3d, but not both at the same time). However, it demonstrates the mechanism by which we can simply change which dimension we want in a single place, and thereby force the compiler to recompile the dimension independent class templates for the dimension we request. The emphasis here lies on the fact that we only need to change a single place. This makes it rather trivial to debug the program in 2d where computations are fast, and then switch a single place to a 3 to run the much more computing intensive program in 3d for `real' computations.
* 

* 
*  Each of the two blocks is enclosed in braces to make sure that the  [2.x.84]  variable goes out of scope (and releases the memory it holds) before we move on to allocate memory for the 3d case. Without the additional braces, the  [2.x.85]  variable would only be destroyed at the end of the function, i.e. after running the 3d problem, and would needlessly hog memory while the 3d run could actually use it.
* 

* 
* [1.x.75]
* [1.x.76][1.x.77]
* 

* 
* The output of the program looks as follows (the number of iterationsmay vary by one or two, depending on your computer, since this isoften dependent on the round-off accuracy of floating pointoperations, which differs between processors):
* [1.x.78]
* It is obvious that in three spatial dimensions the number of cells andtherefore also the number of degrees of freedom ismuch higher. What cannot be seen here, is that besides this highernumber of rows and columns in the matrix, there are also significantlymore entries per row of the matrix in three spacedimensions. Together, this leads to a much higher numerical effort forsolving the system of equation, which you can feel in the run time of the twosolution steps when you actually run the program.
* 

* 
* The program produces two files:  [2.x.86]  and [2.x.87] , which can be viewed using the programsVisIt or Paraview (in case you do not have these programs, you can easilychange theoutput format in the program to something which you can view moreeasily). Visualizing solutions is a bit of an art, but it can also be fun, soyou should play around with your favorite visualization tool to get familiarwith its functionality. Here's what I have come up with for the 2d solution:
*  [2.x.88] 
* ( [2.x.89] The picture shows the solution of the problem under consideration asa 3D plot. As can be seen, the solution is almost flat in the interiorof the domain and has a higher curvature near the boundary. This, ofcourse, is due to the fact that for Laplace's equation the curvatureof the solution is equal to the right hand side and that was chosen asa quartic polynomial which is nearly zero in the interior and is onlyrising sharply when approaching the boundaries of the domain; themaximal values of the right hand side function are at the corners ofthe domain, where also the solution is moving most rapidly.It is also nice to see that the solution follows the desired quadraticboundary values along the boundaries of the domain.It can also be useful to verify a computed solution against an analyticalsolution. For an explanation of this technique, see  [2.x.90] .
* On the other hand, even though the picture does not show the mesh linesexplicitly, you can see them as little kinks in the solution. This clearlyindicates that the solution hasn't been computed to very high accuracy andthat to get a better solution, we may have to compute on a finer mesh.
* In three spatial dimensions, visualization is a bit more difficult. The leftpicture shows the solution and the mesh it was computed on on the surface ofthe domain. This is nice, but it has the drawback that it completely hideswhat is happening on the inside. The picture on the right is an attempt atvisualizing the interior as well, by showing surfaces where the solution hasconstant values (as indicated by the legend at the top left). Isosurfacepictures look best if one makes the individual surfaces slightly transparentso that it is possible to see through them and see what's behind.
*  [2.x.91] 
*  [2.x.92] A final remark on visualization: the idea of visualization is to give insight,which is not the same as displaying information. In particular, it is easy tooverload a picture with information, but while it shows more information itmakes it also more difficult to glean insight. As an example, the program Iused to generate these pictures, VisIt, by default puts tick marks on everyaxis, puts a big fat label "X Axis" on the  [2.x.93]  axis and similar for the otheraxes, shows the file name from which the data was taken in the top left andthe name of the user doing so and the time and date on the bottom right. Noneof this is importanthere: the axes are equally easy to make out because the tripod at the bottomleft is still visible, and we know from the program that the domain is [2.x.94] , so there is no need for tick marks. As a consequence, I haveswitched off all the extraneous stuff in the picture: the art of visualizationis to reduce the picture to those parts that are important to see what onewants to see, but no more.
* 

* 
* [1.x.79][1.x.80][1.x.81]
* 

* 
* Essentially the possibilities for playing around with the program are the sameas for the previous one, except that they will now also apply to the 3dcase. For inspiration read up on [1.x.82].
* 

* [1.x.83][1.x.84] [2.x.95] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-41_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27]
*  [2.x.2] 
* [1.x.28]
* 

* [1.x.29][1.x.30][1.x.31]
* 

* This example is based on the Laplace equation in 2d and deals with thequestion what happens if a membrane is deflected by some external force but isalso constrained by an obstacle. In other words, think of a elastic membraneclamped at the boundary to a rectangular frame (we choose  [2.x.3] ) and that sags through due to gravity acting on it. Whathappens now if there is an obstacle under the membrane that prevents it fromreaching its equilibrium position if gravity was the only existing force? Inthe current example program, we will consider that under the membrane is astair step obstacle against which gravity pushes the membrane.
* This problem is typically called the "obstacle problem" (see also [1.x.32]), and it results in avariational inequality, rather than a variational equation when put into theweak form. We will below derive it from the classical formulation, but before wego on to discuss the mathematics let us show how the solution of the problem wewill consider in this tutorial program looks to gain some intuition of whatwe should expect:
*  [2.x.4] 
* Here, at the left, we see the displacement of the membrane. The shapeof the obstacle underneath is clearly visible. On the right, we overlay whichparts of the membrane are in contact with the obstacle. We will later callthis set of points the "active set" to indicate that an inequality constraintis active there.
* 

* [1.x.33][1.x.34]
* 

* The classical formulation of the problem possesses the following form:
* [1.x.35]
* with  [2.x.5] .   [2.x.6]  is a scalar valued function that denotes thevertical displacement of the membrane. The first equation is called equilibriumcondition with a force of areal density  [2.x.7] . Here, we will consider this forceto be gravity. The second one is known as Hooke's Law that says that the stresses [2.x.8]  are proportional to the gradient of the displacements  [2.x.9]  (theproportionality constant, often denoted by  [2.x.10] , has been set to one here,without loss of generality; if it is constant, it can be put into the righthand side function). At the boundary we have zero Dirichletconditions. Obviously, the first two equations can be combined to yield [2.x.11] .
* Intuitively, gravity acts downward and so  [2.x.12]  is a negativefunction (we choose  [2.x.13]  in this program). The first condition then meansthat the total force acting on the membrane is gravity plus somethingpositive: namely the upward force that the obstacle exerts on the membrane atthose places where the two of them are in contact. How big is this additionalforce? We don't know yet (and neither do we know "where" it actually acts) butit must be so that the membrane doesn't penetrate the obstacle.
* The fourth equality above together with the last inequality forms the obstaclecondition which has to hold at every point of the whole domain. The latter ofthese two means that the membrane must be above the obstacle  [2.x.14] everywhere. The second to last equation, often called the "complementaritycondition" says that where the membrane is not in contact with the obstacle(i.e., those  [2.x.15]  where  [2.x.16] ), then [2.x.17]  at these locations; in other words, no additional forces actthere, as expected. On the other hand, where  [2.x.18]  we can have  [2.x.19] , i.e., there can be additional forces (though there don't have to be:it is possible for the membrane to just touch, not press against, theobstacle).
* 

* [1.x.36][1.x.37]
* 

* An obvious way to obtain the variational formulation of the obstacle problem is to consider the total potential energy:[1.x.38]
* We have to find a solution  [2.x.20]  of the following minimization problem:[1.x.39]
* with the convex set of admissible displacements:[1.x.40]
* This set takes care of the third and fifth conditions above (the boundaryvalues and the complementarity condition).
* Consider now the minimizer  [2.x.21]  of  [2.x.22]  and any other function  [2.x.23] . Then the function[1.x.41]
* takes its minimum at  [2.x.24]  (because  [2.x.25]  is a minimizer of theenergy functional  [2.x.26] ), so that  [2.x.27]  for any choiceof  [2.x.28] . Note that [2.x.29]  because of theconvexity of  [2.x.30] . If we compute  [2.x.31]  ityields the variational formulation we are searching for:
* [1.x.42][1.x.43]
* 
* This is the typical form of variational inequalities, where not just  [2.x.32] appears in the bilinear form but in fact  [2.x.33] . The reason is this: if  [2.x.34]  isnot constrained, then we can find test functions  [2.x.35]  in  [2.x.36]  so that  [2.x.37]  can haveany sign. By choosing test functions  [2.x.38]  so that  [2.x.39]  itfollows that the inequality can only hold for both  [2.x.40]  and  [2.x.41]  if the twosides are in fact equal, i.e., we obtain a variational equality.
* On the other hand, if  [2.x.42]  then  [2.x.43]  only allows test functions  [2.x.44]  so that in fact [2.x.45] . This means that we can't test the equation with both  [2.x.46]  and [2.x.47]  as above, and so we can no longer conclude that the two sides are infact equal. Thus, this mimics the way we have discussed the complementaritycondition above.
* 

* 
* [1.x.44][1.x.45]
* 

* The variational inequality above is awkward to work with. We would thereforelike to reformulate it as an equivalent saddle point problem. We introduce aLagrange multiplier  [2.x.48]  and the convex cone  [2.x.49] ,  [2.x.50] dual space of  [2.x.51] ,  [2.x.52]  ofLagrange multipliers, where  [2.x.53]  denotes the dualitypairing between  [2.x.54]  and  [2.x.55] . Intuitively,  [2.x.56]  is the cone of all "non-positivefunctions", except that  [2.x.57]  and so contains other objectsbesides regular functions as well.This yields:
* [1.x.46]
* [1.x.47]
* [1.x.48]
* [1.x.49]
* In other words, we can consider  [2.x.58]  as the negative of the additional, positive force that theobstacle exerts on the membrane. The inequality in the second line of thestatement above only appears to have the wrong sign because we have [2.x.59]  at points where  [2.x.60] , given the definition of  [2.x.61] .
* The existence and uniqueness of  [2.x.62]  of this saddlepoint problem has been stated in Glowinski, Lions and Tr&eacute;moli&egrave;res: Numerical Analysis of VariationalInequalities, North-Holland, 1981.
* 

* 
* [1.x.50][1.x.51]
* 

* There are different methods to solve the variational inequality. As onepossibility you can understand the saddle point problem as a convex quadratic program (QP) withinequality constraints.
* To get there, let us assume that we discretize both  [2.x.63]  and  [2.x.64]  with thesame finite element space, for example the usual  [2.x.65]  spaces. We would thenget the equations[1.x.52]
* where  [2.x.66]  is the mass matrix on the chosen finite element space and theindices  [2.x.67]  above are for all degrees of freedom in the set  [2.x.68]  of degrees offreedom located in the interior of the domain(we have Dirichlet conditions on the perimeter). However, wecan make our life simpler if we use a particular quadrature rule whenassembling all terms that yield this mass matrix, namely a quadrature formulawhere quadrature points are only located at the interpolation points atwhich shape functions are defined; since all but one shape function are zeroat these locations, we get a diagonal mass matrix with
* [1.x.53]
* To define  [2.x.69]  we use the same technique as for  [2.x.70] . In other words, wedefine
* [1.x.54]
* where  [2.x.71]  is a suitable approximation of  [2.x.72] . The integral in the definitionof  [2.x.73]  and  [2.x.74]  are then approximated by the trapezoidal rule.With this, the equations above can be restated as[1.x.55]
* 
* Now we define for each degree of freedom  [2.x.75]  the function[1.x.56]
* with some  [2.x.76] . (In this program we choose  [2.x.77] . It is a kind of apenalty parameter which depends on the problem itself and needs to be chosenlarge enough; for example there is no convergence for  [2.x.78]  using thecurrent program if we use 7 global refinements.)
* After some head-scratching one can then convince oneself that the inequalitiesabove can equivalently be rewritten as[1.x.57]
* The primal-dual active set strategy we will use here is an iterative scheme which is based onthis condition to predict the next active and inactive sets  [2.x.79]  and [2.x.80]  (that is, those complementary sets of indices  [2.x.81]  for which [2.x.82]  is either equal to or not equal to the value of the obstacle [2.x.83] ). For a more in depth treatment of this approach, see Hintermueller, Ito, Kunisch: The primal-dual active setstrategy as a semismooth newton method, SIAM J. OPTIM., 2003, Vol. 13, No. 3,pp. 865-888.
* [1.x.58][1.x.59]
* 

* The algorithm for the primal-dual active set method works as follows (NOTE:  [2.x.84] ):
* 1. Initialize  [2.x.85]  and  [2.x.86] , such that  [2.x.87]  and  [2.x.88]  and set  [2.x.89] .2. Find the primal-dual pair  [2.x.90]  that satisfies

* 
* [1.x.60]
*  Note that the second and third conditions imply that exactly  [2.x.91]  unknowns are fixed, with the first condition yielding the remaining  [2.x.92]  equations necessary to determine both  [2.x.93]  and  [2.x.94] .3. Define the new active and inactive sets by [1.x.61]
* 4. If  [2.x.95]  (and then, obviously, also  [2.x.96] ) then stop, else set  [2.x.97]  and go to step (2).
* The method is called "primal-dual" because it uses both primal (thedisplacement  [2.x.98] ) as well as dual variables (the Lagrange multiplier [2.x.99] ) to determine the next active set.
* At the end of this section, let us add two observations. First,for any primal-dual pair  [2.x.100]  that satisfies thesecondition, we can distinguish the following cases:
* 1.  [2.x.101]  (i active):   [2.x.102]   Then either  [2.x.103]  and  [2.x.104]  (penetration) or  [2.x.105]  and  [2.x.106]  (pressing load).2.  [2.x.107]  (i inactive):   [2.x.108]   Then either  [2.x.109]  and  [2.x.110]  (no contact) or  [2.x.111]  and  [2.x.112]  (unpressing load).
* Second, the method above appears intuitively correct and useful but a bit adhoc. However, it can be derived in a concisely in the following way. To thisend, note that we'd like to solve the nonlinear system[1.x.62]
* We can iteratively solve this by always linearizing around the previousiterate (i.e., applying a Newton method), but for this we need to linearizethe function  [2.x.113]  that is not differentiable. That said, it isslantly differentiable, and in fact we have[1.x.63]
* [1.x.64]
* This suggest a semismooth Newton step of the form[1.x.65]
* where we have split matrices  [2.x.114]  as well as vectors in the natural way intorows and columns whose indices belong to either the active set [2.x.115]  or the inactive set  [2.x.116] .
* Rather than solving for updates  [2.x.117] , we can also solvefor the variables we are interested in right away by setting  [2.x.118]  and  [2.x.119]  andbringing all known terms to the right hand side. This yields[1.x.66]
* These are the equations outlined above in the description of the basic algorithm.
* We could even drive this a bit further.It's easy to see that we can eliminate the third row and the third columnbecause it implies  [2.x.120] :[1.x.67]
* This shows that one in fact only needs to solve for the Lagrange multiplierslocated on the active set. By considering the second row one would then recoverthe full Lagrange multiplier vector through[1.x.68]
* Because of the third row and the fact that  [2.x.121]  is a diagonal matrix we are ableto calculate  [2.x.122]  directly. We can therefore also write thelinear system as follows:[1.x.69]
* Fortunately, this form is easy to arrive at: we simply build the usual Laplacelinear system[1.x.70]
* and then let the AffineConstraints class eliminate all constrained degrees offreedom, namely  [2.x.123] ,in the same way as if the dofs in  [2.x.124]  were Dirichlet data. Theresult linear system (the second to last one above) is symmetric and positivedefinite and we solve it with a CG-methodand the AMG preconditioner from Trilinos.
* 

* [1.x.71][1.x.72]
* 

* This tutorial is quite similar to  [2.x.125] . The general structure of the programfollows  [2.x.126]  with minor differences:
* 
*  - We need two new methods,  [2.x.127]  and   [2.x.128] .
* 
*  - We need new member variables that denote the constraints we have here.
* 
*  - We change the preconditioner for the solver.
* 

* You may want to read up on  [2.x.129]  if you want to understand thecurrent program.
* 

*  [1.x.73] [1.x.74]
*   [1.x.75]  [1.x.76]
* 

* 
*  As usual, at the beginning we include all the header files we need in here. With the exception of the various files that provide interfaces to the Trilinos library, there are no surprises:
* 

* 
* [1.x.77]
* 
*   [1.x.78]  [1.x.79]
* 

* 
*  This class supplies all function and variables needed to describe the obstacle problem. It is close to what we had to do in  [2.x.130] , and so relatively simple. The only real new components are the update_solution_and_constraints function that computes the active set and a number of variables that are necessary to describe the original (unconstrained) form of the linear system ( [2.x.131]  and  [2.x.132] ) as well as the active set itself and the diagonal of the mass matrix  [2.x.133]  used in scaling Lagrange multipliers in the active set formulation. The rest is as in  [2.x.134] :
* 

* 
* [1.x.80]
* 
*   [1.x.81]  [1.x.82]
* 

* 
*  In the following, we define classes that describe the right hand side function, the Dirichlet boundary values, and the height of the obstacle as a function of  [2.x.135] . In all three cases, we derive these classes from Function [2.x.136]  although in the case of  [2.x.137]  and  [2.x.138]  this is more out of convention than necessity since we never pass such objects to the library. In any case, the definition of the right hand side and boundary values classes is obvious given our choice of  [2.x.139] ,  [2.x.140] :
* 

* 
* [1.x.83]
* 
*  We describe the obstacle function by a cascaded barrier (think: stair steps):
* 

* 
* [1.x.84]
* 
*   [1.x.85]  [1.x.86]
* 

* 
*  
*  
*  [1.x.87]  [1.x.88]
* 

* 
*  To everyone who has taken a look at the first few tutorial programs, the constructor is completely obvious:
* 

* 
* [1.x.89]
* 
*   [1.x.90]  [1.x.91]
* 

* 
*  We solve our obstacle problem on the square  [2.x.141]  in 2D. This function therefore just sets up one of the simplest possible meshes.
* 

* 
* [1.x.92]
* 
*   [1.x.93]  [1.x.94]
* 

* 
*  In this first function of note, we set up the degrees of freedom handler, resize vectors and matrices, and deal with the constraints. Initially, the constraints are, of course, only given by boundary values, so we interpolate them towards the top of the function.
* 

* 
* [1.x.95]
* 
*  The only other thing to do here is to compute the factors in the  [2.x.142]  matrix which is used to scale the residual. As discussed in the introduction, we'll use a little trick to make this mass matrix diagonal, and in the following then first compute all of this as a matrix and then extract the diagonal elements for later use:
* 

* 
* [1.x.96]
* 
*   [1.x.97]  [1.x.98]
* 

* 
*  This function at once assembles the system matrix and right-hand-side and applied the constraints (both due to the active set as well as from boundary values) to our system. Otherwise, it is functionally equivalent to the corresponding function in, for example,  [2.x.143] .
* 

* 
* [1.x.99]
* 
*   [1.x.100]  [1.x.101]
* 

* 
*  The next function is used in the computation of the diagonal mass matrix  [2.x.144]  used to scale variables in the active set method. As discussed in the introduction, we get the mass matrix to be diagonal by choosing the trapezoidal rule for quadrature. Doing so we don't really need the triple loop over quadrature points, indices  [2.x.145]  and indices  [2.x.146]  any more and can, instead, just use a double loop. The rest of the function is obvious given what we have discussed in many of the previous tutorial programs.   
*   Note that at the time this function is called, the constraints object only contains boundary value constraints; we therefore do not have to pay attention in the last copy-local-to-global step to preserve the values of matrix entries that may later on be constrained by the active set.   
*   Note also that the trick with the trapezoidal rule only works if we have in fact  [2.x.147]  elements. For higher order elements, one would need to use a quadrature formula that has quadrature points at all the support points of the finite element. Constructing such a quadrature formula isn't really difficult, but not the point here, and so we simply assert at the top of the function that our implicit assumption about the finite element is in fact satisfied.
* 

* 
* [1.x.102]
* 
*   [1.x.103]  [1.x.104]
* 

* 
*  In a sense, this is the central function of this program.  It updates the active set of constrained degrees of freedom as discussed in the introduction and computes an AffineConstraints object from it that can then be used to eliminate constrained degrees of freedom from the solution of the next iteration. At the same time we set the constrained degrees of freedom of the solution to the correct value, namely the height of the obstacle.   
*   Fundamentally, the function is rather simple: We have to loop over all degrees of freedom and check the sign of the function  [2.x.148]  because in our case  [2.x.149] . To this end, we use the formula given in the introduction by which we can compute the Lagrange multiplier as the residual of the original linear system (given via the variables  [2.x.150] . At the top of this function, we compute this residual using a function that is part of the matrix classes.
* 

* 
* [1.x.105]
* 
*  compute contact_force[i] =
* 
*  - lambda[i] diagonal_of_mass_matrix[i]
* 

* 
* [1.x.106]
* 
*  The next step is to reset the active set and constraints objects and to start the loop over all degrees of freedom. This is made slightly more complicated by the fact that we can't just loop over all elements of the solution vector since there is no way for us then to find out what location a DoF is associated with; however, we need this location to test whether the displacement of a DoF is larger or smaller than the height of the obstacle at this location.     
*   We work around this by looping over all cells and DoFs defined on each of these cells. We use here that the displacement is described using a  [2.x.151]  function for which degrees of freedom are always located on the vertices of the cell; thus, we can get the index of each degree of freedom and its location by asking the vertex for this information. On the other hand, this clearly wouldn't work for higher order elements, and so we add an assertion that makes sure that we only deal with elements for which all degrees of freedom are located in vertices to avoid tripping ourselves with non-functional code in case someone wants to play with increasing the polynomial degree of the solution.     
*   The price to pay for having to loop over cells rather than DoFs is that we may encounter some degrees of freedom more than once, namely each time we visit one of the cells adjacent to a given vertex. We will therefore have to keep track which vertices we have already touched and which we haven't so far. We do so by using an array of flags  [2.x.152] :
* 

* 
* [1.x.107]
* 
*  Now that we know that we haven't touched this DoF yet, let's get the value of the displacement function there as well as the value of the obstacle function and use this to decide whether the current DoF belongs to the active set. For that we use the function given above and in the introduction.           
*   If we decide that the DoF should be part of the active set, we add its index to the active set, introduce an inhomogeneous equality constraint in the AffineConstraints object, and reset the solution value to the height of the obstacle. Finally, the residual of the non-contact part of the system serves as an additional control (the residual equals the remaining, unaccounted forces, and should be zero outside the contact zone), so we zero out the components of the residual vector (i.e., the Lagrange multiplier lambda) that correspond to the area where the body is in contact; at the end of the loop over all cells, the residual will therefore only consist of the residual in the non-contact zone. We output the norm of this residual along with the size of the active set after the loop.
* 

* 
* [1.x.108]
* 
*  In a final step, we add to the set of constraints on DoFs we have so far from the active set those that result from Dirichlet boundary values, and close the constraints object:
* 

* 
* [1.x.109]
* 
*   [1.x.110]  [1.x.111]
* 

* 
*  There is nothing to say really about the solve function. In the context of a Newton method, we are not typically interested in very high accuracy (why ask for a highly accurate solution of a linear problem that we know only gives us an approximation of the solution of the nonlinear problem), and so we use the ReductionControl class that stops iterations when either an absolute tolerance is reached (for which we choose  [2.x.153] ) or when the residual is reduced by a certain factor (here,  [2.x.154] ).
* 

* 
* [1.x.112]
* 
*   [1.x.113]  [1.x.114]
* 

* 
*  We use the vtk-format for the output.  The file contains the displacement and a numerical representation of the active set.
* 

* 
* [1.x.115]
* 
*   [1.x.116]  [1.x.117]
* 

* 
*  This is the function which has the top-level control over everything.  It is not very long, and in fact rather straightforward: in every iteration of the active set method, we assemble the linear system, solve it, update the active set and project the solution back to the feasible set, and then output the results. The iteration is terminated whenever the active set has not changed in the previous iteration.   
*   The only trickier part is that we have to save the linear system (i.e., the matrix and right hand side) after assembling it in the first iteration. The reason is that this is the only step where we can access the linear system as built without any of the contact constraints active. We need this to compute the residual of the solution at other iterations, but in other iterations that linear system we form has the rows and columns that correspond to constrained degrees of freedom eliminated, and so we can no longer access the full residual of the original equation.
* 

* 
* [1.x.118]
* 
*   [1.x.119]  [1.x.120]
* 

* 
*  And this is the main function. It follows the pattern of all other main functions. The call to initialize MPI exists because the Trilinos library upon which we build our linear solvers in this program requires it.
* 

* 
* [1.x.121]
* 
*  This program can only be run in serial. Otherwise, throw an exception.
* 

* 
* [1.x.122]
* [1.x.123][1.x.124]
* 

* Running the program produces output like this:
* [1.x.125]
* 
* The iterations end once the active set doesn't change any more (it has5,399 constrained degrees of freedom at that point). The algebraicprecondition is apparently working nicely since we only need 4-6 CGiterations to solve the linear system (although this also has a lot todo with the fact that we are not asking for very high accuracy of thelinear solver).
* More revealing is to look at a sequence of graphical output files(every third step is shown, with the number of the iteration in theleftmost column):
*  [2.x.155] 
* The pictures show that in the first step, the solution (which has beencomputed without any of the constraints active) bends through so muchthat pretty much every interior point has to be bounced back to thestairstep function, producing a discontinuous solution. Over thecourse of the active set iterations, this unphysical membrane shape issmoothed out, the contact with the lower-most stair step disappears,and the solution stabilizes.
* In addition to this, the program also outputs the values of theLagrange multipliers. Remember that these are the contact forces andso should only be positive on the contact set, and zero outside. If,on the other hand, a Lagrange multiplier is negative in the activeset, then this degree of freedom must be removed from the activeset. The following pictures show the multipliers in iterations 1, 9and 18, where we use red and browns to indicate positive values, andblue for negative values.
*  [2.x.156] 
* It is easy to see that the positive values converge nicely to moderatevalues in the interior of the contact set and large upward forces atthe edges of the steps, as one would expect (to support the largecurvature of the membrane there); at the fringes of the active set,multipliers are initially negative, causing the set to shrink until,in iteration 18, there are no more negative multipliers and thealgorithm has converged.
* 

* 
* [1.x.126][1.x.127][1.x.128]
* 

* As with any of the programs of this tutorial, there are a number ofobvious possibilities for extensions and experiments. The first one isclear: introduce adaptivity. Contact problems are prime candidates foradaptive meshes because the solution has lines along which it is lessregular (the places where contact is established between membrane andobstacle) and other areas where the solution is very smooth (or, inthe present context, constant wherever it is in contact with theobstacle). Adding this to the current program should not pose too manydifficulties, but it is not trivial to find a good error estimator forthat purpose.
* A more challenging task would be an extension to 3d. The problem hereis not so much to simply make everything run in 3d. Rather, it is thatwhen a 3d body is deformed and gets into contact with an obstacle,then the obstacle does not act as a constraining body force within thedomain as is the case here. Rather, the contact force only acts on theboundary of the object. The inequality then is not in the differentialequation but in fact in the (Neumann-type) boundary conditions, thoughthis leads to a similar kind of variationalinequality. Mathematically, this means that the Lagrange multiplieronly lives on the surface, though it can of course be extended by zerointo the domain if that is convenient. As in the current program, onedoes not need to form and store this Lagrange multiplier explicitly.
* A further interesting problem for the 3d case is to consider contact problemswith friction. In almost every mechanical process friction has a big influence.For the modelling we have to take into account tangential stresses at the contactsurface. Also we have to observe that friction adds another nonlinearity toour problem.
* Another nontrivial modification is to implement a more complex constitutivelaw like nonlinear elasticity or elasto-plastic  material behavior.The difficulty here is to handle the additional nonlinearity arisingthrough the nonlinear constitutive law.
* 

* [1.x.129][1.x.130] [2.x.157] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-42_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] ,  [2.x.2] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34][1.x.35][1.x.36][1.x.37][1.x.38]
*  [2.x.3] 
* [1.x.39][1.x.40]
* 

* 
* [1.x.41][1.x.42][1.x.43]
* 

* This example is an extension of  [2.x.4] , considering a 3d contact problem with anelasto-plastic material behavior with isotropic hardening in three dimensions.In other words, it considers how a three-dimensional body deforms if one pushesinto it a rigid obstacle (the contact problem) where deformation is governedby an elasto-plastic material law (a material that can only accommodate a certainmaximal stress) that hardens as deformation accumulates. To show what we intend todo before going into too many details, let us just show a picture of what thesolution will look like (the deformable body is a cube
* 
*  - only half ofwhich is actually shown
* 
*  -  the obstacle correspondsto a Chinese character that is discussed below):
*  [2.x.5] 
* 

* This problem description implies that we have to take care of an additionalnonlinearity compared to  [2.x.6] : thematerial behavior. Since we consider a three dimensional problem here, we alsohave to account for the fact that the contact area is at the boundary ofthe deformable body now, rather than in the interior. Finally, compared to [2.x.7] , we also have to deal with hanging nodes in both the handling of the linearsystem as well as of the inequality constraints as we would like to use anadaptive mesh; in the latter case, we willhave to deal with prioritizing whether the constraints from the hanging nodesor from the inequalities are more important.
* Since you can very easily reach a few million degrees of freedom in threedimensions, even with adaptive mesh refinement, we decided to use Trilinos andp4est to run our code in parallel, building on the framework of  [2.x.8]  forthe parallelization. Additional pointers for parallelization can be found in [2.x.9] .
* 

* [1.x.44][1.x.45]
* 

* The classical formulation of the problem possesses the following form:
* [1.x.46]
* Here, the first of these equations defines therelationship between strain  [2.x.10]  and stress  [2.x.11]  viathe fourth-order compliance tensor  [2.x.12] ;  [2.x.13]  provides the plasticcomponent of the strain to ensure that the stress does not exceed the yieldstress. We will only consider isotropicmaterials for which  [2.x.14]  can be expressed in terms of the Lam&eacute; moduli [2.x.15]  and  [2.x.16]  or alternatively in terms of the bulk modulus [2.x.17]  and  [2.x.18] .The second equation is the force balance; we will herenot consider any body forces and henceforth assume that  [2.x.19] . Thecomplementarity condition in the third line implies that  [2.x.20]  if [2.x.21]  but that  [2.x.22]  may be a nonzero tensor if andonly if  [2.x.23] , and in particular that in this case [2.x.24]  must point in the direction  [2.x.25] . The inequality  [2.x.26]  isa statement of the fact that plastic materials can only support a finite amountof stress; in other words, they react with plastic deformations  [2.x.27] if external forces would result in a stress  [2.x.28]  for which  [2.x.29] would result. A typical form for this [1.x.47] is [2.x.30]  where  [2.x.31]  is the deviatoric part of a tensorand  [2.x.32]  denotes the Frobenius norm.
* Further equations describe afixed, zero displacement on  [2.x.33]  andthat on the surface  [2.x.34]  where contact may appear, the normalforce  [2.x.35]  exerted by the obstacle is inward (no "pull" by the obstacle on ourbody) and with zero tangential component  [2.x.36] .The last condition is again a complementarity condition thatimplies that on  [2.x.37] , the normalforce can only be nonzero if the body is in contact with the obstacle; thesecond part describes the impenetrability of the obstacle and the body.The last two equations are commonly referred to as the Signorini contactconditions.
* Most materials
* 
*  - especially metals
* 
*  - have the property that they show some hardening as a result ofdeformation. In other words,  [2.x.38]  increases with deformation.In practice, it is not the elastic deformation that results in hardening,but the plastic component.There are different constitutive laws to describe those material behaviors. Thesimplest one is called linear isotropic hardening described by the flow function [2.x.39] .
* 

* [1.x.48][1.x.49]
* 

* It is generally rather awkward to deal with inequalities. Here, we have to deal withtwo: plasticity and the contact problem.As described in more detail in the paper mentioned at the top of this page, onecan at least reformulate the plasticity in a way that makes it look like anonlinearity that we can then treat with Newton's method. This is slightlytricky mathematically since the nonlinearity is not just some smoothfunction but instead has kinks where the stress reaches the yield stress;however, it can be shown for such [1.x.50] functions that Newton'smethod still converges.
* Without going into details, we will also get rid of the stress as an independentvariable and instead work exclusively with the displacements  [2.x.40] . Ultimately,the goal of this reformulation is that we will want to end up with a symmetric,positive definite problem
* 
*  - such as a linearized elasticity problem with spatiallyvariable coefficients resulting from the plastic behavior
* 
*  - that needs to be solvedin each Newton step. We want this because there are efficient and scalable methodsfor the solution of such linear systems, such as CG preconditioned with analgebraic multigrid. This is opposed to the saddle point problem akin to the mixedLaplace (see  [2.x.41] ) we would get were we to continue with the mixed formulationcontaining both displacements and stresses, and for which  [2.x.42]  already gives ahint at how difficult it is to construct good solvers and preconditioners.
* With this said, let us simply state the problem we obtain after reformulation(again, details can be found in the paper): Find a displacement  [2.x.43]  so that
* [1.x.51]
* where the projector  [2.x.44]  is defined as
* [1.x.52]
* and the space  [2.x.45]  is the space of all displacements that satisfy the contactcondition:
* [1.x.53]
* 
* In the actual code, we will use the abbreviation  [2.x.46] .
* Given this formulation, we will apply two techniques:
* 
*  - Run a Newton method to iterate out the nonlinearity in the projector.
* 
*  - Run an active set method for the contact condition, in much the same  way as we did in  [2.x.47] .
* A strict approach would keep the active set fixed while we iteratethe Newton method to convergence (or maybe the other way around: find thefinal active set before moving on to the next Newton iteration).In practice, it turns out that it is sufficient to do only a singleNewton step per active set iteration, and so we will iterate over themconcurrently. We will also, every once in a while, refine the mesh.
* 

* [1.x.54][1.x.55]
* 

* As mentioned, we will treat the nonlinearity of the operator  [2.x.48]  byapplying a Newton method, despite the fact that the operator is not differentiablein the strict sense. However, it satisfies the conditions of [1.x.56]differentiability and this turns out to be enough for Newton's method to work.The resulting method then goes by the name [1.x.57],which sounds impressive but is, in reality, just a Newton method applied toa semi-smooth function with an appropriately chosen "derivative".
* In the current case, we will run our iteration by solving in each iteration  [2.x.49] the following equation (still an inequality, but linearized):
* [1.x.58]
* where the rank-4 tensor  [2.x.50]  given by
* [1.x.59]
* This tensor is the (formal) linearization of  [2.x.51]  around  [2.x.52] .For the linear isotropic material we consider here,the bulk and shear components of the projector are given by[1.x.60]
* where  [2.x.53] and  [2.x.54]  are the identity tensors of rank 2 and 4, respectively.
* Note that this problem corresponds to a linear elastic contact problemwhere  [2.x.55]  plays the role of the elasticity tensor  [2.x.56] . Indeed,if the material is not plastic at a point, then  [2.x.57] . However, atplaces where the material is plastic,  [2.x.58]  is a spatially varyingfunction. In any case, the system we have to solve for the Newton iterate [2.x.59]  gets us closer to the goal of rewriting our problem ina way that allows us to use well-known solvers and preconditioners forelliptic systems.
* As a final note about the Newton method let us mention that as is common withNewton methods we need to globalize it by controlling the step length. Inother words, while the system above solves for  [2.x.60] , the finaliterate will rather be
* [1.x.61]
* where the difference in parentheses on the right takes the role of thetraditional Newton direction,  [2.x.61] . We will determine [2.x.62]  using a standard line search.
* 

* [1.x.62][1.x.63]
* 

* This linearized problem to be solved in each Newton step is essentially likein  [2.x.63] . The only difference consists in the fact that the contact areais at the boundary instead of in the domain. But this has no further consequenceso that we refer to the documentation of  [2.x.64]  with the only hint that [2.x.65]  contains all the vertices at the contact boundary  [2.x.66]  thistime. As there, what we need to do is keep a subset of degrees of freedom fixed,leading to additional constraints that one can write as a saddle point problem.However, as discussed in the paper, by writing these constraints in anappropriate way that removes the coupling between degrees of freedom,we end up with a set of nodes that essentially just have Dirichlet valuesattached to them.
* 

* [1.x.64][1.x.65]
* 

* The algorithm outlined above combines the damped semismooth Newton-method,which we use for the nonlinear constitutive law, with the semismooth Newtonmethod for the contact. It works as follows: [2.x.67]   [2.x.68]  Initialize the active and inactive sets  [2.x.69]  and  [2.x.70]  such that  [2.x.71]  and  [2.x.72]  and set  [2.x.73] . Here,  [2.x.74]  is the set of all degrees of freedom located at the surface of the domain where contact may happen. The start value  [2.x.75]  fulfills our obstacle condition, i.e., we project an initial zero displacement onto the set of feasible displacements.
*   [2.x.76]  Assemble the Newton matrix  [2.x.77]  and the right-hand-side  [2.x.78] . These correspond to the linearized Newton step, ignoring for the moment the contact inequality.
*   [2.x.79]  Find the primal-dual pair  [2.x.80]  that satisfies

* 
* [1.x.66]
*  As in  [2.x.81] , we can obtain the solution to this problem by eliminating those degrees of freedom in  [2.x.82]  from the first equation and obtain a linear system  [2.x.83] .
* 

* 
*   [2.x.84]  Damp the Newton iteration for  [2.x.85]  by applying a line search and calculating a linear combination of  [2.x.86]  and  [2.x.87] . This requires finding an  [2.x.88]  so that [1.x.67]
*  satisfies [1.x.68] with  [2.x.89]  with the exceptions of (i) elements  [2.x.90]  where we set  [2.x.91] , and (ii) elements that correspond to hanging nodes, which we eliminate in the usual manner.
*   [2.x.92]  Define the new active and inactive sets by [1.x.69]
*  [1.x.70]
* 
*   [2.x.93] Project  [2.x.94]  so that it satisfies the contact inequality, [1.x.71]
*  Here,  [2.x.95]  is the projection of the active components in  [2.x.96]  to the gap [1.x.72]
*  where  [2.x.97]  is the [1.x.73] denoting the distance of the obstacle from the undisplaced configuration of the body.
*   [2.x.98]  If  [2.x.99]  and  [2.x.100]  then stop, else set  [2.x.101]  and go to step (1). This step ensures that we only stop iterations if both the correct active set has been found and the plasticity has been iterated to sufficient accuracy. [2.x.102] 
* In step 3 of this algorithm,the matrix  [2.x.103] ,  [2.x.104]  describes the coupling of thebases for the displacements and Lagrange multiplier (contact forces)and it is not quadratic in our situation since  [2.x.105]  is only defined on [2.x.106] , i.e., the surface where contact may happen. As shown in the paper,we can choose  [2.x.107]  to be a matrix that has only one entry per row,(see also H&uuml;eber, Wohlmuth: A primal-dual activeset strategy for non-linear multibody contact problems, Comput. Methods Appl. Mech. Engrg.194, 2005, pp. 3147-3166).The vector  [2.x.108]  is defined by a suitable approximation  [2.x.109]  of the gap  [2.x.110] [1.x.74]
* 
* 

* [1.x.75][1.x.76]
* 

* Since we run our program in 3d, the computations the program performs areexpensive. Consequently using adaptive mesh refinement is an important step towardsstaying within acceptable run-times. To make our lives easier we simply choose theKellyErrorEstimator that is already implemented in deal.II. We hand thesolution vector to it which contains the displacement  [2.x.111] . As we will see in theresults it yields a quite reasonable adaptive mesh for the contact zone as wellas for plasticity.
* 

* [1.x.77][1.x.78]
* 

* This tutorial is essentially a mixture of  [2.x.112]  and  [2.x.113]  but instead ofPETSc we let the Trilinos library deal with parallelizing the linear algebra(like in  [2.x.114] ). Since we are trying to solve a similar problem like in [2.x.115]  we will use the same methods but now in parallel.
* A difficulty is handling of the constraints fromthe Dirichlet conditions, hanging nodes and the inequality condition thatarises from the contact. For this purpose we create three objects of typeAffineConstraints that describe the various constraints and that we willcombine as appropriate in each iteration.
* Compared to  [2.x.116] , the programs has a few new classes:
*  [2.x.117]  [2.x.118]   [2.x.119]  describes the plastic behavior of the  material
*  [2.x.120]   [2.x.121]  describes a sphere that serves as the  obstacle that is pushed into the deformable, elastoplastic body.  Whether this or the next class is used to describe the obstacle is  determined from the input parameter file.
*  [2.x.122]   [2.x.123]  (and a helper class) is a class that  allows us to read in an obstacle from a file. In the example we  will show in the results section, this file will be   [2.x.124]  and will correspond to data that shows the  Chinese, Japanese or  Korean symbol for force or power (see http://www.orientaloutpost.com/ :  "This word can be used for motivation
* 
*  - it  can also mean power/motion/propulsion/force. It can be anything  internal or external that keeps you going. This is the safest way to express  motivation in Chinese. If your audience is Japanese, please see the other entry  for motivation. This is a word in Japanese and Korean, but it means "motive  power" or "kinetic energy" (without the motivation meaning that you are  probably looking for)"). In essence, we will pretend that we have a stamp  (i.e., a mask that corresponds to a flat bottomed obstacle with no pieces  of intermediate height) that we press into the body. The symbol in question  looks as follows (see also the picture at  the top of this section on how the end result looks like):
*    [2.x.125]  [2.x.126] 
* Other than that, let us comment only on the following aspects: [2.x.127]  [2.x.128]  The program allows you to select from two different coarse meshes  through the parameter file. These are either a cube  [2.x.129]  or  a half sphere with the open side facing the positive  [2.x.130]  direction.
*  [2.x.131] In either case, we will assume the convention that the part of the  boundary that may be in contact with the obstacle has boundary  indicator one. For both kinds of meshes, we assume that this is a free  surface, i.e., the body is either in contact there or there is no force  acting on it. For the half sphere, the curved part has boundary  indicator zero and we impose zero displacement there. For the box,  we impose zero displacement along the bottom but allow vertical  displacement along the sides (though no horizontal displacement). [2.x.132] 
* 

*  [1.x.79] [1.x.80]
*   [1.x.81]  [1.x.82] The set of include files is not much of a surprise any more at this time:
* 

* 
* [1.x.83]
* 
*  Finally, we include two system headers that let us create a directory for output files. The first header provides the  [2.x.133]  function and the second lets us determine what happened if  [2.x.134]  fails.
* 

* 
* [1.x.84]
* 
*   [1.x.85]  [1.x.86]
* 

* 
*  This class provides an interface for a constitutive law, i.e., for the relationship between strain  [2.x.135]  and stress  [2.x.136] . In this example we are using an elastoplastic material behavior with linear, isotropic hardening. Such materials are characterized by Young's modulus  [2.x.137] , Poisson's ratio  [2.x.138] , the initial yield stress  [2.x.139]  and the isotropic hardening parameter  [2.x.140] .  For  [2.x.141]  we obtain perfect elastoplastic behavior.   
*   As explained in the paper that describes this program, the first Newton steps are solved with a completely elastic material model to avoid having to deal with both nonlinearities (plasticity and contact) at once. To this end, this class has a function  [2.x.142]  that we use later on to simply set  [2.x.143]  to a very large value
* 
*  -  essentially guaranteeing that the actual stress will not exceed it, and thereby producing an elastic material. When we are ready to use a plastic model, we set  [2.x.144]  back to its proper value, using the same function.  As a result of this approach, we need to leave  [2.x.145]  as the only non-const member variable of this class.
* 

* 
* [1.x.87]
* 
*  The constructor of the ConstitutiveLaw class sets the required material parameter for our deformable body. Material parameters for elastic isotropic media can be defined in a variety of ways, such as the pair  [2.x.146]  (elastic modulus and Poisson's number), using the Lam&eacute; parameters  [2.x.147]  or several other commonly used conventions. Here, the constructor takes a description of material parameters in the form of  [2.x.148] , but since this turns out to these are not the coefficients that appear in the equations of the plastic projector, we immediately convert them into the more suitable set  [2.x.149]  of bulk and shear moduli.  In addition, the constructor takes  [2.x.150]  (the yield stress absent any plastic strain) and  [2.x.151]  (the hardening parameter) as arguments. In this constructor, we also compute the two principal components of the stress-strain relation and its linearization.
* 

* 
* [1.x.88]
* 
*   [1.x.89]  [1.x.90]
* 

* 
*  This is the principal component of the constitutive law. It computes the fourth order symmetric tensor that relates the strain to the stress according to the projection given above, when evaluated at a particular strain point. We need this function to calculate the nonlinear residual in  [2.x.152]  where we multiply this tensor with the strain given in a quadrature point. The computations follow the formulas laid out in the introduction. In comparing the formulas there with the implementation below, recall that  [2.x.153]  and that  [2.x.154] .   
*   The function returns whether the quadrature point is plastic to allow for some statistics downstream on how many of the quadrature points are plastic and how many are elastic.
* 

* 
* [1.x.91]
* 
*   [1.x.92]  [1.x.93]
* 

* 
*  This function returns the linearized stress strain tensor, linearized around the solution  [2.x.155]  of the previous Newton step  [2.x.156] .  The parameter  [2.x.157]  (commonly denoted  [2.x.158] ) must be passed as an argument, and serves as the linearization point. The function returns the derivative of the nonlinear constitutive law in the variable stress_strain_tensor, as well as the stress-strain tensor of the linearized problem in stress_strain_tensor_linearized.  See  [2.x.159]  where this function is used.
* 

* 
* [1.x.94]
* 
*  [1.x.95]   
*   The following should be relatively standard. We need classes for the boundary forcing term (which we here choose to be zero) and boundary values on those part of the boundary that are not part of the contact surface (also chosen to be zero here).
* 

* 
* [1.x.96]
* 
*   [1.x.97]  [1.x.98]
* 

* 
*  The following class is the first of two obstacles that can be selected from the input file. It describes a sphere centered at position  [2.x.160]  and radius  [2.x.161] , where  [2.x.162]  is the vertical position of the (flat) surface of the deformable body. The function's  [2.x.163]  returns the location of the obstacle for a given  [2.x.164]  value if the point actually lies below the sphere, or a large positive value that can't possibly interfere with the deformation if it lies outside the "shadow" of the sphere.
* 

* 
* [1.x.99]
* 
*  preceding Assert
* 

* 
* [1.x.100]
* 
*   [1.x.101]  [1.x.102]
* 

* 
*  The following two classes describe the obstacle outlined in the introduction, i.e., the Chinese character. The first of the two,  [2.x.165]  is responsible for reading in data from a picture file stored in pbm ascii format. This data will be bilinearly interpolated and thereby provides a function that describes the obstacle. (The code below shows how one can construct a function by interpolating between given data points. One could use the  [2.x.166]  introduced after this tutorial program was written, which does exactly what we want here, but it is instructive to see how to do it by hand.)     
*   The data which we read from the file will be stored in a double  [2.x.167]  named obstacle_data.  This vector composes the base to calculate a piecewise bilinear function as a polynomial interpolation. The data we will read from a file consists of zeros (white) and ones (black).     
*   The  [2.x.168]  variables denote the spacing between pixels in  [2.x.169]  and  [2.x.170]  directions.  [2.x.171]  are the numbers of pixels in each of these directions.   [2.x.172]  returns the value of the image at a given location, interpolated from the adjacent pixel values.
* 

* 
* [1.x.103]
* 
*  The constructor of this class reads in the data that describes the obstacle from the given file name.
* 

* 
* [1.x.104]
* 
*  The following two functions return the value of a given pixel with coordinates  [2.x.173] , which we identify with the values of a function defined at positions  [2.x.174] , and at arbitrary coordinates  [2.x.175]  where we do a bilinear interpolation between point values returned by the first of the two functions. In the second function, for each  [2.x.176] , we first compute the (integer) location of the nearest pixel coordinate to the bottom left of  [2.x.177] , and then compute the coordinates  [2.x.178]  within this pixel. We truncate both kinds of variables from both below and above to avoid problems when evaluating the function outside of its defined range as may happen due to roundoff errors.
* 

* 
* [1.x.105]
* 
*  Finally, this is the class that actually uses the class above. It has a BitmapFile object as a member that describes the height of the obstacle. As mentioned above, the BitmapFile class will provide us with a mask, i.e., values that are either zero or one (and, if you ask for locations between pixels, values that are interpolated between zero and one). This class translates this to heights that are either 0.001 below the surface of the deformable body (if the BitmapFile class reports a one at this location) or 0.999 above the obstacle (if the BitmapFile class reports a zero). The following function should then be self-explanatory.
* 

* 
* [1.x.106]
* 
*  preceding Assert
* 

* 
* [1.x.107]
* 
*   [1.x.108]  [1.x.109]
* 

* 
*  This is the main class of this program and supplies all functions and variables needed to describe the nonlinear contact problem. It is close to  [2.x.179]  but with some additional features like handling hanging nodes, a Newton method, using Trilinos and p4est for parallel distributed computing. To deal with hanging nodes makes life a bit more complicated since we need another AffineConstraints object now. We create a Newton method for the active set method for the contact situation and to handle the nonlinear operator for the constitutive law.   
*   The general layout of this class is very much like for most other tutorial programs. To make our life a bit easier, this class reads a set of input parameters from an input file. These parameters, using the ParameterHandler class, are declared in the  [2.x.180]  function (which is static so that it can be called before we even create an object of the current type), and a ParameterHandler object that has been used to read an input file will then be passed to the constructor of this class.   
*   The remaining member functions are by and large as we have seen in several of the other tutorial programs, though with additions for the current nonlinear system. We will comment on their purpose as we get to them further below.
* 

* 
* [1.x.110]
* 
*  As far as member variables are concerned, we start with ones that we use to indicate the MPI universe this program runs on, a stream we use to let exactly one processor produce output to the console (see  [2.x.181] ) and a variable that is used to time the various sections of the program:
* 

* 
* [1.x.111]
* 
*  The next group describes the mesh and the finite element space. In particular, for this parallel program, the finite element space has associated with it variables that indicate which degrees of freedom live on the current processor (the index sets, see also  [2.x.182]  and the  [2.x.183]  documentation module) as well as a variety of constraints: those imposed by hanging nodes, by Dirichlet boundary conditions, and by the active set of contact nodes. Of the three AffineConstraints variables defined here, the first only contains hanging node constraints, the second also those associated with Dirichlet boundary conditions, and the third these plus the contact constraints.     
*   The variable  [2.x.184]  consists of those degrees of freedom constrained by the contact, and we use  [2.x.185]  to keep track of the fraction of quadrature points on each cell where the stress equals the yield stress. The latter is only used to create graphical output showing the plastic zone, but not for any further computation; the variable is a member variable of this class since the information is computed as a by-product of computing the residual, but is used only much later. (Note that the vector is a vector of length equal to the number of active cells on the [1.x.112]; it is never used to exchange information between processors and can therefore be a regular deal.II vector.)
* 

* 
* [1.x.113]
* 
*  The next block of variables corresponds to the solution and the linear systems we need to form. In particular, this includes the Newton matrix and right hand side; the vector that corresponds to the residual (i.e., the Newton right hand side) but from which we have not eliminated the various constraints and that is used to determine which degrees of freedom need to be constrained in the next iteration; and a vector that corresponds to the diagonal of the  [2.x.186]  matrix briefly mentioned in the introduction and discussed in the accompanying paper.
* 

* 
* [1.x.114]
* 
*  The next block contains the variables that describe the material response:
* 

* 
* [1.x.115]
* 
*  And then there is an assortment of other variables that are used to identify the mesh we are asked to build as selected by the parameter file, the obstacle that is being pushed into the deformable body, the mesh refinement strategy, whether to transfer the solution from one mesh to the next, and how many mesh refinement cycles to perform. As possible, we mark these kinds of variables as  [2.x.187]  to help the reader identify which ones may or may not be modified later on (the output directory being an exception
* 
*  -  it is never modified outside the constructor but it is awkward to initialize in the member-initializer-list following the colon in the constructor since there we have only one shot at setting it; the same is true for the mesh refinement criterion):
* 

* 
* [1.x.116]
* 
*   [1.x.117]  [1.x.118]
* 

* 
*   [1.x.119]  [1.x.120]
* 

* 
*  Let us start with the declaration of run-time parameters that can be selected in the input file. These values will be read back in the constructor of this class to initialize the member variables of this class:
* 

* 
* [1.x.121]
* 
*   [1.x.122]  [1.x.123]
* 

* 
*  Given the declarations of member variables as well as the declarations of run-time parameters that are read from the input file, there is nothing surprising in this constructor. In the body we initialize the mesh refinement strategy and the output directory, creating such a directory if necessary.
* 

* 
* [1.x.124]
* 
*  If necessary, create a new directory for the output.
* 

* 
* [1.x.125]
* 
*   [1.x.126]  [1.x.127]
* 

* 
*  The next block deals with constructing the starting mesh. We will use the following helper function and the first block of the  [2.x.188]  to construct a mesh that corresponds to a half sphere. deal.II has a function that creates such a mesh, but it is in the wrong location and facing the wrong direction, so we need to shift and rotate it a bit before using it.   
*   For later reference, as described in the documentation of  [2.x.189]  the flat surface of the halfsphere has boundary indicator zero, while the remainder has boundary indicator one.
* 

* 
* [1.x.128]
* 
*  Since we will attach a different manifold below, we immediately clear the default manifold description:
* 

* 
* [1.x.129]
* 
*  Alternatively, create a hypercube mesh. After creating it, assign boundary indicators as follows:  [2.x.190]  In other words, the boundary indicators of the sides of the cube are 8. The boundary indicator of the bottom is 6 and the top has indicator 1. We set these by looping over all cells of all faces and looking at coordinate values of the cell center, and will make use of these indicators later when evaluating which boundary will carry Dirichlet boundary conditions or will be subject to potential contact. (In the current case, the mesh contains only a single cell, and all of its faces are on the boundary, so both the loop over all cells and the query whether a face is on the boundary are, strictly speaking, unnecessary; we retain them simply out of habit: this kind of code can be found in many programs in essentially this form.)
* 

* 
* [1.x.131]
* 
*   [1.x.132]  [1.x.133]
* 

* 
*  The next piece in the puzzle is to set up the DoFHandler, resize vectors and take care of various other status variables such as index sets and constraint matrices.   
*   In the following, each group of operations is put into a brace-enclosed block that is being timed by the variable declared at the top of the block (the constructor of the  [2.x.191]  variable starts the timed section, the destructor that is called at the end of the block stops it again).
* 

* 
* [1.x.134]
* 
*  Finally, we set up sparsity patterns and matrices. We temporarily (ab)use the system matrix to also build the (diagonal) matrix that we use in eliminating degrees of freedom that are in contact with the obstacle, but we then immediately set the Newton matrix back to zero.
* 

* 
* [1.x.135]
* 
*   [1.x.136]  [1.x.137]
* 

* 
*  This function, broken out of the preceding one, computes the constraints associated with Dirichlet-type boundary conditions and puts them into the  [2.x.192]  variable by merging with the constraints that come from hanging nodes.   
*   As laid out in the introduction, we need to distinguish between two cases:
* 

* 
* 
*  - If the domain is a box, we set the displacement to zero at the bottom, and allow vertical movement in z-direction along the sides. As shown in the  [2.x.193]  function, the former corresponds to boundary indicator 6, the latter to 8.
* 

* 
* 
*  - If the domain is a half sphere, then we impose zero displacement along the curved part of the boundary, associated with boundary indicator zero.
* 

* 
* [1.x.138]
* 
*  interpolate all components of the solution
* 

* 
* [1.x.139]
* 
*  interpolate x- and y-components of the solution (this is a bit mask, so apply operator| )
* 

* 
* [1.x.140]
* 
*   [1.x.141]  [1.x.142]
* 

* 
*  The next helper function computes the (diagonal) mass matrix that is used to determine the active set of the active set method we use in the contact algorithm. This matrix is of mass matrix type, but unlike the standard mass matrix, we can make it diagonal (even in the case of higher order elements) by using a quadrature formula that has its quadrature points at exactly the same locations as the interpolation points for the finite element are located. We achieve this by using a QGaussLobatto quadrature formula here, along with initializing the finite element with a set of interpolation points derived from the same quadrature formula. The remainder of the function is relatively straightforward: we put the resulting matrix into the given argument; because we know the matrix is diagonal, it is sufficient to have a loop over only  [2.x.194]  and not over  [2.x.195] . Strictly speaking, we could even avoid multiplying the shape function's values at quadrature point  [2.x.196]  by itself because we know the shape value to be a vector with exactly one one which when dotted with itself yields one. Since this function is not time critical we add this term for clarity.
* 

* 
* [1.x.143]
* 
*   [1.x.144]  [1.x.145]
* 

* 
*  The following function is the first function we call in each Newton iteration in the  [2.x.197]  function. What it does is to project the solution onto the feasible set and update the active set for the degrees of freedom that touch or penetrate the obstacle.   
*   In order to function, we first need to do some bookkeeping: We need to write into the solution vector (which we can only do with fully distributed vectors without ghost elements) and we need to read the Lagrange multiplier and the elements of the diagonal mass matrix from their respective vectors (which we can only do with vectors that do have ghost elements), so we create the respective vectors. We then also initialize the constraints object that will contain constraints from contact and all other sources, as well as an object that contains an index set of all locally owned degrees of freedom that are part of the contact:
* 

* 
* [1.x.146]
* 
*  The second part is a loop over all cells in which we look at each point where a degree of freedom is defined whether the active set condition is true and we need to add this degree of freedom to the active set of contact nodes. As we always do, if we want to evaluate functions at individual points, we do this with an FEValues object (or, here, an FEFaceValues object since we need to check contact at the surface) with an appropriately chosen quadrature object. We create this face quadrature object by choosing the "support points" of the shape functions defined on the faces of cells (for more on support points, see this  [2.x.198]  "glossary entry"). As a consequence, we have as many quadrature points as there are shape functions per face and looping over quadrature points is equivalent to looping over shape functions defined on a face. With this, the code looks as follows:
* 

* 
* [1.x.147]
* 
*  At each quadrature point (i.e., at each support point of a degree of freedom located on the contact boundary), we then ask whether it is part of the z-displacement degrees of freedom and if we haven't encountered this degree of freedom yet (which can happen for those on the edges between faces), we need to evaluate the gap between the deformed object and the obstacle. If the active set condition is true, then we add a constraint to the AffineConstraints object that the next Newton update needs to satisfy, set the solution vector's corresponding element to the correct value, and add the index to the IndexSet object that stores which degree of freedom is part of the contact:
* 

* 
* [1.x.148]
* 
*  At the end of this function, we exchange data between processors updating those ghost elements in the  [2.x.199]  variable that have been written by other processors. We then merge the Dirichlet constraints and those from hanging nodes into the AffineConstraints object that already contains the active set. We finish the function by outputting the total number of actively constrained degrees of freedom for which we sum over the number of actively constrained degrees of freedom owned by each of the processors. This number of locally owned constrained degrees of freedom is of course the number of elements of the intersection of the active set and the set of locally owned degrees of freedom, which we can get by using  [2.x.200]  on two IndexSets:
* 

* 
* [1.x.149]
* 
*   [1.x.150]  [1.x.151]
* 

* 
*  Given the complexity of the problem, it may come as a bit of a surprise that assembling the linear system we have to solve in each Newton iteration is actually fairly straightforward. The following function builds the Newton right hand side and Newton matrix. It looks fairly innocent because the heavy lifting happens in the call to  [2.x.201]  and in particular in  [2.x.202]  using the constraints we have previously computed.
* 

* 
* [1.x.152]
* 
*  Having computed the stress-strain tensor and its linearization, we can now put together the parts of the matrix and right hand side. In both, we need the linearized stress-strain tensor times the symmetric gradient of  [2.x.203] , i.e. the term  [2.x.204] , so we introduce an abbreviation of this term. Recall that the matrix corresponds to the bilinear form  [2.x.205]  in the notation of the accompanying publication, whereas the right hand side is  [2.x.206]  where  [2.x.207]  is the current linearization points (typically the last solution). This might suggest that the right hand side will be zero if the material is completely elastic (where  [2.x.208] ) but this ignores the fact that the right hand side will also contain contributions from non-homogeneous constraints due to the contact.                   
*   The code block that follows this adds contributions that are due to boundary forces, should there be any.
* 

* 
* [1.x.153]
* 
*   [1.x.154]  [1.x.155]
* 

* 
*  The following function computes the nonlinear residual of the equation given the current solution (or any other linearization point). This is needed in the linear search algorithm where we need to try various linear combinations of previous and current (trial) solution to compute the (real, globalized) solution of the current Newton step.   
*   That said, in a slight abuse of the name of the function, it actually does significantly more. For example, it also computes the vector that corresponds to the Newton residual but without eliminating constrained degrees of freedom. We need this vector to compute contact forces and, ultimately, to compute the next active set. Likewise, by keeping track of how many quadrature points we encounter on each cell that show plastic yielding, we also compute the  [2.x.209]  vector that we can later output to visualize the plastic zone. In both of these cases, the results are not necessary as part of the line search, and so we may be wasting a small amount of time computing them. At the same time, this information appears as a natural by-product of what we need to do here anyway, and we want to collect it once at the end of each Newton step, so we may as well do it here.   
*   The actual implementation of this function should be rather obvious:
* 

* 
* [1.x.156]
* 
*   [1.x.157]  [1.x.158]
* 

* 
*  The last piece before we can discuss the actual Newton iteration on a single mesh is the solver for the linear systems. There are a couple of complications that slightly obscure the code, but mostly it is just setup then solve. Among the complications are:
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - For the hanging nodes we have to apply the  [2.x.210]  function to newton_rhs. This is necessary if a hanging node with solution value  [2.x.211]  has one neighbor with value  [2.x.212]  which is in contact with the obstacle and one neighbor  [2.x.213]  which is not in contact. Because the update for the former will be prescribed, the hanging node constraint will have an inhomogeneity and will look like  [2.x.214] . So the corresponding entries in the right-hand-side are non-zero with a meaningless value. These values we have to set to zero.
* 

* 
* 
*  - Like in  [2.x.215] , we need to shuffle between vectors that do and do not have ghost elements when solving or using the solution.   
*   The rest of the function is similar to  [2.x.216]  and  [2.x.217]  except that we use a BiCGStab solver instead of CG. This is due to the fact that for very small hardening parameters  [2.x.218] , the linear system becomes almost semidefinite though still symmetric. BiCGStab appears to have an easier time with such linear systems.
* 

* 
* [1.x.159]
* 
*   [1.x.160]  [1.x.161]
* 

* 
*  This is, finally, the function that implements the damped Newton method on the current mesh. There are two nested loops: the outer loop for the Newton iteration and the inner loop for the line search which will be used only if necessary. To obtain a good and reasonable starting value we solve an elastic problem in the very first Newton step on each mesh (or only on the first mesh if we transfer solutions between meshes). We do so by setting the yield stress to an unreasonably large value in these iterations and then setting it back to the correct value in subsequent iterations.   
*   Other than this, the top part of this function should be reasonably obvious. We initialize the variable  [2.x.219]  to the most negative value representable with double precision numbers so that the comparison whether the current residual is less than that of the previous step will always fail in the first step.
* 

* 
* [1.x.162]
* 
*  It gets a bit more hairy after we have computed the trial solution  [2.x.220]  of the current Newton step. We handle a highly nonlinear problem so we have to damp Newton's method using a line search. To understand how we do this, recall that in our formulation, we compute a trial solution in each Newton step and not the update between old and new solution. Since the solution set is a convex set, we will use a line search that tries linear combinations of the previous and the trial solution to guarantee that the damped solution is in our solution set again. At most we apply 5 damping steps.         
*   There are exceptions to when we use a line search. First, if this is the first Newton step on any mesh, then we don't have any point to compare the residual to, so we always accept a full step. Likewise, if this is the second Newton step on the first mesh (or the second on any mesh if we don't transfer solutions from mesh to mesh), then we have computed the first of these steps using just an elastic model (see how we set the yield stress sigma to an unreasonably large value above). In this case, the first Newton solution was a purely elastic one, the second one a plastic one, and any linear combination would not necessarily be expected to lie in the feasible set
* 
*  -  so we just accept the solution we just got.         
*   In either of these two cases, we bypass the line search and just update residual and other vectors as necessary.
* 

* 
* [1.x.163]
* 
*  The final step is to check for convergence. If the active set has not changed across all processors and the residual is less than a threshold of  [2.x.221] , then we terminate the iteration on the current mesh:
* 

* 
* [1.x.164]
* 
*   [1.x.165]  [1.x.166]
* 

* 
*  If you've made it this far into the deal.II tutorial, the following function refining the mesh should not pose any challenges to you any more. It refines the mesh, either globally or using the Kelly error estimator, and if so asked also transfers the solution from the previous to the next mesh. In the latter case, we also need to compute the active set and other quantities again, for which we need the information computed by  [2.x.222] .
* 

* 
* [1.x.167]
* 
*  enforce constraints to make the interpolated solution conforming on the new mesh:
* 

* 
* [1.x.168]
* 
*   [1.x.169]  [1.x.170]
* 

* 
*  The remaining three functions before we get to  [2.x.223]  have to do with generating output. The following one is an attempt at showing the deformed body in its deformed configuration. To this end, this function takes a displacement vector field and moves every vertex of the (local part) of the mesh by the previously computed displacement. We will call this function with the current displacement field before we generate graphical output, and we will call it again after generating graphical output with the negative displacement field to undo the changes to the mesh so made.   
*   The function itself is pretty straightforward. All we have to do is keep track which vertices we have already touched, as we encounter the same vertices multiple times as we loop over cells.
* 

* 
* [1.x.171]
* 
*   [1.x.172]  [1.x.173]
* 

* 
*  Next is the function we use to actually generate graphical output. The function is a bit tedious, but not actually particularly complicated. It moves the mesh at the top (and moves it back at the end), then computes the contact forces along the contact surface. We can do so (as shown in the accompanying paper) by taking the untreated residual vector and identifying which degrees of freedom correspond to those with contact by asking whether they have an inhomogeneous constraints associated with them. As always, we need to be mindful that we can only write into completely distributed vectors (i.e., vectors without ghost elements) but that when we want to generate output, we need vectors that do indeed have ghost entries for all locally relevant degrees of freedom.
* 

* 
* [1.x.174]
* 
*  Calculation of the contact forces
* 

* 
* [1.x.175]
* 
*  In the remainder of the function, we generate one VTU file on every processor, indexed by the subdomain id of this processor. On the first processor, we then also create a  [2.x.224]  file that indexes [1.x.176] of the VTU files so that the entire set of output files can be read at once. These  [2.x.225]  are used by Paraview to describe an entire parallel computation's output files. We then do the same again for the competitor of Paraview, the VisIt visualization program, by creating a matching  [2.x.226]  file.
* 

* 
* [1.x.177]
* 
*   [1.x.178]  [1.x.179]
* 

* 
*  This last auxiliary function computes the contact force by calculating an integral over the contact pressure in z-direction over the contact area. For this purpose we set the contact pressure lambda to 0 for all inactive dofs (whether a degree of freedom is part of the contact is determined just as we did in the previous function). For all active dofs, lambda contains the quotient of the nonlinear residual (newton_rhs_uncondensed) and corresponding diagonal entry of the mass matrix (diag_mass_matrix_vector). Because it is not unlikely that hanging nodes show up in the contact area it is important to apply constraints_hanging_nodes.distribute to the distributed_lambda vector.
* 

* 
* [1.x.180]
* 
*   [1.x.181]  [1.x.182]
* 

* 
*  As in all other tutorial programs, the  [2.x.227]  function contains the overall logic. There is not very much to it here: in essence, it performs the loops over all mesh refinement cycles, and within each, hands things over to the Newton solver in  [2.x.228]  on the current mesh and calls the function that creates graphical output for the so-computed solution. It then outputs some statistics concerning both run times and memory consumption that has been collected over the course of computations on this mesh.
* 

* 
* [1.x.183]
* 
*   [1.x.184]  [1.x.185]
* 

* 
*  There really isn't much to the  [2.x.229]  function. It looks like they always do:
* 

* 
* [1.x.186]
* [1.x.187][1.x.188]
* 

* The directory that contains this program also contains a number of inputparameter files that can be used to create various differentsimulations. For example, running the program with the [2.x.230]  parameter file (using a ball as obstacle and thebox as domain) on 16 cores produces output like this:
* [1.x.189]
* 
* The tables at the end of each cycle show information about computing time(these numbers are of course specific to the machine on which this outputwas produced)and the number of calls of different parts of the program like assembly orcalculating the residual, for the most recent mesh refinement cycle. Some ofthe numbers above can be improved by transferring the solution from one mesh tothe next, an option we have not exercised here. Of course, you can also makethe program run faster, especially on the later refinement cycles, by justusing more processors: the accompanying paper shows good scaling to at least1000 cores.
* In a typical run, you can observe that for every refinement step, the activeset
* 
*  - the contact points
* 
*  - are iterated out at first. After that the Newtonmethod has only to resolve the plasticity. For the finer meshes,quadratic convergence can be observed for the last 4 or 5 Newton iterations.
* We will not discuss here in all detail what happens with each of the inputfiles. Rather, let us just show pictures of the solution (the left half of thedomain is omitted if cells have zero quadrature points at which the plasticinequality is active):
*  [2.x.231] 
* The picture shows the adaptive refinement and as well how much a cell isplastified during the contact with the ball. Remember that we consider thenorm of the deviator part of the stress in each quadrature point tosee if there is elastic or plastic behavior.The bluecolor means that this cell contains only elastic quadrature points incontrast to the red cells in which all quadrature points are plastified.In the middle of the top surface
* 
*  - here the mesh is finest
* 
*  - a very close look shows the dimple caused by theobstacle. This is the result of the  [2.x.232] function. However, because the indentation of the obstacles we consider hereis so small, it is hard to discern this effect; one could play with displacingvertices of the mesh by a multiple of the computed displacement.
* Further discussion of results that can be obtained using this program isprovided in the publication mentioned at the very top of this page.
* 

* [1.x.190][1.x.191][1.x.192]
* 

* There are, as always, multiple possibilities for extending this program. Froman algorithmic perspective, this program goes about as far as one can at thetime of writing, using the best available algorithms for the contactinequality, the plastic nonlinearity, and the linear solvers. However, thereare things one would like to do with this program as far as more realisticsituations are concerned: [2.x.233]  [2.x.234]  Extend the program from a static to a quasi-static situation, perhaps bychoosing a backward-Euler-scheme for the time discretization. Some theoreticalresults can be found in the PhD thesis by Jörg Frohne, [1.x.193], Universityof Siegen, Germany, 2011.
*  [2.x.235]  It would also be an interesting advance to consider a contact problemwith friction. In almost every mechanical process friction has a biginfluence.  To model this situation, we have to take into account tangentialstresses at the contact surface. Friction also adds another inequality toour problem since body and obstacle will typically stick together as long asthe tangential stress does not exceed a certain limit, beyond which the twobodies slide past each other.
*  [2.x.236]  If we already simulate a frictional contact, the next step to consideris heat generation over the contact zone. The heat that iscaused by friction between two bodies raises the temperature in thedeformable body and entails an change of some material parameters.
*  [2.x.237]  It might be of interest to implement more accurate, problem-adapted errorestimators for contact as well as for the plasticity. [2.x.238] 
* 

* [1.x.194][1.x.195] [2.x.239] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-43_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34][1.x.35][1.x.36][1.x.37][1.x.38][1.x.39][1.x.40][1.x.41][1.x.42][1.x.43]
*  [2.x.2] 
* [1.x.44]
* 

* [1.x.45][1.x.46] [1.x.47]
* 

* The simulation of multiphase flow in porous media is a ubiquitous problem, andwe have previously addressed it already in some form in  [2.x.3]  and [2.x.4] . However, as was easy to see there, it faces two major difficulties:numerical accuracy and efficiency. The first is easy to see in the stationarysolver  [2.x.5] : using lowest order Raviart-Thomas elements can not be expectedto yield highly accurate solutions. We need more accurate methods. The secondreason is apparent from the time dependent  [2.x.6] : that program isexcruciatingly slow, and there is no hope to get highly accurate solutions in3d within reasonable time frames.
* In thisprogram, in order to overcome these two problems, there are five areas whichwe are trying to improve for a high performance simulator:
*  [2.x.7]  [2.x.8]  Higher order spatial discretizations [2.x.9]  Adaptive mesh refinement [2.x.10]  Adaptive time stepping [2.x.11]  Operator splitting [2.x.12]  Efficient solver and preconditioning [2.x.13] 
* Much inspiration for this program comes from  [2.x.14]  but several of thetechniques discussed here are original.
* 

* [1.x.48][1.x.49]
* 

* We consider the flow of a two-phase immiscible, incompressiblefluid. Capillary and gravity effects are neglected, and viscouseffects are assumed dominant. The governing equations for such aflow that are identical to those used in  [2.x.15]  and are
* [1.x.50]
* where  [2.x.16]  is the saturation (volume fraction between zero and one) of the second (wetting) phase,  [2.x.17]  is the pressure,  [2.x.18]  is the permeability tensor,  [2.x.19]  is the total mobility,  [2.x.20]  is the porosity,  [2.x.21]  is the fractional flow of the wetting phase,  [2.x.22]  is the source term and  [2.x.23]  is the total velocity. The total mobility, fractional flow of the wetting phase and total velocity are respectively given by
* [1.x.51]
* where subscripts  [2.x.24]  represent the wetting and non-wetting phases,respectively.
* For convenience, theporosity  [2.x.25]  in the saturation equation, which can be considered ascaling factor for the time variable, is set toone. Following a commonly used prescription for the dependence of the relativepermeabilities  [2.x.26]  and  [2.x.27]  on saturation, we use
* [1.x.52]
* 
* The porous media equations above areaugmented by initial conditions for the saturation and boundary conditions forthe pressure. Since saturation and the gradient of the pressure uniquelydetermine the velocity, no boundary conditions are necessary for the velocity.Since the flow equations do not contain time derivatives, initial conditions for the velocity and pressurevariables are not required. The flow field separates the boundary into inflow or outflowparts. Specifically,[1.x.53]and we arrive at a complete model by also imposing boundary values for thesaturation variable on the inflow boundary  [2.x.28] .
* 

* [1.x.54][1.x.55]
* 

* As seen in  [2.x.29] , solving the flow equations for velocity and pressure arethe parts of the program that take far longer than the (explicit) updatingstep for the saturation variable once we know the flow variables. On the otherhand,  the pressure and velocity depend only weakly on saturation, so one maythink about only solving for pressure and velocity every few time steps whileupdating the saturation in every step. If we can find a criterion for when theflow variables need to be updated, we call this splitting an "adaptiveoperator splitting" scheme.
* Here, we use the following a posteriori criterion to decide when to re-computepressure and velocity variables(detailed derivations and descriptions can be found in [Chueh, Djilaliand Bangerth 2011]):
* [1.x.56]
* where superscripts in parentheses denote the number of the saturation timestep at which any quantity is defined and  [2.x.30]  represents the last stepwhere we actually computed the pressure and velocity. If  [2.x.31] exceeds a certain threshold we re-compute the flow variables; otherwise, weskip this computation in time step  [2.x.32]  and only move the saturation variableone time step forward.
* In short, the algorithm allows us to perform a number ofsaturation time steps of length  [2.x.33]  untilthe criterion above tells us to re-compute velocity and pressurevariables, leading to a macro time step of length[1.x.57]We choose the length of (micro) steps subject to the Courant-Friedrichs-Lewy(CFL) restriction according to the criterion[1.x.58]which we have confirmed to be stable for the choice of finite element and timestepping scheme for the saturation equation discussed below ( [2.x.34]  denotes thediameter of cell  [2.x.35] ).The result is a scheme where neither micro nor macro timesteps are of uniform length, and both are chosen adaptively.
* [1.x.59][1.x.60]
* Using this time discretization, we obtain the following set of equations foreach time step from the IMPES approach (see  [2.x.36] ):
* [1.x.61]
* 
* 

* Using the fact that  [2.x.37] , the time discretesaturation equation becomes
* [1.x.62]
* 
* [1.x.63][1.x.64]
* 

* By multiplying the equations defining the total velocity  [2.x.38]  andthe equation that expresses its divergence in terms of source terms, with testfunctions  [2.x.39]  and  [2.x.40] respectively and then integrating terms by parts as necessary, the weak formof the problem reads: Find  [2.x.41]  so that for all test functions [2.x.42]  there holds[1.x.65]
* Here,  [2.x.43]  represents the unit outward normal vector to  [2.x.44]  and the pressure  [2.x.45]  can be prescribed weakly on the open partof the boundary  [2.x.46]  whereas on those parts where a velocity isprescribed (for example impermeable boundaries with  [2.x.47]  the term disappears altogether because  [2.x.48] .
* We use continuous finite elements to discretize the velocity and pressureequations. Specifically, we use mixed finite elements to ensure high order approximationfor both vector (e.g. a fluid velocity) and scalar variables (e.g. pressure)simultaneously. For saddle point problems, it is well established thatthe so-called Babuska-Brezzi or Ladyzhenskaya-Babuska-Brezzi (LBB) conditions[Brezzi 1991, Chen 2005] need to be satisfied to ensure stability ofthe pressure-velocity system. These stability conditions are satisfied in thepresent work by using elements for velocity that are one order higher than forthe pressure, i.e.  [2.x.49]  and  [2.x.50] , where  [2.x.51] ,  [2.x.52]  isthe space dimension, and  [2.x.53]  denotes the space of tensor product Lagrangepolynomials of degree  [2.x.54]  in each variable.
* [1.x.66][1.x.67]
* The chosen  [2.x.55]  elements for the saturation equation do not lead to a stablediscretization without upwinding or other kinds of stabilization, and spuriousoscillations will appear in the numerical solution. Adding an artificialdiffusion term is one approach to eliminating these oscillations[Chen 2005]. On the other hand, adding too much diffusion smears sharpfronts in the solution and suffers from grid-orientation difficulties[Chen 2005]. To avoid these effects, we use the artificial diffusionterm proposed by [Guermond and Pasquetti 2008] andvalidated in [Chueh, Djilali, Bangerth 2011] and[Kronbichler, Heister and Bangerth, 2011], as well as in  [2.x.56] .
* This method modifies the (discrete) weak form of the saturation equationto read
* [1.x.68]
* where  [2.x.57]  is the artificial diffusion parameter and  [2.x.58]  is anappropriately chosen numerical flux on the boundary of the domain (we choosethe obvious full upwind flux for this).
* Following [Guermond and Pasquetti 2008] (and as detailed in[Chueh, Djilali and Bangerth 2011]), we usethe parameter as a piecewiseconstant function set on each cell  [2.x.59]  with the diameter  [2.x.60]  as[1.x.69]where  [2.x.61]  is a stabilization exponent and  [2.x.62]  is a dimensionlessuser-defined stabilization constant. Following [Guermond and Pasquetti 2008]as well as the implementation in  [2.x.63] , the velocity and saturation globalnormalization constant,  [2.x.64] , and the residual  [2.x.65] are respectively given by[1.x.70]and[1.x.71]where  [2.x.66]  is a second dimensionless user-defined constant, [2.x.67]  is the diameter of the domain and  [2.x.68]  is the range of the presentsaturation values in the entire computational domain  [2.x.69] .
* This stabilization scheme has a number of advantages over simpler schemes suchas finite volume (or discontinuous Galerkin) methods or streamline upwindPetrov Galerkin (SUPG) discretizations. In particular, the artificialdiffusion term acts primarily in the vicinity of discontinuitiessince the residual is small in areas where the saturation is smooth. Ittherefore provides for a higher degree of accuracy. On the other hand, it isnonlinear since  [2.x.70]  depends on the saturation  [2.x.71] . We avoid this difficultyby treating all nonlinear terms explicitly, which leads to the followingfully discrete problem at time step  [2.x.72] :
* [1.x.72]
* where  [2.x.73]  is the velocity linearly extrapolated from [2.x.74]  and  [2.x.75]  to the current time  [2.x.76]  if  [2.x.77]  while  [2.x.78]  is  [2.x.79]  if  [2.x.80] .Consequently, the equation is linear in  [2.x.81]  and all that is requiredis to solve with a mass matrix on the saturation space.
* Since the Dirichlet boundary conditions for saturation are only imposed on theinflow boundaries, the third term on the left hand side of the equation aboveneeds to be split further into two parts:
* [1.x.73]
* where  [2.x.82]  and [2.x.83]  represent inflow and outflow boundaries,respectively. We choose values using anupwind formulation, i.e.  [2.x.84]  and  [2.x.85] correspond to the values taken from the present cell, while the values of [2.x.86]  and  [2.x.87]  are those taken from theneighboring boundary  [2.x.88] .
* 

* [1.x.74][1.x.75]
* 

* Choosing meshes adaptively to resolve sharpsaturation fronts is an essential ingredient to achieve efficiency in ouralgorithm. Here, we use the same shock-type refinement approach used in[Chueh, Djilali and Bangerth 2011] to select those cells that should be refined orcoarsened. The refinement indicator for each cell  [2.x.89]  of the triangulation iscomputed by[1.x.76]where  [2.x.90]  is the gradient of the discrete saturationvariable evaluated at the center  [2.x.91]  of cell  [2.x.92] . This approach isanalogous to ones frequently used in compressible flow problems, where densitygradients are used to indicate refinement. That said, as we willdiscuss at the end of the [1.x.77], this turnsout to not be a very useful criterion since it leads to refinement basicallyeverywhere. We only show it here for illustrative purposes.
* 

* [1.x.78][1.x.79]
* 

* Following the discretization of the governing equationsdiscussed above, weobtain a linear system of equations in time step  [2.x.93]  of the following form:[1.x.80]where the individual matrices and vectors are defined as follows using shape functions  [2.x.94]  for velocity, and  [2.x.95]  for both pressure and saturation:
* [1.x.81]
* and  [2.x.96]  as given in the definition of the stabilized transportequation.
* The linear system above is of block triangular form if we consider the topleft  [2.x.97]  panel of matrices as one block. We can therefore first solvefor the velocity and pressure (unless we decide to use  [2.x.98]  inplace of the velocity)followed by a solve for the saturation variable. The first of these stepsrequires us to solve[1.x.82]We apply the Generalized Minimal Residual (GMRES) method [Saad and Schultz1986] to this linear system. The ideal preconditioner for thevelocity-pressure system is
* [1.x.83]
* where [2.x.99]  isthe Schur complement [Zhang 2005] of the system. This preconditioner isoptimal since
* [1.x.84]
* for which it can be shown that GMRES converges in two iterations.
* However, we cannot of course expect to use exact inverses of thevelocity mass matrix and the Schur complement. We therefore follow theapproach by [Silvester and Wathen 1994] originally proposed forthe Stokes system. Adapting it to the current set of equations yield thepreconditioner
* [1.x.85]
* where a tilde indicates an approximation of the exact inverse matrix. Inparticular, since  [2.x.100] is a sparse symmetric and positive definite matrix, we choose for [2.x.101]  a single application ofa sparse incomplete Cholesky decomposition of this matrix[Golub and Van Loan 1996].We note that the Schur complement that corresponds to the porousmedia flow operator in non-mixed form,  [2.x.102]  and [2.x.103] should be a good approximation of the actual Schur complement matrix  [2.x.104] . Since both of these matrices are again symmetric and positive definite, weuse an incomplete Cholesky decomposition of  [2.x.105]  for  [2.x.106] . It is important to note that  [2.x.107]  needsto be built with Dirichlet boundary conditions to ensure its invertibility.
* Once the velocity  [2.x.108]   is available, wecan assemble  [2.x.109]  and [2.x.110]  and solve for the saturations using
* [1.x.86]
* where the mass matrix  [2.x.111]  is solved by the conjugate gradientmethod, using an incomplete Cholesky decomposition as preconditioner oncemore.
* [1.x.87][1.x.88]
* 

* 
*  [2.x.112] The implementation discussed here uses and extendsparts of the  [2.x.113] ,  [2.x.114]  and  [2.x.115]  tutorial programs of thislibrary. In particular, if you want to understand how it works, pleaseconsult  [2.x.116]  for a discussion of the mathematical problem, and [2.x.117]  from which most of the implementation is derived. We will notdiscuss aspects of the implementation that have already been discussedin  [2.x.118] .
* We show numerical results for some two-phase flow equations augmented byappropriate initial and boundary conditions in conjunction with two differentchoices of the permeability model. In the problems considered, there is nointernal source term ( [2.x.119] ). As mentioned above, quantitative numericalresults are presented in [Chueh, Djilali and Bangerth 2011].
* For simplicity, we choose  [2.x.120] , though all methods (as wellas our implementation) should work equally well on general unstructured meshes.
* Initial conditions are only required for the saturation variable, and wechoose  [2.x.121] , i.e. the porous medium is initially filled by amixture of the non-wetting (80%) and wetting (20%) phases. This differs fromthe initial condition in  [2.x.122]  where we had taken  [2.x.123] , butfor complicated mathematical reasons that are mentioned there in a longishremark, the current method using an entropy-based artificial diffusion termdoes not converge to the viscosity solution with this initial conditionwithout additional modifications to the method. We therefore choose thismodified version for the current program.
* Furthermore, we prescribe a linear pressure onthe boundaries:[1.x.89]Pressure and saturation uniquelydetermine a velocity, and the velocity determines whether a boundary segmentis an inflow or outflow boundary. On the inflow part of the boundary, [2.x.124] , we impose
* [1.x.90]
* In other words, the domain is flooded by the wetting phase from the left.No boundary conditions for the saturation are required for the outflow partsof the boundary.
* All the numerical and physical parameters used for the 2D/3Dcases are listed in the following table:
*  [2.x.125] 
* 

* [1.x.91][1.x.92]
* 

* 
*  [2.x.126]  [2.x.127] CC Chueh, N Djilali and W Bangerth. [2.x.128]  An h-adaptive operator splitting method for two-phase flow in 3D  heterogeneous porous media. [2.x.129]  SIAM Journal on Scientific Computing, vol. 35 (2013), pp. B149-B175
*  [2.x.130] M. Kronbichler, T. Heister, and W. Bangerth [2.x.131]  High Accuracy Mantle Convection Simulation through Modern NumericalMethods. [2.x.132]  Geophysics Journal International, vol. 191 (2012), pp. 12-29
*  [2.x.133] F Brezzi and M Fortin. [2.x.134]  [1.x.93]. [2.x.135]  Springer-Verlag, 1991.
*  [2.x.136] Z Chen. [2.x.137]  [1.x.94]. [2.x.138]  Springer, 2005.
*  [2.x.139] JL Guermond and R Pasquetti. [2.x.140]  Entropy-based nonlinear viscosity for Fourier approximations of  conservation laws. [2.x.141]  [1.x.95], 346(13-14):801-806, 2008.
*  [2.x.142] CC Chueh, M Secanell, W Bangerth, and N Djilali. [2.x.143]  Multi-level adaptive simulation of transient two-phase flow in  heterogeneous porous media. [2.x.144]  [1.x.96], 39:1585-1596, 2010.
*  [2.x.145] Y Saad and MH Schultz. [2.x.146]  Gmres: A generalized minimal residual algorithm for solving  nonsymmetric linear systems. [2.x.147]  [1.x.97],  7(3):856-869, 1986.
*  [2.x.148] F Zhang. [2.x.149]  [1.x.98]. [2.x.150]  Springer, 2005.
*  [2.x.151] D Silvester and A Wathen. [2.x.152]  Fast iterative solution of stabilised Stokes systems part ii: Using  general block preconditioners. [2.x.153]  [1.x.99], 31(5):1352-1367, 1994.
*  [2.x.154] GH Golub and CF van Loan. [2.x.155]  [1.x.100]. [2.x.156]  3rd Edition, Johns Hopkins, 1996.
*  [2.x.157] SE Buckley and MC Leverett. [2.x.158]  Mechanism of fluid displacements in sands. [2.x.159]  [1.x.101], 146:107-116, 1942.
*  [2.x.160] 
* 

*  [1.x.102] [1.x.103]
*   [1.x.104]  [1.x.105]
* 

* 
*  The first step, as always, is to include the functionality of a number of deal.II and C++ header files.
* 

* 
*  The list includes some header files that provide vector, matrix, and preconditioner classes that implement interfaces to the respective Trilinos classes; some more information on these may be found in  [2.x.161] .
* 

* 
* [1.x.106]
* 
*  At the end of this top-matter, we open a namespace for the current project into which all the following material will go, and then import all deal.II names into this namespace:
* 

* 
* [1.x.107]
* 
*   [1.x.108]  [1.x.109]
* 

* 
*  The following part is taken directly from  [2.x.162]  so there is no need to repeat the descriptions found there.
* 

* 
* [1.x.110]
* 
*   [1.x.111]  [1.x.112]
* 

* 
*  In this tutorial, we still use the two permeability models previously used in  [2.x.163]  so we again refrain from commenting in detail about them.
* 

* 
* [1.x.113]
* 
*   [1.x.114]  [1.x.115]
* 

* 
*  The implementations of all the physical quantities such as total mobility  [2.x.164]  and fractional flow of water  [2.x.165]  are taken from  [2.x.166]  so again we don't have do any comment about them. Compared to  [2.x.167]  we have added checks that the saturation passed to these functions is in fact within the physically valid range. Furthermore, given that the wetting phase moves at speed  [2.x.168]  it is clear that  [2.x.169]  must be greater or equal to zero, so we assert that as well to make sure that our calculations to get at the formula for the derivative made sense.
* 

* 
* [1.x.116]
* 
*   [1.x.117]  [1.x.118]
* 

* 
*  In this first part we define a number of classes that we need in the construction of linear solvers and preconditioners. This part is essentially the same as that used in  [2.x.170] . The only difference is that the original variable name stokes_matrix is replaced by another name darcy_matrix to match our problem.
* 

* 
* [1.x.119]
* 
*   [1.x.120]  [1.x.121]
* 

* 
*  The definition of the class that defines the top-level logic of solving the time-dependent advection-dominated two-phase flow problem (or Buckley-Leverett problem [Buckley 1942]) is mainly based on tutorial programs  [2.x.171]  and  [2.x.172] , and in particular on  [2.x.173]  where we have used basically the same general structure as done here. As in  [2.x.174] , the key routines to look for in the implementation below are the  [2.x.175]  functions.   
*   The main difference to  [2.x.176]  is that, since adaptive operator splitting is considered, we need a couple more member variables to hold the last two computed Darcy (velocity/pressure) solutions in addition to the current one (which is either computed directly, or extrapolated from the previous two), and we need to remember the last two times we computed the Darcy solution. We also need a helper function that figures out whether we do indeed need to recompute the Darcy solution.   
*   Unlike  [2.x.177] , this step uses one more AffineConstraints object called darcy_preconditioner_constraints. This constraint object is used only for assembling the matrix for the Darcy preconditioner and includes hanging node constraints as well as Dirichlet boundary value constraints for the pressure variable. We need this because we are building a Laplace matrix for the pressure as an approximation of the Schur complement) which is only positive definite if boundary conditions are applied.   
*   The collection of member functions and variables thus declared in this class is then rather similar to those in  [2.x.178] :
* 

* 
* [1.x.122]
* 
*  We follow with a number of helper functions that are used in a variety of places throughout the program:
* 

* 
* [1.x.123]
* 
*  This all is followed by the member variables, most of which are similar to the ones in  [2.x.179] , with the exception of the ones that pertain to the macro time stepping for the velocity/pressure system:
* 

* 
* [1.x.124]
* 
*  At the very end we declare a variable that denotes the material model. Compared to  [2.x.180] , we do this here as a member variable since we will want to use it in a variety of places and so having a central place where such a variable is declared will make it simpler to replace one class by another (e.g. replace  [2.x.181]  by  [2.x.182] 
* 

* 
* [1.x.125]
* 
*   [1.x.126]  [1.x.127]
* 

* 
*  The constructor of this class is an extension of the constructors in  [2.x.183]  and  [2.x.184] . We need to add the various variables that concern the saturation. As discussed in the introduction, we are going to use  [2.x.185]  (Taylor-Hood) elements again for the Darcy system, an element combination that fulfills the Ladyzhenskaya-Babuska-Brezzi (LBB) conditions [Brezzi and Fortin 1991, Chen 2005], and  [2.x.186]  elements for the saturation. However, by using variables that store the polynomial degree of the Darcy and temperature finite elements, it is easy to consistently modify the degree of the elements as well as all quadrature formulas used on them downstream. Moreover, we initialize the time stepping variables related to operator splitting as well as the option for matrix assembly and preconditioning:
* 

* 
* [1.x.128]
* 
*   [1.x.129]  [1.x.130]
* 

* 
*  This is the function that sets up the DoFHandler objects we have here (one for the Darcy part and one for the saturation part) as well as set to the right sizes the various objects required for the linear algebra in this program. Its basic operations are similar to what  [2.x.187]  did.   
*   The body of the function first enumerates all degrees of freedom for the Darcy and saturation systems. For the Darcy part, degrees of freedom are then sorted to ensure that velocities precede pressure DoFs so that we can partition the Darcy matrix into a  [2.x.188]  matrix.   
*   Then, we need to incorporate hanging node constraints and Dirichlet boundary value constraints into darcy_preconditioner_constraints.  The boundary condition constraints are only set on the pressure component since the Schur complement preconditioner that corresponds to the porous media flow operator in non-mixed form,  [2.x.189] , acts only on the pressure variable. Therefore, we use a component_mask that filters out the velocity component, so that the condensation is performed on pressure degrees of freedom only.   
*   After having done so, we count the number of degrees of freedom in the various blocks. This information is then used to create the sparsity pattern for the Darcy and saturation system matrices as well as the preconditioner matrix from which we build the Darcy preconditioner. As in  [2.x.190] , we choose to create the pattern using the blocked version of DynamicSparsityPattern. So, for this, we follow the same way as  [2.x.191]  did and we don't have to repeat descriptions again for the rest of the member function.
* 

* 
* [1.x.131]
* 
*   [1.x.132]  [1.x.133]
* 

* 
*  The next few functions are devoted to setting up the various system and preconditioner matrices and right hand sides that we have to deal with in this program.
* 

* 
*   [1.x.134]  [1.x.135]
* 

* 
*  This function assembles the matrix we use for preconditioning the Darcy system. What we need are a vector mass matrix weighted by  [2.x.192]  on the velocity components and a mass matrix weighted by  [2.x.193]  on the pressure component. We start by generating a quadrature object of appropriate order, the FEValues object that can give values and gradients at the quadrature points (together with quadrature weights). Next we create data structures for the cell matrix and the relation between local and global DoFs. The vectors phi_u and grad_phi_p are going to hold the values of the basis functions in order to faster build up the local matrices, as was already done in  [2.x.194] . Before we start the loop over all active cells, we have to specify which components are pressure and which are velocity.   
*   The creation of the local matrix is rather simple. There are only a term weighted by  [2.x.195]  (on the velocity) and a Laplace matrix weighted by  [2.x.196]  to be generated, so the creation of the local matrix is done in essentially two lines. Since the material model functions at the top of this file only provide the inverses of the permeability and mobility, we have to compute  [2.x.197]  and  [2.x.198]  by hand from the given values, once per quadrature point.   
*   Once the local matrix is ready (loop over rows and columns in the local matrix on each quadrature point), we get the local DoF indices and write the local information into the global matrix. We do this by directly applying the constraints (i.e. darcy_preconditioner_constraints) that takes care of hanging node and zero Dirichlet boundary condition constraints. By doing so, we don't have to do that afterwards, and we later don't have to use  [2.x.199]  and  [2.x.200]  both functions that would need to modify matrix and vector entries and so are difficult to write for the Trilinos classes where we don't immediately have access to individual memory locations.
* 

* 
* [1.x.136]
* 
*   [1.x.137]  [1.x.138]
* 

* 
*  After calling the above functions to assemble the preconditioner matrix, this function generates the inner preconditioners that are going to be used for the Schur complement block preconditioner. The preconditioners need to be regenerated at every saturation time step since they depend on the saturation  [2.x.201]  that varies with time.   
*   In here, we set up the preconditioner for the velocity-velocity matrix  [2.x.202]  and the Schur complement  [2.x.203] . As explained in the introduction, we are going to use an IC preconditioner based on the vector matrix  [2.x.204]  and another based on the scalar Laplace matrix  [2.x.205]  (which is spectrally close to the Schur complement of the Darcy matrix). Usually, the  [2.x.206]  class can be seen as a good black-box preconditioner which does not need any special knowledge of the matrix structure and/or the operator that's behind it.
* 

* 
* [1.x.139]
* 
*   [1.x.140]  [1.x.141]
* 

* 
*  This is the function that assembles the linear system for the Darcy system.   
*   Regarding the technical details of implementation, the procedures are similar to those in  [2.x.207]  and  [2.x.208] . We reset matrix and vector, create a quadrature formula on the cells, and then create the respective FEValues object.   
*   There is one thing that needs to be commented: since we have a separate finite element and DoFHandler for the saturation, we need to generate a second FEValues object for the proper evaluation of the saturation solution. This isn't too complicated to realize here: just use the saturation structures and set an update flag for the basis function values which we need for evaluation of the saturation solution. The only important part to remember here is that the same quadrature formula is used for both FEValues objects to ensure that we get matching information when we loop over the quadrature points of the two objects.   
*   The declarations proceed with some shortcuts for array sizes, the creation of the local matrix, right hand side as well as the vector for the indices of the local dofs compared to the global system.
* 

* 
* [1.x.142]
* 
*  Next we need a vector that will contain the values of the saturation solution at the previous time level at the quadrature points to assemble the saturation dependent coefficients in the Darcy equations.     
*   The set of vectors we create next hold the evaluations of the basis functions as well as their gradients that will be used for creating the matrices. Putting these into their own arrays rather than asking the FEValues object for this information each time it is needed is an optimization to accelerate the assembly process, see  [2.x.209]  for details.     
*   The last two declarations are used to extract the individual blocks (velocity, pressure, saturation) from the total FE system.
* 

* 
* [1.x.143]
* 
*  Now start the loop over all cells in the problem. We are working on two different DoFHandlers for this assembly routine, so we must have two different cell iterators for the two objects in use. This might seem a bit peculiar, but since both the Darcy system and the saturation system use the same grid we can assume that the two iterators run in sync over the cells of the two DoFHandler objects.     
*   The first statements within the loop are again all very familiar, doing the update of the finite element data as specified by the update flags, zeroing out the local arrays and getting the values of the old solution at the quadrature points.  At this point we also have to get the values of the saturation function of the previous time step at the quadrature points. To this end, we can use the  [2.x.210]  (previously already used in  [2.x.211] ,  [2.x.212]  and  [2.x.213] ), a function that takes a solution vector and returns a list of function values at the quadrature points of the present cell. In fact, it returns the complete vector-valued solution at each quadrature point, i.e. not only the saturation but also the velocities and pressure.     
*   Then we are ready to loop over the quadrature points on the cell to do the integration. The formula for this follows in a straightforward way from what has been discussed in the introduction.     
*   Once this is done, we start the loop over the rows and columns of the local matrix and feed the matrix with the relevant products.     
*   The last step in the loop over all cells is to enter the local contributions into the global matrix and vector structures to the positions specified in local_dof_indices. Again, we let the AffineConstraints class do the insertion of the cell matrix elements to the global matrix, which already condenses the hanging node constraints.
* 

* 
* [1.x.144]
* 
*   [1.x.145]  [1.x.146]
* 

* 
*  This function is to assemble the linear system for the saturation transport equation. It calls, if necessary, two other member functions: assemble_saturation_matrix() and assemble_saturation_rhs(). The former function then assembles the saturation matrix that only needs to be changed occasionally. On the other hand, the latter function that assembles the right hand side must be called at every saturation time step.
* 

* 
* [1.x.147]
* 
*   [1.x.148]  [1.x.149]
* 

* 
*  This function is easily understood since it only forms a simple mass matrix for the left hand side of the saturation linear system by basis functions phi_i_s and phi_j_s only. Finally, as usual, we enter the local contribution into the global matrix by specifying the position in local_dof_indices. This is done by letting the AffineConstraints class do the insertion of the cell matrix elements to the global matrix, which already condenses the hanging node constraints.
* 

* 
* [1.x.150]
* 
*   [1.x.151]  [1.x.152]
* 

* 
*  This function is to assemble the right hand side of the saturation transport equation. Before going about it, we have to create two FEValues objects for the Darcy and saturation systems respectively and, in addition, two FEFaceValues objects for the two systems because we have a boundary integral term in the weak form of saturation equation. For the FEFaceValues object of the saturation system, we also require normal vectors, which we request using the update_normal_vectors flag.   
*   Next, before looping over all the cells, we have to compute some parameters (e.g. global_u_infty, global_S_variation, and global_Omega_diameter) that the artificial viscosity  [2.x.214]  needs. This is largely the same as was done in  [2.x.215] , so you may see there for more information.   
*   The real works starts with the loop over all the saturation and Darcy cells to put the local contributions into the global vector. In this loop, in order to simplify the implementation, we split some of the work into two helper functions: assemble_saturation_rhs_cell_term and assemble_saturation_rhs_boundary_term.  We note that we insert cell or boundary contributions into the global vector in the two functions rather than in this present function.
* 

* 
* [1.x.153]
* 
*   [1.x.154]  [1.x.155]
* 

* 
*  This function takes care of integrating the cell terms of the right hand side of the saturation equation, and then assembling it into the global right hand side vector. Given the discussion in the introduction, the form of these contributions is clear. The only tricky part is getting the artificial viscosity and all that is necessary to compute it. The first half of the function is devoted to this task.   
*   The last part of the function is copying the local contributions into the global vector with position specified in local_dof_indices.
* 

* 
* [1.x.156]
* 
*   [1.x.157]  [1.x.158]
* 

* 
*  The next function is responsible for the boundary integral terms in the right hand side form of the saturation equation.  For these, we have to compute the upwinding flux on the global boundary faces, i.e. we impose Dirichlet boundary conditions weakly only on inflow parts of the global boundary. As before, this has been described in  [2.x.216]  so we refrain from giving more descriptions about that.
* 

* 
* [1.x.159]
* 
*   [1.x.160]  [1.x.161]
* 

* 
*  This function implements the operator splitting algorithm, i.e. in each time step it either re-computes the solution of the Darcy system or extrapolates velocity/pressure from previous time steps, then determines the size of the time step, and then updates the saturation variable. The implementation largely follows similar code in  [2.x.217] . It is, next to the run() function, the central one in this program.   
*   At the beginning of the function, we ask whether to solve the pressure-velocity part by evaluating the a posteriori criterion (see the following function). If necessary, we will solve the pressure-velocity part using the GMRES solver with the Schur complement block preconditioner as is described in the introduction.
* 

* 
* [1.x.162]
* 
*  On the other hand, if we have decided that we don't want to compute the solution of the Darcy system for the current time step, then we need to simply extrapolate the previous two Darcy solutions to the same time as we would have computed the velocity/pressure at. We do a simple linear extrapolation, i.e. given the current length  [2.x.218]  of the macro time step from the time when we last computed the Darcy solution to now (given by  [2.x.219] ), and  [2.x.220]  the length of the last macro time step (given by  [2.x.221] ), then we get  [2.x.222] , where  [2.x.223]  and  [2.x.224]  are the last two computed Darcy solutions. We can implement this formula using just two lines of code.     
*   Note that the algorithm here only works if we have at least two previously computed Darcy solutions from which we can extrapolate to the current time, and this is ensured by requiring re-computation of the Darcy solution for the first 2 time steps.
* 

* 
* [1.x.163]
* 
*  With the so computed velocity vector, compute the optimal time step based on the CFL criterion discussed in the introduction...
* 

* 
* [1.x.164]
* 
*  ...and then also update the length of the macro time steps we use while we're dealing with time step sizes. In particular, this involves: (i) If we have just recomputed the Darcy solution, then the length of the previous macro time step is now fixed and the length of the current macro time step is, up to now, simply the length of the current (micro) time step. (ii) If we have not recomputed the Darcy solution, then the length of the current macro time step has just grown by  [2.x.225] .
* 

* 
* [1.x.165]
* 
*  The last step in this function is to recompute the saturation solution based on the velocity field we've just obtained. This naturally happens in every time step, and we don't skip any of these computations. At the end of computing the saturation, we project back into the allowed interval  [2.x.226]  to make sure our solution remains physical.
* 

* 
* [1.x.166]
* 
*   [1.x.167]  [1.x.168]
* 

* 
*  The next function does the refinement and coarsening of the mesh. It does its work in three blocks: (i) Compute refinement indicators by looking at the gradient of a solution vector extrapolated linearly from the previous two using the respective sizes of the time step (or taking the only solution we have if this is the first time step). (ii) Flagging those cells for refinement and coarsening where the gradient is larger or smaller than a certain threshold, preserving minimal and maximal levels of mesh refinement. (iii) Transferring the solution from the old to the new mesh. None of this is particularly difficult.
* 

* 
* [1.x.169]
* 
*   [1.x.170]  [1.x.171]
* 

* 
*  This function generates graphical output. It is in essence a copy of the implementation in  [2.x.227] .
* 

* 
* [1.x.172]
* 
*   [1.x.173]  [1.x.174]
* 

* 
*   [1.x.175]  [1.x.176]
* 

* 
*  This function implements the a posteriori criterion for adaptive operator splitting. The function is relatively straightforward given the way we have implemented other functions above and given the formula for the criterion derived in the paper.   
*   If one decides that one wants the original IMPES method in which the Darcy equation is solved in every time step, then this can be achieved by setting the threshold value  [2.x.228]  (with a default of  [2.x.229] ) to zero, thereby forcing the function to always return true.   
*   Finally, note that the function returns true unconditionally for the first two time steps to ensure that we have always solved the Darcy system at least twice when skipping its solution, thereby allowing us to extrapolate the velocity from the last two solutions in  [2.x.230] .
* 

* 
* [1.x.177]
* 
*   [1.x.178]  [1.x.179]
* 

* 
*  The next function simply makes sure that the saturation values always remain within the physically reasonable range of  [2.x.231] . While the continuous equations guarantee that this is so, the discrete equations don't. However, if we allow the discrete solution to escape this range we get into trouble because terms like  [2.x.232]  and  [2.x.233]  will produce unreasonable results (e.g.  [2.x.234]  for  [2.x.235] , which would imply that the wetting fluid phase flows [1.x.180] the direction of the bulk fluid velocity)). Consequently, at the end of each time step, we simply project the saturation field back into the physically reasonable region.
* 

* 
* [1.x.181]
* 
*   [1.x.182]  [1.x.183]   
*   Another simpler helper function: Compute the maximum of the total velocity times the derivative of the fraction flow function, i.e., compute  [2.x.236] . This term is used in both the computation of the time step as well as in normalizing the entropy-residual term in the artificial viscosity.
* 

* 
* [1.x.184]
* 
*   [1.x.185]  [1.x.186]   
*   For computing the stabilization term, we need to know the range of the saturation variable. Unlike in  [2.x.237] , this range is trivially bounded by the interval  [2.x.238]  but we can do a bit better by looping over a collection of quadrature points and seeing what the values are there. If we can, i.e., if there are at least two timesteps around, we can even take the values extrapolated to the next time step.   
*   As before, the function is taken with minimal modifications from  [2.x.239] .
* 

* 
* [1.x.187]
* 
*   [1.x.188]  [1.x.189]   
*   The final tool function is used to compute the artificial viscosity on a given cell. This isn't particularly complicated if you have the formula for it in front of you, and looking at the implementation in  [2.x.240] . The major difference to that tutorial program is that the velocity here is not simply  [2.x.241]  but  [2.x.242]  and some of the formulas need to be adjusted accordingly.
* 

* 
* [1.x.190]
* 
*   [1.x.191]  [1.x.192]
* 

* 
*  This function is, besides  [2.x.243] , the primary function of this program as it controls the time iteration as well as when the solution is written into output files and when to do mesh refinement.   
*   With the exception of the startup code that loops back to the beginning of the function through the  [2.x.244]  label, everything should be relatively straightforward. In any case, it mimics the corresponding function in  [2.x.245] .
* 

* 
* [1.x.193]
* 
*   [1.x.194]  [1.x.195]
* 

* 
*  The main function looks almost the same as in all other programs. The need to initialize the MPI subsystem for a program that uses Trilinos
* 
*  -  even for programs that do not actually run in parallel
* 
*  -  is explained in  [2.x.246] .
* 

* 
* [1.x.196]
* 
*  This program can only be run in serial. Otherwise, throw an exception.
* 

* 
* [1.x.197]
* [1.x.198][1.x.199]
* 

* 
* The output of this program is not really much different from that of [2.x.247] : it solves the same problem, after all. Of more importance arequantitative metrics such as the accuracy of the solution as well asthe time needed to compute it. These are documented in detail in thetwo publications listed at the top of this page and we won't repeatthem here.
* That said, no tutorial program is complete without a couple of goodpictures, so here is some output of a run in 3d:
*  [2.x.248] 
* 

* [1.x.200][1.x.201][1.x.202]
* 

* The primary objection one may have to this program is that it is still tooslow: 3d computations on reasonably fine meshes are simply too expensive to bedone routinely and with reasonably quick turn-around. This is similar to thesituation we were in when we wrote  [2.x.249] , from which this program has takenmuch inspiration. The solution is similar as it was there as well: We need toparallelize the program in a way similar to how we derived  [2.x.250]  out of [2.x.251] . In fact, all of the techniques used in  [2.x.252]  would be transferableto this program as well, making the program run on dozens or hundreds ofprocessors immediately.
* A different direction is to make the program more relevant to many otherporous media applications. Specifically, one avenue is to go to the primaryuser of porous media flow simulators, namely the oil industry. There,applications in this area are dominated by multiphase flow (i.e., more thanthe two phases we have here), and the reactions they may have with each other(or any other way phases may exchange mass, such as through dissolution in andbubbling out of gas from the oil phase). Furthermore, the presence of gasoften leads to compressibility effects of the fluid. Jointly, these effectsare typically formulated in the widely-used "black oil model". True reactionsbetween multiple phases also play a role in oil reservoir modeling whenconsidering controlled burns of oil in the reservoir to raise pressure andtemperature. These are much more complex problems, though, and left for futureprojects.
* Finally, from a mathematical perspective, we have derived thecriterion for re-computing the velocity/pressure solution at a giventime step under the assumption that we want to compare the solution wewould get at the current time step with that computed the last time weactually solved this system. However, in the program, whenever we didnot re-compute the solution, we didn't just use the previouslycomputed solution but instead extrapolated from the previous two timeswe solved the system. Consequently, the criterion was pessimisticallystated: what we should really compare is the solution we would get atthe current time step with the extrapolated one. Re-stating thetheorem in this regard is left as an exercise.
* There are also other ways to extend the mathematical foundation ofthis program; for example, one may say that it isn't the velocity wecare about, but in fact the saturation. Thus, one may ask whether thecriterion we use here to decide whether  [2.x.253]  needs to berecomputed is appropriate; one may, for example, suggest that it isalso important to decide whether (and by how much) a wrong velocityfield in fact affects the solution of the saturation equation. Thiswould then naturally lead to a sensitivity analysis.
* From an algorithmic viewpoint, we have here used a criterion for refinementthat is often used in engineering, namely by looking at the gradient ofthe solution. However, if you inspect the solution, you will find thatit quickly leads to refinement almost everywhere, even in regions where itis clearly not necessary: frequently used therefore does not need to implythat it is a useful criterion to begin with. On the other hand, replacingthis criterion by a different and better one should not be very difficult.For example, the KellyErrorEstimator class used in many other programsshould certainly be applicable to the current problem as well.
* 

* [1.x.203][1.x.204] [2.x.254] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-44_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34][1.x.35][1.x.36][1.x.37][1.x.38][1.x.39][1.x.40][1.x.41][1.x.42][1.x.43][1.x.44][1.x.45][1.x.46][1.x.47][1.x.48][1.x.49][1.x.50][1.x.51]
*  [2.x.2] 
* [1.x.52]
*  [2.x.3] 
* [1.x.53][1.x.54][1.x.55]
* 

* The subject of this tutorial is nonlinear solid mechanics.Classical single-field approaches (see e.g.  [2.x.4] ) can not correctly describe the response of quasi-incompressible materials.The response is overly stiff; a phenomenon known as locking.Locking problems can be circumvented using a variety of alternative strategies.One such strategy is the  three-field formulation.It is used here  to model the three-dimensional, fully-nonlinear (geometrical and material) response of an isotropic continuum body.The material response is approximated as hyperelastic.Additionally, the three-field formulation employed is valid for quasi-incompressible as well as compressible materials.
* The objective of this presentation is to provide a basis for using deal.II for problems in nonlinear solid mechanics.The linear problem was addressed in  [2.x.5] .A non-standard, hypoelastic-type form of the geometrically nonlinear problem was partially considered in  [2.x.6] : a rate form of the linearised constitutive relations is used and the problem domain evolves with the motion.Important concepts surrounding the nonlinear kinematics are absent in the theory and implementation. [2.x.7]  does, however, describe many of the key concepts to implement elasticity within the framework of deal.II.
* We begin with a crash-course in nonlinear kinematics.For the sake of simplicity, we restrict our attention to the quasi-static problem.Thereafter, various key stress measures are introduced and the constitutive model described.We then describe the three-field formulation in detail prior to explaining the structure of the class used to manage the material.The setup of the example problem is then presented.
*  [2.x.8]  This tutorial has been developed (and is described in the introduction) for the problem of elasticity in three dimensions. While the space dimension could be changed in the main() routine, care needs to be taken. Two-dimensional elasticity problems, in general, exist only as idealizations of three-dimensional ones. That is, they are either plane strain or plane stress. The assumptions that follow either of these choices needs to be consistently imposed. For more information see the note in  [2.x.9] .
* [1.x.56][1.x.57]
* 

* The three-field formulation implemented here was pioneered by Simo et al. (1985) and is known as the mixed Jacobian-pressure formulation.Important related contributions include those by Simo and Taylor (1991), and Miehe (1994).The notation adopted here draws heavily on the excellent overview of the theoretical aspects of nonlinear solid mechanics by Holzapfel (2001).A nice overview of issues pertaining to incompressible elasticity (at small strains) is given in Hughes (2000).
*  [2.x.10] 	 [2.x.11]  J.C. Simo, R.L. Taylor and K.S. Pister (1985),		Variational and projection methods for the volume constraint in finite deformation elasto-plasticity,		 [2.x.12]  Computer Methods in Applied Mechanics and Engineering  [2.x.13] ,		<strong> 51 </strong>, 1-3,		177-208.		DOI: [1.x.58];	 [2.x.14]  J.C. Simo and R.L. Taylor (1991),  		Quasi-incompressible finite elasticity in principal stretches. Continuum			basis and numerical algorithms,		 [2.x.15]  Computer Methods in Applied Mechanics and Engineering  [2.x.16] ,		<strong> 85 </strong>, 3,		273-310.		DOI: [1.x.59];	 [2.x.17]  C. Miehe (1994),		Aspects of the formulation and finite element implementation of large strain isotropic elasticity		 [2.x.18]  International Journal for Numerical Methods in Engineering  [2.x.19] 		<strong> 37 </strong>, 12,		1981-2004.		DOI: [1.x.60];	 [2.x.20]  G.A. Holzapfel (2001),		Nonlinear Solid Mechanics. A Continuum Approach for Engineering,		John Wiley & Sons.		ISBN: 0-471-82304-X;	 [2.x.21]  T.J.R. Hughes (2000),		The Finite Element Method: Linear Static and Dynamic Finite Element Analysis,		Dover.		ISBN: 978-0486411811 [2.x.22] 
* An example where this three-field formulation is used in a coupled problem is documented in [2.x.23] 	 [2.x.24]  J-P. V. Pelteret, D. Davydov, A. McBride, D. K. Vu, and P. Steinmann (2016),		Computational electro- and magneto-elasticity for quasi-incompressible media immersed in free space,		 [2.x.25]  International Journal for Numerical Methods in Engineering  [2.x.26] .		DOI: [1.x.61] [2.x.27] 
* [1.x.62][1.x.63]
* 

* One can think of fourth-order tensors as linear operators mapping second-ordertensors (matrices) onto themselves in much the same way as matrices mapvectors onto vectors.There are various fourth-order unit tensors that will be required in the forthcoming presentation.The fourth-order unit tensors  [2.x.28]  and  [2.x.29]  are defined by[1.x.64]Note  [2.x.30] .Furthermore, we define the symmetric and skew-symmetric fourth-order unit tensors by[1.x.65]such that[1.x.66]The fourth-order  [2.x.31]  returned by identity_tensor() is  [2.x.32] .
* 

* [1.x.67][1.x.68]
* 

* Let the time domain be denoted  [2.x.33] , where  [2.x.34]  and  [2.x.35]  is the total problem duration.Consider a continuum body that occupies the reference configuration  [2.x.36]  at time  [2.x.37] .%Particles in the reference configuration are identified by the position vector  [2.x.38] .The configuration of the body at a later time  [2.x.39]  is termed the current configuration, denoted  [2.x.40] , with particles identified by the vector  [2.x.41] .The nonlinear map between the reference and current configurations, denoted  [2.x.42] , acts as follows:[1.x.69]The material description of the displacement of a particle is defined by[1.x.70]
* The deformation gradient  [2.x.43]  is defined as the material gradient of the motion:[1.x.71]The determinant of the of the deformation gradient [2.x.44] maps corresponding volume elements in the reference and current configurations, denoted [2.x.45]  and  [2.x.46] ,respectively, as[1.x.72]
* Two important measures of the deformation in terms of the spatial and material coordinates are the left and right Cauchy-Green tensors, respectively,and denoted  [2.x.47]  and  [2.x.48] .They are both symmetric and positive definite.
* The Green-Lagrange strain tensor is defined by[1.x.73]If the assumption of infinitesimal deformations is made, then the second termon the right can be neglected, and  [2.x.49]  (the linearisedstrain tensor) is the only component of the strain tensor.This assumption is, looking at the setup of the problem, not valid in  [2.x.50] ,making the use of the linearized  [2.x.51]  as the strainmeasure in that tutorial program questionable.
* In order to handle the different response that materials exhibit when subjected to bulk and shear type deformations we consider the following decomposition of the deformation gradient  [2.x.52]   and the left Cauchy-Green tensor  [2.x.53]  into volume-changing (volumetric) and volume-preserving (isochoric) parts:[1.x.74]Clearly,  [2.x.54] .
* The spatial velocity field is denoted  [2.x.55] .The derivative of the spatial velocity field with respect to the spatial coordinates gives the spatial velocity gradient  [2.x.56] , that is[1.x.75]where  [2.x.57] .
* 

* [1.x.76][1.x.77]
* 

* Cauchy's stress theorem equates the Cauchy traction  [2.x.58]  acting on an infinitesimal surface element in the current configuration  [2.x.59]  to the product of the Cauchy stress tensor  [2.x.60]  (a spatial quantity)  and the outward unit normal to the surface  [2.x.61]  as[1.x.78]The Cauchy stress is symmetric.Similarly,  the first Piola-Kirchhoff traction  [2.x.62]  which acts on an infinitesimal surface element in the reference configuration  [2.x.63]  is the product of the first Piola-Kirchhoff stress tensor  [2.x.64]  (a two-point tensor)  and the outward unit normal to the surface  [2.x.65]  as[1.x.79]The Cauchy traction  [2.x.66]  and the first Piola-Kirchhoff traction  [2.x.67]  are related as[1.x.80]This can be demonstrated using [1.x.81].
* The first Piola-Kirchhoff stress tensor is related to the Cauchy stress as[1.x.82]Further important stress measures are the (spatial) Kirchhoff stress   [2.x.68] and the (referential) second Piola-Kirchhoff stress [2.x.69] .
* 

* [1.x.83][1.x.84]
* 

* Push-forward and pull-back operators allow one to transform various measures between the material and spatial settings.The stress measures used here are contravariant, while the strain measures are covariant.
* The push-forward and-pull back operations for second-order covariant tensors  [2.x.70]  are respectively given by:[1.x.85]
* The push-forward and pull back operations for second-order contravariant tensors  [2.x.71]  are respectively given by:[1.x.86]For example  [2.x.72] .
* 

* [1.x.87][1.x.88]
* 

* A hyperelastic material response is governed by a Helmholtz free energy function  [2.x.73]  which serves as a potential for the stress.For example, if the Helmholtz free energy depends on the right Cauchy-Green tensor  [2.x.74]  then the isotropic hyperelastic response is[1.x.89]If the Helmholtz free energy depends on the left Cauchy-Green tensor  [2.x.75]  then the isotropic hyperelastic response is[1.x.90]
* Following the multiplicative decomposition of the deformation gradient, the Helmholtz free energy can be decomposed as[1.x.91]Similarly, the Kirchhoff stress can be decomposed into volumetric and isochoric parts as  [2.x.76]  where:
* [1.x.92]
* where [2.x.77]  is the pressure response. [2.x.78]  is the projection tensor which provides the deviatoric operator in the Eulerian setting.The fictitious Kirchhoff stress tensor  [2.x.79]  is defined by[1.x.93]
* 

* 
*  [2.x.80]  The pressure response as defined above differs from the widely-used definition of thepressure in solid mechanics as [2.x.81] .Here  [2.x.82]  is the hydrostatic pressure.We make use of the pressure response throughout this tutorial (although we refer to it as the pressure).
* [1.x.94][1.x.95]
* 

* The Helmholtz free energy corresponding to a compressible [1.x.96] is given by[1.x.97]where  [2.x.83]  is the bulk modulus ( [2.x.84]  and  [2.x.85]  are the Lam&eacute; parameters)and  [2.x.86] .The function  [2.x.87]  is required to be strictly convex and satisfy the condition  [2.x.88] ,among others, see Holzapfel (2001) for further details.In this work  [2.x.89] .
* Incompressibility imposes the isochoric constraint that  [2.x.90]  for all motions  [2.x.91] .The Helmholtz free energy corresponding to an incompressible neo-Hookean material is given by[1.x.98]where  [2.x.92] .Thus, the incompressible response is obtained by removing the volumetric component from the compressible free energy and enforcing  [2.x.93] .
* 

* [1.x.99][1.x.100]
* 

* We will use a Newton-Raphson strategy to solve the nonlinear boundary value problem.Thus, we will need to linearise the constitutive relations.
* The fourth-order elasticity tensor in the material description is defined by[1.x.101]The fourth-order elasticity tensor in the spatial description  [2.x.94]  is obtained from the push-forward of  [2.x.95]  as[1.x.102]The fourth-order elasticity tensors (for hyperelastic materials) possess both major and minor symmetries.
* The fourth-order spatial elasticity tensor can be written in the following decoupled form:[1.x.103]where
* [1.x.104]
* where the fictitious elasticity tensor  [2.x.96]  in the spatial description is defined by[1.x.105]
* [1.x.106][1.x.107]
* 

* The total potential energy of the system  [2.x.97]  is the sum of the internal and external potential energies, denoted  [2.x.98]  and  [2.x.99] , respectively.We wish to find the equilibrium configuration by minimising the potential energy.
* As mentioned above, we adopt a three-field formulation.We denote the set of primary unknowns by [2.x.100] .The independent kinematic variable  [2.x.101]  enters the formulation as a constraint on  [2.x.102]  enforced by the Lagrange multiplier  [2.x.103]  (the pressure, as we shall see).
* The three-field variational principle used here is given by[1.x.108]where the external potential is defined by[1.x.109]The boundary of the current configuration   [2.x.104]  is composed into two parts as [2.x.105] ,where [2.x.106] .The prescribed Cauchy traction, denoted  [2.x.107] , is applied to  [2.x.108]  while the motion is prescribed on the remaining portion of the boundary  [2.x.109] .The body force per unit current volume is denoted  [2.x.110] .
* 

* 
* The stationarity of the potential follows as
* [1.x.110]
* for all virtual displacements  [2.x.111]  subject to the constraint that  [2.x.112]  on  [2.x.113] , and all virtual pressures  [2.x.114]  and virtual dilatations  [2.x.115] .
* One should note that the definitions of the volumetric Kirchhoff stress in the three field formulation [2.x.116]  and the subsequent volumetric tangent differs slightly from the general form given in the section on hyperelastic materials where [2.x.117] .This is because the pressure  [2.x.118]  is now a primary field as opposed to a constitutively derived quantity.One needs to carefully distinguish between the primary fields and those obtained from the constitutive relations.
*  [2.x.119]  Although the variables are all expressed in terms of spatial quantities, the domain of integration is the initial configuration.This approach is called a  [2.x.120]  total-Lagrangian formulation  [2.x.121] .The approach given in  [2.x.122] , where the domain of integration is the current configuration, could be called an  [2.x.123]  updated Lagrangian formulation  [2.x.124] .The various merits of these two approaches are discussed widely in the literature.It should be noted however that they are equivalent.
* 

* The Euler-Lagrange equations corresponding to the residual are:
* [1.x.111]
* The first equation is the (quasi-static) equilibrium equation in the spatial setting.The second is the constraint that  [2.x.125] .The third is the definition of the pressure  [2.x.126] .
*  [2.x.127]  The simplified single-field derivation ( [2.x.128]  is the only primary variable) below makes it clear how we transform the limits of integration to the reference domain:
* [1.x.112]
* where [2.x.129] .
* We will use an iterative Newton-Raphson method to solve the nonlinear residual equation  [2.x.130] .For the sake of simplicity we assume dead loading, i.e. the loading does not change due to the deformation.
* The change in a quantity between the known state at  [2.x.131] and the currently unknown state at  [2.x.132]  is denoted [2.x.133] .The value of a quantity at the current iteration  [2.x.134]  is denoted [2.x.135] .The incremental change between iterations  [2.x.136]  and  [2.x.137]  is denoted [2.x.138] .
* Assume that the state of the system is known for some iteration  [2.x.139] .The linearised approximation to nonlinear governing equations to be solved using the  Newton-Raphson method is:Find  [2.x.140]  such that[1.x.113]then set [2.x.141] .The tangent is given by
* [1.x.114]Thus,
* [1.x.115]
* where
* [1.x.116]
* 
* Note that the following terms are termed the geometrical stress and  the material contributions to the tangent matrix:
* [1.x.117]
* 
* 

* [1.x.118][1.x.119]
* 

* The three-field formulation used here is effective for quasi-incompressible materials,that is where  [2.x.142]  (where  [2.x.143]  is [1.x.120]), subject to a good choice of the interpolation fieldsfor  [2.x.144]  and  [2.x.145] .Typically a choice of  [2.x.146]  is made.Here  [2.x.147]  is the FE_DGPMonomial class.A popular choice is  [2.x.148]  which is known as the mean dilatation method (see Hughes (2000) for an intuitive discussion).This code can accommodate a  [2.x.149]  formulation.The discontinuous approximationallows  [2.x.150]  and  [2.x.151]  to be condensed outand a classical displacement based method is recovered.
* For fully-incompressible materials  [2.x.152]  and the three-field formulation will still exhibitlocking behavior.This can be overcome by introducing an additional constraint into the free energy of the form [2.x.153] .Here  [2.x.154]  is a Lagrange multiplier to enforce the isochoric constraint.For further details see Miehe (1994).
* The linearised problem can be written as[1.x.121]where
* [1.x.122]
* 
* There are no derivatives of the pressure and dilatation (primary) variables present in the formulation.Thus the discontinuous finite element interpolation of the pressure and dilatation yields a blockdiagonal matrix for [2.x.155] , [2.x.156]  and [2.x.157] .Therefore we can easily express the fields  [2.x.158]  and  [2.x.159]  on each cell simplyby inverting a local matrix and multiplying it by the local right handside. We can then insert the result into the remaining equations and recovera classical displacement-based method.In order to condense out the pressure and dilatation contributions at the element level we need the following results:
* [1.x.123]
* and thus[1.x.124]where[1.x.125]Note that due to the choice of  [2.x.160]  and  [2.x.161]  as discontinuous at the element level, all matrices that need to be inverted are defined at the element level.
* The procedure to construct the various contributions is as follows:
* 
*  - Construct  [2.x.162] .
* 
*  - Form  [2.x.163]  for element and store where  [2.x.164]  was stored in  [2.x.165] .
* 
*  - Form  [2.x.166]  and add to  [2.x.167]  to get  [2.x.168] 
* 
*  - The modified system matrix is called  [2.x.169] .  That is  [1.x.126]
* 

* [1.x.127][1.x.128]
* 

* A good object-oriented design of a Material class would facilitate the extension of this tutorial to a wide range of material types.In this tutorial we simply have one Material class named Material_Compressible_Neo_Hook_Three_Field.Ideally this class would derive from a class HyperelasticMaterial which would derive from the base class Material.The three-field nature of the formulation used here also complicates the matter.
* The Helmholtz free energy function for the three field formulation is  [2.x.170] .The isochoric part of the Kirchhoff stress  [2.x.171]  is identical to that obtained using a one-field formulation for a hyperelastic material.However, the volumetric part of the free energy is now a function of the primary variable  [2.x.172] .Thus, for a three field formulation the constitutive response for the volumetric part of the Kirchhoff stress  [2.x.173]  (and the tangent) is not given by the hyperelastic constitutive law as in a one-field formulation.One can label the term [2.x.174] as the volumetric Kirchhoff stress, but the pressure  [2.x.175]  is not derived from the free energy; it is a primary field.
* In order to have a flexible approach, it was decided that the Material_Compressible_Neo_Hook_Three_Field would still be able to calculate and return a volumetric Kirchhoff stress and tangent.In order to do this, we choose to store the interpolated primary fields  [2.x.176]  and  [2.x.177]  in the Material_Compressible_Neo_Hook_Three_Field class associated with the quadrature point.This decision should be revisited at a later stage when the tutorial is extended to account for other materials.
* 

* [1.x.129][1.x.130]
* 

* The numerical example considered here is a nearly-incompressible block under compression.This benchmark problem is taken from
* 
*  - S. Reese, P. Wriggers, B.D. Reddy (2000),  A new locking-free brick element technique for large deformation problems in elasticity,   [2.x.178]  Computers and Structures  [2.x.179] ,  <strong> 75 </strong>,  291-304.  DOI: [1.x.131]
*   [2.x.180] 
* The material is quasi-incompressible neo-Hookean with [1.x.132]  [2.x.181]  and  [2.x.182] .For such a choice of material properties a conventional single-field  [2.x.183]  approach would lock.That is, the response would be overly stiff.The initial and final configurations are shown in the image above.Using symmetry, we solve for only one quarter of the geometry (i.e. a cube with dimension  [2.x.184] ).The inner-quarter of the upper surface of the domain is subject to a load of  [2.x.185] .
* 

*  [1.x.133] [1.x.134]
*  We start by including all the necessary deal.II header files and some C++ related ones. They have been discussed in detail in previous tutorial programs, so you need only refer to past tutorials for details.
* 

* 
* [1.x.135]
* 
*  This header gives us the functionality to store data at quadrature points
* 

* 
* [1.x.136]
* 
*  Here are the headers necessary to use the LinearOperator class. These are also all conveniently packaged into a single header file, namely <deal.II/lac/linear_operator_tools.h> but we list those specifically required here for the sake of transparency.
* 

* 
* [1.x.137]
* 
*  Defined in these two headers are some operations that are pertinent to finite strain elasticity. The first will help us compute some kinematic quantities, and the second provides some stanard tensor definitions.
* 

* 
* [1.x.138]
* 
*  We then stick everything that relates to this tutorial program into a namespace of its own, and import all the deal.II function and class names into it:
* 

* 
* [1.x.139]
* 
*   [1.x.140]  [1.x.141]   
*   There are several parameters that can be set in the code so we set up a ParameterHandler object to read in the choices at run-time.
* 

* 
* [1.x.142]
* 
*   [1.x.143]  [1.x.144]
* 

* 
*  As mentioned in the introduction, a different order interpolation should be used for the displacement  [2.x.186]  than for the pressure  [2.x.187]  and the dilatation  [2.x.188] .  Choosing  [2.x.189]  and  [2.x.190]  as discontinuous (constant) functions at the element level leads to the mean-dilatation method. The discontinuous approximation allows  [2.x.191]  and  [2.x.192]  to be condensed out and a classical displacement based method is recovered. Here we specify the polynomial order used to approximate the solution. The quadrature order should be adjusted accordingly.
* 

* 
* [1.x.145]
* 
*   [1.x.146]  [1.x.147]
* 

* 
*  Make adjustments to the problem geometry and the applied load.  Since the problem modelled here is quite specific, the load scale can be altered to specific values to compare with the results given in the literature.
* 

* 
* [1.x.148]
* 
*   [1.x.149]  [1.x.150]
* 

* 
*  We also need the shear modulus  [2.x.193]  and Poisson ration  [2.x.194]  for the neo-Hookean material.
* 

* 
* [1.x.151]
* 
*   [1.x.152]  [1.x.153]
* 

* 
*  Next, we choose both solver and preconditioner settings.  The use of an effective preconditioner is critical to ensure convergence when a large nonlinear motion occurs within a Newton increment.
* 

* 
* [1.x.154]
* 
*   [1.x.155]  [1.x.156]
* 

* 
*  A Newton-Raphson scheme is used to solve the nonlinear system of governing equations.  We now define the tolerances and the maximum number of iterations for the Newton-Raphson nonlinear solver.
* 

* 
* [1.x.157]
* 
*   [1.x.158]  [1.x.159]
* 

* 
*  Set the timestep size  [2.x.195]  and the simulation end-time.
* 

* 
* [1.x.160]
* 
*   [1.x.161]  [1.x.162]
* 

* 
*  Finally we consolidate all of the above structures into a single container that holds all of our run-time selections.
* 

* 
* [1.x.163]
* 
*   [1.x.164]  [1.x.165]
* 

* 
*  A simple class to store time data. Its functioning is transparent so no discussion is necessary. For simplicity we assume a constant time step size.
* 

* 
* [1.x.166]
* 
*   [1.x.167]  [1.x.168]
* 

* 
*  As discussed in the Introduction, Neo-Hookean materials are a type of hyperelastic materials.  The entire domain is assumed to be composed of a compressible neo-Hookean material.  This class defines the behavior of this material within a three-field formulation.  Compressible neo-Hookean materials can be described by a strain-energy function (SEF)  [2.x.196] .   
*   The isochoric response is given by  [2.x.197]  where  [2.x.198]  and  [2.x.199]  is the first invariant of the left- or right-isochoric Cauchy-Green deformation tensors. That is  [2.x.200] . In this example the SEF that governs the volumetric response is defined as  [2.x.201] , where  [2.x.202]  is the [1.x.169] and  [2.x.203]  is [1.x.170].   
*   The following class will be used to characterize the material we work with, and provides a central point that one would need to modify if one were to implement a different material model. For it to work, we will store one object of this type per quadrature point, and in each of these objects store the current state (characterized by the values or measures  of the three fields) so that we can compute the elastic coefficients linearized around the current state.
* 

* 
* [1.x.171]
* 
*  We update the material model with various deformation dependent data based on  [2.x.204]  and the pressure  [2.x.205]  and dilatation  [2.x.206] , and at the end of the function include a physical check for internal consistency:
* 

* 
* [1.x.172]
* 
*  The second function determines the Kirchhoff stress  [2.x.207] 
* 

* 
* [1.x.173]
* 
*  The fourth-order elasticity tensor in the spatial setting  [2.x.208]  is calculated from the SEF  [2.x.209]  as  [2.x.210]  where  [2.x.211] 
* 

* 
* [1.x.174]
* 
*  Derivative of the volumetric free energy with respect to  [2.x.212]  return  [2.x.213] 
* 

* 
* [1.x.175]
* 
*  Second derivative of the volumetric free energy wrt  [2.x.214] . We need the following computation explicitly in the tangent so we make it public.  We calculate  [2.x.215] 
* 

* 
* [1.x.176]
* 
*  The next few functions return various data that we choose to store with the material:
* 

* 
* [1.x.177]
* 
*  Define constitutive model parameters  [2.x.216]  (bulk modulus) and the neo-Hookean model parameter  [2.x.217] :
* 

* 
* [1.x.178]
* 
*  Model specific data that is convenient to store with the material:
* 

* 
* [1.x.179]
* 
*  The following functions are used internally in determining the result of some of the public functions above. The first one determines the volumetric Kirchhoff stress  [2.x.218] :
* 

* 
* [1.x.180]
* 
*  Next, determine the isochoric Kirchhoff stress  [2.x.219] :
* 

* 
* [1.x.181]
* 
*  Then, determine the fictitious Kirchhoff stress  [2.x.220] :
* 

* 
* [1.x.182]
* 
*  Calculate the volumetric part of the tangent  [2.x.221] :
* 

* 
* [1.x.183]
* 
*  Calculate the isochoric part of the tangent  [2.x.222] :
* 

* 
* [1.x.184]
* 
*  Calculate the fictitious elasticity tensor  [2.x.223] . For the material model chosen this is simply zero:
* 

* 
* [1.x.185]
* 
*   [1.x.186]  [1.x.187]
* 

* 
*  As seen in  [2.x.224] , the  [2.x.225]  class offers a method for storing data at the quadrature points.  Here each quadrature point holds a pointer to a material description.  Thus, different material models can be used in different regions of the domain.  Among other data, we choose to store the Kirchhoff stress  [2.x.226]  and the tangent  [2.x.227]  for the quadrature points.
* 

* 
* [1.x.188]
* 
*  The first function is used to create a material object and to initialize all tensors correctly: The second one updates the stored values and stresses based on the current deformation measure  [2.x.228] , pressure  [2.x.229]  and dilation  [2.x.230]  field values.
* 

* 
* [1.x.189]
* 
*  To this end, we calculate the deformation gradient  [2.x.231]  from the displacement gradient  [2.x.232] , i.e.  [2.x.233]  and then let the material model associated with this quadrature point update itself. When computing the deformation gradient, we have to take care with which data types we compare the sum  [2.x.234] : Since  [2.x.235]  has data type SymmetricTensor, just writing  [2.x.236]  would convert the second argument to a symmetric tensor, perform the sum, and then cast the result to a Tensor (i.e., the type of a possibly nonsymmetric tensor). However, since  [2.x.237]  is nonsymmetric in general, the conversion to SymmetricTensor will fail. We can avoid this back and forth by converting  [2.x.238]  to Tensor first, and then performing the addition as between nonsymmetric tensors:
* 

* 
* [1.x.190]
* 
*  The material has been updated so we now calculate the Kirchhoff stress  [2.x.239] , the tangent  [2.x.240]  and the first and second derivatives of the volumetric free energy.       
*   We also store the inverse of the deformation gradient since we frequently use it:
* 

* 
* [1.x.191]
* 
*  We offer an interface to retrieve certain data.  Here are the kinematic variables:
* 

* 
* [1.x.192]
* 
*  ...and the kinetic variables.  These are used in the material and global tangent matrix and residual assembly operations:
* 

* 
* [1.x.193]
* 
*  And finally the tangent:
* 

* 
* [1.x.194]
* 
*  In terms of member functions, this class stores for the quadrature point it represents a copy of a material type in case different materials are used in different regions of the domain, as well as the inverse of the deformation gradient...
* 

* 
* [1.x.195]
* 
*  ... and stress-type variables along with the tangent  [2.x.241] :
* 

* 
* [1.x.196]
* 
*   [1.x.197]  [1.x.198]
* 

* 
*  The Solid class is the central class in that it represents the problem at hand. It follows the usual scheme in that all it really has is a constructor, destructor and a  [2.x.242]  function that dispatches all the work to private functions of this class:
* 

* 
* [1.x.199]
* 
*  In the private section of this class, we first forward declare a number of objects that are used in parallelizing work using the WorkStream object (see the  [2.x.243]  module for more information on this).     
*   We declare such structures for the computation of tangent (stiffness) matrix and right hand side vector, static condensation, and for updating quadrature points:
* 

* 
* [1.x.200]
* 
*  We start the collection of member functions with one that builds the grid:
* 

* 
* [1.x.201]
* 
*  Set up the finite element system to be solved:
* 

* 
* [1.x.202]
* 
*  Create Dirichlet constraints for the incremental displacement field:
* 

* 
* [1.x.203]
* 
*  Several functions to assemble the system and right hand side matrices using multithreading. Each of them comes as a wrapper function, one that is executed to do the work in the WorkStream model on one cell, and one that copies the work done on this one cell into the global object that represents it:
* 

* 
* [1.x.204]
* 
*  And similar to perform global static condensation:
* 

* 
* [1.x.205]
* 
*  Create and update the quadrature points. Here, no data needs to be copied into a global object, so the copy_local_to_global function is empty:
* 

* 
* [1.x.206]
* 
*  Solve for the displacement using a Newton-Raphson method. We break this function into the nonlinear loop and the function that solves the linearized Newton-Raphson step:
* 

* 
* [1.x.207]
* 
*  Solution retrieval as well as post-processing and writing data to file:
* 

* 
* [1.x.208]
* 
*  Finally, some member variables that describe the current state: A collection of the parameters used to describe the problem setup...
* 

* 
* [1.x.209]
* 
*  ...the volume of the reference configuration...
* 

* 
* [1.x.210]
* 
*  ...and description of the geometry on which the problem is solved:
* 

* 
* [1.x.211]
* 
*  Also, keep track of the current time and the time spent evaluating certain functions
* 

* 
* [1.x.212]
* 
*  A storage object for quadrature point information. As opposed to  [2.x.244] , deal.II's native quadrature point data manager is employed here.
* 

* 
* [1.x.213]
* 
*  A description of the finite-element system including the displacement polynomial degree, the degree-of-freedom handler, number of DoFs per cell and the extractor objects used to retrieve information from the solution vectors:
* 

* 
* [1.x.214]
* 
*  Description of how the block-system is arranged. There are 3 blocks, the first contains a vector DOF  [2.x.245]  while the other two describe scalar DOFs,  [2.x.246]  and  [2.x.247] .
* 

* 
* [1.x.215]
* 
*  Rules for Gauss-quadrature on both the cell and faces. The number of quadrature points on both cells and faces is recorded.
* 

* 
* [1.x.216]
* 
*  Objects that store the converged solution and right-hand side vectors, as well as the tangent matrix. There is an AffineConstraints object used to keep track of constraints.  We make use of a sparsity pattern designed for a block system.
* 

* 
* [1.x.217]
* 
*  Then define a number of variables to store norms and update norms and normalization factors.
* 

* 
* [1.x.218]
* 
*  Methods to calculate error measures
* 

* 
* [1.x.219]
* 
*  Compute the volume in the spatial configuration
* 

* 
* [1.x.220]
* 
*  Print information to screen in a pleasing way...
* 

* 
* [1.x.221]
* 
*   [1.x.222]  [1.x.223]
* 

* 
*   [1.x.224]  [1.x.225]
* 

* 
*  We initialize the Solid class using data extracted from the parameter file.
* 

* 
* [1.x.226]
* 
*  The Finite Element System is composed of dim continuous displacement DOFs, and discontinuous pressure and dilatation DOFs. In an attempt to satisfy the Babuska-Brezzi or LBB stability conditions (see Hughes (2000)), we setup a  [2.x.248]  system.  [2.x.249]  elements satisfy this condition, while  [2.x.250]  elements do not. However, it has been shown that the latter demonstrate good convergence characteristics nonetheless.
* 

* 
* [1.x.227]
* 
*  In solving the quasi-static problem, the time becomes a loading parameter, i.e. we increasing the loading linearly with time, making the two concepts interchangeable. We choose to increment time linearly using a constant time step size.   
*   We start the function with preprocessing, setting the initial dilatation values, and then output the initial grid before starting the simulation proper with the first time (and loading) increment.   
*   Care must be taken (or at least some thought given) when imposing the constraint  [2.x.251]  on the initial solution field. The constraint corresponds to the determinant of the deformation gradient in the undeformed configuration, which is the identity tensor. We use FE_DGPMonomial bases to interpolate the dilatation field, thus we can't simply set the corresponding dof to unity as they correspond to the monomial coefficients. Thus we use the  [2.x.252]  function to do the work for us. The  [2.x.253]  function requires an argument indicating the hanging node constraints. We have none in this program So we have to create a constraint object. In its original state, constraint objects are unsorted, and have to be sorted (using the  [2.x.254]  function) before they can be used. Have a look at  [2.x.255]  for more information. We only need to enforce the initial condition on the dilatation. In order to do this, we make use of a ComponentSelectFunction which acts as a mask and sets the J_component of n_components to 1. This is exactly what we want. Have a look at its usage in  [2.x.256]  for more information.
* 

* 
* [1.x.228]
* 
*  We then declare the incremental solution update  [2.x.257]  and start the loop over the time domain.     
*   At the beginning, we reset the solution update for this time step...
* 

* 
* [1.x.229]
* 
*  ...solve the current time step and update total solution vector  [2.x.258] ...
* 

* 
* [1.x.230]
* 
*  ...and plot the results before moving on happily to the next time step:
* 

* 
* [1.x.231]
* 
*   [1.x.232]  [1.x.233]
* 

* 
*   [1.x.234]  [1.x.235]
* 

* 
*  The first group of private member functions is related to parallelization. We use the Threading Building Blocks library (TBB) to perform as many computationally intensive distributed tasks as possible. In particular, we assemble the tangent matrix and right hand side vector, the static condensation contributions, and update data stored at the quadrature points using TBB. Our main tool for this is the WorkStream class (see the  [2.x.259]  threads module for more information).
* 

* 
*  Firstly we deal with the tangent matrix and right-hand side assembly structures. The PerTaskData object stores local contributions to the global system.
* 

* 
* [1.x.236]
* 
*  On the other hand, the ScratchData object stores the larger objects such as the shape-function values array ( [2.x.260] ) and a shape function gradient and symmetric gradient vector which we will use during the assembly.
* 

* 
* [1.x.237]
* 
*  Then we define structures to assemble the statically condensed tangent matrix. Recall that we wish to solve for a displacement-based formulation. We do the condensation at the element level as the  [2.x.261]  and  [2.x.262]  fields are element-wise discontinuous.  As these operations are matrix-based, we need to setup a number of matrices to store the local contributions from a number of the tangent matrix sub-blocks.  We place these in the PerTaskData struct.   
*   We choose not to reset any data in the  [2.x.263]  function as the matrix extraction and replacement tools will take care of this
* 

* 
* [1.x.238]
* 
*  The ScratchData object for the operations we wish to perform here is empty since we need no temporary data, but it still needs to be defined for the current implementation of TBB in deal.II.  So we create a dummy struct for this purpose.
* 

* 
* [1.x.239]
* 
*  And finally we define the structures to assist with updating the quadrature point information. Similar to the SC assembly process, we do not need the PerTaskData object (since there is nothing to store here) but must define one nonetheless. Note that this is because for the operation that we have here
* 
*  -  updating the data on quadrature points
* 
*  -  the operation is purely local: the things we do on every cell get consumed on every cell, without any global aggregation operation as is usually the case when using the WorkStream class. The fact that we still have to define a per-task data structure points to the fact that the WorkStream class may be ill-suited to this operation (we could, in principle simply create a new task using  [2.x.264]  for each cell) but there is not much harm done to doing it this way anyway. Furthermore, should there be different material models associated with a quadrature point, requiring varying levels of computational expense, then the method used here could be advantageous.
* 

* 
* [1.x.240]
* 
*  The ScratchData object will be used to store an alias for the solution vector so that we don't have to copy this large data structure. We then define a number of vectors to extract the solution values and gradients at the quadrature points.
* 

* 
* [1.x.241]
* 
*   [1.x.242]  [1.x.243]
* 

* 
*  On to the first of the private member functions. Here we create the triangulation of the domain, for which we choose the scaled cube with each face given a boundary ID number.  The grid must be refined at least once for the indentation problem.   
*   We then determine the volume of the reference configuration and print it for comparison:
* 

* 
* [1.x.244]
* 
*  Since we wish to apply a Neumann BC to a patch on the top surface, we must find the cell faces in this part of the domain and mark them with a distinct boundary ID number.  The faces we are looking for are on the +y surface and will get boundary ID 6 (zero through five are already used when creating the six faces of the cube domain):
* 

* 
* [1.x.245]
* 
*   [1.x.246]  [1.x.247]
* 

* 
*  Next we describe how the FE system is setup.  We first determine the number of components per block. Since the displacement is a vector component, the first dim components belong to it, while the next two describe scalar pressure and dilatation DOFs.
* 

* 
* [1.x.248]
* 
*  The DOF handler is then initialized and we renumber the grid in an efficient manner. We also record the number of DOFs per block.
* 

* 
* [1.x.249]
* 
*  Setup the sparsity pattern and tangent matrix
* 

* 
* [1.x.250]
* 
*  The global system matrix initially has the following structure

* 
* [1.x.251]
*  We optimize the sparsity pattern to reflect this structure and prevent unnecessary data creation for the right-diagonal block components.
* 

* 
* [1.x.252]
* 
*  We then set up storage vectors
* 

* 
* [1.x.253]
* 
*  ...and finally set up the quadrature point history:
* 

* 
* [1.x.254]
* 
*   [1.x.255]  [1.x.256] Next we compute some information from the FE system that describes which local element DOFs are attached to which block component.  This is used later to extract sub-blocks from the global matrix.   
*   In essence, all we need is for the FESystem object to indicate to which block component a DOF on the reference cell is attached to.  Currently, the interpolation fields are setup such that 0 indicates a displacement DOF, 1 a pressure DOF and 2 a dilatation DOF.
* 

* 
* [1.x.257]
* 
*   [1.x.258]  [1.x.259] The method used to store quadrature information is already described in  [2.x.265] . Here we implement a similar setup for a SMP machine.   
*   Firstly the actual QPH data objects are created. This must be done only once the grid is refined to its finest level.
* 

* 
* [1.x.260]
* 
*  Next we setup the initial quadrature point data. Note that when the quadrature point data is retrieved, it is returned as a vector of smart pointers.
* 

* 
* [1.x.261]
* 
*   [1.x.262]  [1.x.263] As the update of QP information occurs frequently and involves a number of expensive operations, we define a multithreaded approach to distributing the task across a number of CPU cores.   
*   To start this, we first we need to obtain the total solution as it stands at this Newton increment and then create the initial copy of the scratch and copy data objects:
* 

* 
* [1.x.264]
* 
*  We then pass them and the one-cell update function to the WorkStream to be processed:
* 

* 
* [1.x.265]
* 
*  Now we describe how we extract data from the solution vector and pass it along to each QP storage object for processing.
* 

* 
* [1.x.266]
* 
*  We first need to find the values and gradients at quadrature points inside the current cell and then we update each local QP using the displacement gradient and total pressure and dilatation solution values:
* 

* 
* [1.x.267]
* 
*   [1.x.268]  [1.x.269]
* 

* 
*  The next function is the driver method for the Newton-Raphson scheme. At its top we create a new vector to store the current Newton update step, reset the error storage objects and print solver header.
* 

* 
* [1.x.270]
* 
*  We now perform a number of Newton iterations to iteratively solve the nonlinear problem.  Since the problem is fully nonlinear and we are using a full Newton method, the data stored in the tangent matrix and right-hand side vector is not reusable and must be cleared at each Newton step. We then initially build the linear system and check for convergence (and store this value in the first iteration). The unconstrained DOFs of the rhs vector hold the out-of-balance forces, and collectively determine whether or not the equilibrium solution has been attained.     
*   Although for this particular problem we could potentially construct the RHS vector before assembling the system matrix, for the sake of extensibility we choose not to do so. The benefit to assembling the RHS vector and system matrix separately is that the latter is an expensive operation and we can potentially avoid an extra assembly process by not assembling the tangent matrix when convergence is attained. However, this makes parallelizing the code using MPI more difficult. Furthermore, when extending the problem to the transient case additional contributions to the RHS may result from the time discretization and application of constraints for the velocity and acceleration fields.
* 

* 
* [1.x.271]
* 
*  We construct the linear system, but hold off on solving it (a step that should be significantly more expensive than assembly):
* 

* 
* [1.x.272]
* 
*  We can now determine the normalized residual error and check for solution convergence:
* 

* 
* [1.x.273]
* 
*  If we have decided that we want to continue with the iteration, we solve the linearized system:
* 

* 
* [1.x.274]
* 
*  We can now determine the normalized Newton update error:
* 

* 
* [1.x.275]
* 
*  Lastly, since we implicitly accept the solution step we can perform the actual update of the solution increment for the current time step, update all quadrature point information pertaining to this new displacement and stress state and continue iterating:
* 

* 
* [1.x.276]
* 
*  At the end, if it turns out that we have in fact done more iterations than the parameter file allowed, we raise an exception that can be caught in the main() function. The call <code>AssertThrow(condition, exc_object)</code> is in essence equivalent to <code>if (!cond) throw exc_object;</code> but the former form fills certain fields in the exception object that identify the location (filename and line number) where the exception was raised to make it simpler to identify where the problem happened.
* 

* 
* [1.x.277]
* 
*   [1.x.278]  [1.x.279]
* 

* 
*  This program prints out data in a nice table that is updated on a per-iteration basis. The next two functions set up the table header and footer:
* 

* 
* [1.x.280]
* 
*   [1.x.281]  [1.x.282]
* 

* 
*  Calculate the volume of the domain in the spatial configuration
* 

* 
* [1.x.283]
* 
*  In contrast to that which was previously called for, in this instance the quadrature point data is specifically non-modifiable since we will only be accessing data. We ensure that the right get_data function is called by marking this update function as constant.
* 

* 
* [1.x.284]
* 
*  Calculate how well the dilatation  [2.x.266]  agrees with  [2.x.267]  from the  [2.x.268]  error  [2.x.269] . We also return the ratio of the current volume of the domain to the reference volume. This is of interest for incompressible media where we want to check how well the isochoric constraint has been enforced.
* 

* 
* [1.x.285]
* 
*   [1.x.286]  [1.x.287]
* 

* 
*  Determine the true residual error for the problem.  That is, determine the error in the residual for the unconstrained degrees of freedom.  Note that to do so, we need to ignore constrained DOFs by setting the residual in these vector components to zero.
* 

* 
* [1.x.288]
* 
*   [1.x.289]  [1.x.290]
* 

* 
*  Determine the true Newton update error for the problem
* 

* 
* [1.x.291]
* 
*   [1.x.292]  [1.x.293]
* 

* 
*  This function provides the total solution, which is valid at any Newton step. This is required as, to reduce computational error, the total solution is only updated at the end of the timestep.
* 

* 
* [1.x.294]
* 
*   [1.x.295]  [1.x.296]
* 

* 
*  Since we use TBB for assembly, we simply setup a copy of the data structures required for the process and pass them, along with the assembly functions to the WorkStream object for processing. Note that we must ensure that the matrix and RHS vector are reset before any assembly operations can occur. Furthermore, since we are describing a problem with Neumann BCs, we will need the face normals and so must specify this in the face update flags.
* 

* 
* [1.x.297]
* 
*  The syntax used here to pass data to the WorkStream class is discussed in  [2.x.270] .
* 

* 
* [1.x.298]
* 
*  Of course, we still have to define how we assemble the tangent matrix contribution for a single cell.  We first need to reset and initialize some of the scratch data structures and retrieve some basic information regarding the DOF numbering on this cell.  We can precalculate the cell shape function values and gradients. Note that the shape function gradients are defined with regard to the current configuration.  That is  [2.x.271] .
* 

* 
* [1.x.299]
* 
*  Now we build the local cell stiffness matrix and RHS vector. Since the global and local system matrices are symmetric, we can exploit this property by building only the lower half of the local matrix and copying the values to the upper half.  So we only assemble half of the  [2.x.272] ,  [2.x.273] ,  [2.x.274]  blocks, while the whole  [2.x.275] ,  [2.x.276] ,  [2.x.277]  blocks are built.     
*   In doing so, we first extract some configuration dependent variables from our quadrature history objects for the current quadrature point.
* 

* 
* [1.x.300]
* 
*  These two tensors store some precomputed data. Their use will explained shortly.
* 

* 
* [1.x.301]
* 
*  Next we define some aliases to make the assembly process easier to follow.
* 

* 
* [1.x.302]
* 
*  We first compute the contributions from the internal forces.  Note, by definition of the rhs as the negative of the residual, these contributions are subtracted.
* 

* 
* [1.x.303]
* 
*  Before we go into the inner loop, we have one final chance to introduce some optimizations. We've already taken into account the symmetry of the system, and we can now precompute some common terms that are repeatedly applied in the inner loop. We won't be excessive here, but will rather focus on expensive operations, namely those involving the rank-4 material stiffness tensor and the rank-2 stress tensor.             
*   What we may observe is that both of these tensors are contracted with shape function gradients indexed on the "i" DoF. This implies that this particular operation remains constant as we loop over the "j" DoF. For that reason, we can extract this from the inner loop and save the many operations that, for each quadrature point and DoF index "i" and repeated over index "j" are required to double contract a rank-2 symmetric tensor with a rank-4 symmetric tensor, and a rank-1 tensor with a rank-2 tensor.             
*   At the loss of some readability, this small change will reduce the assembly time of the symmetrized system by about half when using the simulation default parameters, and becomes more significant as the h-refinement level increases.
* 

* 
* [1.x.304]
* 
*  Now we're prepared to compute the tangent matrix contributions:
* 

* 
* [1.x.305]
* 
*  This is the  [2.x.278]  contribution. It comprises a material contribution, and a geometrical stress contribution which is only added along the local matrix diagonals:
* 

* 
* [1.x.306]
* 
*  The material contribution:
* 

* 
* [1.x.307]
* 
*  The geometrical stress contribution:
* 

* 
* [1.x.308]
* 
*  Next is the  [2.x.279]  contribution
* 

* 
* [1.x.309]
* 
*  and lastly the  [2.x.280]  and  [2.x.281]  contributions:
* 

* 
* [1.x.310]
* 
*  Next we assemble the Neumann contribution. We first check to see it the cell face exists on a boundary on which a traction is applied and add the contribution if this is the case.
* 

* 
* [1.x.311]
* 
*  Using the face normal at this quadrature point we specify the traction in reference configuration. For this problem, a defined pressure is applied in the reference configuration. The direction of the applied traction is assumed not to evolve with the deformation of the domain. The traction is defined using the first Piola-Kirchhoff stress is simply  [2.x.282]  We use the time variable to linearly ramp up the pressure load.               
*   Note that the contributions to the right hand side vector we compute here only exist in the displacement components of the vector.
* 

* 
* [1.x.312]
* 
*  Finally, we need to copy the lower half of the local matrix into the upper half:
* 

* 
* [1.x.313]
* 
*   [1.x.314]  [1.x.315] The constraints for this problem are simple to describe. In this particular example, the boundary values will be calculated for the two first iterations of Newton's algorithm. In general, one would build non-homogeneous constraints in the zeroth iteration (that is, when `apply_dirichlet_bc == true` in the code block that follows) and build only the corresponding homogeneous constraints in the following step. While the current example has only homogeneous constraints, previous experiences have shown that a common error is forgetting to add the extra condition when refactoring the code to specific uses. This could lead to errors that are hard to debug. In this spirit, we choose to make the code more verbose in terms of what operations are performed at each Newton step.
* 

* 
* [1.x.316]
* 
*  Since we (a) are dealing with an iterative Newton method, (b) are using an incremental formulation for the displacement, and (c) apply the constraints to the incremental displacement field, any non-homogeneous constraints on the displacement update should only be specified at the zeroth iteration. No subsequent contributions are to be made since the constraints will be exactly satisfied after that iteration.
* 

* 
* [1.x.317]
* 
*  Furthermore, after the first Newton iteration within a timestep, the constraints remain the same and we do not need to modify or rebuild them so long as we do not clear the  [2.x.283]  object.
* 

* 
* [1.x.318]
* 
*  At the zeroth Newton iteration we wish to apply the full set of non-homogeneous and homogeneous constraints that represent the boundary conditions on the displacement increment. Since in general the constraints may be different at each time step, we need to clear the constraints matrix and completely rebuild it. An example case would be if a surface is accelerating; in such a scenario the change in displacement is non-constant between each time step.
* 

* 
* [1.x.319]
* 
*  The boundary conditions for the indentation problem in 3D are as follows: On the
* 
*  - ,
* 
*  -  and
* 
*  -  faces (IDs 0,2,4) we set up a symmetry condition to allow only planar movement while the +x and +z faces (IDs 1,5) are traction free. In this contrived problem, part of the +y face (ID 3) is set to have no motion in the x- and z-component. Finally, as described earlier, the other part of the +y face has an the applied pressure but is also constrained in the x- and z-directions.         
*   In the following, we will have to tell the function interpolation boundary values which components of the solution vector should be constrained (i.e., whether it's the x-, y-, z-displacements or combinations thereof). This is done using ComponentMask objects (see  [2.x.284] ) which we can get from the finite element if we provide it with an extractor object for the component we wish to select. To this end we first set up such extractor objects and later use it when generating the relevant component masks:
* 

* 
* [1.x.320]
* 
*  As all Dirichlet constraints are fulfilled exactly after the zeroth Newton iteration, we want to ensure that no further modification are made to those entries. This implies that we want to convert all non-homogeneous Dirichlet constraints into homogeneous ones.         
*   In this example the procedure to do this is quite straightforward, and in fact we can (and will) circumvent any unnecessary operations when only homogeneous boundary conditions are applied. In a more general problem one should be mindful of hanging node and periodic constraints, which may also introduce some inhomogeneities. It might then be advantageous to keep disparate objects for the different types of constraints, and merge them together once the homogeneous Dirichlet constraints have been constructed.
* 

* 
* [1.x.321]
* 
*  Since the affine constraints were finalized at the previous Newton iteration, they may not be modified directly. So we need to copy them to another temporary object and make modification there. Once we're done, we'll transfer them back to the main  [2.x.285]  object.
* 

* 
* [1.x.322]
* 
*   [1.x.323]  [1.x.324] Solving the entire block system is a bit problematic as there are no contributions to the  [2.x.286]  block, rendering it noninvertible (when using an iterative solver). Since the pressure and dilatation variables DOFs are discontinuous, we can condense them out to form a smaller displacement-only system which we will then solve and subsequently post-process to retrieve the pressure and dilatation solutions.
* 

* 
*  The static condensation process could be performed at a global level but we need the inverse of one of the blocks. However, since the pressure and dilatation variables are discontinuous, the static condensation (SC) operation can also be done on a per-cell basis and we can produce the inverse of the block-diagonal  [2.x.287]  block by inverting the local blocks. We can again use TBB to do this since each operation will be independent of one another.   
*   Using the TBB via the WorkStream class, we assemble the contributions to form  [2.x.288]  from each element's contributions. These contributions are then added to the global stiffness matrix. Given this description, the following two functions should be clear:
* 

* 
* [1.x.325]
* 
*  Now we describe the static condensation process. As per usual, we must first find out which global numbers the degrees of freedom on this cell have and reset some data structures:
* 

* 
* [1.x.326]
* 
*  We now extract the contribution of the dofs associated with the current cell to the global stiffness matrix.  The discontinuous nature of the  [2.x.289]  and  [2.x.290]  interpolations mean that their is no coupling of the local contributions at the global level. This is not the case with the  [2.x.291]  dof.  In other words,  [2.x.292] ,  [2.x.293]  and  [2.x.294] , when extracted from the global stiffness matrix are the element contributions.  This is not the case for  [2.x.295] .     
*   Note: A lower-case symbol is used to denote element stiffness matrices.
* 

* 
*  Currently the matrix corresponding to the dof associated with the current element (denoted somewhat loosely as  [2.x.296] ) is of the form:

* 
* [1.x.327]
*      
*   We now need to modify it such that it appear as

* 
* [1.x.328]
*  with  [2.x.297]  where  [2.x.298]  and  [2.x.299] .     
*   At this point, we need to take note of the fact that global data already exists in the  [2.x.300] ,  [2.x.301]  and  [2.x.302]  sub-blocks.  So if we are to modify them, we must account for the data that is already there (i.e. simply add to it or remove it if necessary).  Since the copy_local_to_global operation is a "+=" operation, we need to take this into account     
*   For the  [2.x.303]  block in particular, this means that contributions have been added from the surrounding cells, so we need to be careful when we manipulate this block.  We can't just erase the sub-blocks.     
*   This is the strategy we will employ to get the sub-blocks we want:
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  -  [2.x.304] : Since we don't have access to  [2.x.305] , but we know its contribution is added to the global  [2.x.306]  matrix, we just want to add the element wise static-condensation  [2.x.307] .
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  -  [2.x.308] : Similarly,  [2.x.309]  exists in the subblock. Since the copy operation is a += operation, we need to subtract the existing  [2.x.310]  submatrix in addition to "adding" that which we wish to replace it with.
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  -  [2.x.311] : Since the global matrix is symmetric, this block is the same as the one above and we can simply use  [2.x.312]  as a substitute for this one.     
*   We first extract element data from the system matrix. So first we get the entire subblock for the cell, then extract  [2.x.313]  for the dofs associated with the current element
* 

* 
* [1.x.329]
* 
*  and next the local matrices for  [2.x.314]   [2.x.315]  and  [2.x.316] :
* 

* 
* [1.x.330]
* 
*  To get the inverse of  [2.x.317] , we invert it directly.  This operation is relatively inexpensive since  [2.x.318]  since block-diagonal.
* 

* 
* [1.x.331]
* 
*  Now we can make condensation terms to add to the  [2.x.319]  block and put them in the cell local matrix  [2.x.320] :
* 

* 
* [1.x.332]
* 
*   [2.x.321] 
* 

* 
* [1.x.333]
* 
*   [2.x.322] 
* 

* 
* [1.x.334]
* 
*   [2.x.323] 
* 

* 
* [1.x.335]
* 
*  Next we place  [2.x.324]  in the  [2.x.325]  block for post-processing.  Note again that we need to remove the contribution that already exists there.
* 

* 
* [1.x.336]
* 
*   [1.x.337]  [1.x.338] We now have all of the necessary components to use one of two possible methods to solve the linearised system. The first is to perform static condensation on an element level, which requires some alterations to the tangent matrix and RHS vector. Alternatively, the full block system can be solved by performing condensation on a global level. Below we implement both approaches.
* 

* 
* [1.x.339]
* 
*  Firstly, here is the approach using the (permanent) augmentation of the tangent matrix. For the following, recall that

* 
* [1.x.340]
*  and

* 
* [1.x.341]
*  and thus [1.x.342] where [1.x.343]
* 

* 
*  At the top, we allocate two temporary vectors to help with the static condensation, and variables to store the number of linear solver iterations and the (hopefully converged) residual.
* 

* 
* [1.x.344]
* 
*  In the first step of this function, we solve for the incremental displacement  [2.x.326] .  To this end, we perform static condensation to make  [2.x.327]  and put  [2.x.328]  in the original  [2.x.329]  block. That is, we make  [2.x.330] .
* 

* 
* [1.x.345]
* 
*   [2.x.331] 
* 

* 
* [1.x.346]
* 
*   [2.x.332] 
* 

* 
* [1.x.347]
* 
*   [2.x.333] 
* 

* 
* [1.x.348]
* 
*   [2.x.334] 
* 

* 
* [1.x.349]
* 
*   [2.x.335] 
* 

* 
* [1.x.350]
* 
*   [2.x.336] 
* 

* 
* [1.x.351]
* 
*  We've chosen by default a SSOR preconditioner as it appears to provide the fastest solver convergence characteristics for this problem on a single-thread machine.  However, this might not be true for different problem sizes.
* 

* 
* [1.x.352]
* 
*  Otherwise if the problem is small enough, a direct solver can be utilised.
* 

* 
* [1.x.353]
* 
*  Now that we have the displacement update, distribute the constraints back to the Newton update:
* 

* 
* [1.x.354]
* 
*  The next step after solving the displacement problem is to post-process to get the dilatation solution from the substitution:  [2.x.337] 
* 

* 
* [1.x.355]
* 
*   [2.x.338] 
* 

* 
* [1.x.356]
* 
*   [2.x.339] 
* 

* 
* [1.x.357]
* 
*   [2.x.340] 
* 

* 
* [1.x.358]
* 
*   [2.x.341] 
* 

* 
* [1.x.359]
* 
*  we ensure here that any Dirichlet constraints are distributed on the updated solution:
* 

* 
* [1.x.360]
* 
*  Finally we solve for the pressure update with the substitution:  [2.x.342] 
* 

* 
* [1.x.361]
* 
*   [2.x.343] 
* 

* 
* [1.x.362]
* 
*   [2.x.344] 
* 

* 
* [1.x.363]
* 
*   [2.x.345] 
* 

* 
* [1.x.364]
* 
*  and finally....  [2.x.346] 
* 

* 
* [1.x.365]
* 
*  We are now at the end, so we distribute all constrained dofs back to the Newton update:
* 

* 
* [1.x.366]
* 
*  Manual condensation of the dilatation and pressure fields on a local level, and subsequent post-processing, took quite a bit of effort to achieve. To recap, we had to produce the inverse matrix  [2.x.347] , which was permanently written into the global tangent matrix. We then permanently modified  [2.x.348]  to produce  [2.x.349] . This involved the extraction and manipulation of local sub-blocks of the tangent matrix. After solving for the displacement, the individual matrix-vector operations required to solve for dilatation and pressure were carefully implemented. Contrast these many sequence of steps to the much simpler and transparent implementation using functionality provided by the LinearOperator class.
* 

* 
*  For ease of later use, we define some aliases for blocks in the RHS vector
* 

* 
* [1.x.367]
* 
*  ... and for blocks in the Newton update vector.
* 

* 
* [1.x.368]
* 
*  We next define some linear operators for the tangent matrix sub-blocks We will exploit the symmetry of the system, so not all blocks are required.
* 

* 
* [1.x.369]
* 
*  We then construct a LinearOperator that represents the inverse of (square block)  [2.x.350] . Since it is diagonal (or, when a higher order ansatz it used, nearly diagonal), a Jacobi preconditioner is suitable.
* 

* 
* [1.x.370]
* 
*  Now we can construct that transpose of  [2.x.351]  and a linear operator that represents the condensed operations  [2.x.352]  and  [2.x.353]  and the final augmented matrix  [2.x.354] . Note that the schur_complement() operator could also be of use here, but for clarity and the purpose of demonstrating the similarities between the formulation and implementation of the linear solution scheme, we will perform these operations manually.
* 

* 
* [1.x.371]
* 
*  Lastly, we define an operator for inverse of augmented stiffness matrix, namely  [2.x.355] . Note that the preconditioner for the augmented stiffness matrix is different to the case when we use static condensation. In this instance, the preconditioner is based on a non-modified  [2.x.356] , while with the first approach we actually modified the entries of this sub-block. However, since  [2.x.357]  and  [2.x.358]  operate on the same space, it remains adequate for this problem.
* 

* 
* [1.x.372]
* 
*  Now we are in a position to solve for the displacement field. We can nest the linear operations, and the result is immediately written to the Newton update vector. It is clear that the implementation closely mimics the derivation stated in the introduction.
* 

* 
* [1.x.373]
* 
*  The operations need to post-process for the dilatation and pressure fields are just as easy to express.
* 

* 
* [1.x.374]
* 
*  Solve the full block system with a direct solver. As it is relatively robust, it may be immune to problem arising from the presence of the zero  [2.x.359]  block.
* 

* 
* [1.x.375]
* 
*  Finally, we again ensure here that any Dirichlet constraints are distributed on the updated solution:
* 

* 
* [1.x.376]
* 
*   [1.x.377]  [1.x.378] Here we present how the results are written to file to be viewed using ParaView or VisIt. The method is similar to that shown in previous tutorials so will not be discussed in detail.
* 

* 
* [1.x.379]
* 
*  Since we are dealing with a large deformation problem, it would be nice to display the result on a displaced grid!  The MappingQEulerian class linked with the DataOut class provides an interface through which this can be achieved without physically moving the grid points in the Triangulation object ourselves.  We first need to copy the solution to a temporary vector and then create the Eulerian mapping. We also specify the polynomial degree to the DataOut object in order to produce a more refined output data set when higher order polynomials are used.
* 

* 
* [1.x.380]
* 
*   [1.x.381]  [1.x.382] Lastly we provide the main driver function which appears no different to the other tutorials.
* 

* 
* [1.x.383]
* [1.x.384][1.x.385]
* 

* Firstly, we present a comparison of a series of 3-d results with thosein the literature (see Reese et al (2000)) to demonstrate that the program works as expected.
* We begin with a comparison of the convergence with mesh refinement for the  [2.x.360]  and [2.x.361]  formulations, as summarised in the figure below.The vertical displacement of the midpoint of the upper surface of the block is used to assess convergence.Both schemes demonstrate good convergence properties for varying values of the load parameter  [2.x.362] .The results agree with those in the literature.The lower-order formulation typically overestimates the displacement for low levels of refinement,while the higher-order interpolation scheme underestimates it, but be a lesser degree.This benchmark, and a series of others not shown here, give us confidence that the code is workingas it should.
*  [2.x.363] 
* 

* A typical screen output generated by running the problem is shown below.The particular case demonstrated is that of the  [2.x.364]  formulation.It is clear that, using the Newton-Raphson method, quadratic convergence of the solution is obtained.Solution convergence is achieved within 5 Newton increments for all time-steps.The converged displacement's  [2.x.365] -norm is several orders of magnitude less than the geometry scale.
* [1.x.386]
* 
* 

* 
* Using the Timer class, we can discern which parts of the code require the highest computational expense.For a case with a large number of degrees-of-freedom (i.e. a high level of refinement), a typical output of the Timer is given below.Much of the code in the tutorial has been developed based on the optimizations described,discussed and demonstrated in  [2.x.366]  and others.With over 93% of the time being spent in the linear solver, it is obvious that it may be necessaryto invest in a better solver for large three-dimensional problems.The SSOR preconditioner is not multithreaded but is effective for this class of solid problems.It may be beneficial to investigate the use of another solver such as those available through the Trilinos library.
* 

* 
* [1.x.387]
* 
* 

* We then used ParaView to visualize the results for two cases.The first was for the coarsest grid and the lowest-order interpolation method:  [2.x.367] .The second was on a refined grid using a  [2.x.368]  formulation.The vertical component of the displacement, the pressure  [2.x.369]  and the dilatation  [2.x.370]  fieldsare shown below.
* 

* For the first case it is clear that the coarse spatial discretization coupled with large displacements leads to a low quality solution(the loading ratio is   [2.x.371] ).Additionally, the pressure difference between elements is very large.The constant pressure field on the element means that the large pressure gradient is not captured.However, it should be noted that locking, which would be present in a standard  [2.x.372]  displacement formulation does not ariseeven in this poorly discretised case.The final vertical displacement of the tracked node on the top surface of the block is still within 12.5% of the converged solution.The pressure solution is very coarse and has large jumps between adjacent cells.It is clear that the volume nearest to the applied traction undergoes compression while the outer extentsof the domain are in a state of expansion.The dilatation solution field and pressure field are clearly linked,with positive dilatation indicating regions of positive pressure and negative showing regions placed in compression.As discussed in the Introduction, a compressive pressure has a negative signwhile an expansive pressure takes a positive sign.This stems from the definition of the volumetric strain energy functionand is opposite to the physically realistic interpretation of pressure.
* 

*  [2.x.373] 
* Combining spatial refinement and a higher-order interpolation scheme results in a high-quality solution.Three grid refinements coupled with a  [2.x.374]  formulation producesa result that clearly captures the mechanics of the problem.The deformation of the traction surface is well resolved.We can now observe the actual extent of the applied traction, with the maximum force being appliedat the central point of the surface causing the largest compression.Even though very high strains are experienced in the domain,especially at the boundary of the region of applied traction,the solution remains accurate.The pressure field is captured in far greater detail than before.There is a clear distinction and transition between regions of compression and expansion,and the linear approximation of the pressure field allows a refined visualizationof the pressure at the sub-element scale.It should however be noted that the pressure field remains discontinuousand could be smoothed on a continuous grid for the post-processing purposes.
* 

* 
*  [2.x.375] 
* This brief analysis of the results demonstrates that the three-field formulation is effectivein circumventing volumetric locking for highly-incompressible media.The mixed formulation is able to accurately simulate the displacement of anear-incompressible block under compression.The command-line output indicates that the volumetric change under extreme compression resulted inless than 0.01% volume change for a Poisson's ratio of 0.4999.
* In terms of run-time, the  [2.x.376]  formulation tends to be more computationally expensivethan the  [2.x.377]  for a similar number of degrees-of-freedom(produced by adding an extra grid refinement level for the lower-order interpolation).This is shown in the graph below for a batch of tests run consecutively on a single 4-core (8-thread) machine.The increase in computational time for the higher-order method is likely due tothe increased band-width required for the higher-order elements.As previously mentioned, the use of a better solver and preconditioner may mitigate theexpense of using a higher-order formulation.It was observed that for the given problem using the multithreaded Jacobi preconditioner can reduce thecomputational runtime by up to 72% (for the worst case being a higher-order formulation with a large numberof degrees-of-freedom) in comparison to the single-thread SSOR preconditioner.However, it is the author's experience that the Jacobi method of preconditioning may not be suitable forsome finite-strain problems involving alternative constitutive models.
* 

*  [2.x.378] 
* 

* Lastly, results for the displacement solution for the 2-d problem are showcased below fortwo different levels of grid refinement.It is clear that due to the extra constraints imposed by simulating in 2-d that the resultingdisplacement field, although qualitatively similar, is different to that of the 3-d case.
* 

*  [2.x.379] 
* [1.x.388][1.x.389][1.x.390]
* 

* There are a number of obvious extensions for this work:
* 
*  - Firstly, an additional constraint could be added to the free-energy  function in order to enforce a high degree of incompressibility in  materials. An additional Lagrange multiplier would be introduced,  but this could most easily be dealt with using the principle of  augmented Lagrange multipliers. This is demonstrated in  [2.x.380] Simo and  Taylor (1991)  [2.x.381] .
* 
*  - The constitutive relationship used in this  model is relatively basic. It may be beneficial to split the material  class into two separate classes, one dealing with the volumetric  response and the other the isochoric response, and produce a generic  materials class (i.e. having abstract virtual functions that derived  classes have to implement) that would allow for the addition of more complex  material models. Such models could include other hyperelastic  materials, plasticity and viscoelastic materials and others.
* 
*  - The program has been developed for solving problems on single-node  multicore machines. With a little effort, the program could be  extended to a large-scale computing environment through the use of  Petsc or Trilinos, using a similar technique to that demonstrated in   [2.x.382] . This would mostly involve changes to the setup, assembly,   [2.x.383]  and linear solver routines.
* 
*  - As this program assumes quasi-static equilibrium, extensions to  include dynamic effects would be necessary to study problems where  inertial effects are important, e.g. problems involving impact.
* 
*  - Load and solution limiting procedures may be necessary for highly  nonlinear problems. It is possible to add a linesearch algorithm to  limit the step size within a Newton increment to ensure optimum  convergence. It may also be necessary to use a load limiting method,  such as the Riks method, to solve unstable problems involving  geometric instability such as buckling and snap-through.
* 
*  - Many physical problems involve contact. It is possible to include  the effect of frictional or frictionless contact between objects  into this program. This would involve the addition of an extra term  in the free-energy functional and therefore an addition to the  assembly routine. One would also need to manage the contact problem  (detection and stress calculations) itself. An alternative to  additional penalty terms in the free-energy functional would be to  use active set methods such as the one used in  [2.x.384] .
* 
*  - The complete condensation procedure using LinearOperators has been  coded into the linear solver routine. This could also have been  achieved through the application of the schur_complement()  operator to condense out one or more of the fields in a more  automated manner.
* 
*  - Finally, adaptive mesh refinement, as demonstrated in  [2.x.385]  and   [2.x.386] , could provide additional solution accuracy.
* 

* [1.x.391][1.x.392] [2.x.387] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-45_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6]
*  [2.x.2] 
* [1.x.7][1.x.8][1.x.9][1.x.10]
* 

* In this example we present how to use periodic boundary conditions indeal.II. Periodic boundary conditions are algebraic constraints thattypically occur in computations on representative regions of a largerdomain that repeat in one or more directions.
* An example is the simulation of the electronic structure of photoniccrystals, because they have a lattice-like structure and, thus, it oftensuffices to do the actual computation on only one box of the lattice. Tobe able to proceed this way one has to assume that the model can beperiodically extended to the other boxes; this requires the solution tohave a periodic structure.
* [1.x.11][1.x.12][1.x.13]
* 

* deal.II provides a number of high level entry points to impose periodicboundary conditions.The general approach to apply periodic boundary conditions consists ofthree steps (see also the [2.x.3]  "Glossary entry on periodic boundary conditions"):
* 
*  - Create a mesh
* 
*  - Identify those pairs of faces on different parts of the boundary across which   the solution should be symmetric, using  [2.x.4] 
* 
*  - Add the periodicity information to the mesh   using  [2.x.5] 
* 
*  - Add periodicity constraints using  [2.x.6] 
* The second and third step are necessary for parallel meshes using the [2.x.7]  classto ensure that cells on opposite sides of the domain but connected by periodicfaces are part of the ghost layer if one of them is stored on the local processor.If the Triangulation is not a  [2.x.8] these steps are not necessary.
* The first step consists of collecting matching periodic faces and storing them ina  [2.x.9]  of  [2.x.10]  This is done with thefunction  [2.x.11]  that can be invoked for examplelike this:
* [1.x.14]
* 
* This call loops over all faces of the container dof_handler on the periodicboundaries with boundary indicator  [2.x.12]  and  [2.x.13] respectively. (You can assign these boundary indicators by hand aftercreating the coarse mesh, see [2.x.14]  "Boundary indicator". Alternatively, youcan also let many of the functions in namespace GridGenerator do thisfor if you specify the "colorize" flag; in that case, these functionswill assign different boundary indicators to different parts of theboundary, with the details typically spelled out in the documentationof these functions.)
* Concretely, if  [2.x.15]  are the vertices of two faces [2.x.16] , then the function call above will match pairs offaces (and dofs) such that the difference between  [2.x.17] and  [2.x.18]  vanishes in everycomponent apart from direction and stores the resulting pairs withassociated data in  [2.x.19]  (See [2.x.20]  for detailed information about thematching process.)
* Consider, for example, the colored unit square  [2.x.21]  with boundaryindicator 0 on the left, 1 on the right, 2 on the bottom and 3 on the topfaces. (See the documentation of  [2.x.22]  for thisconvention on how boundary indicators are assigned.) Then,
* [1.x.15]
* would yield periodicity constraints such that  [2.x.23]  for all [2.x.24] .
* If we instead consider the parallelogram given by the convex hull of [2.x.25] ,  [2.x.26] ,  [2.x.27] ,  [2.x.28]  we can achieve the constraints [2.x.29]  by specifying an  [2.x.30] 
* [1.x.16]
* or
* [1.x.17]
* Here, again, the assignment of boundary indicators 0 and 1 stems fromwhat  [2.x.31]  documents.
* The resulting  [2.x.32]  can be used in [2.x.33]  for populating an AffineConstraintsobject with periodicity constraints:
* [1.x.18]
* 
* Apart from this high level interface there are also variants of [2.x.34]  available that combine those twosteps (see the variants of  [2.x.35] 
* There is also a low level interface to [2.x.36]  if more flexibility is needed. Thelow level variant allows to directly specify two faces that shall beconstrained:
* [1.x.19]
* Here, we need to specify the orientation of the two faces using [2.x.37]   [2.x.38]  and  [2.x.39]  For a closer descriptionhave a look at the documentation of  [2.x.40] The remaining parameters are the same as for the high level interface apartfrom the self-explaining  [2.x.41]  and  [2.x.42] 
* 

* [1.x.20][1.x.21][1.x.22]
* 

* In the following, we show how to use the above functions in a more involvedexample. The task is to enforce rotated periodicity constraints for thevelocity component of a Stokes flow.
* On a quarter-circle defined by  [2.x.43]  we aregoing to solve the Stokes problem[1.x.23]
* where the boundary  [2.x.44]  is defined as  [2.x.45] .For the remaining parts of the boundary we are going to use periodic boundary conditions, i.e.
* [1.x.24]
* 
* The mesh will be generated by  [2.x.46] which also documents how it assigns boundary indicators to its variousboundaries if its `colorize` argument is set to `true`.
* 

*  [1.x.25] [1.x.26]
*  This example program is a slight modification of  [2.x.47]  running in parallel using Trilinos to demonstrate the usage of periodic boundary conditions in deal.II. We thus omit to discuss the majority of the source code and only comment on the parts that deal with periodicity constraints. For the rest have a look at  [2.x.48]  and the full source code at the bottom.
* 

* 
*  In order to implement periodic boundary conditions only two functions have to be modified:
* 

* 
* 
*  -  [2.x.49] : To populate an AffineConstraints object with periodicity constraints
* 

* 
* 
*  -  [2.x.50] : To supply a distributed triangulation with periodicity information.
* 

* 
*  The rest of the program is identical to  [2.x.51] , so let us skip this part and only show these two functions in the following. (The full program can be found in the "Plain program" section below, though.)
* 

* 
*  
*     
*  
*  [1.x.27]  [1.x.28]
* 

* 
* [1.x.29]
* 
*  Before we can prescribe periodicity constraints, we need to ensure that cells on opposite sides of the domain but connected by periodic faces are part of the ghost layer if one of them is stored on the local processor. At this point we need to think about how we want to prescribe periodicity. The vertices  [2.x.52]  of a face on the left boundary should be matched to the vertices  [2.x.53]  of a face on the lower boundary given by  [2.x.54]  where the rotation matrix  [2.x.55]  and the offset  [2.x.56]  are given by

* 
* [1.x.30]
*  The data structure we are saving the resulting information into is here based on the Triangulation.
* 

* 
* [1.x.31]
* 
*  Now telling the triangulation about the desired periodicity is particularly easy by just calling  [2.x.57] 
* 

* 
* [1.x.32]
* 
*  After we provided the mesh with the necessary information for the periodicity constraints, we are now able to actual create them. For describing the matching we are using the same approach as before, i.e., the  [2.x.58]  of a face on the left boundary should be matched to the vertices  [2.x.59]  of a face on the lower boundary given by  [2.x.60]  where the rotation matrix  [2.x.61]  and the offset  [2.x.62]  are given by

* 
* [1.x.33]
*  These two objects not only describe how faces should be matched but also in which sense the solution should be transformed from  [2.x.63]  to  [2.x.64] .
* 

* 
* [1.x.34]
* 
*  For setting up the constraints, we first store the periodicity information in an auxiliary object of type  [2.x.65]   [2.x.66]  </code>. The periodic boundaries have the boundary indicators 2 (x=0) and 3 (y=0). All the other parameters we have set up before. In this case the direction does not matter. Due to  [2.x.67]  this is exactly what we want.
* 

* 
* [1.x.35]
* 
*  Next, we need to provide information on which vector valued components of the solution should be rotated. Since we choose here to just constraint the velocity and this starts at the first component of the solution vector, we simply insert a 0:
* 

* 
* [1.x.36]
* 
*  After setting up all the information in periodicity_vector all we have to do is to tell make_periodicity_constraints to create the desired constraints.
* 

* 
* [1.x.37]
* 
*  The rest of the program is then again identical to  [2.x.68] . We will omit it here now, but as before, you can find these parts in the "Plain program" section below.
* 

* 
* [1.x.38][1.x.39]
* 

* The created output is not very surprising. We simply see that the solution isperiodic with respect to the left and lower boundary:
*  [2.x.69] 
* Without the periodicity constraints we would have ended up with the following solution:
*  [2.x.70] 
* 

* [1.x.40][1.x.41] [2.x.71] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-46_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] ,  [2.x.2] ,  [2.x.3] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34]
*  [2.x.4] 
* [1.x.35]
* 

* [1.x.36][1.x.37][1.x.38]
* 

* This program deals with the problem of coupling different physics in differentparts of the domain. Specifically, let us consider the followingsituation that couples a Stokes fluid with an elastic solid (these twoproblems were previously discussed separately in  [2.x.5]  and  [2.x.6] ,where you may want to read up on the individual equations):
* 
*  - In a part  [2.x.7]  of  [2.x.8] , we have a fluid flowing that satisfies the  time independent Stokes equations (in the form that involves the strain  tensor): 
* [1.x.39]
*   Here,  [2.x.9]  are the fluid velocity and pressure, respectively.  We prescribe the velocity on part of the external boundary, 
* [1.x.40]
*   while we assume free-flow conditions on the remainder of the external  boundary, 
* [1.x.41]
* 
* 
*  - The remainder of the domain,  [2.x.10]  is  occupied by a solid whose deformation field  [2.x.11]  satisfies the  elasticity equation, 
* [1.x.42]
*   where  [2.x.12]  is the rank-4 elasticity tensor (for which we will use a  particularly simple form by assuming that the solid is isotropic).  It deforms in reaction to the forces exerted by the  fluid flowing along the boundary of the solid. We assume this deformation to  be so small that it has no feedback effect on the fluid, i.e. the coupling  is only in one direction. For simplicity, we will assume that the  solid's external boundary is clamped, i.e. 
* [1.x.43]
* 
* 
*  - As a consequence of the small displacement assumption, we will pose the  following boundary conditions on the interface between the fluid and solid:  first, we have no slip boundary conditions for the fluid, 
* [1.x.44]
*   Secondly, the forces (traction) on the solid equal the normal stress from the fluid, 
* [1.x.45]
*   where  [2.x.13]  is the normal vector on  [2.x.14]  pointing from  the solid to the fluid.
* We get a weak formulation of this problem by following our usual rule ofmultiplying from the left by a test function and integrating over thedomain. It then looks like this: Find  [2.x.15]  such that
* [1.x.46]
* for all test functions  [2.x.16] ; the first, second, andthird lines correspond to the fluid, solid, and interfacecontributions, respectively.Note that  [2.x.17]  is only a subspace of the spaces listed above to accommodate forthe various Dirichlet boundary conditions.
* This sort of coupling is of course possible by simply having two Triangulationand two DoFHandler objects, one each for each of the two subdomains. On theother hand, deal.II is much simpler to use if there is a single DoFHandlerobject that knows about the discretization of the entire problem.
* This program is about how this can be achieved. Note that the goal is not topresent a particularly useful physical model (a realistic fluid-structureinteraction model would have to take into account the finite deformation ofthe solid and the effect this has on the fluid): this is, after all, just atutorial program intended to demonstrate techniques, not to solve actualproblems. Furthermore, we will make the assumption that the interface betweenthe subdomains is aligned with coarse mesh cell faces.
* 

* [1.x.47][1.x.48]
* 

* Before going into more details let us state the obvious: this is aproblem with multiple solution variables; for this, you will probablywant to read the  [2.x.18]  documentation module first, whichpresents the basic philosophical framework in which we addressproblems with more than one solution variable. But back to the problemat hand:
* The fundamental idea to implement these sort of problems in deal.II goes asfollows: in the problem formulation, the velocity and pressure variables [2.x.19]  only live in the fluid subdomain  [2.x.20] . But let's assumethat we extend them by zero to the entire domain  [2.x.21]  (in the general casethis means that they will be discontinuous along  [2.x.22] ). So what is theappropriate function space for these variables? We know that on  [2.x.23]  weshould require  [2.x.24] , so forthe extensions  [2.x.25]  to the whole domain the followingappears a useful set of function spaces:
* [1.x.49]
* (Since this is not important for the current discussion, we have omitted thequestion of boundary values from the choice of function spaces; this questionalso affects whether we can choose  [2.x.26]  for the pressure or whether we haveto choose the space  [2.x.27]  for the pressure. None of these questions are relevant to the followingdiscussion, however.)
* Note that these are indeed a linear function spaces with obvious norm. Since noconfusion is possible in practice, we will henceforth omit the tilde again todenote the extension of a function to the whole domain and simply refer by [2.x.28]  to both the original and the extended function.
* For discretization, we need finite dimensional subspaces  [2.x.29]  of [2.x.30] . For Stokes, we know from  [2.x.31]  that an appropriate choice is [2.x.32]  but this only holds for that part of the domainoccupied by the fluid. For the extended field, let's use the followingsubspaces defined on the triangulation  [2.x.33] :
* [1.x.50]
* In other words, on  [2.x.34]  we choose the usual discrete spaces butwe keep the (discontinuous) extension by zero. The point to make isthat we now need a description of a finite element space for functionsthat are zero on a cell &mdash; and this is where the FE_Nothing classcomes in: it describes a finite dimensional function space offunctions that are constant zero. A particular property of thispeculiar linear vector space is that it has no degrees of freedom: itisn't just finite dimensional, it is in fact zero dimensional, andconsequently for objects of this type,  [2.x.35] will return zero. For discussion below, let us give this space aproper symbol:[1.x.51]The symbol  [2.x.36]  reminds of the fact that functions in this space arezero. Obviously, we choose  [2.x.37] .
* This entire discussion above can be repeated for the variables we use todescribe the elasticity equation. Here, for the extended variables, wehave
* [1.x.52]
* and we will typically use a finite element space of the kind
* [1.x.53]
* of polynomial degree  [2.x.38] .
* So to sum up, we are going to look for a discrete vector-valuedsolution  [2.x.39]  in the followingspace:
* [1.x.54]
* 
* 

* 
* [1.x.55][1.x.56]
* 

* So how do we implement this sort of thing? First, we realize that the discretespace  [2.x.40]  essentially calls for two different finite elements: First, on thefluid subdomain, we need the element  [2.x.41]  whichin deal.II is readily implemented by
* [1.x.57]
* where  [2.x.42]  implements the space of functions that arealways zero. Second, on the solid subdomain, we need the element [2.x.43] , which we get using
* [1.x.58]
* 
* The next step is that we associate each of these two elements with the cellsthat occupy each of the two subdomains. For this we realize that in a sensethe two elements are just variations of each other in that they have the samenumber of vector components but have different polynomial degrees &mdash; thissmells very much like what one would do in  [2.x.44]  finite element methods, and itis exactly what we are going to do here: we are going to (ab)use the classesand facilities of the hp-namespace to assign different elements to differentcells. In other words, we will use collect the two finite elements in an [2.x.45]  will integrate with an appropriate  [2.x.46]  using an [2.x.47]  object, and our DoFHandler will be in [1.x.59]-mode. Youmay wish to take a look at  [2.x.48]  for an overview of all of these concepts.
* Before going on describing the testcase, let us clarify a bit [1.x.60] thisapproach of extending the functions by zero to the entire domain and thenmapping the problem on to the hp-framework makes sense:
* 
*  - It makes things uniform: On all cells, the number of vector components is  the same (here,  [2.x.49] ). This makes all sorts of  things possible since a uniform description allows for code  re-use. For example, counting degrees of freedom per vector  component  [2.x.50]  sorting degrees of  freedom by component  [2.x.51]  subsequent  partitioning of matrices and vectors into blocks and many other  functions work as they always did without the need to add special  logic to them that describes cases where some of the variables only  live on parts of the domain. Consequently, you have all sorts of  tools already available to you in programs like the current one that  weren't originally written for the multiphysics case but work just  fine in the current context.
* 
*  - It allows for easy graphical output: All graphical output formats we support  require that each field in the output is defined on all nodes of the  mesh. But given that now all solution components live everywhere,  our existing DataOut routines work as they always did, and produce  graphical output suitable for visualization
* 
*  -  the fields will  simply be extended by zero, a value that can easily be filtered out  by visualization programs if not desired.
* 
*  - There is essentially no cost: The trick with the FE_Nothing does not add any  degrees of freedom to the overall problem, nor do we ever have to handle a  shape function that belongs to these components &mdash; the FE_Nothing has  no degrees of freedom, not does it have shape functions, all it does is take  up vector components.
* 

* [1.x.61][1.x.62]
* 

* More specifically, in the program we have to address the followingpoints:
* 
*  - Implementing the bilinear form, and in particular dealing with the  interface term, both in the matrix and the sparsity pattern.
* 
*  - Implementing Dirichlet boundary conditions on the external and  internal parts of the boundaries   [2.x.52] .
* 

* [1.x.63][1.x.64]
* 

* Let us first discuss implementing the bilinear form, which at thediscrete level we recall to be
* [1.x.65]
* Given that we have extended the fields by zero, we could in principlewrite the integrals over subdomains to the entire domain  [2.x.53] ,though it is little additional effort to first ask whether a cell ispart of the elastic or fluid region before deciding which terms tointegrate. Actually integrating these terms is not very difficult; forthe Stokes equations, the relevant steps have been shown in  [2.x.54] ,whereas for the elasticity equation we take essentially the form shownin the  [2.x.55]  module (rather than the one from  [2.x.56] ).
* The term that is of more interest is the interface term,[1.x.66]Based on our assumption that the interface  [2.x.57]  coincides withcell boundaries, this can in fact be written as a set of faceintegrals. If we denote the velocity, pressure and displacementcomponents of shape function  [2.x.58]  using the extractornotation  [2.x.59] , then theterm above yields the following contribution to the global matrixentry  [2.x.60] :[1.x.67]Although it isn't immediately obvious, this term presents a slightcomplication: while  [2.x.61]  and  [2.x.62]  are evaluatedon the solid side of the interface (they are test functions for thedisplacement and the normal vector to  [2.x.63] , respectively, weneed to evaluate  [2.x.64]  on the fluidside of the interface since they correspond to the stress/forceexerted by the fluid. In other words, in our implementation, we willneed FEFaceValue objects for both sides of the interface. To makethings slightly worse, we may also have to deal with the fact that oneside or the other may be refined, leaving us with the need tointegrate over parts of a face. Take a look at the implementationbelow on how to deal with this.
* As an additional complication, the matrix entries that result from this termneed to be added to the sparsity pattern of the matrix somehow. This is therealm of various functions in the DoFTools namespace like [2.x.65]  and [2.x.66]  Essentially, what these functions do issimulate what happens during assembly of the system matrix: whenever assemblywould write a nonzero entry into the global matrix, the functions in DoFToolswould add an entry to the sparsity pattern. We could therefore do thefollowing: let  [2.x.67]  add all those entries to thesparsity pattern that arise from the regular cell-by-cell integration, andthen do the same by hand that arise from the interface terms. If you look atthe implementation of the interface integrals in the program below, it shouldbe obvious how to do that and would require no more than maybe 100 lines ofcode at most.
* But we're lazy people: the interface term couples degrees of freedom from twoadjacent cells along a face, which is exactly the kind of thing one would doin discontinuous Galerkin schemes for which the function [2.x.68]  was written. This is a superset of matrixentries compared to the usual  [2.x.69]  it will alsoadd all entries that result from computing terms coupling the degrees offreedom from both sides of all faces. Unfortunately, for the simplest versionof this function, this is a pretty big superset. Consider for example thefollowing mesh with two cells and a  [2.x.70]  finite element:
* [1.x.68]
* Here, the sparsity pattern produced by  [2.x.71]  willonly have entries for degrees of freedom that couple on a cell. However, itwill not have sparsity pattern entries  [2.x.72] . The sparsitypattern generated by  [2.x.73]  will have theseentries, however: it assumes that you want to build a sparsity pattern for abilinear form that couples [1.x.69] degrees of freedom from adjacentcells. This is not what we want: our interface term acts only on a smallsubset of cells, and we certainly don't need all the extra couplings betweentwo adjacent fluid cells, or two adjacent solid cells. Furthermore, the fact that weuse higher order elements means that we would really generate many many moreentries than we actually need: on the coarsest mesh, in 2d, 44,207 nonzeroentries instead of 16,635 for  [2.x.74]  leading toplenty of zeros in the matrix we later build (of course, the 16,635 are notenough since they don't include the interface entries). This ratio would beeven worse in 3d.
* So being extremely lazy comes with a cost: too many entries in the matrix. Butwe can get away with being moderately lazy: there is a variant of [2.x.75]  that allows usto specify which vector components of the finite element couple with whichother components, both in cell terms as well as in face terms. For cells thatare in the solid subdomain, we couple all displacements with each other; forfluid cells, all velocities with all velocities and the pressure, but not thepressure with itself. Since no cell has both sets ofvariables, there is no need to distinguish between the two kinds of cells, sowe can write the mask like this:
* [1.x.70]
* Here, we have used the fact that the first  [2.x.76]  components of thefinite element are the velocities, then the pressure, and then the [2.x.77]  displacements. (We could as well have stated that thevelocities/pressure also couple with the displacements since no cell ever hasboth sets of variables.) On the other hand, the interface terms require a masklike this:
* [1.x.71]
* In other words, all displacement test functions (components [2.x.78] ) couple with all velocity and pressure shape functionson the other side of an interface. This is not entirely true, though close: infact, the exact form of the interface term only those pressure displacementshape functions that are indeed nonzero on the common interface, which is nottrue for all shape functions; on the other hand, it really couples allvelocities (since the integral involves gradients of the velocity shapefunctions, which are all nonzero on all faces of the cell). However, the mask webuild above, is not capable of these subtleties. Nevertheless, through thesemasks we manage to get the number of sparsity pattern entries down to 21,028&mdash; good enough for now.
* 

* 
* [1.x.72][1.x.73]
* 

* The second difficulty is that while we know how to enforce a zerovelocity or stress on the external boundary (using [2.x.79]  called with an appropriatecomponent mask and setting different boundary indicators for solid andfluid external boundaries), we now also needed the velocity to be zeroon the interior interface, i.e.  [2.x.80] . At the timeof writing this, there is no function in deal.II that handles thispart, but it isn't particularly difficult to implement by hand:essentially, we just have to loop over all cells, and if it is a fluidcell and its neighbor is a solid cell, then add constraints thatensure that the velocity degrees of freedom on this face arezero. Some care is necessary to deal with the case that the adjacentsolid cell is refined, yielding the following code:
* [1.x.74]
* 
* The call  [2.x.81]  tells theAffineConstraints to start a new constraint for degree of freedom [2.x.82]  of the form  [2.x.83] . Typically, one would then proceed to set individual coefficients [2.x.84]  to nonzero values (using  [2.x.85]  or set [2.x.86]  to something nonzero (using [2.x.87]  doing nothing as above, funny asit looks, simply leaves the constraint to be  [2.x.88] , which is exactlywhat we need in the current context. The call to [2.x.89]  makes sure that we only setboundary values to zero for velocity but not pressure components.
* Note that there are cases where this may yield incorrect results:notably, once we find a solid neighbor child to a current fluid cell,we assume that all neighbor children on the common face are in thesolid subdomain. But that need not be so; consider, for example, thefollowing mesh:
* [1.x.75]
* 
* In this case, we would set all velocity degrees of freedom on theright face of the left cell to zero, which is incorrect for the topdegree of freedom on that face. That said, that can only happen if thefluid and solid subdomains do not coincide with a set of completecoarse mesh cells &mdash; but this is a contradiction to theassumption stated at the end of the first section of thisintroduction.
* 

* 
* [1.x.76][1.x.77]
* 

* We will consider the following situation as a testcase:
*  [2.x.90] 
* As discussed at the top of this document, we need to assume in a few placesthat a cell is either entirely in the fluid or solid part of the domain and,furthermore, that all children of an inactive cell also belong to the samesubdomain. This can definitely be ensured if the coarse mesh alreadysubdivides the mesh into solid and fluid coarse mesh cells; given the geometryoutlined above, we can do that by using an  [2.x.91]  coarse mesh,conveniently provided by the  [2.x.92] function.
* The fixed boundary at the bottom implies  [2.x.93] , and we alsoprescribe Dirichlet conditions for the flow at the top so that we getinflow at the left and outflow at the right. At the left and rightboundaries, no boundary conditions are imposed explicitly for theflow, yielding the implicit no-stress condition  [2.x.94] .The conditions on the interface between the two domains has already beendiscussed above.
* For simplicity, we choose the material parameters to be [2.x.95] . In the results section below, we will also showa 3d simulation that can be obtained from the same program. Theboundary conditions and geometry are defined nearly analogously to the2d situation above.
* 

* [1.x.78][1.x.79]
* 

* In the program, we need a way to identify which part of the domain a cell isin. There are many different ways of doing this. A typical way would be to usethe  [2.x.96]  "subdomain_id" tag available with each cell, thoughthis field has a special meaning in %parallel computations. An alternativeis the  [2.x.97]  "material_id" field also available withevery cell. It has the additional advantage that it is inherited from themother to the child cell upon mesh refinement; in other words, we would setthe material id once upon creating the mesh and it will be correct for allactive cells even after several refinement cycles. We therefore go with thisalternative: we define an  [2.x.98]  with symbolic names formaterial_id numbers and will use them to identify which part of the domain acell is on.
* Secondly, we use an object of type DoFHandler operating in [1.x.80]-mode. Thisclass needs to know which cells will use the Stokes and which the elasticityfinite element. At the beginning of each refinement cycle we will thereforehave to walk over all cells and set the (in hp-parlance) active FE index towhatever is appropriate in the current situation. While we can use symbolicnames for the material id, the active FE index is in fact a number that willfrequently be used to index into collections of objects (e.g. of type [2.x.99]  and  [2.x.100]  that means that the active FE indexactually has to have value zero for the fluid and one for the elastic part ofthe domain.
* 

* [1.x.81][1.x.82]
* 

* This program is primarily intended to show how to deal with differentphysics in different parts of the domain, and how to implement suchmodels in deal.II. As a consequence, we won't bother coming up with agood solver: we'll just use the SparseDirectUMFPACK class which alwaysworks, even if not with optimal complexity. We will, however, commenton possible other solvers in the [1.x.83] section.
* 

* [1.x.84][1.x.85]
* 

* One of the trickier aspects of this program is how to estimate theerror. Because it works on almost any program, we'd like to use theKellyErrorEstimator, and we can relatively easily do that here as well usingcode like the following:
* [1.x.86]
* This gives us two sets of error indicators for each cell. We would thensomehow combine them into one for mesh refinement, for example using somethinglike the following (note that we normalize the squared error indicator in thetwo vectors because error quantities have physical units that do not match inthe current situation, leading to error indicators that may differ by ordersof magnitude between the two subdomains):
* [1.x.87]
* (In the code, we actually weigh the error indicators 4:1 in favor of the onescomputed on the Stokes subdomain since refinement is otherwise heavily biasedtowards the elastic subdomain, but this is just a technicality. The factor 4has been determined heuristically to work reasonably well.)
* While this principle is sound, it doesn't quite work as expected. The reasonis that the KellyErrorEstimator class computes error indicators by integratingthe jump in the solution's gradient around the faces of each cell. This jumpis likely to be very large at the locations where the solution isdiscontinuous and extended by zero; it also doesn't become smaller as the meshis refined. The KellyErrorEstimator class can't just ignore the interfacebecause it essentially only sees a DoFHandler in [1.x.88]-mode where the elementtype changes from one cell to another &mdash; precisely the thing that the[1.x.89]-mode was designed for, the interface in the current program looks nodifferent than the interfaces in  [2.x.101] , for example, and certainly no lesslegitimate. Be that as it may, the end results is that there is a layer ofcells on both sides of the interface between the two subdomains where errorindicators are irrationally large. Consequently, most of the mesh refinementis focused on the interface.
* This clearly wouldn't happen if we had a refinement indicator that actuallyunderstood something about the problem and simply ignore the interface betweensubdomains when integrating jump terms.On the other hand, this program isabout showing how to represent problems where we have different physics indifferent subdomains, not about the peculiarities of the KellyErrorEstimator,and so we resort to the big hammer called "heuristics": we simply set theerror indicators of cells at the interface to zero. This cuts off the spikesin the error indicators. At first sight one would also think that it preventsthe mesh from being refined at the interface, but the requirement thatneighboring cells may only differ by one level of refinement will still leadto a reasonably refined mesh.
* While this is clearly a suboptimal solution, it works for now and leaves roomfor future improvement.
* 

*  [1.x.90] [1.x.91]
*   [1.x.92]  [1.x.93]
* 

* 
*  The include files for this program are the same as for many others before. The only new one is the one that declares FE_Nothing as discussed in the introduction. The ones in the hp directory have already been discussed in  [2.x.102] .
* 

* 
*  

* 
* [1.x.94]
* 
*   [1.x.95]  [1.x.96]
* 

* 
*  This is the main class. It is, if you want, a combination of  [2.x.103]  and  [2.x.104]  in that it has member variables that either address the global problem (the Triangulation and DoFHandler objects, as well as the  [2.x.105]  and various linear algebra objects) or that pertain to either the elasticity or Stokes sub-problems. The general structure of the class, however, is like that of most of the other programs implementing stationary problems.   
*   There are a few helper functions (<code>cell_is_in_fluid_domain, cell_is_in_solid_domain</code>) of self-explanatory nature (operating on the symbolic names for the two subdomains that will be used as material_ids for cells belonging to the subdomains, as explained in the introduction) and a few functions (<code>make_grid, set_active_fe_indices, assemble_interface_terms</code>) that have been broken out of other functions that can be found in many of the other tutorial programs and that will be discussed as we get to their implementation.   
*   The final set of variables ( [2.x.106] ) describes the material properties used for the two physics models.
* 

* 
* [1.x.97]
* 
*   [1.x.98]  [1.x.99]
* 

* 
*  The following class does as its name suggests. The boundary values for the velocity are  [2.x.107]  in 2d and  [2.x.108]  in 3d, respectively. The remaining boundary conditions for this problem are all homogeneous and have been discussed in the introduction. The right hand side forcing term is zero for both the fluid and the solid so we don't need an extra class for it.
* 

* 
* [1.x.100]
* 
*   [1.x.101]  [1.x.102]
* 

* 
*   [1.x.103]  [1.x.104]
* 

* 
*  Let's now get to the implementation of the primary class of this program. The first few functions are the constructor and the helper functions that can be used to determine which part of the domain a cell is in. Given the discussion of these topics in the introduction, their implementation is rather obvious. In the constructor, note that we have to construct the  [2.x.109]  object from the base elements for Stokes and elasticity; using the  [2.x.110]  function assigns them spots zero and one in this collection, an order that we have to remember and use consistently in the rest of the program.
* 

* 
* [1.x.105]
* 
*   [1.x.106]  [1.x.107]
* 

* 
*  The next pair of functions deals with generating a mesh and making sure all flags that denote subdomains are correct.  [2.x.111] , as discussed in the introduction, generates an  [2.x.112]  mesh (or an  [2.x.113]  mesh in 3d) to make sure that each coarse mesh cell is completely within one of the subdomains. After generating this mesh, we loop over its boundary and set the boundary indicator to one at the top boundary, the only place where we set nonzero Dirichlet boundary conditions. After this, we loop again over all cells to set the material indicator &mdash; used to denote which part of the domain we are in, to either the fluid or solid indicator.
* 

* 
* [1.x.108]
* 
*  The second part of this pair of functions determines which finite element to use on each cell. Above we have set the material indicator for each coarse mesh cell, and as mentioned in the introduction, this information is inherited from mother to child cell upon mesh refinement.   
*   In other words, whenever we have refined (or created) the mesh, we can rely on the material indicators to be a correct description of which part of the domain a cell is in. We then use this to set the active FE index of the cell to the corresponding element of the  [2.x.114]  member variable of this class: zero for fluid cells, one for solid cells.
* 

* 
* [1.x.109]
* 
*   [1.x.110]  [1.x.111]
* 

* 
*  The next step is to setup the data structures for the linear system. To this end, we first have to set the active FE indices with the function immediately above, then distribute degrees of freedom, and then determine constraints on the linear system. The latter includes hanging node constraints as usual, but also the inhomogeneous boundary values at the top fluid boundary, and zero boundary values along the perimeter of the solid subdomain.
* 

* 
* [1.x.112]
* 
*  There are more constraints we have to handle, though: we have to make sure that the velocity is zero at the interface between fluid and solid. The following piece of code was already presented in the introduction:
* 

* 
* [1.x.113]
* 
*  At the end of all this, we can declare to the constraints object that we now have all constraints ready to go and that the object can rebuild its internal data structures for better efficiency:
* 

* 
* [1.x.114]
* 
*  In the rest of this function we create a sparsity pattern as discussed extensively in the introduction, and use it to initialize the matrix; then also set vectors to their correct sizes:
* 

* 
* [1.x.115]
* 
*   [1.x.116]  [1.x.117]
* 

* 
*  Following is the central function of this program: the one that assembles the linear system. It has a long section of setting up auxiliary functions at the beginning: from creating the quadrature formulas and setting up the FEValues, FEFaceValues and FESubfaceValues objects necessary to integrate the cell terms as well as the interface terms for the case where cells along the interface come together at same size or with differing levels of refinement...
* 

* 
* [1.x.118]
* 
*  ...to objects that are needed to describe the local contributions to the global linear system...
* 

* 
* [1.x.119]
* 
*  ...to variables that allow us to extract certain components of the shape functions and cache their values rather than having to recompute them at every quadrature point:
* 

* 
* [1.x.120]
* 
*  Then comes the main loop over all cells and, as in  [2.x.115] , the initialization of the  [2.x.116]  object for the current cell and the extraction of a FEValues object that is appropriate for the current cell:
* 

* 
* [1.x.121]
* 
*  With all of this done, we continue to assemble the cell terms for cells that are part of the Stokes and elastic regions. While we could in principle do this in one formula, in effect implementing the one bilinear form stated in the introduction, we realize that our finite element spaces are chosen in such a way that on each cell, one set of variables (either velocities and pressure, or displacements) are always zero, and consequently a more efficient way of computing local integrals is to do only what's necessary based on an  [2.x.117]  clause that tests which part of the domain we are in.         
*   The actual computation of the local matrix is the same as in  [2.x.118]  as well as that given in the  [2.x.119]  documentation module for the elasticity equations:
* 

* 
* [1.x.122]
* 
*  Once we have the contributions from cell integrals, we copy them into the global matrix (taking care of constraints right away, through the  [2.x.120]  function). Note that we have not written anything into the  [2.x.121]  variable, though we still need to pass it along since the elimination of nonzero boundary values requires the modification of local and consequently also global right hand side values:
* 

* 
* [1.x.123]
* 
*  The more interesting part of this function is where we see about face terms along the interface between the two subdomains. To this end, we first have to make sure that we only assemble them once even though a loop over all faces of all cells would encounter each part of the interface twice. We arbitrarily make the decision that we will only evaluate interface terms if the current cell is part of the solid subdomain and if, consequently, a face is not at the boundary and the potential neighbor behind it is part of the fluid domain. Let's start with these conditions:
* 

* 
* [1.x.124]
* 
*  At this point we know that the current cell is a candidate for integration and that a neighbor behind face  [2.x.122]  exists. There are now three possibilities:
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - The neighbor is at the same refinement level and has no children.
* 

* 
* 
*  - The neighbor has children.
* 

* 
* 
*  - The neighbor is coarser.                 
*   In all three cases, we are only interested in it if it is part of the fluid subdomain. So let us start with the first and simplest case: if the neighbor is at the same level, has no children, and is a fluid cell, then the two cells share a boundary that is part of the interface along which we want to integrate interface terms. All we have to do is initialize two FEFaceValues object with the current face and the face of the neighboring cell (note how we find out which face of the neighboring cell borders on the current cell) and pass things off to the function that evaluates the interface terms (the third through fifth arguments to this function provide it with scratch arrays). The result is then again copied into the global matrix, using a function that knows that the DoF indices of rows and columns of the local matrix result from different cells:
* 

* 
* [1.x.125]
* 
*  The second case is if the neighbor has further children. In that case, we have to loop over all the children of the neighbor to see if they are part of the fluid subdomain. If they are, then we integrate over the common interface, which is a face for the neighbor and a subface of the current cell, requiring us to use an FEFaceValues for the neighbor and an FESubfaceValues for the current cell:
* 

* 
* [1.x.126]
* 
*  The last option is that the neighbor is coarser. In that case we have to use an FESubfaceValues object for the neighbor and a FEFaceValues for the current cell; the rest is the same as before:
* 

* 
* [1.x.127]
* 
*  In the function that assembles the global system, we passed computing interface terms to a separate function we discuss here. The key is that even though we can't predict the combination of FEFaceValues and FESubfaceValues objects, they are both derived from the FEFaceValuesBase class and consequently we don't have to care: the function is simply called with two such objects denoting the values of the shape functions on the quadrature points of the two sides of the face. We then do what we always do: we fill the scratch arrays with the values of shape functions and their derivatives, and then loop over all entries of the matrix to compute the local integrals. The details of the bilinear form we evaluate here are given in the introduction.
* 

* 
* [1.x.128]
* 
*   [1.x.129]  [1.x.130]
* 

* 
*  As discussed in the introduction, we use a rather trivial solver here: we just pass the linear system off to the SparseDirectUMFPACK direct solver (see, for example,  [2.x.123] ). The only thing we have to do after solving is ensure that hanging node and boundary value constraints are correct.
* 

* 
* [1.x.131]
* 
*   [1.x.132]  [1.x.133]
* 

* 
*  Generating graphical output is rather trivial here: all we have to do is identify which components of the solution vector belong to scalars and/or vectors (see, for example,  [2.x.124]  for a previous example), and then pass it all on to the DataOut class:
* 

* 
* [1.x.134]
* 
*   [1.x.135]  [1.x.136]
* 

* 
*  The next step is to refine the mesh. As was discussed in the introduction, this is a bit tricky primarily because the fluid and the solid subdomains use variables that have different physical dimensions and for which the absolute magnitude of error estimates is consequently not directly comparable. We will therefore have to scale them. At the top of the function, we therefore first compute error estimates for the different variables separately (using the velocities but not the pressure for the fluid domain, and the displacements in the solid domain):
* 

* 
* [1.x.137]
* 
*  We then normalize error estimates by dividing by their norm and scale the fluid error indicators by a factor of 4 as discussed in the introduction. The results are then added together into a vector that contains error indicators for all cells:
* 

* 
* [1.x.138]
* 
*  The second to last part of the function, before actually refining the mesh, involves a heuristic that we have already mentioned in the introduction: because the solution is discontinuous, the KellyErrorEstimator class gets all confused about cells that sit at the boundary between subdomains: it believes that the error is large there because the jump in the gradient is large, even though this is entirely expected and a feature that is in fact present in the exact solution as well and therefore not indicative of any numerical error.     
*   Consequently, we set the error indicators to zero for all cells at the interface; the conditions determining which cells this affects are slightly awkward because we have to account for the possibility of adaptively refined meshes, meaning that the neighboring cell can be coarser than the current one, or could in fact be refined some more. The structure of these nested conditions is much the same as we encountered when assembling interface terms in  [2.x.125] .
* 

* 
* [1.x.139]
* 
*   [1.x.140]  [1.x.141]
* 

* 
*  This is, as usual, the function that controls the overall flow of operation. If you've read through tutorial programs  [2.x.126]  through  [2.x.127] , for example, then you are already quite familiar with the following structure:
* 

* 
* [1.x.142]
* 
*   [1.x.143]  [1.x.144]
* 

* 
*  This, final, function contains pretty much exactly what most of the other tutorial programs have:
* 

* 
* [1.x.145]
* [1.x.146][1.x.147][1.x.148]
* 

* [1.x.149][1.x.150]
* 

* 
* When running the program, you should get output like the following:
* [1.x.151]
* 
* The results are easily visualized:
*  [2.x.128] 
* The plots are easily interpreted: as the flow drives down on the left side andup on the right side of the upright part of the solid, it produces apressure that is high on the left and low on the right, and theseforces bend the vertical part of the solid to the right.
* 

* [1.x.152][1.x.153]
* 

* By changing the dimension of the  [2.x.129] class in  [2.x.130]  to 3, we can also run the same problem3d. You'd get output along the following lines:
* [1.x.154]
* You'll notice that the big bottleneck is the solver: SparseDirectUmfpack needsnearly 5 hours and some 80 GB of memory to solve the last iteration ofthis problem on a 2016 workstation (the second to last iteration took only 16minutes). Clearly a better solver is needed here, a topic discussed below.
* The results can also be visualized and yield good pictures aswell. Here is one, showing both a vector plot for the velocity (inoranges), the solid displacement (in blues), and shading the solid region:
*  [2.x.131] 
* In addition to the lack of a good solver, the mesh is a bitunbalanced: mesh refinement heavily favors the fluid subdomain (in 2d,it was the other way around, prompting us to weigh the fluid errorindicators higher). Clearly, some tweaking of the relative importanceof error indicators in the two subdomains is important if one wantedto go on doing more 3d computations.
* 

* [1.x.155][1.x.156][1.x.157]
* 

* [1.x.158][1.x.159]
* 

* An obvious place to improve the program would be to use a moresophisticated solver &mdash; in particular one that scales well andwill also work for realistic 3d problems. This shouldn't actually betoo hard to achieve here, because of the one-way coupling from fluidinto solid. To this end, assume we had re-ordered degrees of freedomin such a way that we first have all velocity and pressure degrees offreedom, and then all displacements (this is easily possible using [2.x.132]  Then the system matrix could be splitinto the following block form:[1.x.160]where  [2.x.133]  is the Stokes matrix for velocity and pressure (itcould be further subdivided into a  [2.x.134]  matrix as in  [2.x.135] , thoughthis is immaterial for the current purpose), [2.x.136]  results from the elasticity equations for thedisplacements, and  [2.x.137]  is the matrix that comes from the interfaceconditions. Now notice that the matrix[1.x.161]is the inverse of  [2.x.138] . Applying this matrix requiresonly one solve with  [2.x.139]  and  [2.x.140]  each since[1.x.162]can be computed as  [2.x.141]  followed by [2.x.142] .
* One can therefore expect that[1.x.163]would be a good preconditioner if  [2.x.143] .
* That means, we only need good preconditioners for Stokes and theelasticity equations separately. These are well known: forStokes, we can use the preconditioner discussed in the results sectionof  [2.x.144] ; for elasticity, a good preconditioner would be a singleV-cycle of a geometric or algebraic multigrid. There are more openquestions, however: For an "optimized" solver block-triangularpreconditioner built from two sub-preconditioners, one point thatoften comes up is that, when choosing parameters for thesub-preconditioners, values that work well when solving the twoproblems separately may not be optimal when combined into amultiphysics preconditioner.  In particular, when solving just a solidor fluid mechanics problem separately, the balancing act between thenumber of iterations to convergence and the cost of applying thepreconditioner on a per iteration basis may lead one to choose anexpensive preconditioner for the Stokes problem and a cheappreconditioner for the elasticity problem (or vice versa).  Whencombined, however, there is the additional constraint that you wantthe two sub-preconditioners to converge at roughly the same rate, orelse the cheap one may drive up the global number of iterations whilethe expensive one drives up the cost-per-iteration. For example, while a single AMGV-cycle is a good approach for elasticity by itself, when combinedinto a multiphysics problem there may be an incentive to using a fullW-cycle or multiple cycles to help drive down the total solve time.
* 

* [1.x.164][1.x.165]
* 

* As mentioned in the introduction, the refinement indicator we use for thisprogram is rather ad hoc. A better one would understand that the jump in thegradient of the solution across the interface is not indicative of the errorbut to be expected and ignore the interface when integrating the jumpterms. Nevertheless, this is not what the KellyErrorEstimator classdoes. Another, bigger question, is whether this kind of estimator is a goodstrategy in the first place: for example, if we want to have maximal accuracyin one particular aspect of the displacement (e.g. the displacement at the topright corner of the solid), then is it appropriate to scale the errorindicators for fluid and solid to the same magnitude? Maybe it is necessary tosolve the fluid problem with more accuracy than the solid because the fluidsolution directly affects the solids solution? Maybe the other way around?
* Consequently, an obvious possibility for improving the program would be toimplement a better refinement criterion. There is some literature on thistopic; one of a variety of possible starting points would be the paper byThomas Wick on "Adaptive finite elements for monolithic fluid-structureinteraction on a prolongated domain: Applied to an heart valve simulation",Proceedings of the Computer Methods in Mechanics Conference 2011 (CMM-2011),9-12 May 2011, Warszaw, Poland.
* 

* [1.x.166][1.x.167]
* 

* The results above are purely qualitative as there is no evidence that ourscheme in fact converges. An obvious thing to do would therefore be to addsome quantitative measures to check that the scheme at least converges to[1.x.168]. For example, we could output for each refinement cycle thedeflection of the top right corner of the part of the solid that protrudesinto the fluid subdomain. Or we could compute the net force vector or torquethe fluid exerts on the solid.
* 

* [1.x.169][1.x.170]
* 

* In reality, most fluid structure interaction problems are so that the movementof the solid does affect the flow of the fluid. For example, the forces of theair around an air foil cause it to flex and to change its shape. Likewise, aflag flaps in the wind, completely changing its shape.
* Such problems where the coupling goes both ways are typically handled in anArbitrary Lagrangian Eulerian (ALE) framework, in which the displacement ofthe solid is extended into the fluid domain in some smooth way, rather than byzero as we do here. The extended displacement field is then used to deform themesh on which we compute the fluid flow. Furthermore, the boundary conditionsfor the fluid on the interface are no longer that the velocity is zero;rather, in a time dependent program, the fluid velocity must be equal to thetime derivative of the displacement along the interface.
* 

* [1.x.171][1.x.172] [2.x.145] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-47_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34]
*  [2.x.2] 
* [1.x.35]
* [1.x.36][1.x.37][1.x.38]
* 

* This program deals with the [1.x.39],
* [1.x.40]
* This equation appears in the modeling of thin structures such as roofsof stadiums. These objects are of course in realitythree-dimensional with a large aspect ratio of lateral extent toperpendicular thickness, but one can often very accurately model thesestructures as two dimensional by making assumptions about how internalforces vary in the perpendicular direction. These assumptions lead to theequation above.
* The model typically comes in two different kinds, depending on whatkinds of boundary conditions are imposed. The first case,
* [1.x.41]
* corresponds to the edges of the thin structure attached to the top ofa wall of height  [2.x.3]  in such a way that the bending forcesthat act on the structure are  [2.x.4] ; in most physicalsituations, one will have  [2.x.5] , corresponding to the structure simplysitting atop the wall.
* In the second possible case of boundary values, one would have
* [1.x.42]
* This corresponds to a "clamped" structure for which a nonzero [2.x.6]  implies a certain angle against the horizontal.
* As with Dirichlet and Neumann boundary conditions for the Laplaceequation, it is of course possible to have one kind of boundaryconditions on one part of the boundary, and the other on theremainder.
* 

* [1.x.43][1.x.44]
* 

* The fundamental issue with the equation is that it takes fourderivatives of the solution. In the case of the Laplace equationwe treated in  [2.x.7] ,  [2.x.8] , and several other tutorial programs,one multiplies by a test function, integrates, integrates by parts,and ends up with only one derivative on both the test function andtrial function
* 
*  -  something one can do with functions that arecontinuous globally, but may have kinks at the interfaces betweencells: The derivative may not be defined at the interfaces, butthat is on a lower-dimensional manifold (and so doesn't show upin the integrated value).
* But for the biharmonic equation, if one followed the same procedureusing integrals over the entire domain (i.e., the union of all cells),one would end up with two derivatives on the test functions and trialfunctions each. If one were to use the usual piecewise polynomialfunctions with their kinks on cell interfaces, the first derivativewould yield a discontinuous gradient, and the second derivative withdelta functions on the interfaces
* 
*  -  but because both the secondderivatives of the test functions and of the trial functions yield adelta function, we would try to integrate the product of two deltafunctions. For example, in 1d, where  [2.x.9]  are the usualpiecewise linear "hat functions", we would get integrals of the sort
* [1.x.45]
* where  [2.x.10]  is the node location at which the shape function [2.x.11]  is defined, and  [2.x.12]  is the mesh size (assumeduniform). The problem is that delta functions in integrals are definedusing the relationship
* [1.x.46]
* But that only works if (i)  [2.x.13]  is actually well defined at [2.x.14] , and (ii) if it is finite. On the other hand, an integral ofthe form
* [1.x.47]
* does not make sense. Similar reasoning can be applied for 2d and 3dsituations.
* In other words: This approach of trying to integrate over the entiredomain and then integrating by parts can't work.
* Historically, numerical analysts have tried to address this byinventing finite elements that are "C<sup>1</sup> continuous", i.e., that useshape functions that are not just continuous but also have continuousfirst derivatives. This is the realm of elements such as the Argyriselement, the Clough-Tocher element and others, all developed in thelate 1960s. From a twenty-first century perspective, they can only bedescribed as bizarre in their construction. They are also exceedinglycumbersome to implement if one wants to use general meshes. As aconsequence, they have largely fallen out of favor and deal.II currentlydoes not contain implementations of these shape functions.
* 

* [1.x.48][1.x.49]
* 

* So how does one approach solving such problems then? That depends abit on the boundary conditions. If one has the first set of boundaryconditions, i.e., if the equation is
* [1.x.50]
* then the following trick works (at least if the domain is convex, seebelow): In the same way as we obtained themixed Laplace equation of  [2.x.15]  from the regular Laplace equation byintroducing a second variable, we can here introduce a variable [2.x.16]  and can then replace the equations above by thefollowing, "mixed" system:
* [1.x.51]
* In other words, we end up with what is in essence a system of twocoupled Laplace equations for  [2.x.17] , each with Dirichlet-type boundaryconditions. We know how to solve such problems, and it should not bevery difficult to construct good solvers and preconditioners for thissystem either using the techniques of  [2.x.18]  or  [2.x.19] . So thiscase is pretty simple to deal with.
*  [2.x.20]  It is worth pointing out that this only works for domains whose  boundary has corners if the domain is also convex
* 
*  -  in other words,  if there are no re-entrant corners.  This sounds like a rather random condition, but it makes  sense in view of the following two facts: The solution of the  original biharmonic equation must satisfy  [2.x.21] . On the  other hand, the mixed system reformulation above suggests that both   [2.x.22]  and  [2.x.23]  satisfy  [2.x.24]  because both variables only  solve a Poisson equation. In other words, if we want to ensure that  the solution  [2.x.25]  of the mixed problem is also a solution of the  original biharmonic equation, then we need to be able to somehow  guarantee that the solution of  [2.x.26]  is in fact more smooth  than just  [2.x.27] . This can be argued as follows: For convex  domains,  [1.x.52] implies that if the right hand side  [2.x.28] , then   [2.x.29]  if the domain is convex and the boundary is smooth  enough. (This could also be guaranteed if the domain boundary is  sufficiently smooth
* 
*  -  but domains whose boundaries have no corners  are not very practical in real life.)  We know that  [2.x.30]  because it solves the equation   [2.x.31] , but we are still left with the condition on convexity  of the boundary; one can show that polygonal, convex domains are  good enough to guarantee that  [2.x.32]  in this case (smoothly  bounded, convex domains would result in  [2.x.33] , but we don't  need this much regularity). On the other hand, if the domain is not  convex, we can not guarantee that the solution of the mixed system  is in  [2.x.34] , and consequently may obtain a solution that can't be  equal to the solution of the original biharmonic equation.
* The more complicated situation is if we have the "clamped" boundaryconditions, i.e., if the equation looks like this:
* [1.x.53]
* The same trick with the mixed system does not work here, because wewould end up with [1.x.54] Dirichlet and Neumann boundary conditions for [2.x.35] , but none for  [2.x.36] .
* 

* The solution to this conundrum arrived with the Discontinuous Galerkinmethod wave in the 1990s and early 2000s: In much the same way as onecan use [1.x.55] shape functions for the Laplace equationby penalizing the size of the discontinuity to obtain a scheme for anequation that has one derivative on each shape function, we can use ascheme that uses [1.x.56] (but not  [2.x.37]  continuous) shapefunctions and penalize the jump in the derivative to obtain a schemefor an equation that has two derivatives on each shape function. Inanalogy to the Interior Penalty (IP) method for the Laplace equation,this scheme for the biharmonic equation is typically called the  [2.x.38]  IP(or C0IP) method, since it uses  [2.x.39]  (continuous but not continuouslydifferentiable) shape functions with an interior penalty formulation.
* 

* [1.x.57][1.x.58]
* 

* We base this program on the  [2.x.40]  IP method presented by SusanneBrenner and Li-Yeng Sung in the paper "C [2.x.41]  Interior Penalty Methodfor Linear Fourth Order Boundary Value Problems on polygonaldomains''  [2.x.42]  , where the method isderived for the biharmonic equation with "clamped" boundaryconditions.
* As mentioned, this method relies on the use of  [2.x.43]  Lagrange finiteelements where the  [2.x.44]  continuity requirement is relaxed and hasbeen replaced with interior penalty techniques. To derive this method,we consider a  [2.x.45]  shape function  [2.x.46]  which vanishes on [2.x.47] . We introduce notation  [2.x.48]  as the set ofall faces of  [2.x.49] ,  [2.x.50]  as the set of boundary faces,and  [2.x.51]  as the set of interior faces for use further down below.Since the higher order derivatives of  [2.x.52]  have twovalues on each interface  [2.x.53]  (shared by the two cells [2.x.54] ), we cope with this discontinuity bydefining the following single-valued functions on  [2.x.55] :
* [1.x.59]
* for  [2.x.56]  (i.e., for the gradient and the matrix of secondderivatives), and where  [2.x.57]  denotes a unit vector normal to [2.x.58]  pointing from  [2.x.59]  to  [2.x.60] . In theliterature, these functions are referred to as the "jump" and"average" operations, respectively.
* To obtain the  [2.x.61]  IP approximation  [2.x.62] , we left multiply thebiharmonic equation by  [2.x.63] , and then integrate over  [2.x.64] . Asexplained above, we can't do the integration by parts on all of [2.x.65]  with these shape functions, but we can do it on each cellindividually since the shape functions are just polynomials on eachcell. Consequently, we start by using the followingintegration-by-parts formula on each mesh cell  [2.x.66] :
* [1.x.60]
* At this point, we have two options: We can integrate the domain term's [2.x.67]  one more time to obtain
* [1.x.61]
* For a variety of reasons, this turns out to be a variation that is notuseful for our purposes.
* Instead, what we do is recognize that [2.x.68] , and wecan re-sort these operations as [2.x.69]  where wetypically write  [2.x.70]  to indicatethat this is the "Hessian" matrix of second derivatives. With thisre-ordering, we can now integrate the divergence, rather than thegradient operator, and we get the following instead:
* [1.x.62]
* Here, the colon indicates a double-contraction over the indices of thematrices to its left and right, i.e., the scalar product between twotensors. The outer product of two vectors  [2.x.71]  yields thematrix  [2.x.72] .
* Then, we sum over all cells  [2.x.73] , and take into accountthat this means that every interior face appears twice in thesum. If we therefore split everything into a sum of integrals overcell interiors and a separate sum over cell interfaces, we can usethe jump and average operators defined above. There are two stepsleft: First, because our shape functions are continuous, the gradientsof the shape functions may be discontinuous, but the continuityguarantees that really only the normal component of the gradient isdiscontinuous across faces whereas the tangential component(s) arecontinuous. Second, the discrete formulation that results is notstable as the mesh size goes to zero, and to obtain a stableformulation that converges to the correct solution, we need to addthe following terms:
* [1.x.63]
* Then, after making cancellations that arise, we arrive at the followingC0IP formulation of the biharmonic equation: find  [2.x.74]  such that  [2.x.75]  on  [2.x.76]  and
* [1.x.64]
* where
* [1.x.65]
* and
* [1.x.66]
* Here,  [2.x.77]  is the penalty parameter which both weakly enforces theboundary condition
* [1.x.67]
* on the boundary interfaces  [2.x.78] , and also ensures thatin the limit  [2.x.79] ,  [2.x.80]  converges to a  [2.x.81]  continuousfunction.  [2.x.82]  is chosen to be large enough to guarantee thestability of the method. We will discuss our choice in the program below.
* 

* [1.x.68][1.x.69]
* On polygonal domains, the weak solution  [2.x.83]  to the biharmonic equationlives in  [2.x.84]  where  [2.x.85]  isdetermined by the interior angles at the corners of  [2.x.86] . Forinstance, whenever  [2.x.87]  is convex,  [2.x.88] ;  [2.x.89]  may be lessthan one if the domain has re-entrant corners but [2.x.90]  is close to  [2.x.91]  if one of all interior angles is close to [2.x.92] .
* Now suppose that the  [2.x.93]  IP solution  [2.x.94]  is approximated by  [2.x.95] shape functions with polynomial degree  [2.x.96] . Then thediscretization outlined above yields the convergence rates asdiscussed below.
* 

* [1.x.70]
* Ideally, we would like to measure convergence in the "energy norm" [2.x.97] . However, this does not work because, again, thediscrete solution  [2.x.98]  does not have two (weak) derivatives. Instead,one can define a discrete ( [2.x.99]  IP) seminorm that is "equivalent" to theenergy norm, as follows:
* [1.x.71]
* 
* In this seminorm, the theory in the paper mentioned above yields that wecan expect
* [1.x.72]
* much as one would expect given the convergence rates we know are truefor the usual discretizations of the Laplace equation.
* Of course, this is true only if the exact solution is sufficientlysmooth. Indeed, if  [2.x.100]  with  [2.x.101] , [2.x.102]  where  [2.x.103] ,then the convergence rate of the  [2.x.104]  IP method is [2.x.105] . In other words, the optimalconvergence rate can only be expected if the solution is so smooththat  [2.x.106] ; this canonly happen if (i) the domain is convex with a sufficiently smoothboundary, and (ii)  [2.x.107] . In practice, of course, the solution iswhat it is (independent of the polynomial degree we choose), and thelast condition can then equivalently be read as saying that there isdefinitely no point in choosing  [2.x.108]  large if  [2.x.109]  is not alsolarge. In other words, the only reasonably choices for  [2.x.110]  are  [2.x.111]  because larger polynomial degrees do not result in higherconvergence orders.
* For the purposes of this program, we're a bit too lazy to actuallyimplement this equivalent seminorm
* 
*  -  though it's not very difficult andwould make for a good exercise. Instead, we'll simply check in theprogram what the "broken"  [2.x.112]  seminorm
* [1.x.73]
* yields. The convergence rate in this norm can, from a theoreticalperspective, of course not be [1.x.74] than the one for [2.x.113]  because it contains only a subset of the necessary terms,but it could at least conceivably be better. It could also be the case thatwe get the optimal convergence rate even though there is a bug in theprogram, and that that bug would only show up in sub-optimal rates forthe additional terms present in  [2.x.114] . But, one might hopethat if we get the optimal rate in the broken norm and the normsdiscussed below, then the program is indeed correct. The resultssection will demonstrate that we obtain optimal rates in all normsshown.
* 

* [1.x.75]
* The optimal convergence rate in the  [2.x.115] -norm is  [2.x.116] provided  [2.x.117] . More details can be found in Theorem 4.6 of [2.x.118]  .
* The default in the program below is to choose  [2.x.119] . In that case, thetheorem does not apply, and indeed one only gets  [2.x.120] instead of  [2.x.121]  as we will show in the results section.
* 

* [1.x.76]
* Given that we expect [2.x.122]  in the best of cases for a norm equivalent tothe  [2.x.123]  seminorm, and  [2.x.124]  for the  [2.x.125]  norm, onemay ask about what happens in the  [2.x.126]  seminorm that is intermediateto the two others. A reasonable guess is that one should expect [2.x.127] . There is probably a paper somewhere that provesthis, but we also verify that this conjecture is experimentally truebelow.
* 

* 
* [1.x.77][1.x.78]
* 

* We remark that the derivation of the  [2.x.128]  IP method for thebiharmonic equation with other boundary conditions
* 
*  -  for instance,for the first set of boundary conditions namely  [2.x.129]  and  [2.x.130]  on [2.x.131] 
* 
*  -  can be obtained with suitable modifications to [2.x.132]  and  [2.x.133]  described inthe book chapter  [2.x.134]  .
* 

* [1.x.79][1.x.80]
* 

* The last step that remains to describe is what this program solvesfor. As always, a trigonometric function is both a good and a badchoice because it does not lie in any polynomial space in which we mayseek the solution while at the same time being smoother than realsolutions typically are (here, it is in  [2.x.135]  while realsolutions are typically only in  [2.x.136]  or so on convex polygonaldomains, or somewhere between  [2.x.137]  and  [2.x.138]  if the domain is notconvex). But, since we don't have the means to describe solutions ofrealistic problems in terms of relatively simple formulas, we just gowith the following, on the unit square for the domain  [2.x.139] :
* [1.x.81]
* As a consequence, we then need choose as boundary conditions the following:
* [1.x.82]
* The right hand side is easily computed as
* [1.x.83]
* The program has classes  [2.x.140]  and [2.x.141]  that encode this information.
* 

*  [1.x.84] [1.x.85]
*   [1.x.86]  [1.x.87]
* 

* 
*  The first few include files have already been used in the previous example, so we will not explain their meaning here again. The principal structure of the program is very similar to that of, for example,  [2.x.142]  and so we include many of the same header files.
* 

* 
*  

* 
* [1.x.88]
* 
*  The two most interesting header files will be these two:
* 

* 
* [1.x.89]
* 
*  The first of these is responsible for providing the class FEInterfaceValues that can be used to evaluate quantities such as the jump or average of shape functions (or their gradients) across interfaces between cells. This class will be quite useful in evaluating the penalty terms that appear in the C0IP formulation.
* 

* 
*  
*  
* 

* 
* [1.x.90]
* 
*  In the following namespace, let us define the exact solution against which we will compare the numerically computed one. It has the form  [2.x.143]  (only the 2d case is implemented), and the namespace also contains a class that corresponds to the right hand side that produces this solution.
* 

* 
* [1.x.91]
* 
*   [1.x.92]  [1.x.93]   
*   The following is the principal class of this tutorial program. It has the structure of many of the other tutorial programs and there should really be nothing particularly surprising about its contents or the constructor that follows it.
* 

* 
* [1.x.94]
* 
*  Next up are the functions that create the initial mesh (a once refined unit square) and set up the constraints, vectors, and matrices on each mesh. Again, both of these are essentially unchanged from many previous tutorial programs.
* 

* 
* [1.x.95]
* 
*   [1.x.96]  [1.x.97]   
*   The following pieces of code are more interesting. They all relate to the assembly of the linear system. While assembling the cell-interior terms is not of great difficulty
* 
*  -  that works in essence like the assembly of the corresponding terms of the Laplace equation, and you have seen how this works in  [2.x.144]  or  [2.x.145] , for example
* 
*  -  the difficulty is with the penalty terms in the formulation. These require the evaluation of gradients of shape functions at interfaces of cells. At the least, one would therefore need to use two FEFaceValues objects, but if one of the two sides is adaptively refined, then one actually needs an FEFaceValues and one FESubfaceValues objects; one also needs to keep track which shape functions live where, and finally we need to ensure that every face is visited only once. All of this is a substantial overhead to the logic we really want to implement (namely the penalty terms in the bilinear form). As a consequence, we will make use of the FEInterfaceValues class
* 
*  -  a helper class in deal.II that allows us to abstract away the two FEFaceValues or FESubfaceValues objects and directly access what we really care about: jumps, averages, etc.   
*   But this doesn't yet solve our problem of having to keep track of which faces we have already visited when we loop over all cells and all of their faces. To make this process simpler, we use the  [2.x.146]  function that provides a simple interface for this task: Based on the ideas outlined in the WorkStream namespace documentation,  [2.x.147]  requires three functions that do work on cells, interior faces, and boundary faces. These functions work on scratch objects for intermediate results, and then copy the result of their computations into copy data objects from where a copier function copies them into the global matrix and right hand side objects.   
*   The following structures then provide the scratch and copy objects that are necessary for this approach. You may look up the WorkStream namespace as well as the  [2.x.148]  "Parallel computing with multiple processors" module for more information on how they typically work.
* 

* 
* [1.x.98]
* 
*  The more interesting part is where we actually assemble the linear system. Fundamentally, this function has five parts:
* 

* 
* 
*  - The definition of the `cell_worker` lambda function, a small function that is defined within the `assemble_system()` function and that will be responsible for computing the local integrals on an individual cell. It will work on a copy of the `ScratchData` class and put its results into the corresponding `CopyData` object.
* 

* 
* 
*  - The definition of the `face_worker` lambda function that does the integration of all terms that live on the interfaces between cells.
* 

* 
* 
*  - The definition of the `boundary_worker` function that does the same but for cell faces located on the boundary of the domain.
* 

* 
* 
*  - The definition of the `copier` function that is responsible for copying all of the data the previous three functions have put into copy objects for a single cell, into the global matrix and right hand side.   
*   The fifth part is the one where we bring all of this together.   
*   Let us go through each of these pieces necessary for the assembly in turns.
* 

* 
* [1.x.99]
* 
*  The first piece is the `cell_worker` that does the assembly on the cell interiors. It is a (lambda) function that takes a cell (input), a scratch object, and a copy object (output) as arguments. It looks like the assembly functions of many other of the tutorial programs, or at least the body of the loop over all cells.     
*   The terms we integrate here are the cell contribution

* 
* [1.x.100]
*  to the global matrix, and

* 
* [1.x.101]
*  to the right hand side vector.     
*   We use the same technique as used in the assembly of  [2.x.149]  to accelerate the function: Instead of calling `fe_values.shape_hessian(i, qpoint)` in the innermost loop, we create a variable `hessian_i` that evaluates this value once in the loop over `i` and re-use the so-evaluated value in the loop over `j`. For symmetry, we do the same with a variable `hessian_j`, although it is indeed only used once and we could have left the call to `fe_values.shape_hessian(j,qpoint)` in the instruction that computes the scalar product between the two terms.
* 

* 
* [1.x.102]
* 
*  The next building block is the one that assembles penalty terms on each of the interior faces of the mesh. As described in the documentation of  [2.x.150]  this function receives arguments that denote a cell and its neighboring cell, as well as (for each of the two cells) the face (and potentially sub-face) we have to integrate over. Again, we also get a scratch object, and a copy object for putting the results in.     
*   The function has three parts itself. At the top, we initialize the FEInterfaceValues object and create a new  [2.x.151]  object to store our input in. This gets pushed to the end of the `copy_data.face_data` variable. We need to do this because the number of faces (or subfaces) over which we integrate for a given cell differs from cell to cell, and the sizes of these matrices also differ, depending on what degrees of freedom are adjacent to the face or subface. As discussed in the documentation of  [2.x.152]  the copy object is reset every time a new cell is visited, so that what we push to the end of `copy_data.face_data()` is really all that the later `copier` function gets to see when it copies the contributions of each cell to the global matrix and right hand side objects.
* 

* 
* [1.x.103]
* 
*  The second part deals with determining what the penalty parameter should be. By looking at the units of the various terms in the bilinear form, it is clear that the penalty has to have the form  [2.x.153]  (i.e., one over length scale), but it is not a priori obvious how one should choose the dimension-less number  [2.x.154] . From the discontinuous Galerkin theory for the Laplace equation, one might conjecture that the right choice is  [2.x.155]  is the right choice, where  [2.x.156]  is the polynomial degree of the finite element used. We will discuss this choice in a bit more detail in the results section of this program.       
*   In the formula above,  [2.x.157]  is the size of cell  [2.x.158] . But this is not quite so straightforward either: If one uses highly stretched cells, then a more involved theory says that  [2.x.159]  should be replaced by the diameter of cell  [2.x.160]  normal to the direction of the edge in question.  It turns out that there is a function in deal.II for that. Secondly,  [2.x.161]  may be different when viewed from the two different sides of a face.       
*   To stay on the safe side, we take the maximum of the two values. We will note that it is possible that this computation has to be further adjusted if one were to use hanging nodes resulting from adaptive mesh refinement.
* 

* 
* [1.x.104]
* 
*  Finally, and as usual, we loop over the quadrature points and indices `i` and `j` to add up the contributions of this face or sub-face. These are then stored in the `copy_data.face_data` object created above. As for the cell worker, we pull the evaluation of averages and jumps out of the loops if possible, introducing local variables that store these results. The assembly then only needs to use these local variables in the innermost loop. Regarding the concrete formula this code implements, recall that the interface terms of the bilinear form were as follows:

* 
* [1.x.105]
* 
* 

* 
* [1.x.106]
* 
*  The third piece is to do the same kind of assembly for faces that are at the boundary. The idea is the same as above, of course, with only the difference that there are now penalty terms that also go into the right hand side.     
*   As before, the first part of the function simply sets up some helper objects:
* 

* 
* [1.x.107]
* 
*  Positively, because we now only deal with one cell adjacent to the face (as we are on the boundary), the computation of the penalty factor  [2.x.162]  is substantially simpler:
* 

* 
* [1.x.108]
* 
*  The third piece is the assembly of terms. This is now slightly more involved since these contains both terms for the matrix and for the right hand side. The former is exactly the same as for the interior faces stated above if one just defines the jump and average appropriately (which is what the FEInterfaceValues class does). The latter requires us to evaluate the boundary conditions  [2.x.163] , which in the current case (where we know the exact solution) we compute from  [2.x.164] . The term to be added to the right hand side vector is then  [2.x.165] .
* 

* 
* [1.x.109]
* 
*  Part 4 is a small function that copies the data produced by the cell, interior, and boundary face assemblers above into the global matrix and right hand side vector. There really is not very much to do here: We distribute the cell matrix and right hand side contributions as we have done in almost all of the other tutorial programs using the constraints objects. We then also have to do the same for the face matrix contributions that have gained content for the faces (interior and boundary) and that the `face_worker` and `boundary_worker` have added to the `copy_data.face_data` array.
* 

* 
* [1.x.110]
* 
*  Having set all of this up, what remains is to just create a scratch and copy data object and call the  [2.x.166]  function that then goes over all cells and faces, calls the respective workers on them, and then the copier function that puts things into the global matrix and right hand side. As an additional benefit,  [2.x.167]  does all of this in parallel, using as many processor cores as your machine happens to have.
* 

* 
* [1.x.111]
* 
*   [1.x.112]  [1.x.113]   
*   The show is essentially over at this point: The remaining functions are not overly interesting or novel. The first one simply uses a direct solver to solve the linear system (see also  [2.x.168] ):
* 

* 
* [1.x.114]
* 
*  The next function evaluates the error between the computed solution and the exact solution (which is known here because we have chosen the right hand side and boundary values in a way so that we know the corresponding solution). In the first two code blocks below, we compute the error in the  [2.x.169]  norm and the  [2.x.170]  semi-norm.
* 

* 
* [1.x.115]
* 
*  Now also compute an approximation to the  [2.x.171]  seminorm error. The actual  [2.x.172]  seminorm would require us to integrate second derivatives of the solution  [2.x.173] , but given the Lagrange shape functions we use,  [2.x.174]  of course has kinks at the interfaces between cells, and consequently second derivatives are singular at interfaces. As a consequence, we really only integrate over the interior of cells and ignore the interface contributions. This isnot* an equivalent norm to the energy norm for the problem, but still gives us an idea of how fast the error converges.     
*   We note that one could address this issue by defining a norm that is equivalent to the energy norm. This would involve adding up not only the integrals over cell interiors as we do below, but also adding penalty terms for the jump of the derivative of  [2.x.175]  across interfaces, with an appropriate scaling of the two kinds of terms. We will leave this for later work.
* 

* 
* [1.x.116]
* 
*  Equally uninteresting is the function that generates graphical output. It looks exactly like the one in  [2.x.176] , for example.
* 

* 
* [1.x.117]
* 
*  The same is true for the `run()` function: Just like in previous programs.
* 

* 
* [1.x.118]
* 
*   [1.x.119]  [1.x.120]
* 

* 
*  Finally for the `main()` function. There is, again, not very much to see here: It looks like the ones in previous tutorial programs. There is a variable that allows selecting the polynomial degree of the element we want to use for solving the equation. Because the C0IP formulation we use requires the element degree to be at least two, we check with an assertion that whatever one sets for the polynomial degree actually makes sense.
* 

* 
* [1.x.121]
* [1.x.122][1.x.123]
* 

* We run the program with right hand side and boundary values asdiscussed in the introduction. These will produce thesolution  [2.x.177]  on the domain  [2.x.178] .We test this setup using  [2.x.179] ,  [2.x.180] , and  [2.x.181]  elements, which one canchange via the `fe_degree` variable in the `main()` function. With meshrefinement, the  [2.x.182]  convergence rates,  [2.x.183] -seminorm rate,and  [2.x.184] -seminorm convergence of  [2.x.185] should then be around 2, 2, 1 for  [2.x.186]  (with the  [2.x.187]  normsub-optimal as discussed in the introduction); 4, 3, 2 for [2.x.188] ; and 5, 4, 3 for  [2.x.189] , respectively.
* From the literature, it is not immediately clear whatthe penalty parameter  [2.x.190]  should be. For example, [2.x.191]  state that it needs to be larger than one, andchoose  [2.x.192] . The FEniCS/Dolphin tutorial chooses it as [2.x.193] , seehttps://fenicsproject.org/docs/dolfin/1.6.0/python/demo/documented/biharmonic/python/documentation.html.  [2.x.194]  uses a value for  [2.x.195]  larger than thenumber of edges belonging to an element for Kirchhoff plates (seetheir Section 4.2). This suggests that maybe [2.x.196] ,  [2.x.197] , are too small; on the other hand, a value [2.x.198]  would be reasonable,where  [2.x.199]  is the degree of polynomials. The last of these choices isthe one one would expect to work by comparingto the discontinuous Galerkin formulations for the Laplace equation(see, for example, the discussions in  [2.x.200]  and  [2.x.201] ),and it will turn out to also work here.But we should check what value of  [2.x.202]  is right, and we will do sobelow; changing  [2.x.203]  is easy in the two `face_worker` and`boundary_worker` functions defined in `assemble_system()`.
* 

* [1.x.124][1.x.125][1.x.126][1.x.127]
* 

* We run the code with differently refined meshesand get the following convergence rates.
*  [2.x.204] We can see that the  [2.x.205]  convergence rates are around 2, [2.x.206] -seminorm convergence rates are around 2,and  [2.x.207] -seminorm convergence rates are around 1. The latter twomatch the theoretically expected rates; for the former, we have notheorem but are not surprised that it is sub-optimal given the remarkin the introduction.
* 

* [1.x.128][1.x.129][1.x.130][1.x.131]
* 

* 
*  [2.x.208] We can see that the  [2.x.209]  convergence rates are around 4, [2.x.210] -seminorm convergence rates are around 3,and  [2.x.211] -seminorm convergence rates are around 2.This, of course, matches our theoretical expectations.
* 

* [1.x.132][1.x.133][1.x.134][1.x.135]
* 

*  [2.x.212] We can see that the  [2.x.213]  norm convergence rates are around 5, [2.x.214] -seminorm convergence rates are around 4,and  [2.x.215] -seminorm convergence rates are around 3.On the finest mesh, the  [2.x.216]  norm convergence rateis much smaller than our theoretical expectationsbecause the linear solver becomes the limiting factor dueto round-off. Of course the  [2.x.217]  error is also very small already inthat case.
* 

* [1.x.136][1.x.137][1.x.138][1.x.139]
* 

* For comparison with the results above, let us now also consider thecase where we simply choose  [2.x.218] :
*  [2.x.219] Although  [2.x.220]  norm convergence rates of  [2.x.221]  more or lessfollows the theoretical expectations,the  [2.x.222] -seminorm and  [2.x.223] -seminorm do not seem to converge as expected.Comparing results from  [2.x.224]  and  [2.x.225] , it is clear that [2.x.226]  is a better penalty.Given that  [2.x.227]  is already too small for  [2.x.228]  elements, it may not be surprising that if one repeated theexperiment with a  [2.x.229]  element, the results are even more disappointing: One again only obtains convergencerates of 2, 1, zero
* 
*  -  i.e., no better than for the  [2.x.230]  element (although the errors are smaller in magnitude).Maybe surprisingly, however, one obtains more or less the expected convergence orders when using  [2.x.231] elements. Regardless, this uncertainty suggests that  [2.x.232]  is at best a risky choice, and at worst anunreliable one and that we should choose  [2.x.233]  larger.
* 

* [1.x.140][1.x.141][1.x.142][1.x.143]
* 

* Since  [2.x.234]  is clearly too small, one might conjecture that [2.x.235]  might actually work better. Here is what one obtains inthat case:
*  [2.x.236] In this case, the convergence rates more or less follow thetheoretical expectations, but, compared to the results from  [2.x.237] , are more variable.Again, we could repeat this kind of experiment for  [2.x.238]  and  [2.x.239]  elements. In both cases, we will find that weobtain roughly the expected convergence rates. Of more interest may then be to compare the absolutesize of the errors. While in the table above, for the  [2.x.240]  case, the errors on the finest grid are comparable betweenthe  [2.x.241]  and  [2.x.242]  case, for  [2.x.243]  the errors are substantially larger for  [2.x.244]  than for [2.x.245] . The same is true for the  [2.x.246]  case.
* 

* [1.x.144][1.x.145]
* 

* The conclusions for which of the "reasonable" choices one should use for the penalty parameteris that  [2.x.247]  yields the expected results. It is, consequently, what the codeuses as currently written.
* 

* [1.x.146][1.x.147]
* 

* There are a number of obvious extensions to this program that wouldmake sense:
* 
*  - The program uses a square domain and a uniform mesh. Real problems  don't come this way, and one should verify convergence also on  domains with other shapes and, in particular, curved boundaries. One  may also be interested in resolving areas of less regularity by  using adaptive mesh refinement.
* 
*  - From a more theoretical perspective, the convergence results above  only used the "broken"  [2.x.248]  seminorm  [2.x.249]  instead  of the "equivalent" norm  [2.x.250] . This is good enough to  convince ourselves that the program isn't fundamentally  broken. However, it might be interesting to measure the error in the  actual norm for which we have theoretical results. Implementing this  addition should not be overly difficult using, for example, the  FEInterfaceValues class combined with  [2.x.251]  in the  same spirit as we used for the assembly of the linear system.
* 

* [1.x.148]  [1.x.149]
* 

*   Similar to the "clamped" boundary condition addressed in the implementation,  we will derive the  [2.x.252]  IP finite element scheme for simply supported plates: 
* [1.x.150]
*   We multiply the biharmonic equation by the test function  [2.x.253]  and integrate over  [2.x.254]  and get: 
* [1.x.151]
* 
*   Summing up over all cells  [2.x.255] ,since normal directions of  [2.x.256]  are pointing at  opposite directions on each interior edge shared by two cells and  [2.x.257]  on  [2.x.258] , 
* [1.x.152]
*   and by the definition of jump over cell interfaces, 
* [1.x.153]
*   We separate interior faces and boundary faces of the domain, 
* [1.x.154]
*   where  [2.x.259]  is the set of interior faces.  This leads us to 
* [1.x.155]
* 
*   In order to symmetrize and stabilize the discrete problem,  we add symmetrization and stabilization term.  We finally get the  [2.x.260]  IP finite element scheme for the biharmonic equation:  find  [2.x.261]  such that  [2.x.262]  on  [2.x.263]  and 
* [1.x.156]
*   where 
* [1.x.157]
*   and 
* [1.x.158]
*   The implementation of this boundary case is similar to the "clamped" version  except that `boundary_worker` is no longer needed for system assembling  and the right hand side is changed according to the formulation.
* 

* [1.x.159][1.x.160] [2.x.264] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-48_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] ,  [2.x.2] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23]
* 
* [1.x.24][1.x.25]
* [1.x.26][1.x.27][1.x.28]
* 

* This program demonstrates how to use the cell-based implementation of finiteelement operators with the MatrixFree class, first introduced in  [2.x.3] , tosolve nonlinear partial differential equations. Moreover, we have another lookat the handling of constraints within the matrix-free framework.Finally, we will use an explicit time-steppingmethod to solve the problem and introduce Gauss-Lobatto finite elements thatare very convenient in this case since their mass matrix can be accuratelyapproximated by a diagonal, and thus trivially invertible, matrix. The twoingredients to this property are firstly a distribution of the nodal points ofLagrange polynomials according to the point distribution of the Gauss-Lobattoquadrature rule. Secondly, the quadrature is done with the same Gauss-Lobattoquadrature rule. In this formula, the integrals  [2.x.4]  becomezero whenever  [2.x.5] , because exactly one function  [2.x.6]  is one andall others zero in the points defining the Lagrange polynomials.Moreover, the Gauss-Lobatto distribution of nodes of Lagrangepolynomials clusters the nodes towards the element boundaries. This results ina well-conditioned polynomial basis for high-order discretizationmethods. Indeed, the condition number of an FE_Q elements with equidistantnodes grows exponentially with the degree, which destroys any benefit fororders of about five and higher. For this reason, Gauss-Lobatto points are thedefault distribution for the FE_Q element (but at degrees one and two, thoseare equivalent to the equidistant points).
* [1.x.29][1.x.30]
* 

* As an example, we choose to solve the sine-Gordon soliton equation[1.x.31]
* that was already introduced in  [2.x.7] . As a simple explicit timeintegration method, we choose leap frog scheme using the second-orderformulation of the equation. With this time stepping, the scheme reads inweak form
* [1.x.32]where [1.x.33] denotes a test function and the index [1.x.34] stands forthe time step number.
* For the spatial discretization, we choose FE_Q elementswith basis functions defined to interpolate the support points of theGauss-Lobatto quadrature rule. Moreover, when we compute the integralsover the basis functions to form the mass matrix and the operator onthe right hand side of the equation above, we use theGauss-Lobatto quadrature rule with the same support points as thenode points of the finite element to evaluate the integrals. Since thefinite element is Lagrangian, this will yield a diagonal mass matrixon the left hand side of the equation, making the solution of thelinear system in each time step trivial.
* Using this quadrature rule, for a [1.x.35]th order finite element, we use a[1.x.36]th order accurate formula to evaluate the integrals. Since theproduct of two [1.x.37]th order basis functions when computing a mass matrixgives a function with polynomial degree [1.x.38] in each direction, theintegrals are not computed exactly.  However, the overall convergenceproperties are not disturbed by the quadrature error on meshes with affineelement shapes with L2 errors proportional to [1.x.39]. Notehowever that order reduction with sub-optimal convergence rates of the L2error of [1.x.40] or even [1.x.41] for some 3Dsetups has been reported [1.x.42] on deformed (non-affine) element shapes for wave equationswhen the integrand is not a polynomial any more.
* Apart from the fact that we avoid solving linear systems with thistype of elements when using explicit time-stepping, they come with twoother advantages. When we are using the sum-factorization approach toevaluate the finite element operator (cf.  [2.x.8] ), we have toevaluate the function at the quadrature points. In the case ofGauss-Lobatto elements, where quadrature points and node points of thefinite element coincide, this operation is trivial since the valueof the function at the quadrature points is given by its one-dimensionalcoefficients. In this way, the arithmetic work for the finite element operatorevaluation is reduced by approximately a factor of two compared to the genericGaussian quadrature.
* To sum up the discussion, by using the right finite element andquadrature rule combination, we end up with a scheme where weonly need to compute the right hand side vector correspondingto the formulation above and then multiply it by the inverse of thediagonal mass matrix in each time step. In practice, of course, we extractthe diagonal elements and invert them only once at the beginning of theprogram.
* [1.x.43][1.x.44]
* 

* The usual way to handle constraints in  [2.x.9]  is to usethe AffineConstraints class that builds a sparse matrix storinginformation about which degrees of freedom (DoF) are constrained andhow they are constrained. This format uses an unnecessarily largeamount of memory since there are not so many different types ofconstraints: for example, in the case of hanging nodes when usinglinear finite element on every cell, most constraints have the form [2.x.10]  where the coefficients  [2.x.11] are always the same and only  [2.x.12]  are different. While storing thisredundant information is not a problem in general because it is onlyneeded once during matrix and right hand side assembly, it becomes abottleneck in the matrix-free approach since there thisinformation has to be accessed every time we apply the operator, and theremaining components of the operator evaluation are so fast. Thus,instead of an AffineConstraints object, MatrixFree uses a variable thatwe call  [2.x.13]  that collects the weights of thedifferent constraints. Then, only an identifier of each constraint in themesh instead of all the weights have to be stored. Moreover,the constraints are not applied in a pre- and postprocessing stepbut rather as we evaluate the finite elementoperator. Therefore, the constraint information is embedded into thevariable  [2.x.14]  that is used to extractthe cell information from the global vector. If a DoF is constrained,the  [2.x.15]  variable contains the globalindices of the DoFs that it is constrained to. Then, we have anothervariable  [2.x.16]  at hand that holds, foreach cell, the local indices of DoFs that are constrained as well asthe identifier of the type of constraint. Fortunately, you will not seethese data structures in the example program since the class [2.x.17]  takes care of the constraints without userinteraction.
* In the presence of hanging nodes, the diagonal mass matrix obtained on theelement level via the Gauss-Lobatto quadrature/node point procedure does notdirectly translate to a diagonal global mass matrix, as following theconstraints on rows and columns would also add off-diagonal entries. Asexplained in [1.x.45], interpolating constraints on a vector, which maintains thediagonal shape of the mass matrix, is consistent with the equations up to anerror of the same magnitude as the quadrature error. In the program below, wewill simply assemble the diagonal of the mass matrix as if it were a vector toenable this approximation.
* 

* [1.x.46][1.x.47]
* 

* The MatrixFree class comes with the option to be parallelized on three levels:MPI parallelization on clusters of distributed nodes, thread parallelizationscheduled by the Threading Building Blocks library, and finally with avectorization by working on a batch of two (or more) cells via SIMD data type(sometimes called cross-element or external vectorization).As we have already discussed in  [2.x.18] , you willget best performance by using an instruction set specific to your system,e.g. with the cmake variable <tt>-DCMAKE_CXX_FLAGS="-march=native"</tt>. TheMPI parallelization was already exploited in  [2.x.19] . Here, we additionallyconsider thread parallelization with TBB. This is fairly simple, as all weneed to do is to tell the initialization of the MatrixFree object about thefact that we want to use a thread parallel scheme through the variable [2.x.20]  During setup, a dependencygraph is set up similar to the one described in the  [2.x.21]  ,which allows to schedule the work of the  [2.x.22]  function on chunks ofcells without several threads accessing the same vector indices. As opposed tothe WorkStream loops, some additional clever tricks to avoid globalsynchronizations as described in [1.x.48] are also applied.
* Note that this program is designed to be run with a distributed triangulation [2.x.23]  which requires deal.II to beconfigured with [1.x.49] as describedin the [1.x.50] file. However, anon-distributed triangulation is also supported, in which case thecomputation will be run in serial.
* [1.x.51][1.x.52]
* 

* In our example, we choose the initial value to be [1.x.53] and solve the equation over the time interval [-10,10]. Theconstants are chosen to be  [2.x.24]  and [1.x.54]. As mentionedin  [2.x.25] , in one dimension [1.x.55] as a function of [1.x.56] is the exactsolution of the sine-Gordon equation. For higher dimension, this is howevernot the case.
* 

*  [1.x.57] [1.x.58]
*  The necessary files from the deal.II library.
* 

* 
* [1.x.59]
* 
*  This includes the data structures for the efficient implementation of matrix-free methods.
* 

* 
* [1.x.60]
* 
*  We start by defining two global variables to collect all parameters subject to changes at one place: One for the dimension and one for the finite element degree. The dimension is used in the main function as a template argument for the actual classes (like in all other deal.II programs), whereas the degree of the finite element is more crucial, as it is passed as a template argument to the implementation of the Sine-Gordon operator. Therefore, it needs to be a compile-time constant.
* 

* 
* [1.x.61]
* 
*   [1.x.62]  [1.x.63]
* 

* 
*  The  [2.x.26]  class implements the cell-based operation that is needed in each time step. This nonlinear operation can be implemented straight-forwardly based on the  [2.x.27]  class, in the same way as a linear operation would be treated by this implementation of the finite element operator application. We apply two template arguments to the class, one for the dimension and one for the degree of the finite element. This is a difference to other functions in deal.II where only the dimension is a template argument. This is necessary to provide the inner loops in  [2.x.28]  with information about loop lengths etc., which is essential for efficiency. On the other hand, it makes it more challenging to implement the degree as a run-time parameter.
* 

* 
* [1.x.64]
* 
*   [1.x.65]  [1.x.66]
* 

* 
*  This is the constructor of the SineGordonOperation class. It receives a reference to the MatrixFree holding the problem information and the time step size as input parameters. The initialization routine sets up the mass matrix. Since we use Gauss-Lobatto elements, the mass matrix is a diagonal matrix and can be stored as a vector. The computation of the mass matrix diagonal is simple to achieve with the data structures provided by FEEvaluation: Just loop over all cell batches, i.e., collections of cells due to SIMD vectorization, and integrate over the function that is constant one on all quadrature points by using the  [2.x.29]  function with  [2.x.30]  argument at the slot for values. Finally, we invert the diagonal entries to have the inverse mass matrix directly available in each time step.
* 

* 
* [1.x.67]
* 
*   [1.x.68]  [1.x.69]
* 

* 
*  This operator implements the core operation of the program, the integration over a range of cells for the nonlinear operator of the Sine-Gordon problem. The implementation is based on the FEEvaluation class as in  [2.x.31] . Due to the special structure in Gauss-Lobatto elements, certain operations become simpler, in particular the evaluation of shape function values on quadrature points which is simply the injection of the values of cell degrees of freedom. The MatrixFree class detects possible structure of the finite element at quadrature points when initializing, which is then automatically used by FEEvaluation for selecting the most appropriate numerical kernel.
* 

* 
*  The nonlinear function that we have to evaluate for the time stepping routine includes the value of the function at the present time  [2.x.32]  as well as the value at the previous time step  [2.x.33]  Both values are passed to the operator in the collection of source vectors  [2.x.34]  which is simply a  [2.x.35]  of pointers to the actual solution vectors. This construct of collecting several source vectors into one is necessary as the cell loop in  [2.x.36]  takes exactly one source and one destination vector, even if we happen to use many vectors like the two in this case. Note that the cell loop accepts any valid class for input and output, which does not only include vectors but general data types.  However, only in case it encounters a  [2.x.37]  or a  [2.x.38]  collecting these vectors, it calls functions that exchange ghost data due to MPI at the beginning and the end of the loop. In the loop over the cells, we first have to read in the values in the vectors related to the local values.  Then, we evaluate the value and the gradient of the current solution vector and the values of the old vector at the quadrature points. Next, we combine the terms in the scheme in the loop over the quadrature points. Finally, we integrate the result against the test function and accumulate the result to the global solution vector  [2.x.39]  dst.
* 

* 
* [1.x.70]
* 
*   [1.x.71]  [1.x.72]
* 

* 
*  This function performs the time stepping routine based on the cell-local strategy. Note that we need to set the destination vector to zero before we add the integral contributions of the current time step (via the  [2.x.40]  call). In this tutorial, we let the cell-loop do the zero operation via the fifth `true` argument passed to  [2.x.41]  The loop can schedule the zero operation closer to the operations on vector entries for supported vector entries, thereby possibly increasing data locality (the vector entries that first get zeroed are later re-used in the `distribute_local_to_global()` call). The structure of the cell loop is implemented in the cell finite element operator class. On each cell it applies the routine defined as the  [2.x.42]  method of the class  [2.x.43] . One could also provide a function with the same signature that is not part of a class. Finally, the result of the integration is multiplied by the inverse mass matrix.
* 

* 
* [1.x.73]
* 
*   [1.x.74]  [1.x.75]
* 

* 
*  We define a time-dependent function that is used as initial value. Different solutions can be obtained by varying the starting time. This function, taken from  [2.x.44] , would represent an analytic solution in 1D for all times, but is merely used for setting some starting solution of interest here. More elaborate choices that could test the convergence of this program are given in  [2.x.45] .
* 

* 
* [1.x.76]
* 
*   [1.x.77]  [1.x.78]
* 

* 
*  This is the main class that builds on the class in  [2.x.46] .  However, we replaced the SparseMatrix<double> class by the MatrixFree class to store the geometry data. Also, we use a distributed triangulation in this example.
* 

* 
* [1.x.79]
* 
*   [1.x.80]  [1.x.81]
* 

* 
*  This is the constructor of the SineGordonProblem class. The time interval and time step size are defined here. Moreover, we use the degree of the finite element that we defined at the top of the program to initialize a FE_Q finite element based on Gauss-Lobatto support points. These points are convenient because in conjunction with a QGaussLobatto quadrature rule of the same order they give a diagonal mass matrix without compromising accuracy too much (note that the integration is inexact, though), see also the discussion in the introduction. Note that FE_Q selects the Gauss-Lobatto nodal points by default due to their improved conditioning versus equidistant points. To make things more explicit, we state the selection of the nodal points nonetheless.
* 

* 
* [1.x.82]
* 
*   [1.x.83]  [1.x.84]
* 

* 
*  As in  [2.x.47]  this functions sets up a cube grid in  [2.x.48]  dimensions of extent  [2.x.49] . We refine the mesh more in the center of the domain since the solution is concentrated there. We first refine all cells whose center is within a radius of 11, and then refine once more for a radius 6.  This simple ad hoc refinement could be done better by adapting the mesh to the solution using error estimators during the time stepping as done in other example programs, and using  [2.x.50]  to transfer the solution to the new mesh.
* 

* 
* [1.x.85]
* 
*  We generate hanging node constraints for ensuring continuity of the solution. As in  [2.x.51] , we need to equip the constraint matrix with the IndexSet of locally relevant degrees of freedom to avoid it to consume too much memory for big problems. Next, the <code> MatrixFree </code> object for the problem is set up. Note that we specify a particular scheme for shared-memory parallelization (hence one would use multithreading for intra-node parallelism and not MPI; we here choose the standard option &mdash; if we wanted to disable shared memory parallelization even in case where there is more than one TBB thread available in the program, we would choose  [2.x.52]  Also note that, instead of using the default QGauss quadrature argument, we supply a QGaussLobatto quadrature formula to enable the desired behavior. Finally, three solution vectors are initialized. MatrixFree expects a particular layout of ghost indices (as it handles index access in MPI-local numbers that need to match between the vector and MatrixFree), so we just ask it to initialize the vectors to be sure the ghost exchange is properly handled.
* 

* 
* [1.x.86]
* 
*   [1.x.87]  [1.x.88]
* 

* 
*  This function prints the norm of the solution and writes the solution vector to a file. The norm is standard (except for the fact that we need to accumulate the norms over all processors for the parallel grid which we do via the  [2.x.53]  function), and the second is similar to what we did in  [2.x.54]  or  [2.x.55] . Note that we can use the same vector for output as the one used during computations: The vectors in the matrix-free framework always provide full information on all locally owned cells (this is what is needed in the local evaluations, too), including ghost vector entries on these cells. This is the only data that is needed in the  [2.x.56]  function as well as in DataOut. The only action to take at this point is to make sure that the vector updates its ghost values before we read from them, and to reset ghost values once done. This is a feature present only in the  [2.x.57]  class. Distributed vectors with PETSc and Trilinos, on the other hand, need to be copied to special vectors including ghost values (see the relevant section in  [2.x.58] ). If we also wanted to access all degrees of freedom on ghost cells (e.g. when computing error estimators that use the jump of solution over cell boundaries), we would need more information and create a vector initialized with locally relevant dofs just as in  [2.x.59] . Observe also that we need to distribute constraints for output
* 
*  - they are not filled during computations (rather, they are interpolated on the fly in the matrix-free method  [2.x.60] 
* 

* 
* [1.x.89]
* 
*   [1.x.90]  [1.x.91]
* 

* 
*  This function is called by the main function and steps into the subroutines of the class.   
*   After printing some information about the parallel setup, the first action is to set up the grid and the cell operator. Then, the time step is computed from the CFL number given in the constructor and the finest mesh size. The finest mesh size is computed as the diameter of the last cell in the triangulation, which is the last cell on the finest level of the mesh. This is only possible for meshes where all elements on a level have the same size, otherwise, one needs to loop over all cells. Note that we need to query all the processors for their finest cell since not all processors might hold a region where the mesh is at the finest level. Then, we readjust the time step a little to hit the final time exactly.
* 

* 
* [1.x.92]
* 
*  Next the initial value is set. Since we have a two-step time stepping method, we also need a value of the solution at time-time_step. For accurate results, one would need to compute this from the time derivative of the solution at initial time, but here we ignore this difficulty and just set it to the initial value function at that artificial time.
* 

* 
*  We then go on by writing the initial state to file and collecting the two starting solutions in a  [2.x.61]  of pointers that get later consumed by the  [2.x.62]  function. Next, an instance of the  [2.x.63]  based on the finite element degree specified at the top of this file is set up.
* 

* 
* [1.x.93]
* 
*  Now loop over the time steps. In each iteration, we shift the solution vectors by one and call the `apply` function of the `SineGordonOperator` class. Then, we write the solution to a file. We clock the wall times for the computational time needed as wall as the time needed to create the output and report the numbers when the time stepping is finished.     
*   Note how this shift is implemented: We simply call the swap method on the two vectors which swaps only some pointers without the need to copy data around, a relatively expensive operation within an explicit time stepping method. Let us see what happens in more detail: First, we exchange  [2.x.64] , which means that  [2.x.65]  gets  [2.x.66] , which is what we expect. Similarly,  [2.x.67]  in the next step. After this,  [2.x.68]  holds  [2.x.69] , but that will be overwritten during this step.
* 

* 
* [1.x.94]
* 
*   [1.x.95]  [1.x.96]
* 

* 
*  As in  [2.x.70] , we initialize MPI at the start of the program. Since we will in general mix MPI parallelization with threads, we also set the third argument in MPI_InitFinalize that controls the number of threads to an invalid number, which means that the TBB library chooses the number of threads automatically, typically to the number of available cores in the system. As an alternative, you can also set this number manually if you want to set a specific number of threads (e.g. when MPI-only is required).
* 

* 
* [1.x.97]
* [1.x.98][1.x.99]
* 

* [1.x.100][1.x.101]
* 

* In order to demonstrate the gain in using the MatrixFree class instead ofthe standard  [2.x.71]  assembly routines for evaluating theinformation from old time steps, we study a simple serial run of the code on anonadaptive mesh. Since much time is spent on evaluating the sine function, wedo not only show the numbers of the full sine-Gordon equation but also for thewave equation (the sine-term skipped from the sine-Gordon equation). We useboth second and fourth order elements. The results are summarized in thefollowing table.
*  [2.x.72] 
* It is apparent that the matrix-free code outperforms the standard assemblyroutines in deal.II by far. In 3D and for fourth order elements, one operatorevaluation is also almost ten times as fast as a sparse matrix-vectorproduct.
* [1.x.102][1.x.103]
* 

* We start with the program output obtained on a workstation with 12 cores / 24threads (one Intel Xeon E5-2687W v4 CPU running at 3.2 GHz, hyperthreadingenabled), running the program in release mode:
* [1.x.104]
* 
* In 3D, the respective output looks like
* [1.x.105]
* 
* It takes 0.008 seconds for one time step with more than a milliondegrees of freedom (note that we would need many processors to reach suchnumbers when solving linear systems).
* If we replace the thread-parallelization by a pure MPI parallelization, thetimings change into:
* [1.x.106]
* 
* We observe a dramatic speedup for the output (which makes sense, given thatmost code of the output is not parallelized via threads, whereas it is forMPI), but less than the theoretical factor of 12 we would expect from theparallelism. More interestingly, the computations also get faster whenswitching from the threads-only variant to the MPI-only variant. This is ageneral observation for the MatrixFree framework (as of updating this data in2019). The main reason is that the decisions regarding work on conflictingcell batches made to enable execution in parallel are overly pessimistic:While they ensure that no work on neighboring cells is done on differentthreads at the same time, this conservative setting implies that data fromneighboring cells is also evicted from caches by the time neighbors gettouched. Furthermore, the current scheme is not able to provide a constantload for all 24 threads for the given mesh with 17,592 cells.
* The current program allows to also mix MPI parallelization with threadparallelization. This is most beneficial when running programs on clusterswith multiple nodes, using MPI for the inter-node parallelization and threadsfor the intra-node parallelization. On the workstation used above, we can runthreads in the hyperthreading region (i.e., using 2 threads for each of the 12MPI ranks). An important setting for mixing MPI with threads is to ensureproper binning of tasks to CPUs. On many clusters the placing is eitherautomatically via the `mpirun/mpiexec` environment, or there can be manualsettings. Here, we simply report the run times the plain version of theprogram (noting that things could be improved towards the timings of theMPI-only program when proper pinning is done):
* [1.x.107]
* 
* 

* 
* [1.x.108][1.x.109]
* 

* There are several things in this program that could be improved to make iteven more efficient (besides improved boundary conditions and physicalstuff as discussed in  [2.x.73] ):
*  [2.x.74]   [2.x.75]  [1.x.110] As becomes obvious  from the comparison of the plain wave equation and the sine-Gordon  equation above, the evaluation of the sine terms dominates the total  time for the finite element operator application. There are a few  reasons for this: Firstly, the deal.II sine computation of a  VectorizedArray field is not vectorized (as opposed to the rest of  the operator application). This could be cured by handing the sine  computation to a library with vectorized sine computations like  Intel's math kernel library (MKL). By using the function   [2.x.76]  in MKL, the program uses half the computing time  in 2D and 40 percent less time in 3D. On the other hand, the sine  computation is structurally much more complicated than the simple  arithmetic operations like additions and multiplications in the rest  of the local operation.
*    [2.x.77]  [1.x.111] While the implementation allows for  arbitrary order in the spatial part (by adjusting the degree of the finite  element), the time stepping scheme is a standard second-order leap-frog  scheme. Since solutions in wave propagation problems are usually very  smooth, the error is likely dominated by the time stepping part. Of course,  this could be cured by using smaller time steps (at a fixed spatial  resolution), but it would be more efficient to use higher order time  stepping as well. While it would be straight-forward to do so for a  first-order system (use some Runge&ndash;Kutta scheme of higher order,  probably combined with adaptive time step selection like the [1.x.112]), it is more challenging for the second-order formulation. At  least in the finite difference community, people usually use the PDE to find  spatial correction terms that improve the temporal error.
*  [2.x.78] 
* 

* [1.x.113][1.x.114] [2.x.79] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-49_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31]
* [1.x.32]
* [1.x.33][1.x.34][1.x.35]
* This tutorial is an extension to  [2.x.2]  and demonstrates several ways toobtain more involved meshes than the ones shown there.
*  [2.x.3]  This tutorial is also available as a Jupyter Python notebook that  uses the deal.II python interface. The notebook is available in the  same directory as the original C++ program.
* Generating complex geometries is a challenging task, especially in three spacedimensions. We will discuss several ways to do this, but this list is notexhaustive. Additionally, there is not one approach that fits all problems.
* This example program shows some of ways to create and modify meshes forcomputations and outputs them as  [2.x.4]  files in much the same wayas we do in  [2.x.5] . No other computations or adaptiverefinements are done; the idea is that you can use the techniques used here asbuilding blocks in other, more involved simulators. Please note that theexample program does not show all the ways to generate meshes that arediscussed in this introduction.
* 

* [1.x.36][1.x.37]
* 

* When you use adaptive mesh refinement, you definitely want the initial mesh tobe as coarse as possible. The reason is that you can make it as fine as youwant using adaptive refinement as long as you have memory and CPU timeavailable. However, this requires that you don't waste mesh cells in parts ofthe domain where they don't pay off. As a consequence, you don't want to startwith a mesh that is too fine to start with, because that takes up a good partof your cell budget already, and because you can't coarsen away cells that arein the initial mesh.
* That said, your mesh needs to capture the given geometry adequately.
* 

* [1.x.38][1.x.39]
* 

* There are several ways to create an initial mesh. Meshes can be modified orcombined in many ways as discussed later on.
* [1.x.40][1.x.41]
* 

* The easiest way to generate meshes is to use the functions in namespaceGridGenerator, as already discussed in  [2.x.6] .  There are many differenthelper functionsavailable, including  [2.x.7]  [2.x.8]   [2.x.9] and  [2.x.10] 
* 

* [1.x.42][1.x.43]
* 

* If there is no good fit in the GridGenerator namespace for what you want todo, you can always create aTriangulation in your program "by hand". For that, you need a list of verticeswith their coordinates and a list of cells referencing those vertices. You canfind an example in the function <tt>create_coarse_grid()</tt> in  [2.x.11] .All the functions in GridGenerator are implemented in this fashion.
* We are happy to accept more functions to be added to GridGenerator. So, ifyou end up writing a function that might be useful for a larger audience,please contribute it.
* 

* [1.x.44][1.x.45]
* 

* The class GridIn can read many different mesh formats from a file fromdisk. How this is done is explained in  [2.x.12]  and can be seen in the function [2.x.13]  in this example, see the code below.
* Meshes can be generated from different tools like [1.x.46], [1.x.47] and [1.x.48]. See thedocumentation of GridIn for more information. The problem is that deal.IIneeds meshes that only consist of quadrilaterals and hexahedra
* 
*  -  tetrahedralmeshes won't work (this means tools like tetgen can not be used directly).
* We will describe a possible workflow using %Gmsh. %Gmsh is the smallest andmost quickly set up open source tool we are aware of. It can generateunstructured 2d quad meshes. In 3d, it can extrude 2d meshes toget hexahedral meshes; 3D meshing of unstructured geometry into hexahedra ispossible, though there are some issues with the quality of these meshesthat imply that these meshes only sometimes work in deal.II.
* In %Gmsh, a mesh is fundamentally described in a text-based [2.x.14]  file whose format cancontain computations, loops, variables, etc. This format is quite flexiblein allowing the description of complex geometries. The mesh is thengenerated from a surface representation, which is built from a list of lineloops, which is built from a list of lines, which are in turn built frompoints. The  [2.x.15]  script can be written and edited by hand or itcan be generated automatically by creating objects graphically inside %Gmsh. Inmany cases it is best to combine both approaches. The file can be easilyreloaded by pressing "reload" under the "Geometry" tab if you want to writeit by hand and see the effects in the graphical user interface of gmsh.
* This tutorial contains an example  [2.x.16]  file that describes a boxwith two objects cut out in the interior. This is how [2.x.17]  looks like in %Gmsh (displaying the boundaryindicators as well as the mesh discussed further down below):
*  [2.x.18] 
* You might want to open the  [2.x.19]  file in a text editor (itis located in the same directory as the <tt> [2.x.20] .cc</tt> source file) tosee how it is structured. You can see how the boundary of the domain iscomposed of a number of lines and how later on we combine several lines into"physical lines" (or "physical surfaces") that list the logical lines'numbers. "Physical" object are the ones that carry information about theboundary indicator (see  [2.x.21]  "this glossary entry").
*  [2.x.22]  It is important that this file contain "physical lines" and "physical  surfaces". These give the boundary indicators and material ids for use  in deal.II. Without these physical entities, nothing will be imported into  deal.II.
* deal.II's GridIn class can read the  [2.x.23]  format written by%Gmsh and that contains a mesh created for the geometry described by the [2.x.24]  from the [2.x.25]  by running the commands
* [1.x.49]
* 
* on the command line, or by clicking "Mesh" and then "2D" inside %Gmsh afterloading the file.  Now this is the mesh read from the  [2.x.26]  fileand saved again by deal.II as an image (see the  [2.x.27]  functionof the current program):
*  [2.x.28] 
*  [2.x.29]  %Gmsh has a number of other interfaces by which one can describe  geometries to it. In particular, it has the ability to interface with  scripting languages like Python and Julia, but it can also be scripted  from C++. These interfaces are useful if one doesn't just want to generate  a mesh for a single geometry (in which case the graphical interface or,  in simple cases, a hand-written `.geo` file is probably the simplest  approach), but instead wants to do parametric studies over the geometry  for which it is necessary to generate many meshes for geometries that  differ in certain parameters. Another case where this is useful is if there  is already a CAD geometry for which one only needs a mesh; indeed, this  can be done from within deal.II using the   [2.x.30]  function.
* 

* [1.x.50][1.x.51]
* 

* After acquiring one (or several) meshes in the ways described above, there aremany ways to manipulate them before using them in a finite elementcomputation.
* 

* [1.x.52][1.x.53]
* 

* The GridTools namespace contains a collection of small functions to transforma given mesh in various ways. The usage of the functions  [2.x.31]  [2.x.32]   [2.x.33]  is fairly obvious, so we won't discussthose functions here.
* The function  [2.x.34]  allows you to transform the vertices of agiven mesh using a smooth function. An example of its use is also given in theresults section of  [2.x.35]  but let us show a simpler example here:In the function  [2.x.36]  of the current program, we perturb the ycoordinate of a mesh with a sine curve:
*  [2.x.37] 
* Similarly, we can transform a regularly refinedunit square to a wall-adapted mesh in y direction using the formula [2.x.38] . This is done in  [2.x.39] of this tutorial: [2.x.40] 
* Finally, the function  [2.x.41]  allows you to move vertices in themesh (optionally ignoring boundary nodes) by a random amount. This isdemonstrated in  [2.x.42]  and the result is as follows:
*  [2.x.43] 
* This function is primarily intended to negate some of the superconvergenceeffects one gets when studying convergence on regular meshes, as well as tosuppress some optimizations in deal.II that can exploit the fact that cellsare similar in shape. (Superconvergence refers to the fact that if a meshhas certain symmetries
* 
*  -  for example, if the edges running into a vertexare symmetric to this vertex, and if this is so for all vertices of a cell
* 
*  -  that the solution is then often convergent with a higher order than onewould have expected from the usual error analysis. In the end, thisis a result of the fact that if one were to make a Taylor expansion of theerror, the symmetry leads to the fact that the expected next term of theexpansion happens to be zero, and the error order is determined by the
*second next* term. A distorted mesh does not have these symmetries andconsequently the error reflects what one will see when solving the equationonany* kind of mesh, rather than showing something that is only reflectiveof a particular situation.)
* 

* [1.x.54][1.x.55]
* 

* The function  [2.x.44]  allows you to merge twogiven Triangulation objects into a single one.  For this to work, the verticesof the shared edge or face have to match exactly.  Lining up the two meshescan be achieved using  [2.x.45]  and  [2.x.46]   In the function [2.x.47]  of this tutorial, we merge a square with a round hole(generated with  [2.x.48]  and arectangle (generated with  [2.x.49]  Thefunction  [2.x.50]  allows you to specify thenumber of repetitions and the positions of the corners, so there is no need toshift the triangulation manually here. You should inspect the mesh graphicallyto make sure that cells line up correctly and no unpaired nodes exist in themerged Triangulation.
* These are the input meshes and the output mesh:
*  [2.x.51] 
* 

* [1.x.56][1.x.57]
* 

* The function  [2.x.52]  demonstrates the ability to pick individual vertices andmove them around in an existing mesh. Note that this has the potential to produce degenerateor inverted cells and you shouldn't expect anything useful to come of usingsuch meshes. Here, we create a box with a cylindrical hole that is not exactlycentered by moving the top vertices upwards:
*  [2.x.53] 
* For the exact way how this is done, see the code below.
* 

* [1.x.58][1.x.59]
* 

* If you need a 3d mesh that can be created by extruding a given 2d mesh (thatcan be created in any of the ways given above), you can use the function [2.x.54]  See the  [2.x.55]  functionin this tutorial for an example. Note that for this particular case, the givenresult could also be achieved using the 3d version of [2.x.56]  The main usage is a 2dmesh, generated for example with %Gmsh, that is read in from a [2.x.57]  file as described above. This is the output from grid_4():
*  [2.x.58] 
* 

* [1.x.60][1.x.61]
* 

* Creating a coarse mesh using the methods discussed above is only the firststep. When you have it, it will typically serve as the basis for further meshrefinement. This is not difficult &mdash; in fact, there is nothing else to do&mdash; if your geometry consists of only straight faces. However, this isoften not the case if you have a more complex geometry and more steps thanjust creating the mesh are necessary. We will go over some of these steps inthe [1.x.62] below.
* 

*  [1.x.63] [1.x.64]
*  This tutorial program is odd in the sense that, unlike for most other steps, the introduction already provides most of the information on how to use the various strategies to generate meshes. Consequently, there is little that remains to be commented on here, and we intersperse the code with relatively little text. In essence, the code here simply provides a reference implementation of what has already been described in the introduction.
* 

* 
*   [1.x.65]  [1.x.66]
* 

* 
*  

* 
* [1.x.67]
* 
*   [1.x.68]  [1.x.69]
* 

* 
*  The following function generates some output for any of the meshes we will be generating in the remainder of this program. In particular, it generates the following information:
* 

* 
* 

* 
* 
*  - Some general information about the number of space dimensions in which this mesh lives and its number of cells.
* 

* 
* 
*  - The number of boundary faces that use each boundary indicator, so that it can be compared with what we expect.
* 

* 
*  Finally, the function outputs the mesh in VTU format that can easily be visualized in Paraview or VisIt.
* 

* 
* [1.x.70]
* 
*  Next loop over all faces of all cells and find how often each boundary indicator is used (recall that if you access an element of a  [2.x.59]  object that doesn't exist, it is implicitly created and default initialized
* 
*  -  to zero, in the current case
* 
*  -  before we then increment it):
* 

* 
* [1.x.71]
* 
*  Finally, produce a graphical representation of the mesh to an output file:
* 

* 
* [1.x.72]
* 
*   [1.x.73]  [1.x.74]
* 

* 
*   [1.x.75]  [1.x.76]
* 

* 
*  In this first example, we show how to load the mesh for which we have discussed in the introduction how to generate it. This follows the same pattern as used in  [2.x.60]  to load a mesh, although there it was written in a different file format (UCD instead of MSH).
* 

* 
* [1.x.77]
* 
*   [1.x.78]  [1.x.79]
* 

* 
*  Here, we first create two triangulations and then merge them into one.  As discussed in the introduction, it is important to ensure that the vertices at the common interface are located at the same coordinates.
* 

* 
* [1.x.80]
* 
*   [1.x.81]  [1.x.82]
* 

* 
*  In this function, we move vertices of a mesh. This is simpler than one usually expects: if you ask a cell using  [2.x.61]  for the coordinates of its  [2.x.62] th vertex, it doesn't just provide the location of this vertex but in fact a reference to the location where these coordinates are stored. We can then modify the value stored there.
* 

* 
*  So this is what we do in the first part of this function: We create a square of geometry  [2.x.63]  with a circular hole with radius 0.25 located at the origin. We then loop over all cells and all vertices and if a vertex has a  [2.x.64]  coordinate equal to one, we move it upward by 0.5.
* 

* 
*  Note that this sort of procedure does not usually work this way because one will typically encounter the same vertices multiple times and may move them more than once. It works here because we select the vertices we want to use based on their geometric location, and a vertex moved once will fail this test in the future. A more general approach to this problem would have been to keep a  [2.x.65]  of those vertex indices that we have already moved (which we can obtain using  [2.x.66]  and only move those vertices whose index isn't in the set yet.
* 

* 
* [1.x.83]
* 
*  In the second step we will refine the mesh twice. To do this correctly, we should place new points on the interior boundary along the surface of a circle centered at the origin. Fortunately,  [2.x.67]  already attaches a Manifold object to the interior boundary, so we do not need to do anything but refine the mesh (see the [1.x.84] for a fully worked example where we  [2.x.68] do [2.x.69]  attach a Manifold object).
* 

* 
* [1.x.85]
* 
*  There is one snag to doing things as shown above: If one moves the nodes on the boundary as shown here, one often ends up with cells in the interior that are badly distorted since the interior nodes were not moved around. This is not that much of a problem in the current case since the mesh did not contain any internal nodes when the nodes were moved
* 
*  -  it was the coarse mesh and it so happened that all vertices are at the boundary. It's also the case that the movement we had here was, compared to the average cell size not overly dramatic. Nevertheless, sometimes one does want to move vertices by a significant distance, and in that case one needs to move internal nodes as well. One way to do that automatically is to call the function  [2.x.70]  that takes a set of transformed vertex coordinates and moves all of the other vertices in such a way that the resulting mesh has, in some sense, a small distortion.
* 

* 
*  
*  
* 

* 
*   [1.x.86]  [1.x.87]
* 

* 
*  This example takes the initial grid from the previous function and simply extrudes it into the third space dimension:
* 

* 
* [1.x.88]
* 
*   [1.x.89]  [1.x.90]
* 

* 
*  This and the next example first create a mesh and then transform it by moving every node of the mesh according to a function that takes a point and returns a mapped point. In this case, we transform  [2.x.71] .
* 

* 
*   [2.x.72]  takes a triangulation and an argument that can be called like a function taking a Point and returning a Point. There are different ways of providing such an argument: It could be a pointer to a function; it could be an object of a class that has an `operator()`; it could be a lambda function; or it could be anything that is described via a  [2.x.73]  object.
* 

* 
*  Decidedly the more modern way is to use a lambda function that takes a Point and returns a Point, and that is what we do in the following:
* 

* 
* [1.x.91]
* 
*   [1.x.92]  [1.x.93]
* 

* 
*  In this second example of transforming points from an original to a new mesh, we will use the mapping  [2.x.74] . To make things more interesting, rather than doing so in a single function as in the previous example, we here create an object with an  [2.x.75]  that will be called by  [2.x.76]  Of course, this object may in reality be much more complex: the object may have member variables that play a role in computing the new locations of vertices.
* 

* 
* [1.x.94]
* 
*   [1.x.95]  [1.x.96]
* 

* 
*  In this last example, we create a mesh and then distort its (interior) vertices by a random perturbation. This is not something you want to do for production computations (because results are generally better on meshes with "nicely shaped" cells than on the deformed cells produced by  [2.x.77]  but it is a useful tool for testing discretizations and codes to make sure they don't work just by accident because the mesh happens to be uniformly structured and supporting superconvergence properties.
* 

* 
* [1.x.97]
* 
*   [1.x.98]  [1.x.99]
* 

* 
*  Finally, the main function. There isn't much to do here, only to call all the various functions we wrote above.
* 

* 
* [1.x.100]
* [1.x.101][1.x.102]
* 

* The program produces a series of  [2.x.78]  files of thetriangulations. The methods are discussed above.
* 

* [1.x.103][1.x.104]
* 

* As mentioned in the introduction, creating a coarse mesh using the methodsdiscussed here is only the first step. In order to refine a mesh, theTriangulation needs to know where to put new vertices on the mid-points ofedges, faces, and cells. By default, these new points will be placed at thearithmetic mean of the surrounding points, but this isn't what you want if youneed curved boundaries that aren't already adequately resolved by the coarsemesh. For example, for this mesh the central hole is supposed to be round:
*  [2.x.79] 
* If you simply refine it, the Triangulation class can not know whether you wantedthe hole to be round or to be an octagon. The default is to place new pointsalong existing straight lines. After two mesh refinement steps, this would yieldthe following mesh, which is not what we wanted:
*  [2.x.80] 
* What needs to happen is that you tell the triangulation that you in fact wantto use a curved geometry. The way to do this requires three steps:
* 
*  - Create an object that describes the desired geometry. This object will be  queried when refining the Triangulation for new point placement. It will also  be used to calculate shape function values if a high degree mapping, like  MappingQ or MappingQGeneric, is used during system assembly.  In deal.II the Manifold class and classes inheriting from it (e.g.,  PolarManifold and FlatManifold) perform these calculations.
* 
*  - Notify the Triangulation object which Manifold classes to use. By default, a  Triangulation uses FlatManifold to do all geometric calculations,  which assumes that all cell edges are straight lines and all quadrilaterals  are flat. You can attach Manifold classes to a Triangulation by calling   [2.x.81]  function, which associates a   [2.x.82]  with a Manifold object. For more information on this  see the  [2.x.83]  "glossary entry on this topic".
* 
*  - Finally, you must mark cells and cell faces with the correct   [2.x.84] . For example, you could get an annular sector with  curved cells in Cartesian coordinates (but rectangles in polar coordinates)  by doing the following: 
* [1.x.105]
*   Now, when the grid is refined, all cell splitting calculations will be done in  polar coordinates.
* All functions in the GridGenerator namespace which create a mesh where somecells should be curved also attach the correct Manifold object to the providedTriangulation: i.e., for those functions we get the correct behavior bydefault. For a hand-generated mesh, however, the situation is much moreinteresting.
* To illustrate this process in more detail, let us consider an example createdby Yuhan Zhou as part of a 2013 semester project at Texas A&amp;M University.The goal was to generate (and use) a geometry that describes amicrostructured electric device. In a CAD program, the geometry looks likethis:
*  [2.x.85] 
* In the following, we will walk you through the entire process of creating amesh for this geometry, including a number of common pitfalls by showing thethings that can go wrong.
* The first step in getting there was to create a coarse mesh, which was done bycreating a 2d coarse mesh for each of cross sections, extruding them into thethird direction, and gluing them together. The following code does this, usingthe techniques previously described:
* [1.x.106]
* 
* This creates the following mesh:
* <img src="https://www.dealii.org/images/steps/developer/ [2.x.86] .yuhan.8.png"     alt="" width="400" height="355">
* This mesh has the right general shape, but the top cells are now polygonal: theiredges are no longer along circles and we do not have a very accuraterepresentation of the original geometry. The next step is to teach the top partof the domain that it should be curved. Put another way, all calculations doneon the top boundary cells should be done in cylindrical coordinates rather thanCartesian coordinates. We can do this by creating a CylindricalManifold objectand associating it with the cells above  [2.x.87] . This way, when we refine thecells on top, we will place new points along concentric circles instead ofstraight lines.
* In deal.II we describe all geometries with classes that inherit fromManifold. The default geometry is Cartesian and is implemented in theFlatManifold class. As the name suggests, Manifold and its inheriting classesprovide a way to describe curves and curved cells in a general way with ideasand terminology from differential geometry: for example, CylindricalManifoldinherits from ChartManifold, which describes a geometry through pull backsand push forwards. In general, one should think that the Triangulation classdescribes the topology of a domain (in addition, of course, to storing thelocations of the vertices) while the Manifold classes describe the geometry of adomain (e.g., whether or not a pair of vertices lie along a circular arc or astraight line). A Triangulation will refine cells by doing computations with theManifold associated with that cell regardless of whether or not the cell is onthe boundary. Put another way: the Manifold classes do not need any informationabout where the boundary of the Triangulation actually is: it is up to theTriangulation to query the right Manifold for calculations on a cell. MostManifold functions (e.g.,  [2.x.88]  know nothing aboutthe domain itself and just assume that the points given to it lie along ageodesic. In this case, with the CylindricalManifold constructed below, thegeodesics are arcs along circles orthogonal to the  [2.x.89] -axis centered along theline  [2.x.90] .
* Since all three top parts of the domain use the same geodesics, we willmark all cells with centers above the  [2.x.91]  line as being cylindrical innature:
* [1.x.107]
* 
* With this code, we get a mesh that looks like this:
* <img src="https://www.dealii.org/images/steps/developer/ [2.x.92] .yuhan.9.png"     alt="" width="400" height="355">
* This change fixes the boundary but creates a new problem: the cells adjacent tothe cylinder's axis are badly distorted. We should use Cartesian coordinates forcalculations on these central cells to avoid this issue. The cells along thecenter line all have a face that touches the line  [2.x.93]  so, to implementthis, we go back and overwrite the  [2.x.94] s on these cells tobe zero (which is the default):
* [1.x.108]
* 
* This gives us the following grid:
* <img src="https://www.dealii.org/images/steps/developer/ [2.x.95] .yuhan.10.png"     alt="" width="400" height="355">
* This gives us a good mesh, where cells at the center of each circle are stillCartesian and cells around the boundary lie along a circle. We can really seethe nice detail of the boundary fitted mesh if we refine two more times:
* <img src="https://www.dealii.org/images/steps/developer/ [2.x.96] .yuhan.11.png"     alt="" width="400" height="355">
* 

* [1.x.109][1.x.110]
* 

* [1.x.111][1.x.112]
* 

* It is often useful to assign different boundary ids to a mesh that isgenerated in one form or another as described in this tutorial to applydifferent boundary conditions.
* For example, you might want to apply a different boundary condition for theright boundary of the first grid in this program. To do this, iterate over thecells and their faces and identify the correct faces (for example using`cell->center()` to query the coordinates of the center of a cell as wedo in  [2.x.97] , or using `cell->face(f)->get_boundary_id()` to query the currentboundary indicator of the  [2.x.98] th face of the cell). You can then use`cell->face(f)->set_boundary_id()` to set the boundary id to something different.You can take a look back at  [2.x.99]  how iteration over the meshes is done there.
* [1.x.113][1.x.114]
* 

* Computations on manifolds, like they are done in  [2.x.100] , require a surfacemesh embedded into a higher dimensional space. While some can be constructedusing the GridGenerator namespace or loaded from a file, it is sometimesuseful to extract a surface mesh from a volume mesh.
* Use the function  [2.x.101]  to extract the surfaceelements of a mesh. Using the function on a 3d mesh (a `Triangulation<3,3>`, forexample from `grid_4()`), this will return a `Triangulation<2,3>` that you can usein  [2.x.102] .  Also try extracting the boundary mesh of a `Triangulation<2,2>`.
* 

* <!--
* Possible Extensions for this tutorial:
* 
*  - Database of unstructured meshes for convergence studies
* 
*  - how to remove or disable a cell inside a mesh
* 
*  - >
* 

* [1.x.115][1.x.116] [2.x.103] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-50_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] ,  [2.x.2] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27]
*  [2.x.3] 
* [1.x.28]
*  [2.x.4] 
*  [2.x.5]  As a prerequisite of this program, you need to have both p4est and either the PETScor Trilinos library installed. The installation of deal.II together with these additionallibraries is described in the [1.x.29] file.
* 

* [1.x.30][1.x.31][1.x.32]
* 

* 
* This example shows the usage of the multilevel functions in deal.II onparallel, distributedmeshes and gives a comparison between geometric and algebraic multigrid methods.The algebraic multigrid (AMG) preconditioner is the same used in  [2.x.6] . Two geometricmultigrid (GMG) preconditioners are considered: a matrix-based version similar to thatin  [2.x.7]  (but for parallel computations) and a matrix-free versiondiscussed in  [2.x.8] . The goal is to find out which approach leads tothe best solver for large parallel computations.
* This tutorial is based on one of the numerical examples in [2.x.9] . Please see that publication for a detailed backgroundon the multigrid implementation in deal.II. We will summarize some of theresults in the following text.
* Algebraic multigrid methods are obviously the easiest to implementwith deal.II since classes such as  [2.x.10] and  [2.x.11]  are, in essence, black boxpreconditioners that require only a couple of lines to set up even forparallel computations. On the other hand, geometric multigrid methodsrequire changes throughout a code base
* 
*  -  not very many, but one hasto know what one is doing.
* What the results of this program will showis that algebraic and geometric multigrid methods are roughlycomparable in performance [1.x.33],and that matrix-free geometric multigrid methods are vastly better forthe problem under consideration here. A secondary conclusion will bethat matrix-based geometric multigrid methods really don't scale wellstrongly when the number of unknowns per processor becomes smaller than20,000 or so.
* 

* [1.x.34][1.x.35]
* 

* We consider the variable-coefficient Laplacian weak formulation
* [1.x.36]
* on the domain  [2.x.12]  (an L-shaped domainfor 2D and a Fichera corner for 3D) with  [2.x.13]  if  [2.x.14]  and [2.x.15]  otherwise. In other words,  [2.x.16]  is small along the edgesor faces of the domain that run into the reentrant corner, as will be visiblein the figure below.
* The boundary conditions are  [2.x.17]  on the whole boundary andthe right-hand side is  [2.x.18] . We use continuous  [2.x.19]  elements for thediscrete finite element space  [2.x.20] , and use aresidual-based, cell-wise a posteriori error estimator [2.x.21]  from  [2.x.22]  with
* [1.x.37]
* to adaptively refine the mesh. (This is a generalization of the Kellyerror estimator used in the KellyErrorEstimator class that drives meshrefinement in most of the other tutorial programs.)The following figure visualizes the solution and refinement for 2D: [2.x.23] In 3D, the solution looks similar (see below). On the left you can see the solution and on the right we show a slice for  [2.x.24]  close to thecenter of the domain showing the adaptively refined mesh. [2.x.25] Both in 2D and 3D you can see the adaptive refinement picking up the corner singularity and the inner singularity where the viscosity jumps, while the interface along the line that separates the two viscosities is (correctly) not refined as it is resolved adequately.This is because the kink in the solution that results from the jumpin the coefficient is aligned with cell interfaces.
* 

* [1.x.38][1.x.39]
* 

* As mentioned above, the purpose of this program is to demonstrate theuse of algebraic and geometric multigrid methods for this problem, andto do so for parallel computations. An important component of makingalgorithms scale to large parallel machines is ensuring that everyprocessor has the same amount of work to do. (More precisely, whatmatters is that there are no small fraction of processors that havesubstantially more work than the rest since, if that were so, a largefraction of processors will sit idle waiting for the small fraction tofinish. Conversely, a small fraction of processors havingsubstantially [1.x.40] work is not a problem because the majorityof processors continues to be productive and only the small fractionsits idle once finished with their work.)
* For the active mesh, we use the  [2.x.26]  class as donein  [2.x.27]  which uses functionality in the external library[1.x.41] for the distribution of the active cellsamong processors. For the non-active cells in the multilevel hierarchy, deal.IIimplements what we will refer to as the "first-child rule" where, for each cellin the hierarchy, we recursively assign the parent of a cell to the owner of thefirst child cell. The following figures give an example of such a distribution. Herethe left image represents the active cells for a sample 2D mesh partitioned using aspace-filling curve (which is what p4est uses to partition cells);the center image gives the tree representationof the active mesh; and the right image gives the multilevel hierarchy of cells. Thecolors and numbers represent the different processors. The circular nodes in the treeare the non-active cells which are distributed using the "first-child rule".
*  [2.x.28] 
* Included among the output to screen in this example is a value "Partition efficiency"given by one over  [2.x.29]  This value, which will be denotedby  [2.x.30] ,  quantifies the overhead produced by not having a perfect work balanceon each level of the multigrid hierarchy. This imbalance is evident from theexample above: while level  [2.x.31]  is about as well balanced as is possiblewith four cells among three processors, the coarselevel  [2.x.32]  has work for only one processor, and level  [2.x.33]  has workfor only two processors of which one has three times as much work asthe other.
* For defining  [2.x.34] , it is important to note that, as we are using local smoothingto define the multigrid hierarchy (see the  [2.x.35]  "multigrid paper" for a description oflocal smoothing), the refinement level of a cell corresponds to that cell's multigridlevel. Now, let  [2.x.36]  be the number of cells on level  [2.x.37] (both active and non-active cells) and  [2.x.38]  be the subset owned by process [2.x.39] . We will also denote by  [2.x.40]  the total number of processors.Assuming that the workload for any one processor is proportional to the numberof cells owned by that processor, the optimal workload per processor is given by
* [1.x.42]
* Next, assuming a synchronization of work on each level (i.e., on each level of a V-cycle,work must be completed by all processors before moving on to the next level), thelimiting effort on each level is given by
* [1.x.43]
* and the total parallel complexity
* [1.x.44]
* Then we define  [2.x.41]  as a ratio of the optimal partition to the parallelcomplexity of the current partition
* [1.x.45]
* For the example distribution above, we have
* [1.x.46]
* The value  [2.x.42]  1/\mathbb{E} [2.x.43] \mathbb{E} [2.x.44] \mathbb{E} \approx 1 [2.x.45] \mathbb{E} [2.x.46] \mathbb{E} [2.x.47] 1/\mathbb{E} [2.x.48] W_\ell [2.x.49] W [2.x.50] r_0 = f-Au_0 [2.x.51] u_0 [2.x.52] u = u_0 + A^{-1}r_0 [2.x.53] u_0 [2.x.54] A^{-1}r_0 [2.x.55] u_0 [2.x.56] A [2.x.57] u_0 [2.x.58] f [2.x.59] u_0 [2.x.60] h^2 \| f + \epsilon \triangle u \|_K^2 [2.x.61] \sum_F h_F \| \jump{\epsilon \nabla u \cdot n} \|_F^2 [2.x.62] \mathbb{E} [2.x.63] 0.371/0.161=2.3 [2.x.64] \mathbb{E} [2.x.65] {\cal O}(N\log N) [2.x.66] {\cal O}(N) [2.x.67] 9\times 9 [2.x.68] 27\times 27 [2.x.69] L [2.x.70] 21\times 21 [2.x.71] 117\times 117$ in 3d. But if the coarse meshconsists of hundreds or thousands of cells, this approach will nolonger work and might start to dominate the overall run-time of each V-cyle.A common approach is then to solve the coarse mesh problem using analgebraic multigrid preconditioner; this would then, however, requireassembling the coarse matrix (even for the matrix-free version) asinput to the AMG implementation.
* 

* [1.x.134][1.x.135] [2.x.72] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-5_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16]
* [1.x.17][1.x.18][1.x.19]
* 

*  [2.x.2] 
* This example does not show revolutionary new things, but it shows manysmall improvements over the previous examples, and also many smallthings that can usually be found in finite element programs. Amongthem are: [2.x.3]    [2.x.4]  Computations on successively refined grids. At least in the       mathematical sciences, it is common to compute solutions on       a hierarchy of grids, in order to get a feeling for the accuracy       of the solution; if you only have one solution on a single grid, you       usually can't guess the accuracy of the       solution. Furthermore, deal.II is designed to support adaptive       algorithms where iterative solution on successively refined       grids is at the heart of algorithms. Although adaptive grids       are not used in this example, the foundations for them is laid       here.   [2.x.5]  In practical applications, the domains are often subdivided       into triangulations by automatic mesh generators. In order to       use them, it is important to read coarse grids from a file. In       this example, we will read a coarse grid in UCD (unstructured       cell data) format. When this program was first written around       2000, UCD format was what the AVS Explorer used
* 
*  -  a program       reasonably widely used at the time but now no longer of       importance. (Nonetheless, the file format has survived and is       still understood by a number of programs.)   [2.x.6]  Finite element programs usually use extensive amounts of       computing time, so some optimizations are sometimes       necessary. We will show some of them.   [2.x.7]  On the other hand, finite element programs tend to be rather       complex, so debugging is an important aspect. We support safe       programming by using assertions that check the validity of       parameters and %internal states in a debug mode, but are removed       in optimized mode. ( [2.x.8]    [2.x.9]  Regarding the mathematical side, we show how to support a       variable coefficient in the elliptic operator and how to use       preconditioned iterative solvers for the linear systems of       equations. [2.x.10] 
* The equation to solve here is as follows:
* [1.x.20]
* If  [2.x.11]  was a constant coefficient, this would simply be the Poissonequation. However, if it is indeed spatially variable, it is a more complexequation (often referred to as the "extended Poisson equation"). Depending onwhat the variable  [2.x.12]  refers to it models a variety of situations with wideapplicability:
* 
*  - If  [2.x.13]  is the electric potential, then  [2.x.14]  is the electric current  in a medium and the coefficient  [2.x.15]  is the conductivity of the medium at any  given point. (In this situation, the right hand side of the equation would  be the electric source density and would usually be zero or consist of  localized, Delta-like, functions.)
* 
*  - If  [2.x.16]  is the vertical deflection of a thin membrane, then  [2.x.17]  would be a  measure of the local stiffness. This is the interpretation that will allow  us to interpret the images shown in the results section below.
* Since the Laplace/Poisson equation appears in so many contexts, there are manymore interpretations than just the two listed above.
* When assembling the linear system for this equation, we need the weak formwhich here reads as follows:
* [1.x.21]
* The implementation in the  [2.x.18]  function followsimmediately from this.
* 

*  [1.x.22] [1.x.23]
*   [1.x.24]  [1.x.25]
* 

* 
*  Again, the first few include files are already known, so we won't comment on them:
* 

* 
* [1.x.26]
* 
*  This one is new. We want to read a triangulation from disk, and the class which does this is declared in the following file:
* 

* 
* [1.x.27]
* 
*  We will use a circular domain, and the object describing the boundary of it comes from this file:
* 

* 
* [1.x.28]
* 
*  This is C++ ...
* 

* 
* [1.x.29]
* 
*  Finally, this has been discussed in previous tutorial programs before:
* 

* 
* [1.x.30]
* 
*   [1.x.31]  [1.x.32]
* 

* 
*  The main class is mostly as in the previous example. The most visible change is that the function  [2.x.19]  has been removed, since creating the grid is now done in the  [2.x.20]  function and the rest of its functionality is now in  [2.x.21] . Apart from this, everything is as before.
* 

* 
* [1.x.33]
* 
*   [1.x.34]  [1.x.35]
* 

* 
*  In  [2.x.22] , we showed how to use non-constant boundary values and right hand side.  In this example, we want to use a variable coefficient in the elliptic operator instead. Since we have a function which just depends on the point in space we can do things a bit more simply and use a plain function instead of inheriting from Function.
* 

* 
*  This is the implementation of the coefficient function for a single point. We let it return 20 if the distance to the origin is less than 0.5, and 1 otherwise.
* 

* 
* [1.x.36]
* 
*   [1.x.37]  [1.x.38]
* 

* 
*   [1.x.39]  [1.x.40]
* 

* 
*  This function is as before.
* 

* 
* [1.x.41]
* 
*   [1.x.42]  [1.x.43]
* 

* 
*  This is the function  [2.x.23]  from the previous example, minus the generation of the grid. Everything else is unchanged:
* 

* 
* [1.x.44]
* 
*   [1.x.45]  [1.x.46]
* 

* 
*  As in the previous examples, this function is not changed much with regard to its functionality, but there are still some optimizations which we will show. For this, it is important to note that if efficient solvers are used (such as the preconditioned CG method), assembling the matrix and right hand side can take a comparable time, and you should think about using one or two optimizations at some places.
* 

* 
*  The first parts of the function are completely unchanged from before:
* 

* 
* [1.x.47]
* 
*  Next is the typical loop over all cells to compute local contributions and then to transfer them into the global matrix and vector. The only change in this part, compared to  [2.x.24] , is that we will use the  [2.x.25]  function defined above to compute the coefficient value at each quadrature point.
* 

* 
* [1.x.48]
* 
*  With the matrix so built, we use zero boundary values again:
* 

* 
* [1.x.49]
* 
*   [1.x.50]  [1.x.51]
* 

* 
*  The solution process again looks mostly like in the previous examples. However, we will now use a preconditioned conjugate gradient algorithm. It is not very difficult to make this change. In fact, the only thing we have to alter is that we need an object which will act as a preconditioner. We will use SSOR (symmetric successive overrelaxation), with a relaxation factor of 1.2. For this purpose, the  [2.x.26]  class has a function which does one SSOR step, and we need to package the address of this function together with the matrix on which it should act (which is the matrix to be inverted) and the relaxation factor into one object. The  [2.x.27]  class does this for us. ( [2.x.28]  class takes a template argument denoting the matrix type it is supposed to work on. The default value is  [2.x.29] , which is exactly what we need here, so we simply stick with the default and do not specify anything in the angle brackets.)
* 

* 
*  Note that for the present case, SSOR doesn't really perform much better than most other preconditioners (though better than no preconditioning at all). A brief comparison of different preconditioners is presented in the Results section of the next tutorial program,  [2.x.30] .
* 

* 
*  With this, the rest of the function is trivial: instead of the  [2.x.31]  object we have created before, we now use the preconditioner we have declared, and the CG solver will do the rest for us:
* 

* 
* [1.x.52]
* 
*   [1.x.53]  [1.x.54]
* 

* 
*  Writing output to a file is mostly the same as for the previous tutorial. The only difference is that we now need to construct a different filename for each refinement cycle.
* 

* 
*  The function writes the output in VTU format, a variation of the VTK format that requires less disk space because it compresses the data. Of course, there are many other formats supported by the DataOut class if you desire to use a program for visualization that doesn't understand VTK or VTU.
* 

* 
* [1.x.55]
* 
*   [1.x.56]  [1.x.57]
* 

* 
*  The second to last thing in this program is the definition of the  [2.x.32]  function. In contrast to the previous programs, we will compute on a sequence of meshes that after each iteration is globally refined. The function therefore consists of a loop over 6 cycles. In each cycle, we first print the cycle number, and then have to decide what to do with the mesh. If this is not the first cycle, we simply refine the existing mesh once globally. Before running through these cycles, however, we have to generate a mesh:
* 

* 
*  In previous examples, we have already used some of the functions from the  [2.x.33]  class. Here we would like to read a grid from a file where the cells are stored and which may originate from someone else, or may be the product of a mesh generator tool.
* 

* 
*  In order to read a grid from a file, we generate an object of data type GridIn and associate the triangulation to it (i.e. we tell it to fill our triangulation object when we ask it to read the file). Then we open the respective file and initialize the triangulation with the data in the file:
* 

* 
* [1.x.58]
* 
*  We would now like to read the file. However, the input file is only for a two-dimensional triangulation, while this function is a template for arbitrary dimension. Since this is only a demonstration program, we will not use different input files for the different dimensions, but rather quickly kill the whole program if we are not in 2D. Of course, since the main function below assumes that we are working in two dimensions we could skip this check, in this version of the program, without any ill effects.   
*   It turns out that more than 90 per cent of programming errors are invalid function parameters such as invalid array sizes, etc, so we use assertions heavily throughout deal.II to catch such mistakes. For this, the  [2.x.34]  macro is a good choice, since it makes sure that the condition which is given as first argument is valid, and if not throws an exception (its second argument) which will usually terminate the program giving information where the error occurred and what the reason was. (A longer discussion of what exactly the  [2.x.35]  macro does can be found in the  [2.x.36]  "exception documentation module".) This generally reduces the time to find programming errors dramatically and we have found assertions an invaluable means to program fast.   
*   On the other hand, all these checks (there are over 10,000 of them in the library at present) should not slow down the program too much if you want to do large computations. To this end, the  [2.x.37]  macro is only used in debug mode and expands to nothing if in optimized mode. Therefore, while you test your program on small problems and debug it, the assertions will tell you where the problems are. Once your program is stable, you can switch off debugging and the program will run your real computations without the assertions and at maximum speed. More precisely: turning off all the checks in the library (which prevent you from calling functions with wrong arguments, walking off of arrays, etc.) by compiling your program in optimized mode usually makes things run about four times faster. Even though optimized programs are more performant, we still recommend developing in debug mode since it allows the library to find lots of common programming errors automatically. For those who want to try: The way to switch from debug mode to optimized mode is to recompile your program with the command <code>make release</code>. The output of the  [2.x.38]  program should now indicate to you that the program is now compiled in optimized mode, and it will later also be linked to libraries that have been compiled for optimized mode. In order to switch back to debug mode, simply recompile with the command  [2.x.39] .
* 

* 
* [1.x.59]
* 
*  ExcInternalError is a globally defined exception, which may be thrown whenever something is terribly wrong. Usually, one would like to use more specific exceptions, and particular in this case one would of course try to do something else if  [2.x.40]  is not equal to two, e.g. create a grid using library functions. Aborting a program is usually not a good idea and assertions should really only be used for exceptional cases which should not occur, but might due to stupidity of the programmer, user, or someone else. The situation above is not a very clever use of Assert, but again: this is a tutorial and it might be worth to show what not to do, after all.
* 

* 
*  So if we got past the assertion, we know that dim==2, and we can now actually read the grid. It is in UCD (unstructured cell data) format (though the convention is to use the suffix  [2.x.41]  for UCD files):
* 

* 
* [1.x.60]
* 
*  If you like to use another input format, you have to use one of the other  [2.x.42]  function. (See the documentation of the  [2.x.43]  class to find out what input formats are presently supported.)
* 

* 
*  The grid in the file describes a circle. Therefore we have to use a manifold object which tells the triangulation where to put new points on the boundary when the grid is refined. Unlike  [2.x.44] , since GridIn does not know that the domain has a circular boundary (unlike  [2.x.45]  we have to explicitly attach a manifold to the boundary after creating the triangulation to get the correct result when we refine the mesh.
* 

* 
* [1.x.61]
* 
*  Now that we have a mesh for sure, we write some output and do all the things that we have already seen in the previous examples.
* 

* 
* [1.x.62]
* 
*   [1.x.63]  [1.x.64]
* 

* 
*  The main function looks mostly like the one in the previous example, so we won't comment on it further:
* 

* 
* [1.x.65]
* [1.x.66][1.x.67]
* 

* 
* Here is the console output:
* [1.x.68]
* 
* 

* 
* In each cycle, the number of cells quadruples and the number of CGiterations roughly doubles.Also, in each cycle, the program writes one output graphic file in VTUformat. They are depicted in the following:
*  [2.x.46] 
* 

* 
* Due to the variable coefficient (the curvature there is reduced by thesame factor by which the coefficient is increased), the top region ofthe solution is flattened. The gradient of the solution isdiscontinuous along the interface, although this is not very clearlyvisible in the pictures above. We will look at this in more detail inthe next example.
* The pictures also show that the solution computed by this program isactually pretty wrong on a very coarse mesh (its magnitude iswrong). That's because no numerical method guarantees that the solutionon a coarse mesh is particularly accurate
* 
*  -  but we know that thesolution [1.x.69] to the exact solution, and indeed you cansee how the solutions from one mesh to the next seem to not changevery much any more at the end.
* 

* [1.x.70][1.x.71] [2.x.47] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-51_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] ,  [2.x.2] ,  [2.x.3] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34][1.x.35][1.x.36][1.x.37][1.x.38]
*  [2.x.4] 
* [1.x.39]
* [1.x.40][1.x.41][1.x.42]
* 

* This tutorial program presents the implementation of a hybridizablediscontinuous Galkerin method for the convection-diffusion equation.
* [1.x.43][1.x.44]
* 

* One common argument against the use of discontinuous Galerkin elementsis the large number of globally coupled degrees of freedom that onemust solve in an implicit system.  This is because, unlike continuous finiteelements, in typical discontinuous elements there is one degree of freedom ateach vertex [1.x.45], rather than just one,and similarly for edges and faces.  As an example of how fast the number ofunknowns grows, consider the FE_DGPMonomial basis: eachscalar solution component is represented by polynomials of degree  [2.x.5] with  [2.x.6]  degrees of freedom perelement. Typically, all degrees of freedom in an element are coupledto all of the degrees of freedom in the adjacent elements.  The resultingdiscrete equations yield very large linear systems very quickly, especiallyfor systems of equations in 2 or 3 dimensions.
* [1.x.46][1.x.47]
* To alleviate the computational cost of solving such large linear systems,the hybridizable discontinuous Galerkin (HDG) methodology was introducedby Cockburn and co-workers (see the references in the recent HDG overviewarticle by Nguyen and Peraire  [2.x.7] ).
* The HDG method achieves this goal by formulating the mathematical problem usingDirichlet-to-Neumann mappings.  The partial differential equations are firstwritten as a first order system, and each field is then discretized via a DGmethod.  At this point, the single-valued "trace" values on the skeleton of themesh, i.e., element faces, are taken to be independent unknown quantities.This yields unknowns in the discrete formulation that fall into two categories:
* 
*  - Face unknowns that only couple with the cell unknowns from both sides of the face;
* 
*  - Cell unknowns that only couple with the cell and face unknowns  defined within the same cell. Crucially, no cell interior degree of freedom  on one cell ever couples to any interior cell degree of freedom of a  different cell.
* The Dirichlet-to-Neumann map concept then permits the following solution procedure: [2.x.8]    [2.x.9]   Use local element interior data to enforce a Neumann condition on theskeleton of the triangulation.  The global problem is then to solve for thetrace values, which are the only globally coupled unknowns.   [2.x.10]   Use the known skeleton values as Dirichlet data for solving localelement-level solutions.  This is known as the 'local solver', and is an[1.x.48] element-by-element solution process. [2.x.11] 
* [1.x.49][1.x.50]
* The above procedure also has a linear algebra interpretation---referred toas [1.x.51]---that was exploited to reduce the size of theglobal linear system by Guyan in the context of continuous Finite Elements [2.x.12] , and by Fraeijs de Veubeke for mixed methods  [2.x.13] . In thelatter case (mixed formulation), the system reduction was achieved through theuse of discontinuous fluxes combined with the introduction of an additionalauxiliary [1.x.52] variable that approximates the trace of the unknownat the boundary of every element. This procedure became known as hybridizationand---by analogy---is the reason why the local discontinuous Galerkin methodintroduced by Cockburn, Gopalakrishnan, and Lazarov in 2009  [2.x.14] , andsubsequently developed by their collaborators, eventually came to be known asthe [1.x.53] (HDG) method.
* Let us write the complete linear system associated to the HDG problem as ablock system with the discrete DG (cell interior) variables  [2.x.15]  as first blockand the skeleton (face) variables  [2.x.16]  as the second block:[1.x.54]
* Our aim is now to eliminate the  [2.x.17]  block with a Schur complementapproach similar to  [2.x.18] , which results in the following two steps:[1.x.55]
* The point is that the presence of  [2.x.19]  is not a problem because  [2.x.20]  is ablock diagonal matrix where each block corresponds to one cell and istherefore easy enough to invert.The coupling to other cells is introduced by the matrices [2.x.21]  and  [2.x.22]  over the skeleton variable. The block-diagonality of [2.x.23]  and the structure in  [2.x.24]  and  [2.x.25]  allow us to invert thematrix  [2.x.26]  element by element (the local solution of the Dirichletproblem) and subtract  [2.x.27]  from  [2.x.28] . The steps in the Dirichlet-to-Neumannmap concept hence correspond to [2.x.29]    [2.x.30]  constructing the Schur complement matrix  [2.x.31]  and right hand    side  [2.x.32]   [1.x.56]    and inserting the contribution into the global trace matrix in the usual way,   [2.x.33]  solving the Schur complement system for  [2.x.34] , and   [2.x.35]  solving for  [2.x.36]  using the second equation, given  [2.x.37] . [2.x.38] 
* 

* [1.x.57][1.x.58]
* Another criticism of traditional DG methods is that the approximate fluxesconverge suboptimally.  The local HDG solutions can be shown to convergeas  [2.x.39] , i.e., at optimal order.  Additionally, asuper-convergence property can be used to post-process a new approximatesolution that converges at the rate  [2.x.40] .
* 

* [1.x.59][1.x.60]
* 

* The hybridizable discontinuous Galerkin method is only one way inwhich the problems of the discontinuous Galerkin method can beaddressed. Another idea is what is called the "weak Galerkin"method. It is explored in  [2.x.41] .
* 

* [1.x.61][1.x.62]
* 

* The HDG formulation used for this example is taken from [2.x.42] [1.x.63][1.x.64][1.x.65]
* We consider the convection-diffusion equation over the domain  [2.x.43] with Dirichlet boundary  [2.x.44]  and Neumann boundary [2.x.45] :[1.x.66]
* 
* Introduce the auxiliary variable  [2.x.46]  and rewritethe above equation as the first order system:[1.x.67]
* 
* We multiply these equations by the weight functions  [2.x.47] and integrate by parts over every element  [2.x.48]  to obtain:[1.x.68]
* 
* The terms decorated with a hat denote the numerical traces (also commonly referredto as numerical fluxes).  They are approximationsto the interior values on the boundary of the element.  To ensure conservation,these terms must be single-valued on any given element edge  [2.x.49]  eventhough, with discontinuous shape functions, there may of course be multiplevalues coming from the cells adjacent to an interface.We eliminate the numerical trace  [2.x.50]  by using traces of the form:[1.x.69]
* 
* The variable  [2.x.51]  is introduced as an additional independent variableand is the one for which we finally set up a globally coupled linearsystem. As mentioned above, it is defined on the element faces anddiscontinuous from one face to another wherever faces meet (atvertices in 2d, and at edges and vertices in 3d).Values for  [2.x.52]  and  [2.x.53]  appearing in the numerical trace functionare taken to be the cell's interior solution restrictedto the boundary  [2.x.54] .
* The local stabilization parameter  [2.x.55]  has effects on stability and accuracyof HDG solutions; see the literature for a further discussion. A stabilizationparameter of unity is reported to be the choice which gives best results. Astabilization parameter  [2.x.56]  that tends to infinity prohibits jumps in thesolution over the element boundaries, making the HDG solution approach theapproximation with continuous finite elements. In the program below, we choosethe stabilization parameter as[1.x.70]
* where we set the diffusion  [2.x.57]  and the diffusion length scale to [2.x.58] .
* The trace/skeleton variables in HDG methods are single-valued on elementfaces.  As such, they must strongly represent the Dirichlet data on [2.x.59] .  This means that[1.x.71]
* where the equal sign actually means an  [2.x.60]  projection of the boundaryfunction  [2.x.61]  onto the space of the face variables (e.g. linear functions onthe faces). This constraint is then applied to the skeleton variable  [2.x.62] using inhomogeneous constraints by the method [2.x.63] 
* Summing the elementalcontributions across all elements in the triangulation, enforcing the normalcomponent of the numerical flux, and integrating by partson the equation weighted by  [2.x.64] , we arrive at the final form of the problem:Find  [2.x.65]  such that
* [1.x.72]
* 
* The unknowns  [2.x.66]  are referred to as local variables; they arerepresented as standard DG variables.  The unknown  [2.x.67]  is the skeletonvariable which has support on the codimension-1 surfaces (faces) of the mesh.
* We use the notation  [2.x.68] to denote the sum of integrals over all cells and  [2.x.69]  to denote integration over all faces of all cells,i.e., interior faces are visited twice, once from each side and withthe corresponding normal vectors. When combining the contribution fromboth elements sharing a face, the above equation yields terms familiarfrom the DG method, with jumps of the solution over the cell boundaries.
* In the equation above, the space  [2.x.70]  for the scalar variable [2.x.71]  is defined as the space of functions that are tensorproduct polynomials of degree  [2.x.72]  on each cell and discontinuous over theelement boundaries  [2.x.73] , i.e., the space described by [2.x.74] . The space for the gradient or flux variable [2.x.75]  is a vector element space where each component isa locally polynomial and discontinuous  [2.x.76] . In the code below,we collect these two local parts together in one FESystem where the first  [2.x.77] dim components denote the gradient part and the last scalar componentcorresponds to the scalar variable. For the skeleton component  [2.x.78] , wedefine a space that consists of discontinuous tensor product polynomials thatlive on the element faces, which in deal.II is implemented by the classFE_FaceQ. This space is otherwise similar to FE_DGQ, i.e., the solutionfunction is not continuous between two neighboring faces, see also the resultssection below for an illustration.
* In the weak form given above, we can note the following coupling patterns: [2.x.79]    [2.x.80]  The matrix  [2.x.81]  consists of local-local coupling terms.  These arise when the  local weighting functions  [2.x.82]  multiply the local solution terms   [2.x.83] . Because the elements are discontinuous,  [2.x.84]   is block diagonal.   [2.x.85]  The matrix  [2.x.86]  represents the local-face coupling.  These are the terms  with weighting functions  [2.x.87]  multiplying the skeleton variable   [2.x.88] .   [2.x.89]  The matrix  [2.x.90]  represents the face-local coupling, which involves the  weighting function  [2.x.91]  multiplying the local solutions  [2.x.92] .   [2.x.93]   The matrix  [2.x.94]  is the face-face coupling;  terms involve both  [2.x.95]  and  [2.x.96] . [2.x.97] 
* [1.x.73][1.x.74]
* 

* One special feature of the HDG methods is that they typically allow forconstructing an enriched solution that gains accuracy. This post-processingtakes the HDG solution in an element-by-element fashion and combines it suchthat one can get  [2.x.98]  order of accuracy when usingpolynomials of degree  [2.x.99] . For this to happen, there are two necessaryingredients: [2.x.100]    [2.x.101]  The computed solution gradient  [2.x.102]  converges at optimal rate,   i.e.,  [2.x.103] .   [2.x.104]  The cell-wise average of the scalar part of the solution,    [2.x.105] , super-converges at rate    [2.x.106] . [2.x.107] 
* We now introduce a new variable  [2.x.108] , which we findby minimizing the expression  [2.x.109]  over the cell [2.x.110]  under the constraint  [2.x.111] . The constraint is necessary because the minimizationfunctional does not determine the constant part of  [2.x.112] . Thistranslates to the following system of equations:[1.x.75]
* 
* Since we test by the whole set of basis functions in the space of tensorproduct polynomials of degree  [2.x.113]  in the second set of equations, thisis an overdetermined system with one more equation than unknowns. We fix thisin the code below by omitting one of these equations (since the rows in theLaplacian are linearly dependent when representing a constant function). As wewill see below, this form of the post-processing gives the desiredsuper-convergence result with rate  [2.x.114] .  It should benoted that there is some freedom in constructing  [2.x.115]  and this minimizationapproach to extract the information from the gradient is not the only one. Inparticular, the post-processed solution defined here does not satisfy theconvection-diffusion equation in any sense. As an alternative, the paper byNguyen, Peraire and Cockburn cited above suggests another somewhat moreinvolved formula for convection-diffusion that can also post-process the fluxvariable into an  [2.x.116] -conforming variant and betterrepresents the local convection-diffusion operator when the diffusion issmall. We leave the implementation of a more sophisticated post-processing asa possible extension to the interested reader.
* Note that for vector-valued problems, the post-processing works similarly. Onesimply sets the constraint for the mean value of each vector componentseparately and uses the gradient as the main source of information.
* [1.x.76][1.x.77]
* 

* For this tutorial program, we consider almost the same test case as in [2.x.117] . The computational domain is  [2.x.118]  and the exactsolution corresponds to the one in  [2.x.119] , except for a scaling. We use thefollowing source centers  [2.x.120]  for the exponentials [2.x.121]    [2.x.122]  1D:   [2.x.123] ,   [2.x.124]  2D:  [2.x.125] ,   [2.x.126]  3D:  [2.x.127] . [2.x.128] 
* With the exact solution given, we then choose the forcing on the right handside and the Neumann boundary condition such that we obtain this solution(manufactured solution technique). In this example, we choose the diffusionequal to one and the convection as[1.x.78]Note that the convection is divergence-free,  [2.x.129] .
* [1.x.79][1.x.80]
* 

* Besides implementing the above equations, the implementation below providesthe following features: [2.x.130]    [2.x.131]  WorkStream to parallelize local solvers. Workstream has been presented  in detail in  [2.x.132] .   [2.x.133]  Reconstruct the local DG solution from the trace.   [2.x.134]  Post-processing the solution for superconvergence.   [2.x.135]  DataOutFaces for direct output of the global skeleton solution. [2.x.136] 
* 

*  [1.x.81] [1.x.82]
*   [1.x.83]  [1.x.84]
* 

* 
*  Most of the deal.II include files have already been covered in previous examples and are not commented on.
* 

* 
* [1.x.85]
* 
*  However, we do have a few new includes for the example. The first one defines finite element spaces on the faces of the triangulation, which we refer to as the 'skeleton'. These finite elements do not have any support on the element interior, and they represent polynomials that have a single value on each codimension-1 surface, but admit discontinuities on codimension-2 surfaces.
* 

* 
* [1.x.86]
* 
*  The second new file we include defines a new type of sparse matrix.  The regular  [2.x.137]  type stores indices to all non-zero entries.  The  [2.x.138]  takes advantage of the coupled nature of DG solutions.  It stores an index to a matrix sub-block of a specified size.  In the HDG context, this sub-block-size is actually the number of degrees of freedom per face defined by the skeleton solution field. This reduces the memory consumption of the matrix by up to one third and results in similar speedups when using the matrix in solvers.
* 

* 
* [1.x.87]
* 
*  The final new include for this example deals with data output.  Since we have a finite element field defined on the skeleton of the mesh, we would like to visualize what that solution actually is. DataOutFaces does exactly this; the interface is the almost the same as the familiar DataOut, but the output only has codimension-1 data for the simulation.
* 

* 
* [1.x.88]
* 
*  We start by putting all of our classes into their own namespace.
* 

* 
* [1.x.89]
* 
*   [1.x.90]  [1.x.91]   
*   The structure of the analytic solution is the same as in  [2.x.139] . There are two exceptions. Firstly, we also create a solution for the 3d case, and secondly, we scale the solution so its norm is of order unity for all values of the solution width.
* 

* 
* [1.x.92]
* 
*  This class implements a function where the scalar solution and its negative gradient are collected together. This function is used when computing the error of the HDG approximation and its implementation is to simply call value and gradient function of the Solution class.
* 

* 
* [1.x.93]
* 
*  Next comes the implementation of the convection velocity. As described in the introduction, we choose a velocity field that is  [2.x.140]  in 2D and  [2.x.141]  in 3D. This gives a divergence-free velocity field.
* 

* 
* [1.x.94]
* 
*  The last function we implement is the right hand side for the manufactured solution. It is very similar to  [2.x.142] , with the exception that we now have a convection term instead of the reaction term. Since the velocity field is incompressible, i.e.,  [2.x.143] , the advection term simply reads  [2.x.144] .
* 

* 
* [1.x.95]
* 
*   [1.x.96]  [1.x.97]
* 

* 
*  The HDG solution procedure follows closely that of  [2.x.145] . The major difference is the use of three different sets of DoFHandler and FE objects, along with the ChunkSparseMatrix and the corresponding solutions vectors. We also use WorkStream to enable a multithreaded local solution process which exploits the embarrassingly parallel nature of the local solver. For WorkStream, we define the local operations on a cell and a copy function into the global matrix and vector. We do this both for the assembly (which is run twice, once when we generate the system matrix and once when we compute the element-interior solutions from the skeleton values) and for the postprocessing where we extract a solution that converges at higher order.
* 

* 
* [1.x.98]
* 
*  Data for the assembly and solution of the primal variables.
* 

* 
* [1.x.99]
* 
*  Post-processing the solution to obtain  [2.x.146]  is an element-by-element procedure; as such, we do not need to assemble any global data and do not declare any 'task data' for WorkStream to use.
* 

* 
* [1.x.100]
* 
*  The following three functions are used by WorkStream to do the actual work of the program.
* 

* 
* [1.x.101]
* 
*  The 'local' solutions are interior to each element.  These represent the primal solution field  [2.x.147]  as well as the auxiliary field  [2.x.148] .
* 

* 
* [1.x.102]
* 
*  The new finite element type and corresponding  [2.x.149]  are used for the global skeleton solution that couples the element-level local solutions.
* 

* 
* [1.x.103]
* 
*  As stated in the introduction, HDG solutions can be post-processed to attain superconvergence rates of  [2.x.150] .  The post-processed solution is a discontinuous finite element solution representing the primal variable on the interior of each cell.  We define a FE type of degree  [2.x.151]  to represent this post-processed solution, which we only use for output after constructing it.
* 

* 
* [1.x.104]
* 
*  The degrees of freedom corresponding to the skeleton strongly enforce Dirichlet boundary conditions, just as in a continuous Galerkin finite element method. We can enforce the boundary conditions in an analogous manner via an AffineConstraints object. In addition, hanging nodes are handled in the same way as for continuous finite elements: For the face elements which only define degrees of freedom on the face, this process sets the solution on the refined side to coincide with the representation on the coarse side.     
*   Note that for HDG, the elimination of hanging nodes is not the only possibility &mdash; in terms of the HDG theory, one could also use the unknowns from the refined side and express the local solution on the coarse side through the trace values on the refined side. However, such a setup is not as easily implemented in terms of deal.II loops and not further analyzed.
* 

* 
* [1.x.105]
* 
*  The usage of the ChunkSparseMatrix class is similar to the usual sparse matrices: You need a sparsity pattern of type ChunkSparsityPattern and the actual matrix object. When creating the sparsity pattern, we just have to additionally pass the size of local blocks.
* 

* 
* [1.x.106]
* 
*  Same as  [2.x.152] :
* 

* 
* [1.x.107]
* 
*   [1.x.108]  [1.x.109]
* 

* 
*   [1.x.110]  [1.x.111] The constructor is similar to those in other examples, with the exception of handling multiple DoFHandler and FiniteElement objects. Note that we create a system of finite elements for the local DG part, including the gradient/flux part and the scalar part.
* 

* 
* [1.x.112]
* 
*   [1.x.113]  [1.x.114] The system for an HDG solution is setup in an analogous manner to most of the other tutorial programs.  We are careful to distribute dofs with all of our DoFHandler objects.  The  [2.x.153]  and  [2.x.154]  objects go with the global skeleton solution.
* 

* 
* [1.x.115]
* 
*  When creating the chunk sparsity pattern, we first create the usual dynamic sparsity pattern and then set the chunk size, which is equal to the number of dofs on a face, when copying this into the final sparsity pattern.
* 

* 
* [1.x.116]
* 
*   [1.x.117]  [1.x.118] Next comes the definition of the local data structures for the parallel assembly. The first structure  [2.x.155]  contains the local vector and matrix that are written into the global matrix, whereas the ScratchData contains all data that we need for the local assembly. There is one variable worth noting here, namely the boolean variable  [2.x.156]  trace_reconstruct. As mentioned in the introduction, we solve the HDG system in two steps. First, we create a linear system for the skeleton system where we condense the local part into it via the Schur complement  [2.x.157] . Then, we solve for the local part using the skeleton solution. For these two steps, we need the same matrices on the elements twice, which we want to compute by two assembly steps. Since most of the code is similar, we do this with the same function but only switch between the two based on a flag that we set when starting the assembly. Since we need to pass this information on to the local worker routines, we store it once in the task data.
* 

* 
* [1.x.119]
* 
*   [1.x.120]  [1.x.121]  [2.x.158]  contains persistent data for each thread within WorkStream.  The FEValues, matrix, and vector objects should be familiar by now.  There are two objects that need to be discussed:  [2.x.159]  int> > fe_local_support_on_face` and  [2.x.160]  int> > fe_support_on_face`.  These are used to indicate whether or not the finite elements chosen have support (non-zero values) on a given face of the reference cell for the local part associated to  [2.x.161]  and the skeleton part  [2.x.162]  We extract this information in the constructor and store it once for all cells that we work on.  Had we not stored this information, we would be forced to assemble a large number of zero terms on each cell, which would significantly slow the program.
* 

* 
* [1.x.122]
* 
*   [1.x.123]  [1.x.124]  [2.x.163]  contains the data used by WorkStream when post-processing the local solution  [2.x.164] .  It is similar, but much simpler, than  [2.x.165] 
* 

* 
* [1.x.125]
* 
*   [1.x.126]  [1.x.127] The  [2.x.166]  function is similar to the one on  [2.x.167] , where the quadrature formula and the update flags are set up, and then  [2.x.168]  is used to do the work in a multi-threaded manner.  The  [2.x.169]  input parameter is used to decide whether we are solving for the global skeleton solution (false) or the local solution (true).   
*   One thing worth noting for the multi-threaded execution of assembly is the fact that the local computations in `assemble_system_one_cell()` call into BLAS and LAPACK functions if those are available in deal.II. Thus, the underlying BLAS/LAPACK library must support calls from multiple threads at the same time. Most implementations do support this, but some libraries need to be built in a specific way to avoid problems. For example, OpenBLAS compiled without multithreading inside the BLAS/LAPACK calls needs to built with a flag called `USE_LOCKING` set to true.
* 

* 
* [1.x.128]
* 
*   [1.x.129]  [1.x.130] The real work of the HDG program is done by  [2.x.170]  Assembling the local matrices  [2.x.171]  is done here, along with the local contributions of the global matrix  [2.x.172] .
* 

* 
* [1.x.131]
* 
*  Construct iterator for dof_handler_local for FEValues reinit function.
* 

* 
* [1.x.132]
* 
*  We first compute the cell-interior contribution to  [2.x.173]  matrix (referred to as matrix  [2.x.174]  in the introduction) corresponding to local-local coupling, as well as the local right-hand-side vector.  We store the values at each quadrature point for the basis functions, the right-hand-side value, and the convection velocity, in order to have quick access to these fields.
* 

* 
* [1.x.133]
* 
*  Face terms are assembled on all faces of all elements. This is in contrast to more traditional DG methods, where each face is only visited once in the assembly procedure.
* 

* 
* [1.x.134]
* 
*  The already obtained  [2.x.175]  values are needed when solving for the local variables.
* 

* 
* [1.x.135]
* 
*  Here we compute the stabilization parameter discussed in the introduction: since the diffusion is one and the diffusion length scale is set to 1/5, it simply results in a contribution of 5 for the diffusion part and the magnitude of convection through the element boundary in a centered scheme for the convection part.
* 

* 
* [1.x.136]
* 
*  We store the non-zero flux and scalar values, making use of the support_on_face information we created in  [2.x.176] 
* 

* 
* [1.x.137]
* 
*  When  [2.x.177]  we are preparing to assemble the system for the skeleton variable  [2.x.178] . If this is the case, we must assemble all local matrices associated with the problem: local-local, local-face, face-local, and face-face.  The face-face matrix is stored as  [2.x.179]  so that it can be assembled into the global system by  [2.x.180]  copy_local_to_global.
* 

* 
* [1.x.138]
* 
*  Note the sign of the face_no-local matrix.  We negate the sign during assembly here so that we can use the  [2.x.181]  with addition when computing the Schur complement.
* 

* 
* [1.x.139]
* 
*  This last term adds the contribution of the term  [2.x.182]  to the local matrix. As opposed to the face matrices above, we need it in both assembly stages.
* 

* 
* [1.x.140]
* 
*  When  [2.x.183]  we are solving for the local solutions on an element by element basis.  The local right-hand-side is calculated by replacing the basis functions  [2.x.184]  tr_phi in the  [2.x.185]  computation by the computed values  [2.x.186]  trace_values.  Of course, the sign of the matrix is now minus since we have moved everything to the other side of the equation.
* 

* 
* [1.x.141]
* 
*  Once assembly of all of the local contributions is complete, we must either: (1) assemble the global system, or (2) compute the local solution values and save them. In either case, the first step is to invert the local-local matrix.
* 

* 
* [1.x.142]
* 
*  For (1), we compute the Schur complement and add it to the  [2.x.187]  cell_matrix, matrix  [2.x.188]  in the introduction.
* 

* 
* [1.x.143]
* 
*  For (2), we are simply solving (ll_matrix).(solution_local) = (l_rhs). Hence, we multiply  [2.x.189]  by our already inverted local-local matrix and store the result using the  [2.x.190]  function.
* 

* 
* [1.x.144]
* 
*   [1.x.145]  [1.x.146] If we are in the first step of the solution, i.e.  [2.x.191]  then we assemble the local matrices into the global system.
* 

* 
* [1.x.147]
* 
*   [1.x.148]  [1.x.149] The skeleton solution is solved for by using a BiCGStab solver with identity preconditioner.
* 

* 
* [1.x.150]
* 
*  Once we have solved for the skeleton solution, we can solve for the local solutions in an element-by-element fashion.  We do this by re-using the same  [2.x.192]  function but switching  [2.x.193]  to true.
* 

* 
* [1.x.151]
* 
*   [1.x.152]  [1.x.153]
* 

* 
*  The postprocess method serves two purposes. First, we want to construct a post-processed scalar variables in the element space of degree  [2.x.194]  that we hope will converge at order  [2.x.195] . This is again an element-by-element process and only involves the scalar solution as well as the gradient on the local cell. To do this, we introduce the already defined scratch data together with some update flags and run the work stream to do this in parallel.   
*   Secondly, we want to compute discretization errors just as we did in  [2.x.196] . The overall procedure is similar with calls to  [2.x.197]  The difference is in how we compute the errors for the scalar variable and the gradient variable. In  [2.x.198] , we did this by computing  [2.x.199]  or  [2.x.200]  contributions. Here, we have a DoFHandler with these two contributions computed and sorted by their vector component,  [2.x.201]  for the gradient and  [2.x.202]  for the scalar. To compute their value, we hence use a ComponentSelectFunction with either of them, together with the  [2.x.203]  SolutionAndGradient class introduced above that contains the analytic parts of either of them. Eventually, we also compute the L2-error of the post-processed solution and add the results into the convergence table.
* 

* 
* [1.x.154]
* 
*   [1.x.155]  [1.x.156]   
*   This is the actual work done for the postprocessing. According to the discussion in the introduction, we need to set up a system that projects the gradient part of the DG solution onto the gradient of the post-processed variable. Moreover, we need to set the average of the new post-processed variable to equal the average of the scalar DG solution on the cell.   
*   More technically speaking, the projection of the gradient is a system that would potentially fills our  [2.x.204]  times  [2.x.205]  matrix but is singular (the sum of all rows would be zero because the constant function has zero gradient). Therefore, we take one row away and use it for imposing the average of the scalar value. We pick the first row for the scalar part, even though we could pick any row for  [2.x.206]  elements. However, had we used FE_DGP elements instead, the first row would correspond to the constant part already and deleting e.g. the last row would give us a singular system. This way, our program can also be used for those elements.
* 

* 
* [1.x.157]
* 
*  Having assembled all terms, we can again go on and solve the linear system. We invert the matrix and then multiply the inverse by the right hand side. An alternative (and more numerically stable) method would have been to only factorize the matrix and apply the factorization.
* 

* 
* [1.x.158]
* 
*   [1.x.159]  [1.x.160] We have 3 sets of results that we would like to output:  the local solution, the post-processed local solution, and the skeleton solution. The former 2 both 'live' on element volumes, whereas the latter lives on codimension-1 surfaces of the triangulation.  Our  [2.x.207]  function writes all local solutions to the same vtk file, even though they correspond to different DoFHandler objects.  The graphical output for the skeleton variable is done through use of the DataOutFaces class.
* 

* 
* [1.x.161]
* 
*  We first define the names and types of the local solution, and add the data to  [2.x.208] 
* 

* 
* [1.x.162]
* 
*  The second data item we add is the post-processed solution. In this case, it is a single scalar variable belonging to a different DoFHandler.
* 

* 
* [1.x.163]
* 
*  The  [2.x.209]  class works analogously to the  [2.x.210]  that defines the solution on the skeleton of the triangulation.  We treat it as such here, and the code is similar to that above.
* 

* 
* [1.x.164]
* 
*   [1.x.165]  [1.x.166]
* 

* 
*  We implement two different refinement cases for HDG, just as in  [2.x.211] : adaptive_refinement and global_refinement.  The global_refinement option recreates the entire triangulation every time. This is because we want to use a finer sequence of meshes than what we would get with one refinement step, namely 2, 3, 4, 6, 8, 12, 16, ... elements per direction.
* 

* 
*  The adaptive_refinement mode uses the  [2.x.212]  to give a decent indication of the non-regular regions in the scalar local solutions.
* 

* 
* [1.x.167]
* 
*  Just as in  [2.x.213] , we set the boundary indicator of two of the faces to 1 where we want to specify Neumann boundary conditions instead of Dirichlet conditions. Since we re-create the triangulation every time for global refinement, the flags are set in every refinement step, not just at the beginning.
* 

* 
* [1.x.168]
* 
*   [1.x.169]  [1.x.170] The functionality here is basically the same as  [2.x.214] . We loop over 10 cycles, refining the grid on each one.  At the end, convergence tables are created.
* 

* 
* [1.x.171]
* 
*  There is one minor change for the convergence table compared to  [2.x.215] : Since we did not refine our mesh by a factor two in each cycle (but rather used the sequence 2, 3, 4, 6, 8, 12, ...), we need to tell the convergence rate evaluation about this. We do this by setting the number of cells as a reference column and additionally specifying the dimension of the problem, which gives the necessary information for the relation between number of cells and mesh size.
* 

* 
* [1.x.172]
* 
*  Now for the three calls to the main class in complete analogy to  [2.x.216] .
* 

* 
* [1.x.173]
* [1.x.174][1.x.175]
* 

* [1.x.176][1.x.177]
* 

* We first have a look at the output generated by the program when run in 2D. Inthe four images below, we show the solution for polynomial degree  [2.x.217] and cycles 2, 3, 4, and 8 of the program. In the plots, we overlay the datagenerated from the internal data (DG part) with the skeleton part ( [2.x.218] )into the same plot. We had to generate two different data sets because cellsand faces represent different geometric entities, the combination of which (inthe same file) is not supported in the VTK output of deal.II.
* The images show the distinctive features of HDG: The cell solution (coloredsurfaces) is discontinuous between the cells. The solution on the skeletonvariable sits on the faces and ties together the local parts. The skeletonsolution is not continuous on the vertices where the faces meet, even thoughits values are quite close along lines in the same coordinate direction. Theskeleton solution can be interpreted as a rubber spring between the two sidesthat balances the jumps in the solution (or rather, the flux  [2.x.219] ). From the picture at the top left, it is clear thatthe bulk solution frequently over- and undershoots and that theskeleton variable in indeed a better approximation to the exactsolution; this explains why we can get a better solution using apostprocessing step.
* As the mesh is refined, the jumps between the cells getsmall (we represent a smooth solution), and the skeleton solution approachesthe interior parts. For cycle 8, there is no visible difference in the twovariables. We also see how boundary conditions are implemented weakly and thatthe interior variables do not exactly satisfy boundary conditions. On thelower and left boundaries, we set Neumann boundary conditions, whereas we setDirichlet conditions on the right and top boundaries.
*  [2.x.220] 
* Next, we have a look at the post-processed solution, again at cycles 2, 3, 4,and 8. This is a discontinuous solution that is locally described by secondorder polynomials. While the solution does not look very good on the mesh ofcycle two, it looks much better for cycles three and four. As shown by theconvergence table below, we find that is also converges more quickly to theanalytical solution.
*  [2.x.221] 
* Finally, we look at the solution for  [2.x.222]  at cycle 2. Despite the coarsemesh with only 64 cells, the post-processed solution is similar in qualityto the linear solution (not post-processed) at cycle 8 with 4,096cells. This clearly shows the superiority of high order methods for smoothsolutions.
*  [2.x.223] 
* [1.x.178][1.x.179]
* 

* When the program is run, it also outputs information about the respectivesteps and convergence tables with errors in the various components in theend. In 2D, the convergence tables look the following:
* [1.x.180]
* 
* 

* One can see the error reduction upon grid refinement, and for the cases whereglobal refinement was performed, also the convergence rates. The quadraticconvergence rates of Q1 elements in the  [2.x.224]  norm for both the scalarvariable and the gradient variable is apparent, as is the cubic rate for thepostprocessed scalar variable in the  [2.x.225]  norm. Note this distinctivefeature of an HDG solution. In typical continuous finite elements, thegradient of the solution of order  [2.x.226]  converges at rate  [2.x.227]  only, asopposed to  [2.x.228]  for the actual solution. Even though superconvergenceresults for finite elements are also available (e.g. superconvergent patchrecovery first introduced by Zienkiewicz and Zhu), these are typically limitedto structured meshes and other special cases. For Q3 HDG variables, the scalarvariable and gradient converge at fourth order and the postprocessed scalarvariable at fifth order.
* The same convergence rates are observed in 3d.
* [1.x.181]
* 
* [1.x.182][1.x.183]
* 

* [1.x.184][1.x.185]
* 

* The convergence tables verify the expected convergence rates stated in theintroduction. Now, we want to show a quick comparison of the computationalefficiency of the HDG method compared to a usual finite element (continuousGalkerin) method on the problem of this tutorial. Of course, stability aspectsof the HDG method compared to continuous finite elements fortransport-dominated problems are also important in practice, which is anaspect not seen on a problem with smooth analytic solution. In the picturebelow, we compare the  [2.x.229]  error as a function of the number of degrees offreedom (left) and of the computing time spent in the linear solver (right)for two space dimensions of continuous finite elements (CG) and the hybridizeddiscontinuous Galerkin method presented in this tutorial. As opposed to thetutorial where we only use unpreconditioned BiCGStab, the times shown in thefigures below use the Trilinos algebraic multigrid preconditioner in [2.x.230]  For the HDG part, a wrapper aroundChunkSparseMatrix for the trace variable has been used in order to utilize theblock structure in the matrix on the finest level.
*  [2.x.231] 
* The results in the graphs show that the HDG method is slower than continuousfinite elements at  [2.x.232] , about equally fast for cubic elements andfaster for sixth order elements. However, we have seen above that the HDGmethod actually produces solutions which are more accurate than what isrepresented in the original variables. Therefore, in the next two plots belowwe instead display the error of the post-processed solution for HDG (denotedby  [2.x.233]  for example). We now see a clear advantage of HDG for the sameamount of work for both  [2.x.234]  and  [2.x.235] , and about the same qualityfor  [2.x.236] .
*  [2.x.237] 
* Since the HDG method actually produces results converging as [2.x.238] , we should compare it to a continuous Galerkinsolution with the same asymptotic convergence behavior, i.e., FE_Q with degree [2.x.239] . If we do this, we get the convergence curves below. We see thatCG with second order polynomials is again clearly better than HDG withlinears. However, the advantage of HDG for higher orders remains.
*  [2.x.240] 
* The results are in line with properties of DG methods in general: Bestperformance is typically not achieved for linear elements, but rather atsomewhat higher order, usually around  [2.x.241] . This is because of avolume-to-surface effect for discontinuous solutions with too much of thesolution living on the surfaces and hence duplicating work when the elementsare linear. Put in other words, DG methods are often most efficient when usedat relatively high order, despite their focus on a discontinuous (and hence,seemingly low accurate) representation of solutions.
* [1.x.186][1.x.187]
* 

* We now show the same figures in 3D: The first row shows the number of degreesof freedom and computing time versus the  [2.x.242]  error in the scalar variable [2.x.243]  for CG and HDG at order  [2.x.244] , the second row shows thepost-processed HDG solution instead of the original one, and the third rowcompares the post-processed HDG solution with CG at order  [2.x.245] . In 3D,the volume-to-surface effect makes the cost of HDG somewhat higher and the CGsolution is clearly better than HDG for linears by any metric. For cubics, HDGand CG are of similar quality, whereas HDG is again more efficient for sixthorder polynomials. One can alternatively also use the combination of FE_DGPand FE_FaceP instead of (FE_DGQ, FE_FaceQ), which do not use tensor productpolynomials of degree  [2.x.246]  but Legendre polynomials of [1.x.188]degree  [2.x.247] . There are fewer degrees of freedom on the skeleton variablefor FE_FaceP for a given mesh size, but the solution quality (error vs. numberof DoFs) is very similar to the results for FE_FaceQ.
*  [2.x.248] 
* One final note on the efficiency comparison: We tried to use general-purposesparse matrix structures and similar solvers (optimal AMG preconditioners forboth without particular tuning of the AMG parameters on any of them) to give afair picture of the cost versus accuracy of two methods, on a toy example. Itshould be noted however that geometric multigrid (GMG) for continuous finiteelements is about a factor four to five faster for  [2.x.249]  and  [2.x.250] . As of2019, optimal-complexity iterative solvers for HDG are still under developmentin the research community. Also, there are other implementation aspects for CGavailable such as fast matrix-free approaches as shown in  [2.x.251]  that makehigher order continuous elements more competitive. Again, it is not clear tothe authors of the tutorial whether similar improvements could be made forHDG. We refer to [1.x.189] for a recent efficiency evaluation.
* 

* [1.x.190][1.x.191]
* 

* As already mentioned in the introduction, one possibility is to implementanother post-processing technique as discussed in the literature.
* A second item that is not done optimally relates to the performance of thisprogram, which is of course an issue in practical applications (weighing inalso the better solution quality of (H)DG methods for transport-dominatedproblems). Let us look atthe computing time of the tutorial program and the share of the individualcomponents:
*  [2.x.252] 
* As can be seen from the table, the solver and assembly calls dominate theruntime of the program. This also gives a clear indication of whereimprovements would make the most sense:
*  [2.x.253]    [2.x.254]  Better linear solvers: We use a BiCGStab iterative solver without  preconditioner, where the number of iteration increases with increasing  problem size (the number of iterations for Q1 elements and global  refinements starts at 35 for the small sizes but increase up to 701 for the  largest size). To do better, one could for example use an algebraic  multigrid preconditioner from Trilinos, or some more advanced variants as  the one discussed in [1.x.192]. For diffusion-dominated problems such as the problem at hand  with finer meshes, such a solver can be designed that uses the matrix-vector  products from the more efficient ChunkSparseMatrix on the finest level, as  long as we are not working in parallel with MPI. For MPI-parallelized  computations, a standard  [2.x.255]  can be used.
*    [2.x.256]  Speed up assembly by pre-assembling parts that do not change from one  cell to another (those that do neither contain variable coefficients nor  mapping-dependent terms). [2.x.257] 
* 

* [1.x.193][1.x.194] [2.x.258] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-52_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25]
*  [2.x.2] 
* [1.x.26]
*  [2.x.3]  In order to run this program, deal.II must be configured to usethe UMFPACK sparse direct solver. Refer to the [1.x.27] for instructions how to do this.
* [1.x.28][1.x.29][1.x.30]
* 

* This program shows how to use Runge-Kutta methods to solve a time-dependentproblem. It solves a small variation of the heat equation discussed first in [2.x.4]  but, since the purpose of this program is only to demonstrate usingmore advanced ways to interface with deal.II's time stepping algorithms, onlysolves a simple problem on a uniformly refined mesh.
* 

* [1.x.31][1.x.32]
* 

* In this example, we solve the one-group time-dependent diffusionapproximation of the neutron transport equation (see  [2.x.5]  for thetime-independent multigroup diffusion). This is a model for how neutrons movearound highly scattering media, and consequently it is a variant of thetime-dependent diffusion equation
* 
*  -  which is just a different name for theheat equation discussed in  [2.x.6] , plus some extra terms.We assume that the medium is notfissible and therefore, the neutron flux satisfies the following equation:[1.x.33]
* augmented by appropriate boundary conditions. Here,  [2.x.7]  is the velocity ofneutrons (for simplicity we assume it is equal to 1 which can be achieved bysimply scaling the time variable),  [2.x.8]  is the diffusion coefficient, [2.x.9]  is the absorption cross section, and  [2.x.10]  is a source. Because we areonly interested in the time dependence, we assume that  [2.x.11]  and  [2.x.12]  areconstant.
* Since this program only intends to demonstrate how to use advanced timestepping algorithms, we will only look for the solutions of relatively simpleproblems. Specifically, we are looking for a solution on a square domain [2.x.13]  of the form[1.x.34]
* By using quadratic finite elements, we can represent this function exactly atany particular time, and all the error will be due to the timediscretization. We do this because it is then easy to observe the order ofconvergence of the various time stepping schemes we will consider, withouthaving to separate spatial and temporal errors.
* We impose the following boundary conditions: homogeneous Dirichlet for  [2.x.14]  and [2.x.15]  and homogeneous Neumann conditions for  [2.x.16]  and  [2.x.17] . We choose thesource term so that the corresponding solution isin fact of the form stated above:[1.x.35]
* Because the solution is a sine in time, we know that the exact solutionsatisfies  [2.x.18] .Therefore, the error at time  [2.x.19]  is simply the norm of the numericalsolution, i.e.,  [2.x.20] ,and is particularly easily evaluated. In the code, we evaluate the  [2.x.21]  normof the vector of nodal values of  [2.x.22]  instead of the  [2.x.23]  norm of theassociated spatial function, since the former is simpler to compute; however,on uniform meshes, the two are just related by a constant and we canconsequently observe the temporal convergence order with either.
* 

* [1.x.36][1.x.37]
* 

* The Runge-Kutta methods implemented in deal.II assume that the equation to besolved can be written as:[1.x.38]
* On the other hand, when using finite elements, discretized time derivatives always result in thepresence of a mass matrix on the left hand side. This can easily be seen byconsidering that if the solution vector  [2.x.24]  in the equation above is in fact the vectorof nodal coefficients  [2.x.25]  for a variable of the form[1.x.39]
* with spatial shape functions  [2.x.26] , then multiplying an equation ofthe form[1.x.40]
* by test functions, integrating over  [2.x.27] , substituting  [2.x.28] and restricting the test functions to the  [2.x.29]  from above, then thisspatially discretized equation has the form[1.x.41]
* where  [2.x.30]  is the mass matrix and  [2.x.31]  is the spatially discretized versionof  [2.x.32]  (where  [2.x.33]  is typically the place where spatialderivatives appear, but this is not of much concern for the moment given thatwe only consider time derivatives). In other words, this form fits the generalscheme above if we write[1.x.42]
* 
* Runke-Kutta methods are time stepping schemes that approximate  [2.x.34]  through a particular one-step approach. They are typically written in the form[1.x.43]
* where for the form of the right hand side above[1.x.44]
* Here  [2.x.35] ,  [2.x.36] , and  [2.x.37]  are known coefficients that identify whichparticular Runge-Kutta scheme you want to use, and  [2.x.38]  is the time stepused. Different time stepping methods of the Runge-Kutta class differ in thenumber of stages  [2.x.39]  and the values they use for the coefficients  [2.x.40] , [2.x.41] , and  [2.x.42]  but are otherwise easy to implement since one can look uptabulated values for these coefficients. (These tables are often calledButcher tableaus.)
* At the time of the writing of this tutorial, the methods implemented indeal.II can be divided in three categories: [2.x.43]  [2.x.44]  Explicit Runge-Kutta; in order for a method to be explicit, it isnecessary that in the formula above defining  [2.x.45] ,  [2.x.46]  does not appearon the right hand side. In other words, these methods have to satisfy [2.x.47] . [2.x.48]  Embedded (or adaptive) Runge-Kutta; we will discuss their properties below. [2.x.49]  Implicit Runge-Kutta; this class of methods require the solution of apossibly nonlinear system the stages  [2.x.50]  above, i.e., they have [2.x.51]  for at least one of the stages  [2.x.52] . [2.x.53] Many well known time stepping schemes that one does not typically associatewith the names Runge or Kutta can in fact be written in a way so that they,too, can be expressed in these categories. They oftentimes represent thelowest-order members of these families.
* 

* [1.x.45][1.x.46]
* 

* These methods, only require a function to evaluate  [2.x.54]  but not(as implicit methods) to solve an equation that involves [2.x.55]  for  [2.x.56] . As all explicit time stepping methods, they become unstablewhen the time step chosen is too large.
* Well known methods in this class include forward Euler, third orderRunge-Kutta, and fourth order Runge-Kutta (often abbreviated as RK4).
* 

* [1.x.47][1.x.48]
* 

* These methods use both a lower and a higher order method toestimate the error and decide if the time step needs to be shortened or can beincreased. The term "embedded" refers to the fact that the lower-order methoddoes not require additional evaluates of the function  [2.x.57] but reuses data that has to be computed for the high order method anyway. Itis, in other words, essentially free, and we get the error estimate as a sideproduct of using the higher order method.
* This class of methods include Heun-Euler, Bogacki-Shampine, Dormand-Prince (ode45 inMatlab and often abbreviated as RK45 to indicate that the lower and higher order methodsused here are 4th and 5th order Runge-Kutta methods, respectively), Fehlberg,and Cash-Karp.
* At the time of the writing, only embedded explicit methods have been implemented.
* 

* [1.x.49][1.x.50]
* 

* Implicit methods require the solution of (possibly nonlinear) systems of theform  [2.x.58] for  [2.x.59]  in each (sub-)timestep. Internally, this isdone using a Newton-type method and, consequently, they require that the userprovide functions that can evaluate  [2.x.60]  and [2.x.61]  or equivalently [2.x.62] .
* The particular form of this operator results from the fact that each Newtonstep requires the solution of an equation of the form
* [1.x.51]
* for some (given)  [2.x.63] . Implicit methods arealways stable, regardless of the time step size, but too large time steps ofcourse affect the [1.x.52] of the solution, even if the numericalsolution remains stable and bounded.
* Methods in this class include backward Euler, implicit midpoint,Crank-Nicolson, and the two stage SDIRK method (short for "singly diagonallyimplicit Runge-Kutta", a term coined to indicate that the diagonal elements [2.x.64]  defining the time stepping method are all equal; this propertyallows for the Newton matrix  [2.x.65]  tobe re-used between stages because  [2.x.66]  is the same every time).
* 

* [1.x.53][1.x.54]
* 

* By expanding the solution of our model problemas always using shape functions  [2.x.67]  and writing[1.x.55]
* we immediately get the spatially discretized version of the diffusion equation as[1.x.56]
* where[1.x.57]
* See also  [2.x.68]  and  [2.x.69]  to understand how we arrive here.Boundary terms are not necessary due to the chosen boundary conditions forthe current problem. To use the Runge-Kutta methods, we recast thisas follows:[1.x.58]
* In the code, we will need to be able to evaluate this function  [2.x.70]  alongwith its derivative,[1.x.59]
* 
* 

* [1.x.60][1.x.61]
* 

* To simplify the problem, the domain is two dimensional and the mesh isuniformly refined (there is no need to adapt the mesh since we use quadraticfinite elements and the exact solution is quadratic). Going from a twodimensional domain to a three dimensional domain is not verychallenging. However if you intend to solve more complex problems where themesh must be adapted (as is done, for example, in  [2.x.71] ), then it isimportant to remember the following issues:
*  [2.x.72]  [2.x.73]  You will need to project the solution to the new mesh when the mesh is changed. Of course,     the mesh     used should be the same from the beginning to the end of each time step,     a question that arises because Runge-Kutta methods use multiple     evaluations of the equations within each time step. [2.x.74]  You will need to update the mass matrix and its inverse every time the     mesh is changed. [2.x.75] The techniques for these steps are readily available by looking at  [2.x.76] .
* 

*  [1.x.62] [1.x.63]
*   [1.x.64]  [1.x.65]
* 

* 
*  The first task as usual is to include the functionality of these well-known deal.II library files and some C++ header files.
* 

* 
* [1.x.66]
* 
*  This is the only include file that is new: It includes all the Runge-Kutta methods.
* 

* 
* [1.x.67]
* 
*  The next step is like in all previous tutorial programs: We put everything into a namespace of its own and then import the deal.II classes and functions into it.
* 

* 
* [1.x.68]
* 
*   [1.x.69]  [1.x.70]
* 

* 
*  The next piece is the declaration of the main class. Most of the functions in this class are not new and have been explained in previous tutorials. The only interesting functions are  [2.x.77]  and  [2.x.78]  evaluates the diffusion equation,  [2.x.79] , at a given time and a given  [2.x.80] .  [2.x.81]  evaluates  [2.x.82]  or equivalently  [2.x.83]  at a given time, for a given  [2.x.84]  and  [2.x.85] . This function is needed when an implicit method is used.
* 

* 
* [1.x.71]
* 
*  The next three functions are the drivers for the explicit methods, the implicit methods, and the embedded explicit methods respectively. The driver function for embedded explicit methods returns the number of steps executed given that it only takes the number of time steps passed as an argument as a hint, but internally computed the optimal time step itself.
* 

* 
* [1.x.72]
* 
*  We choose quadratic finite elements and we initialize the parameters.
* 

* 
* [1.x.73]
* 
*   [1.x.74]  [1.x.75] Now, we create the constraint matrix and the sparsity pattern. Then, we initialize the matrices and the solution vector.
* 

* 
* [1.x.76]
* 
*   [1.x.77]  [1.x.78] In this function, we compute  [2.x.86]  and the mass matrix  [2.x.87] . The mass matrix is then inverted using a direct solver; the  [2.x.88]  variable will then store the inverse of the mass matrix so that  [2.x.89]  can be applied to a vector using the  [2.x.90]  function of that object. (Internally, UMFPACK does not really store the inverse of the matrix, but its LU factors; applying the inverse matrix is then equivalent to doing one forward and one backward solves with these two factors, which has the same complexity as applying an explicit inverse of the matrix).
* 

* 
* [1.x.79]
* 
*   [1.x.80]  [1.x.81]   
*   In this function, the source term of the equation for a given time and a given point is computed.
* 

* 
* [1.x.82]
* 
*   [1.x.83]  [1.x.84]   
*   Next, we evaluate the weak form of the diffusion equation at a given time  [2.x.91]  and for a given vector  [2.x.92] . In other words, as outlined in the introduction, we evaluate  [2.x.93] . For this, we have to apply the matrix  [2.x.94]  (previously computed and stored in the variable  [2.x.95] ) to  [2.x.96]  and then add the source term which we integrate as we usually do. (Integrating up the solution could be done using  [2.x.97]  if you wanted to save a few lines of code, or wanted to take advantage of doing the integration in parallel.) The result is then multiplied by  [2.x.98] .
* 

* 
* [1.x.85]
* 
*   [1.x.86]  [1.x.87]   
*   We compute  [2.x.99] . This is done in several steps:
* 

* 
* 
*  - compute  [2.x.100] 
* 

* 
* 
*  - invert the matrix to get  [2.x.101] 
* 

* 
* 
*  - compute  [2.x.102] 
* 

* 
* 
*  - compute  [2.x.103] 
* 

* 
* 
*  - return z.
* 

* 
* [1.x.88]
* 
*   [1.x.89]  [1.x.90]   
*   The following function then outputs the solution in vtu files indexed by the number of the time step and the name of the time stepping method. Of course, the (exact) result should really be the same for all time stepping method, but the output here at least allows us to compare them.
* 

* 
* [1.x.91]
* 
*   [1.x.92]  [1.x.93]   
*   This function is the driver for all the explicit methods. At the top it initializes the time stepping and the solution (by setting it to zero and then ensuring that boundary value and hanging node constraints are respected; of course, with the mesh we use here, hanging node constraints are not in fact an issue). It then calls  [2.x.104]  which performs one time step. Time is stored and incremented through a DiscreteTime object.   
*   For explicit methods,  [2.x.105]  needs to evaluate  [2.x.106] , i.e, it needs  [2.x.107] . Because  [2.x.108]  is a member function, it needs to be bound to  [2.x.109] . After each evolution step, we again apply the correct boundary values and hanging node constraints.   
*   Finally, the solution is output every 10 time steps.
* 

* 
* [1.x.94]
* 
*   [1.x.95]  [1.x.96] This function is equivalent to  [2.x.110]  but for implicit methods. When using implicit methods, we need to evaluate  [2.x.111]  and  [2.x.112]  for which we use the two member functions previously introduced.
* 

* 
* [1.x.97]
* 
*   [1.x.98]  [1.x.99] This function is the driver for the embedded explicit methods. It requires more parameters:
* 

* 
* 
*  - coarsen_param: factor multiplying the current time step when the error is below the threshold.
* 

* 
* 
*  - refine_param: factor multiplying the current time step when the error is above the threshold.
* 

* 
* 
*  - min_delta: smallest time step acceptable.
* 

* 
* 
*  - max_delta: largest time step acceptable.
* 

* 
* 
*  - refine_tol: threshold above which the time step is refined.
* 

* 
* 
*  - coarsen_tol: threshold below which the time step is coarsen.   
*   Embedded methods use a guessed time step. If the error using this time step is too large, the time step will be reduced. If the error is below the threshold, a larger time step will be tried for the next time step.  [2.x.113]  is the guessed time step produced by the embedded method. In summary, time step size is potentially modified in three ways:
* 

* 
* 
*  - Reducing or increasing time step size within  [2.x.114] 
* 

* 
* 
*  - Using the calculated  [2.x.115] .
* 

* 
* 
*  - Automatically adjusting the step size of the last time step to ensure simulation ends precisely at  [2.x.116] . This adjustment is handled inside the DiscreteTime instance.
* 

* 
* [1.x.100]
* 
*   [1.x.101]  [1.x.102]   
*   The following is the main function of the program. At the top, we create the grid (a [0,5]x[0,5] square) and refine it four times to get a mesh that has 16 by 16 cells, for a total of 256.  We then set the boundary indicator to 1 for those parts of the boundary where  [2.x.117]  and  [2.x.118] .
* 

* 
* [1.x.103]
* 
*  Next, we set up the linear systems and fill them with content so that they can be used throughout the time stepping process:
* 

* 
* [1.x.104]
* 
*  Finally, we solve the diffusion problem using several of the Runge-Kutta methods implemented in namespace TimeStepping, each time outputting the error at the end time. (As explained in the introduction, since the exact solution is zero at the final time, the error equals the numerical solution and can be computed by just taking the  [2.x.119]  norm of the solution vector.)
* 

* 
* [1.x.105]
* 
*   [1.x.106]  [1.x.107]
* 

* 
*  The following  [2.x.120]  function is similar to previous examples and need not be commented on.
* 

* 
* [1.x.108]
* [1.x.109][1.x.110]
* 

* The point of this program is less to show particular results, but instead toshow how it is done. This we have already demonstrated simply by discussingthe code above. Consequently, the output the program yields is relativelysparse and consists only of the console output and the solutions given in VTUformat for visualization.
* The console output contains both errors and, for some of the methods, thenumber of steps they performed:
* [1.x.111]
* 
* As expected the higher order methods give (much) more accurate solutions. Wealso see that the (rather inaccurate) Heun-Euler method increased the number oftime steps in order to satisfy the tolerance. On the other hand, the otherembedded methods used a lot less time steps than what was prescribed.
* 

* [1.x.112][1.x.113] [2.x.121] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-53_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13]
*  [2.x.2] 
* [1.x.14]
*  [2.x.3]  This program elaborates on concepts of geometry and the classes thatimplement it. These classes are grouped into the documentation module on  [2.x.4]  "Manifold description for triangulations". See there for additionalinformation.
*  [2.x.5]  This tutorial is also available as a Jupyter Python notebook that  uses the deal.II python interface. The notebook is available in the  same directory as the original C++ program. Rendered notebook can also  be viewed on the [1.x.15].
* 

* [1.x.16][1.x.17][1.x.18]
* 

* Partial differential equations for realistic problems are often posed ondomains with complicated geometries. To provide just a few examples, considerthese cases:
* 
*  - Among the two arguably most important industrial applications for the finite  element method, aerodynamics and more generally fluid dynamics is  one. Computer simulations today are used in the design of every airplane,  car, train and ship. The domain in which the partial differential equation  is posed is, in these cases, the air surrounding the plane with its wings,  flaps and engines; the air surrounding the car with its wheel, wheel wells,  mirrors and, in the case of race cars, all sorts of aerodynamic equipment;  the air surrounding the train with its wheels and gaps between cars. In the  case of ships, the domain is the water surrounding the ship with its rudders  and propellers.
* 
*  - The other of the two big applications of the finite element method is  structural engineering in which the domains are bridges, airplane nacelles  and wings, and other solid bodies of often complicated shapes.
* 
*  - Finite element modeling is also often used to describe the generation and  propagation of earthquake waves. In these cases, one needs to accurately  represent the geometry of faults in the Earth crust. Since faults intersect,  dip at angles, and are often not completely straight, domains are frequently  very complex.One could cite many more examples of complicated geometries in which one wantsto pose and solve a partial differential equation. What this shows is that the"real" world is much more complicated than what we have shown in almost all ofthe tutorial programs preceding this one.
* This program is therefore devoted to showing how one deals with complexgeometries using a concrete application. In particular, what it shows is howwe make a mesh fit the domain we want to solve on. On the other hand, what theprogram does not show is how to create a coarse for a domain. The process toarrive at a coarse mesh is called "mesh generation" and there are a number ofhigh-quality programs that do this much better than we could everimplement. However, deal.II does have the ability to read in meshes in manyformats generated by mesh generators and then make them fit a given shape,either by deforming a mesh or refining it a number of times until it fits. Thedeal.II Frequently Asked Questions page referenced from http://www.dealii.org/provides resources to mesh generators.
* 

* [1.x.19][1.x.20]
* 

* Let us assume that you have a complex domain and that you already have acoarse mesh that somehow represents the general features of the domain. Thenthere are two situations in which it is necessary to describe to a deal.IIprogram the details of your geometry:
* 
*  - Mesh refinement: Whenever a cell is refined, it is necessary to introduce  new vertices in the Triangulation. In the simplest case, one assumes that  the objects that make up the Triangulation are straight line segments, a  bi-linear surface or a tri-linear volume. The next vertex is then simply put  into the middle of the old ones. However, for curved boundaries or if we  want to solve a PDE on a curved, lower-dimensional manifold embedded in a  higher-dimensional space, this is insufficient since it will not respect the  actual geometry. We will therefore have to tell Triangulation where to put  new points.
* 
*  - Integration: When using higher order finite element methods, it is often  necessary to compute integrals using curved approximations of the boundary,  i.e., describe each edge or face of cells as curves, instead of straight  line segments or bilinear patches. The same is, of course, true when  integrating boundary terms (e.g., inhomogeneous Neumann boundary  conditions). For the purpose of integration, the various Mapping classes  then provide the transformation from the reference cell to the actual cell.
* In both cases, we need a way to provide information about the geometry of thedomain at the level of an individual cell, its faces and edges. This is wherethe Manifold class comes into play. Manifold is an abstract base class thatonly defines an interface by which the Triangulation and Mapping classes canquery geometric information about the domain. Conceptually, Manifold sees theworld in a way not dissimilar to how the mathematical subdiscipline geometrysees it: a domain is essentially just a collection of points that is somehowequipped with the notion of a distance between points so that we can obtain apoint "in the middle" of some other points.
* deal.II provides a number of classes that implement the interface provided byManifold for a variety of common geometries. On the other hand, in thisprogram we will consider only a very common and much simpler case, namely thesituation where (a part of) the domain we want to solve on can be described bytransforming a much simpler domain (we will call this the "reference domain").In the language of mathematics, this meansthat the (part of the) domain is a [1.x.21]. Charts aredescribed by a smooth function that maps from the simpler domain to the chart(the "push-forward" function) and its inverse (the "pull-back" function). Ifthe domain as a whole is not a chart (e.g., the surface of a sphere), then itcan often be described as a collection of charts (e.g., the northernhemisphere and the southern hemisphere are each charts) and the domain can thenbe describe by an [1.x.22].
* If a domain can be decomposed into an atlas, all we need to do is provide thepull-back and push-forward functions for each of the charts. In deal.II, thismeans providing a class derived from ChartManifold, and this is precisely whatwe will do in this program.
* 

* [1.x.23][1.x.24]
* 

* To illustrate how one describes geometries using charts in deal.II, we willconsider a case that originates in an application of the [1.x.25], using adata set provided by D. Sarah Stamps. In the concrete application, we wereinterested in describing flow in the Earth mantle under the [1.x.26], azone where two continental plates drift apart. Not to beat around the bush,the geometry we want to describe looks like this:
*  [2.x.6] 
* In particular, though you cannot see this here, the top surface is notjust colored by the elevation but is, in fact, deformed to follow thecorrect topography.While the actual application is not relevant here, the geometry is. The domainwe are interested in is a part of the Earth that ranges from the surface to adepth of 500km, from 26 to 35 degrees East of the Greenwich meridian, and from5 degrees North of the equator to 10 degrees South.
* This description of the geometry suggests to start with a box [2.x.7]  (measured in degrees,degrees, and meters) and to provide a map  [2.x.8]  sothat  [2.x.9]  where  [2.x.10]  is the domain weseek.  [2.x.11]  is then a chart,  [2.x.12]  the pull-back operator, and [2.x.13]  the push-forward operator. If we need a point  [2.x.14]  that is the"average" of other points  [2.x.15] , the ChartManifold class then firstapplies the pull-back to obtain  [2.x.16] , averages these to apoint  [2.x.17]  and then computes  [2.x.18] .
* Our goal here is therefore to implement a class that describes  [2.x.19]  and [2.x.20] . If Earth was a sphere, then this would not be difficult: if wedenote by  [2.x.21]  the points of  [2.x.22]  (i.e.,longitude counted eastward, latitude counted northward, and elevation relativeto zero depth), then[1.x.27]provides coordinates in a Cartesian coordinate system, where  [2.x.23]  is the radiusof the sphere. However, the Earth is not a sphere:
*  [2.x.24]  [2.x.25]  It is flattened at the poles and larger at the equator: the semi-major axis  is approximately 22km longer than the semi-minor axis. We will account for  this using the [1.x.28]  reference standard for the Earth shape. The formula used in WGS 84 to obtain  a position in Cartesian coordinates from longitude, latitude, and elevation  is[1.x.29]  where  [2.x.26] , and radius and  ellipticity are given by  [2.x.27] . In this formula,  we assume that the arguments to sines and cosines are evaluated in degree, not  radians (though we will have to change this assumption in the code).
*  [2.x.28]  It has topography in the form of mountains and valleys. We will account for  this using real topography data (see below for a description of where  this data comes from). Using this data set, we can look up elevations on a  latitude-longitude mesh laid over the surface of the Earth. Starting with  the box  [2.x.29] , we will therefore  first stretch it in vertical direction before handing it off to the WGS 84  function: if  [2.x.30]  is the height at longitude  [2.x.31]   and latitude  [2.x.32] , then we define[1.x.30]  Using this function, the top surface of the box  [2.x.33]  is displaced to the  correct topography, the bottom surface remains where it was, and points in  between are linearly interpolated. [2.x.34] 
* Using these two functions, we can then define the entire push-forward function [2.x.35]  as[1.x.31]In addition, we will have to define the inverse of this function, thepull-back operation, which we can write as[1.x.32]We can obtain one of the components of this function by inverting the formula above:[1.x.33]Computing  [2.x.36]  is also possible though a lot moreawkward. We won't show the formula here but instead only provide the implementationin the program.
* 

* [1.x.34][1.x.35]
* 

* There are a number of issues we need to address in the program. At the largest scale,we need to write a class that implements the interface of ChartManifold. This involvesa function  [2.x.37]  that takes a pointin the reference domain  [2.x.38]  and transform it into real space using the function [2.x.39]  outlined above, and its inverse function  [2.x.40] implementing  [2.x.41] . We will do so in the  [2.x.42]  class belowthat looks, in essence, like this:
* [1.x.36]
* 
* The transformations above have two parts: the WGS 84 transformations and the topographytransformation. Consequently, the  [2.x.43]  class will haveadditional (non-virtual) member functions [2.x.44]  and [2.x.45]  that implement these two pieces, andcorresponding pull back functions.
* The WGS 84 transformation functions are not particularly interesting (even though theformulas they implement are impressive). The more interesting part is the topographytransformation. Recall that for this, we needed to evaluate the elevation function [2.x.46] . There is of course no formula for this: Earth is what it is,the best one can do is look up the altitude from some table. This is, in fact what wewill do.
* The data we use was originally created by the  [1.x.37], was downloaded from the US Geologic Survey(USGS) and processed by D. Sarah Stamps who also wrote the initial version ofthe WGS 84 transformation functions. The topography data so processed isstored in a file  [2.x.47]  that, when unpackedlooks like this:
* [1.x.38]
* The data is formatted as  [2.x.48]  where the first twocolumns are provided in degrees North of the equator and degrees East of the Greenwichmeridian. The final column is given in meters above the WGS 84 zero elevation.
* In the transformation functions, we need to evaluate  [2.x.49]  for a givenlongitude  [2.x.50]  and latitude  [2.x.51] . In general, this data point will not beavailable and we will have to interpolate between adjacent data points. Writing such aninterpolation routine is not particularly difficult, but it is a bit tedious and errorprone. Fortunately, we can somehow shoehorn this data set into an existing class: [2.x.52]  . Unfortunately, the class does not fit the billquite exactly and so we need to work around it a bit. The problem comes from the waywe initialize this class: in its simplest form, it takes a stream of values that itassumes form an equispaced mesh in the  [2.x.53]  plane (or, here, the  [2.x.54]  plane).Which is what they do here, sort of: they are ordered latitude first, longitude second;and more awkwardly, the first column starts at the largest values and counts down,rather than the usual other way around.
* Now, while tutorial programs are meant to illustrate how to code with deal.II, they donot necessarily have to satisfy the same quality standards as one would have to dowith production codes. In a production code, we would write a function that reads thedata and (i) automatically determines the extents of the first and second column,(ii) automatically determines the number of data points in each direction, (iii) doesthe interpolation regardless of the order in which data is arranged, if necessaryby switching the order between reading and presenting it to the [2.x.55]  class.
* On the other hand, tutorial programs are best if they are short and demonstrate keypoints rather than dwell on unimportant aspects and, thereby, obscure what we reallywant to show. Consequently, we will allow ourselves a bit of leeway:
* 
*  - since this program is intended solely for a particular geometry around the area  of the East-African rift and since this is precisely the area described by the data  file, we will hardcode in the program that there are   [2.x.56]  pieces of data;
* 
*  - we will hardcode the boundaries of the data   [2.x.57] ;
* 
*  - we will lie to the  [2.x.58]  class: the class will  only see the data in the last column of this data file, and we will pretend that  the data is arranged in a way that there are 1139 data points in the first  coordinate direction that are arranged in [1.x.39] order but in an  interval  [2.x.59]  (not the negated bounds). Then,  when we need to look something up for a latitude  [2.x.60] , we can ask the  interpolating table class for a value at  [2.x.61] . With this little  trick, we can avoid having to switch around the order of data as read from  file.
* All of this then calls for a class that essentially looks like this:
* [1.x.40]
* 
* Note how the  [2.x.62]  function negates the latitude. It also switchesfrom the format  [2.x.63]  that we use everywhere else to the latitude-longitudeformat used in the table. Finally, it takes its arguments in radians as that is whatwe do everywhere else in the program, but then converts them to the degree-basedsystem used for table lookup. As you will see in the implementation below, the functionhas a few more (static) member functions that we will call in the initializationof the  [2.x.64]  member variable: the class type of this variablehas a constructor that allows us to set everything right at construction time,rather than having to fill data later on, but this constructor takes a number ofobjects that can't be constructed in-place (at least not in C++98). Consequently,the construction of each of the objects we want to pass in the initialization happensin a number of static member functions.
* Having discussed the general outline of how we want to implement things, let us goto the program and show how it is done in practice.
* 

*  [1.x.41] [1.x.42]
*  Let us start with the include files we need here. Obviously, we need the ones that describe the triangulation ( [2.x.65] ), and that allow us to create and output triangulations ( [2.x.66]  and  [2.x.67] ). Furthermore, we need the header file that declares the Manifold and ChartManifold classes that we will need to describe the geometry ( [2.x.68] ). We will then also need the  [2.x.69]  function from the last of the following header files; the purpose for this function will become discussed at the point where we use it.
* 

* 
* [1.x.43]
* 
*  The remainder of the include files relate to reading the topography data. As explained in the introduction, we will read it from a file and then use the  [2.x.70]  class that is declared in the first of the following header files. Because the data is large, the file we read from is stored as gzip compressed data and we make use of some BOOST-provided functionality to read directly from gzipped data.
* 

* 
* [1.x.44]
* 
*  The final part of the top matter is to open a namespace into which to put everything, and then to import the dealii namespace into it.
* 

* 
* [1.x.45]
* 
*   [1.x.46]  [1.x.47]   
*   The first significant part of this program is the class that describes the topography  [2.x.71]  as a function of longitude and latitude. As discussed in the introduction, we will make our life a bit easier here by not writing the class in the most general way possible but by only writing it for the particular purpose we are interested in here: interpolating data obtained from one very specific data file that contains information about a particular area of the world for which we know the extents.   
*   The general layout of the class has been discussed already above. Following is its declaration, including three static member functions that we will need in initializing the  [2.x.72]  member variable.
* 

* 
* [1.x.48]
* 
*  Let us move to the implementation of the class. The interesting parts of the class are the constructor and the  [2.x.73]  function. The former initializes the  [2.x.74]  member variable and we will use the constructor that requires us to pass in the end points of the 2-dimensional data set we want to interpolate (which are here given by the intervals  [2.x.75] , using the trick of switching end points discussed in the introduction, and  [2.x.76] , both given in degrees), the number of intervals into which the data is split (379 in latitude direction and 219 in longitude direction, for a total of  [2.x.77]  data points), and a Table object that contains the data. The data then of course has size  [2.x.78]  and we initialize it by providing an iterator to the first of the 83,600 elements of a  [2.x.79]  object returned by the  [2.x.80]  function below. Note that all of the member functions we call here are static because (i) they do not access any member variables of the class, and (ii) because they are called at a time when the object is not initialized fully anyway.
* 

* 
* [1.x.49]
* 
*  The only other function of greater interest is the  [2.x.81]  function. It returns a temporary vector that contains all 83,600 data points describing the altitude and is read from the file  [2.x.82] . Because the file is compressed by gzip, we cannot just read it through an object of type  [2.x.83]  but there are convenient methods in the BOOST library (see http://www.boost.org) that allows us to read from compressed files without first having to uncompress it on disk. The result is, basically, just another input stream that, for all practical purposes, looks just like the ones we always use.   
*   When reading the data, we read the three columns but throw ignore the first two. The datum in the last column is appended to an array that we the return and that will be copied into the table from which  [2.x.84]  is initialized. Since the BOOST.iostreams library does not provide a very useful exception when the input file does not exist, is not readable, or does not contain the correct number of data lines, we catch all exceptions it may produce and create our own one. To this end, in the  [2.x.85]  clause, we let the program run into an  [2.x.86]  statement. Since the condition is always false, this always triggers an exception. In other words, this is equivalent to writing  [2.x.87]  but it also fills certain fields in the exception object that will later be printed on the screen identifying the function, file and line where the exception happened.
* 

* 
* [1.x.50]
* 
*  create a stream where we read from gzipped data
* 

* 
* [1.x.51]
* 
*   [1.x.52]  [1.x.53]   
*   The following class is then the main one of this program. Its structure has been described in much detail in the introduction and does not need much introduction any more.
* 

* 
* [1.x.54]
* 
*  The implementation, as well, is pretty straightforward if you have read the introduction. In particular, both of the pull back and push forward functions are just concatenations of the respective functions of the WGS 84 and topography mappings:
* 

* 
* [1.x.55]
* 
*  The next function is required by the interface of the Manifold base class, and allows cloning the AfricaGeometry class. Notice that, while the function returns a  [2.x.88]  we internally create a `unique_ptr<AfricaGeometry>`. In other words, the library requires a pointer-to-base-class, which we provide by creating a pointer-to-derived-class.
* 

* 
* [1.x.56]
* 
*  The following two functions then define the forward and inverse transformations that correspond to the WGS 84 reference shape of Earth. The forward transform follows the formula shown in the introduction. The inverse transform is significantly more complicated and is, at the very least, not intuitive. It also suffers from the fact that it returns an angle that at the end of the function we need to clip back into the interval  [2.x.89]  if it should have escaped from there.
* 

* 
* [1.x.57]
* 
*  In contrast, the topography transformations follow exactly the description in the introduction. There is not consequently not much to add:
* 

* 
* [1.x.58]
* 
*   [1.x.59]  [1.x.60]   
*   Having so described the properties of the geometry, not it is time to deal with the mesh used to discretize it. To this end, we create objects for the geometry and triangulation, and then proceed to create a  [2.x.90]  rectangular mesh that corresponds to the reference domain  [2.x.91] . We choose this number of subdivisions because it leads to cells that are roughly like cubes instead of stretched in one direction or another.   
*   Of course, we are not actually interested in meshing the reference domain. We are interested in meshing the real domain. Consequently, we will use the  [2.x.92]  function that simply moves every point of a triangulation according to a given transformation. The transformation function it wants is a function that takes as its single argument a point in the reference domain and returns the corresponding location in the domain that we want to map to. This is, of course, exactly the push forward function of the geometry we use. We wrap it by a lambda function to obtain the kind of function object required for the transformation.
* 

* 
* [1.x.61]
* 
*  The next step is to explain to the triangulation to use our geometry object whenever a new point is needed upon refining the mesh. We do this by telling the triangulation to use our geometry for everything that has manifold indicator zero, and then proceed to mark all cells and their bounding faces and edges with manifold indicator zero. This ensures that the triangulation consults our geometry object every time a new vertex is needed. Since manifold indicators are inherited from mother to children, this also happens after several recursive refinement steps.
* 

* 
* [1.x.62]
* 
*  The last step is to refine the mesh beyond its initial  [2.x.93]  coarse mesh. We could just refine globally a number of times, but since for the purpose of this tutorial program we're really only interested in what is happening close to the surface, we just refine 6 times all of the cells that have a face at a boundary with indicator 5. Looking this up in the documentation of the  [2.x.94]  function we have used above reveals that boundary indicator 5 corresponds to the top surface of the domain (and this is what the last  [2.x.95]  argument in the call to  [2.x.96]  above meant: to "color" the boundaries by assigning each boundary a unique boundary indicator).
* 

* 
* [1.x.63]
* 
*  Having done this all, we can now output the mesh into a file of its own:
* 

* 
* [1.x.64]
* 
*   [1.x.65]  [1.x.66]
* 

* 
*  Finally, the main function, which follows the same scheme used in all tutorial programs starting with  [2.x.97] . There isn't much to do here, only to call the single  [2.x.98]  function.
* 

* 
* [1.x.67]
* [1.x.68][1.x.69]
* 

* Running the program produces a mesh file  [2.x.99]  that we canvisualize with any of the usual visualization programs that can read the VTUfile format. If one just looks at the mesh itself, it is actually very difficultto see anything that doesn't just look like a perfectly round piece of asphere (though if one modified the program so that it does produce a sphere andlooked at them at the same time, the difference between the overall sphere andWGS 84 shape is quite apparent). Apparently, Earth is actually quite a flat place.Of course we already know this from satellite pictures.However, we can tease out something more bycoloring cells by their volume. This both produces slight variations in huealong the top surface and something for the visualization programs to applytheir shading algorithms to (because the top surfaces of the cells are now nolonger just tangential to a sphere but tilted):
*  [2.x.100] 
* Yet, at least as far as visualizations are concerned, this is still not tooimpressive. Rather, let us visualize things in a way so that we show theactual elevation along the top surface. In other words, we want a picture likethis, with an incredible amount of detail:
*  [2.x.101] 
* A zoom-in of this picture shows the vertical displacement quite clearly (here,looking from the West-Northwest over the rift valley, the triple peaksof[1.x.70],[1.x.71], and[1.x.72]in the[1.x.73],[1.x.74]and toward the great flatness of[1.x.75]):
*  [2.x.102] 
* 

* These image were produced with three small modifications: [2.x.103]    [2.x.104]  An additional seventh mesh refinement towards the top surface for the  first of these two pictures, and a total of nine for the second. In the  second image, the horizontal mesh size is approximately 1.5km, and just  under 1km in vertical direction. (The picture was also created using a  more resolved data set; however, it is too big to distribute as part of  the tutorial.)
*    [2.x.105]  The addition of the following function that, given a point   [2.x.106]  computes the elevation by converting the point to  reference WGS 84 coordinates and only keeping the depth variable (the  function is, consequently, a simplified version of the   [2.x.107]  function):
* [1.x.76]
* 
*    [2.x.108] Adding the following piece to the bottom of the  [2.x.109]  function:
* [1.x.77]
*  [2.x.110] This last piece of code first creates a  [2.x.111]  finite element space on the mesh.It then (ab)uses  [2.x.112]  to evaluate theelevation function for every node at the top boundary (the one with boundaryindicator 5). We here wrap the call to  [2.x.113]  with theScalarFunctionFromFunctionObject class to make a regular C++ function looklike an object of a class derived from the Function class that we wantto use in  [2.x.114]  Having so gotten a listof degrees of freedom located at the top boundary and corresponding elevationvalues, we just go down this list and set these elevations in the [2.x.115]  vector (leaving all interior degrees of freedom attheir original zero value). This vector is then output using DataOut asusual and can be visualized as shown above.
* 

* [1.x.78][1.x.79]
* 

* If you zoomed in on the mesh shown above and looked closely enough, you wouldfind that at hanging nodes, the two small edges connecting to the hangingnodes are not in exactly the same location as the large edge of theneighboring cell. This can be shown more clearly by using a different surfacedescription in which we enlarge the vertical topography to enhance the effect(courtesy of Alexander Grayver):
*  [2.x.116] 
* So what is happening here? Partly, this is only a result of visualization, butthere is an underlying real cause as well:
*  [2.x.117]    [2.x.118] When you visualize a mesh using any of the common visualization  programs, what they really show you is just a set of edges that are plotted  as straight lines in three-dimensional space. This is so because almost all  data file formats for visualizing data only describe hexahedral cells as a  collection of eight vertices in 3d space, and do not allow to any more  complicated descriptions. (This is the main reason why   [2.x.119]  takes an argument that can be set to something  larger than one.) These linear edges may be the edges of the cell you do  actual computations on, or they may not, depending on what kind of mapping  you use when you do your integrations using FEValues. By default, of course,  FEValues uses a linear mapping (i.e., an object of class MappingQ1) and in  that case a 3d cell is indeed described exclusively by its 8 vertices and  the volume it fills is a trilinear interpolation between these points,  resulting in linear edges. But, you could also have used tri-quadratic,  tri-cubic, or even higher order mappings and in these cases the volume of  each cell will be bounded by quadratic, cubic or higher order polynomial  curves. Yet, you only get to see these with linear edges in the  visualization program because, as mentioned, file formats do not allow to  describe the real geometry of cells.
*    [2.x.120] That said, let us for simplicity assume that you are indeed using a  trilinear mapping, then the image shown above is a faithful representation  of the cells on which you form your integrals. In this case, indeed the  small cells at a hanging nodes do not, in general, snugly fit against the  large cell but leave a gap or may intersect the larger cell. Why is this?  Because when the triangulation needs a new vertex on an edge it wants to  refine, it asks the manifold description where this new vertex is supposed  to be, and the manifold description duly returns such a point by (in the  case of a geometry derived from ChartManifold) pulling the adjacent points  of the line back to the reference domain, averaging their locations, and  pushing forward this new location to the real domain. But this new location  is not usually along a straight line (in real space) between the adjacent  vertices and consequently the two small straight lines forming the refined  edge do not lie exactly on the one large straight line forming the unrefined  side of the hanging node. [2.x.121] 
* The situation is slightly more complicated if you use a higher order mappingusing the MappingQ class, but not fundamentally different. Let's take aquadratic mapping for the moment (nothing fundamental changes with even higherorder mappings). Then you need to imagine each edge of the cells you integrateon as a quadratic curve despite the fact that you will never actually see itplotted that way by a visualization program. But imagine it that way for asecond. So which quadratic curve does MappingQ take? It is the quadratic curvethat goes through the two vertices at the end of the edge as well as a pointin the middle that it queries from the manifold. In the case of the long edgeon the unrefined side, that's of course exactly the location of the hangingnode, so the quadratic curve describing the long edge does go through thehanging node, unlike in the case of the linear mapping. But the two smalledges are also quadratic curves; for example, the left small edge will gothrough the left vertex of the long edge and the hanging node, plus a point itqueries halfway in between from the manifold. Because, as before, the pointthe manifold returns halfway along the left small edge is rarely exactly onthe quadratic curve describing the long edge, the quadratic short edge willtypically not coincide with the left half of the quadratic long edge, and thesame is true for the right short edge. In other words, again, the geometriesof the large cell and its smaller neighbors at hanging nodes do not touchsnuggly.
* This all begs two questions: first, does it matter, and second, could this befixed. Let us discuss these in the following:
*  [2.x.122]    [2.x.123] Does it matter? It is almost certainly true that this depends on the  equation you are solving. For example, it is known that solving the Euler  equations of gas dynamics on complex geometries requires highly accurate  boundary descriptions to ensure convergence of quantities that are measure  the flow close to the boundary. On the other hand, equations with elliptic  components (e.g., the Laplace or Stokes equations) are typically rather  forgiving of these issues: one does quadrature anyway to approximate  integrals, and further approximating the geometry may not do as much harm as  one could fear given that the volume of the overlaps or gaps at every  hanging node is only  [2.x.124]  even with a linear mapping and  [2.x.125]  for a mapping of degree  [2.x.126] . (You can see this by considering  that in 2d the gap/overlap is a triangle with base  [2.x.127]  and height  [2.x.128] ; in 3d, it is a pyramid-like structure with base area  [2.x.129]  and  height  [2.x.130] . Similar considerations apply for higher order mappings  where the height of the gaps/overlaps is  [2.x.131] .) In other words,  if you use a linear mapping with linear elements, the error in the volume  you integrate over is already at the same level as the integration error  using the usual Gauss quadrature. Of course, for higher order elements one  would have to choose matching mapping objects.
*   Another point of view on why it is probably not worth worrying too much  about the issue is that there is certainly no narrative in the community of  numerical analysts that these issues are a major concern one needs to watch  out for when using complex geometries. If it does not seem to be discussed  often among practitioners, if ever at all, then it is at least not something  people have identified as a common problem.
*   This issue is not dissimilar to having hanging nodes at curved boundaries  where the geometry description of the boundary typically pulls a hanging  node onto the boundary whereas the large edge remains straight, making the  adjacent small and large cells not match each other. Although this behavior  existed in deal.II since its beginning, 15 years before manifold  descriptions became available, it did not ever come up in mailing list  discussions or conversations with colleagues.
*    [2.x.132] Could it be fixed? In principle, yes, but it's a complicated  issue. Let's assume for the moment that we would only ever use the MappingQ1  class, i.e., linear mappings. In that case, whenever the triangulation class  requires a new vertex along an edge that would become a hanging node, it  would just take the mean value of the adjacent vertices [1.x.80], i.e., without asking the manifold description. This way, the  point lies on the long straight edge and the two short straight edges would  match the one long edge. Only when all adjacent cells have been refined and  the point is no longer a hanging node would we replace its coordinates by  coordinates we get by a manifold. This may be awkward to implement, but it  would certainly be possible.
*   The more complicated issue arises because people may want to use a higher  order MappingQ object. In that case, the Triangulation class may freely  choose the location of the hanging node (because the quadratic curve for the  long edge can be chosen in such a way that it goes through the hanging node)  but the MappingQ class, when determining the location of mid-edge points  must make sure that if the edge is one half of a long edge of a neighboring  coarser cell, then the midpoint cannot be obtained from the manifold but  must be chosen along the long quadratic edge. For cubic (and all other odd)  mappings, the matter is again a bit complicated because one typically  arranges the cubic edge to go through points 1/3 and 2/3 along the edge, and  thus necessarily through the hanging node, but this could probably be worked  out. In any case, even then, there are two problems with this:
* 

* 
* 

* 
* 
*  - When refining the triangulation, the Triangulation class can not know what    mapping will be used. In fact it is not uncommon for a triangulation to be    used differently in different contexts within the same program. If the    mapping used determines whether we can freely choose a point or not, how,    then, should the triangulation locate new vertices?
* 

* 
* 

* 
* 
*  - Mappings are purely local constructs: they only work on a cell in    isolation, and this is one of the important features of the finite element    method. Having to ask whether one of the vertices of an edge is a hanging    node requires querying the neighborhood of a cell; furthermore, such a    query does not just involve the 6 face neighbors of a cell in 3d, but may    require traversing a possibly very large number of other cells that    connect to an edge. Even if it can be done, one still needs to do    different things depending on how the neighborhood looks like, producing    code that is likely very complex, hard to maintain, and possibly slow.
*   Consequently, at least for the moment, none of these ideas are  implemented. This leads to the undesirable consequence of discontinuous  geometries, but, as discussed above, the effects of this do not appear to  pose problem in actual practice.
*  [2.x.133] 
* 

* [1.x.81][1.x.82] [2.x.134] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-54_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16]
*  [2.x.2] 
* [1.x.17]
*  [2.x.3]  This program elaborates on concepts of industrial geometry, using toolsthat interface with the OpenCASCADE library (http://www.opencascade.org) thatallow the specification of arbitrary IGES files to describe the boundaries foryour geometries.
*  [2.x.4] 
* [1.x.18][1.x.19][1.x.20]
* 

* 
* In some of the previous tutorial programs ( [2.x.5] ,  [2.x.6] ,  [2.x.7] ,  [2.x.8]  and [2.x.9]  among others) we have learned how to use the mesh refinement methodsprovided in deal.II. These tutorials have shown how to employ such tools toproduce a fine grid for a single simulation, as done in  [2.x.10] ; or to start froma coarse grid and carry out a series of simulations on adaptively refined grids,as is the case of  [2.x.11] . Regardless of which approach is taken, the meshrefinement requires a suitable geometrical description of the computationaldomain boundary in order to place, at each refinement, the new mesh nodes ontothe boundary surface. For instance,  [2.x.12]  shows how creating a circular gridautomatically attaches a circular manifold object to the computational domain,so that the faces lying on the boundary are refined onto the circle.  [2.x.13] shows how to do this with a Manifold defined by experimentally obtained data.But, at least as far as elementary boundary shapes are concerned, deal.II reallyonly provides circles, spheres, boxes and other elementary combinations. In thistutorial, we will show how to use a set of classes developed to import arbitraryCAD geometries, assign them to the desired boundary of the computational domain,and refine a computational grid on such complex shapes.
* 

* [1.x.21][1.x.22]
* 

* In the most common industrial practice, the geometrical models of arbitrarilyshaped objects are realized by means of Computer Aided Design (CAD) tools. Theuse of CAD modelers has spread in the last decades, as they allow for thegeneration of a full virtual model of each designed object, which through acomputer can be visualized, inspected, and analyzed in its finest details wellbefore it is physically crafted.  From a mathematical perspective, the enginelying under the hood of CAD modelers is represented by analytical geometry,and in particular by parametric curves and surfaces such as B-splines andNURBS that are rich enough that they can represent most surfaces of practicalinterest.  Once a virtual model is ready, all the geometrical features of thedesired object are stored in files which materially contain the coefficientsof the parametric surfaces and curves composing the object. Depending on thespecific CAD tool used to define the geometrical model, there are of courseseveral different file formats in which the information of a CAD model can beorganized. To provide a common ground to exchange data across CAD tools, theU.S. National Bureau of Standards published in 1980 the Initial GraphicsExchange Representation (IGES) neutral file format, which is used in thisexample.
* [1.x.23][1.x.24]
* 

* To import and interrogate CAD models, the deal.II library implements a series ofwrapper functions for the OpenCASCADE open source library for CAD modeling.These functions allow to import IGES files into OpenCASCADE native objects, andwrap them inside a series of Manifold classes.
* Once imported from an IGES file, the model is stored in a [2.x.14] , which is the generic topological entity defined inthe OpenCASCADE framework. From a  [2.x.15] , it is then possibleto access all the sub-shapes (such as vertices, edges and faces) composing it,along with their geometrical description. In the deal.II framework, thetopological entities composing a shape are used to create a correspondingManifold representation. In  [2.x.16]  we saw how to use  [2.x.17] to create a hyper sphere, which automatically attaches a SphericalManifoldto all boundary faces. This guarantees that boundary faces stay on asphere or circle during mesh refinement. The functions of the CAD modeling interfacehave been designed to retain the same structure, allowing the user to build aprojector object using the imported CAD shapes, maintaining the same procedurewe used in other tutorial programs, i.e., assigning such projector object tocells, faces or edges of a coarse mesh. At each refinement cycle, the new meshnodes will be then automatically generated by projecting a midpoint of anexisting object onto the specified geometry.
* Differently from a spherical or circular boundary, a boundary with a complexgeometry poses problems as to where it is best to place the new nodes createdupon refinement on the prescribed shape. PolarManifold, for example, transformsthe surrounding points to polar coordinates, calculates the average in thatcoordinate system (for each coordinate individually) and finally transformsthe point back to Cartesian coordinates.
* In the case of an arbitrary and complex shape though, an appropriate choice forthe placement of a new node cannot be identified that easily. The OpenCASCADEwrappers in deal.II provide several projector classes that employ differentprojection strategies. A first projector, implemented in the [2.x.18]  class, is to be used only foredge refinement. It is built assigning it a topological shape of dimensionone, either a  [2.x.19]  (which isa compound shape, made of several connected  [2.x.20] s) andrefines a mesh edge finding the new vertex as the point splitting in two evenparts the curvilinear length of the CAD curve portion that lies between thevertices of the original edge.
*  [2.x.21] 
* 

* A different projection strategy has been implemented in the [2.x.22]  class. The  [2.x.23] assigned at construction time can be arbitrary (a collection of shapes, faces,edges or a single face or edge will all work). The new cell nodes are firstcomputed by averaging the surrounding points in the same way as FlatManifolddoes. In a second step, all the new nodes will be projected onto the [2.x.24]  along the direction normal to the shape. If nonormal projection is available, the point which is closest to theshape---typically lying on the shape boundary---is selected.  If the shape iscomposed of several sub-shapes, the projection is carried out onto everysingle sub-shape, and the closest projection point is selected.
*  [2.x.25]  [2.x.26] 
* As we are about to experience, for some shapes, setting the projectiondirection as that normal to the CAD surface will not lead to surface meshelements of suitable quality. This is because the direction normal to the CADsurface has in principle nothing to do with the direction along which the meshneeds the new nodes to be located. The [2.x.27]  class, in this case, can help. Thisclass is constructed assigning a  [2.x.28]  (containing atleast a face) and a direction along which all the projections will be carriedout. New points will be computed by first averaging the surrounding points (asin the FlatManifold case), and then taking the closest intersection betweenthe topological shape and the line passing through the resulting point, alongthe direction used at construction time.  In this way, the user will have ahigher control on the projection direction to be enforced to ensure good meshquality.
*  [2.x.29] 
* 

* Of course the latter approach is effective only when the orientation of thesurface is rather uniform, so that a single projection direction can beidentified. In cases in which the surface direction is approaching theprojection direction, it is even possible that the directional projection isnot found. To overcome these problems, the [2.x.30]  class implements a thirdprojection algorithm. The  [2.x.31]  class isbuilt assigning a  [2.x.32]  (containing at least one face) tothe constructor, and works exactly like a [2.x.33]  But, as the name of the class suggests, [2.x.34]  tries to come up with a suitableestimate of the direction normal to the mesh elements to be refined, and usesit for the projection of the new nodes onto the CAD surface. If we consider amesh edge in a 2D space, the direction of its axis is a direction along whichto split it in order to give rise to two new cells of the same length. We hereextended this concept in 3D, and project all new nodes in a direction thatapproximates the cell normal.
* In the next figure, which is inspired by the geometry considered in thistutorial, we make an attempt to compare the behavior of the three projectorsconsidered. As can be seen on the left, given the original cell (in blue), thenew point found with the normal projection is in a position which does notallow for the generation of evenly spaced new elements (in red). The situationwill get worse in further refinement steps.  Since the geometry we consideredis somehow perpendicular to the horizontal direction, the directionalprojection (central image) defined with horizontal direction as the projectiondirection, does a rather good job in getting the new mesh point. Yet, sincethe surface is almost horizontal at the bottom of the picture, we can expectproblems in those regions when further refinement steps are carriedout. Finally, the picture on the right shows that a node located on the cellaxis will result in two new cells having the same length. Of course thesituation in 3D gets a little more complicated than that described in thissimple 2D case. Nevertheless, the results of this test confirm that the normalto the mesh direction is the best approach among the three tested, whenarbitrarily shaped surfaces are considered, and unless you have a geometry forwhich a more specific approach is known to be appropriate.
* 

*  [2.x.35] 
* 

* [1.x.25][1.x.26]
* 

* In this program, we will consider creating a surface mesh for a real geometrydescribing the bow of a ship (this geometry is frequently used in CAD and meshgeneration comparisons and is freely available). The surface mesh we get fromthis could then be used to solve a boundary element equation to simulate theflow of water around the ship (in a way similar to  [2.x.36] ) but we will nottry to do this here. To already give you an idea of the geometry we consider,here is a picture:
*  [2.x.37] 
* In the program, we read both the geometry and a coarse mesh from files, andthen employ several of the options discussed above to place new vertices for asequence of mesh refinement steps.
* 

*  [1.x.27] [1.x.28]
*   [1.x.29]  [1.x.30]
* 

* 
*  We start with including a bunch of files that we will use in the various parts of the program. Most of them have been discussed in previous tutorials already:
* 

* 
* [1.x.31]
* 
*  These are the headers of the opencascade support classes and functions. Notice that these will contain sensible data only if you compiled your deal.II library with support for OpenCASCADE, i.e., specifying  [2.x.38]  and  [2.x.39]  when calling  [2.x.40]  during deal.II configuration.
* 

* 
* [1.x.32]
* 
*  Finally, a few C++ standard header files
* 

* 
* [1.x.33]
* 
*  We isolate the rest of the program in its own namespace
* 

* 
* [1.x.34]
* 
*   [1.x.35]  [1.x.36]
* 

* 
*  This is the main class. All it really does is store names for input and output files, and a triangulation. It then provides a function that generates such a triangulation from a coarse mesh, using one of the strategies discussed in the introduction and listed in the enumeration type at the top of the class.   
*   The member functions of this class are similar to what you can find in most of the other tutorial programs in the setup stage of the grid for the simulations.
* 

* 
*  

* 
* [1.x.37]
* 
*   [1.x.38]  [1.x.39]
* 

* 
*  The constructor of the TriangulationOnCAD class is very simple. The input arguments are strings for the input and output file names, and the enumeration type that determines which kind of surface projector is used in the mesh refinement cycles (see below for details).
* 

* 
*  

* 
* [1.x.40]
* 
*   [1.x.41]  [1.x.42]
* 

* 
*  
*   The following function represents the core of this program.  In this function we import the CAD shape upon which we want to generate and refine our triangulation. We assume that the CAD surface is contained in the  [2.x.41]  file (we provide an example IGES file in the input directory called "input/DTMB-5415_bulbous_bow.iges" that represents the bulbous bow of a ship). The presence of several convex and concave high curvature regions makes the geometry we provided a particularly meaningful example.   
*   After importing the hull bow surface, we extract some of the curves and surfaces composing it, and use them to generate a set of projectors. Such projectors define the rules the Triangulation has to follow to position each new node during cell refinement.   
*   To initialize the Triangulation, as done in previous tutorial programs, we import a pre-existing grid saved in VTK format. We assume here that the user has generated a coarse mesh externally, which matches the IGES geometry. At the moment of writing this tutorial, the deal.II library does not automatically support generation of such meshes, but there are several tools which can provide you with reasonable initial meshes starting from CAD files. In our example, the imported mesh is composed of a single quadrilateral cell whose vertices have been placed on the CAD shape.   
*   After importing both the IGES geometry and the initial mesh, we assign the projectors previously discussed to each of the edges and cells which will have to be refined on the CAD surface.   
*   In this tutorial, we will test the three different CAD surface projectors described in the introduction, and will analyze the results obtained with each of them.  As mentioned, each of these projection strategies has been implemented in a different class, and objects of these types can be assigned to a triangulation using the  [2.x.42]  method.   
*   The following function then first imports the given CAD file. The function arguments are a string containing the desired file name, and a scale factor. In this example, the scale factor is set to 1e-3, as the original geometry is written in millimeters (which is the typical unit of measure for most IGES files), while we prefer to work in meters.  The output of the function is an object of OpenCASCADE generic topological shape class, namely a  [2.x.43] 
* 

* 
* [1.x.43]
* 
*  Each CAD geometrical object is defined along with a tolerance, which indicates possible inaccuracy of its placement. For instance, the tolerance  [2.x.44]  of a vertex indicates that it can be located in any point contained in a sphere centered in the nominal position and having radius  [2.x.45]  While projecting a point onto a surface (which will in turn have its tolerance) we must keep in mind that the precision of the projection will be limited by the tolerance with which the surface is built.
* 

* 
*  The following method extracts the tolerance of the given shape and makes it a bit bigger to stay our of trouble:
* 

* 
* [1.x.44]
* 
*  We now want to extract a set of composite sub-shapes from the generic shape. In particular, each face of the CAD file is composed of a trimming curve of type  [2.x.46]  which is the collection of  [2.x.47]  that compose the boundary of a surface, and a NURBS description of the surface itself. We will use a line projector to associate the boundary of our Triangulation to the wire delimiting the surface.  To extract all compound sub-shapes, like wires, shells, or solids, we resort to a method of the OpenCASCADE namespace.  The input of  [2.x.48]  is a shape and a set of empty  [2.x.49]  of subshapes, which will be filled with all compound shapes found in the given topological shape:
* 

* 
* [1.x.45]
* 
*  The next few steps are more familiar, and allow us to import an existing mesh from an external VTK file, and convert it to a deal triangulation.
* 

* 
* [1.x.46]
* 
*  We output this initial mesh saving it as the refinement step 0.
* 

* 
* [1.x.47]
* 
*  The mesh imported has a single, two-dimensional cell located in three-dimensional space. We now want to ensure that it is refined according to the CAD geometry imported above. This this end, we get an iterator to that cell and assign to it the manifold_id 1 (see  [2.x.50]  "this glossary entry"). We also get an iterator to its four faces, and assign each of them the manifold_id 2:
* 

* 
* [1.x.48]
* 
*  Once both the CAD geometry and the initial mesh have been imported and digested, we use the CAD surfaces and curves to define the projectors and assign them to the manifold ids just specified.
* 

* 
*  A first projector is defined using the single wire contained in our CAD file.  The ArclengthProjectionLineManifold will make sure that every mesh edge located on the wire is refined with a point that lies on the wire and splits it into two equal arcs lying between the edge vertices. We first check that the wires vector contains at least one element and then create a Manifold object for it.     
*   Once the projector is created, we then assign it to all the parts of the triangulation with manifold_id = 2:
* 

* 
* [1.x.49]
* 
*  The surface projector is created according to what is specified with the  [2.x.51]  option of the constructor. In particular, if the surface_projection_kind value equals  [2.x.52]  we select the  [2.x.53]  The new mesh points will then initially be generated at the barycenter of the cell/edge considered, and then projected on the CAD surface along its normal direction.  The NormalProjectionManifold constructor only needs a shape and a tolerance, and we then assign it to the triangulation for use with all parts that manifold having id 1:
* 

* 
* [1.x.50]
* 
*   [2.x.54]  surface_projection_kind value is  [2.x.55]  we select the  [2.x.56]  class. The new mesh points will then initially be generated at the barycenter of the cell/edge considered, and then projected on the CAD surface along a direction that is specified to the  [2.x.57]  constructor. In this case, the projection is done along the y-axis.
* 

* 
* [1.x.51]
* 
*  As a third option, if  [2.x.58]  value is  [2.x.59]  we select the  [2.x.60]  The new mesh points will again initially be generated at the barycenter of the cell/edge considered, and then projected on the CAD surface along a direction that is an estimate of the mesh normal direction. The  [2.x.61]  constructor only requires a shape (containing at least a face) and a tolerance.
* 

* 
* [1.x.52]
* 
*  Finally, we use good software cleanliness by ensuring that this really covers all possible options of the  [2.x.62]  statement. If we get any other value, we simply abort the program:
* 

* 
* [1.x.53]
* 
*   [1.x.54]  [1.x.55]
* 

* 
*  This function globally refines the mesh. In other tutorials, it would typically also distribute degrees of freedom, and resize matrices and vectors. These tasks are not carried out here, since we are not running any simulation on the Triangulation produced.   
*   While the function looks innocent, this is where most of the work we are interested in for this tutorial program actually happens. In particular, when refining the quads and lines that define the surface of the ship's hull, the Triangulation class will ask the various objects we have assigned to handle individual manifold ids for where the new vertices should lie.
* 

* 
* [1.x.56]
* 
*   [1.x.57]  [1.x.58]
* 

* 
*  Outputting the results of our computations is a rather mechanical task. All the components of this function have been discussed before:
* 

* 
* [1.x.59]
* 
*   [1.x.60]  [1.x.61]
* 

* 
*  This is the main function. It should be self explanatory in its briefness:
* 

* 
* [1.x.62]
* 
*   [1.x.63]  [1.x.64]
* 

* 
*  This is the main function of this program. It is in its basic structure like all previous tutorial programs, but runs the main class through the three possibilities of new vertex placement:
* 

* 
* [1.x.65]
* [1.x.66][1.x.67]
* 

* The program execution produces a series of mesh files  [2.x.63] that we can visualize with any of the usual visualization programs that can read the VTKfile format.
* The following table illustrates the results obtained employing the normal projection strategy. The first tworows of the table show side views of the grids obtained for progressive levelsof refinement, overlain on a very fine rendering of the exact geometry. Thedark and light red areas simply indicate whether the current mesh or the finegeometry is closer to the observer; the distinction does not carry anyparticularly deep meaning. The last rowof pictures depict front views (mirrored to both sides of the geometry) of thesame grids shown in the second row.
* 

*  [2.x.64] 
* As can be seen in the pictures---and as we anticipated---the normal refinement strategy is unable to produce nicely shaped elementswhen applied to surfaces with significant curvature changes. This isparticularly apparent at the bulb of the hull where all new points have beenplaced in the upper part of the bulb and the lower part remains completelyunresolved.
* The following table, which is arranged as the previous one, illustratesthe results obtained adopting the directional projection approach, in which the projection direction selected was the y-axis (whichis indicated with a small yellow arrow at the bottom left of each image).
* 

*  [2.x.65] 
* The images confirm that the quality of the mesh obtained with a directional projection is sensibly higher than that obtained projecting along thesurface normal. Yet, a number of elements elongated in the y-direction are observed around the bottom of the bulb, where the surface is almost parallel to thedirection chosen for the projection.
* The final test shows results using instead the projection normal to the faces:
*  [2.x.66] 
* The pictures confirm that the normal to mesh projection approach leads to grids that remain evenly spacedthroughtout the refinement steps. At the same time, these meshes represent rather well the original geometry even in the bottom regionof the bulb, which is not well recovered employing the directional projector or the normal projector.
* 

* [1.x.68][1.x.69] [2.x.67] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-55_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] ,  [2.x.2] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19]
*  [2.x.3] 
* [1.x.20]
* 

* 
*  [2.x.4]  As a prerequisite of this program, you need to have PETSc or Trilinosand the p4est library installed. The installation of deal.II together withthese additional libraries is described in the [1.x.21] file.
* [1.x.22][1.x.23][1.x.24]
* 

* Building on  [2.x.5] , this tutorial shows how to solve linear PDEs with severalcomponents in parallel using MPI with PETSc or Trilinos for the linearalgebra. For this, we return to the Stokes equations as discussed in [2.x.6] . The motivation for writing this tutorial is to provide anintermediate step (pun intended) between  [2.x.7]  (parallel Laplace) and [2.x.8]  (parallel coupled Stokes with Boussinesq for a time dependentproblem).
* The learning outcomes for this tutorial are:
* 
*  - You are able to solve PDEs with several variables in parallel and can  apply this to different problems.
* 
*  - You understand the concept of optimal preconditioners and are able to check  this for a particular problem.
* 
*  - You are able to construct manufactured solutions using the free computer  algreba system SymPy (https://sympy.org).
* 
*  - You can implement various other tasks for parallel programs: error  computation, writing graphical output, etc.
* 
*  - You can visualize vector fields, stream lines, and contours of vector  quantities.
* We are solving for a velocity  [2.x.9]  and pressure  [2.x.10]  that satisfy theStokes equation, which reads[1.x.25]
* 
* 

* [1.x.26][1.x.27]
* 

* Make sure that you read (even better: try) what is described in "Block Schurcomplement preconditioner" in the "Possible Extensions" section in  [2.x.11] .Like described there, we are going to solve the block system using a Krylovmethod and a block preconditioner.
* Our goal here is to construct a very simple (maybe the simplest?) optimalpreconditioner for the linear system. A preconditioner is called "optimal" or"of optimal complexity", if the number of iterations of the preconditionedsystem is independent of the mesh size  [2.x.12] . You can extend that definition toalso require indepence of the number of processors used (we will discuss thatin the results section), the computational domain and the mesh quality, thetest case itself, the polynomial degree of the finite element space, and more.
* Why is a constant number of iterations considered to be "optimal"? Assume thediscretized PDE gives a linear system with N unknowns. Because the matrixcoming from the FEM discretization is sparse, a matrix-vector product can bedone in O(N) time. A preconditioner application can also only be O(N) at best(for example doable with multigrid methods). If the number of iterationsrequired to solve the linear system is independent of  [2.x.13]  (and therefore N),the total cost of solving the system will be O(N). It is not possible to beatthis complexity, because even looking at all the entries of the right-handside already takes O(N) time. For more information see  [2.x.14] ,Chapter 2.5 (Multigrid).
* The preconditioner described here is even simpler than the one described in [2.x.15]  and will typically require more iterations and consequently time tosolve. When considering preconditioners, optimality is not the only importantmetric. But an optimal and expensive preconditioner is typically moredesirable than a cheaper, non-optimal one. This is because, eventually, as themesh size becomes smaller and smaller and linear problems become bigger andbigger, the former will eventually beat the latter.
* [1.x.28][1.x.29]
* 

* We precondition the linear system[1.x.30]
* 
* with the block diagonal preconditioner[1.x.31]
* where  [2.x.16]  is the Schur complement.
* With this choice of  [2.x.17] , assuming that we handle  [2.x.18]  and  [2.x.19]  exactly(which is an "idealized" situation), the preconditioned linear system hasthree distinct eigenvalues independent of  [2.x.20]  and is therefore "optimal".  Seesection 6.2.1 (especially p. 292) in  [2.x.21] . For comparison,using the ideal version of the upper block-triangular preconditioner in [2.x.22]  (also used in  [2.x.23] ) would have all eigenvalues be equal to one.
* We will use approximations of the inverse operations in  [2.x.24]  that are(nearly) independent of  [2.x.25] . In this situation, one can again show, that theeigenvalues are independent of  [2.x.26] . For the Krylov method we choose MINRES,which is attractive for the analysis (iteration count is proven to beindependent of  [2.x.27] , see the remainder of the chapter 6.2.1 in the bookmentioned above), great from the computational standpoint (simpler and cheaperthan GMRES for example), and applicable (matrix and preconditioner aresymmetric).
* For the approximations we will use a CG solve with the mass matrix in thepressure space for approximating the action of  [2.x.28] . Note that the massmatrix is spectrally equivalent to  [2.x.29] . We can expect the number of CGiterations to be independent of  [2.x.30] , even with a simple preconditioner likeILU.
* For the approximation of the velocity block  [2.x.31]  we will perform a single AMGV-cycle. In practice this choice is not exactly independent of  [2.x.32] , which canexplain the slight increase in iteration numbers. A possible explanation isthat the coarsest level will be solved exactly and the number of levels andsize of the coarsest matrix is not predictable.
* 

* [1.x.32][1.x.33]
* 

* We will construct a manufactured solution based on the classical Kovasznay problem,see  [2.x.33] . Hereis an image of the solution colored by the x velocity includingstreamlines of the velocity:
*   [2.x.34] 
* We have to cheat here, though, because we are not solving the non-linearNavier-Stokes equations, but the linear Stokes system without convectiveterm. Therefore, to recreate the exact same solution, we use the method ofmanufactured solutions with the solution of the Kovasznay problem. This willeffectively move the convective term into the right-hand side  [2.x.35] .
* The right-hand side is computed using the script "reference.py" and we usethe exact solution for boundary conditions and error computation.
* 

*  [1.x.34] [1.x.35]
*  

* 
* [1.x.36]
* 
*  The following chunk out code is identical to  [2.x.36]  and allows switching between PETSc and Trilinos:
* 

* 
*  

* 
* [1.x.37]
* 
*   [1.x.38]  [1.x.39]
* 

* 
*  We need a few helper classes to represent our solver strategy described in the introduction.
* 

* 
*  

* 
* [1.x.40]
* 
*  This class exposes the action of applying the inverse of a giving matrix via the function  [2.x.37]  Internally, the inverse is not formed explicitly. Instead, a linear solver with CG is performed. This class extends the InverseMatrix class in  [2.x.38]  with an option to specify a preconditioner, and to allow for different vector types in the vmult function.
* 

* 
* [1.x.41]
* 
*  The class A template class for a simple block diagonal preconditioner for 2x2 matrices.
* 

* 
* [1.x.42]
* 
*   [1.x.43]  [1.x.44]
* 

* 
*  The following classes represent the right hand side and the exact solution for the test problem.
* 

* 
*  

* 
* [1.x.45]
* 
*   [1.x.46]  [1.x.47]   
*   The main class is very similar to  [2.x.39] , except that matrices and vectors are now block versions, and we store a  [2.x.40]  for owned and relevant DoFs instead of a single IndexSet. We have exactly two IndexSets, one for all velocity unknowns and one for all pressure unknowns.
* 

* 
* [1.x.48]
* 
*  The Kovasnay flow is defined on the domain [-0.5, 1.5]^2, which we create by passing the min and max values to  [2.x.41] 
* 

* 
* [1.x.49]
* 
*   [1.x.50]  [1.x.51]   
*   The construction of the block matrices and vectors is new compared to  [2.x.42]  and is different compared to serial codes like  [2.x.43] , because we need to supply the set of rows that belong to our processor.
* 

* 
* [1.x.52]
* 
*  Put all dim velocities into block 0 and the pressure into block 1, then reorder the unknowns by block. Finally count how many unknowns we have per block.
* 

* 
* [1.x.53]
* 
*  We split up the IndexSet for locally owned and locally relevant DoFs into two IndexSets based on how we want to create the block matrices and vectors.
* 

* 
* [1.x.54]
* 
*  Setting up the constraints for boundary conditions and hanging nodes is identical to  [2.x.44] . Even though we don't have any hanging nodes because we only perform global refinement, it is still a good idea to put this function call in, in case adaptive refinement gets introduced later.
* 

* 
* [1.x.55]
* 
*  Now we create the system matrix based on a BlockDynamicSparsityPattern. We know that we won't have coupling between different velocity components (because we use the laplace and not the deformation tensor) and no coupling between pressure with its test functions, so we use a Table to communicate this coupling information to  [2.x.45] 
* 

* 
* [1.x.56]
* 
*  The preconditioner matrix has a different coupling (we only fill in the 1,1 block with the mass matrix), otherwise this code is identical to the construction of the system_matrix above.
* 

* 
* [1.x.57]
* 
*  owned_partitioning,
* 

* 
* [1.x.58]
* 
*  Finally, we construct the block vectors with the right sizes. The function call with two  [2.x.46]  will create a ghosted vector.
* 

* 
* [1.x.59]
* 
*   [1.x.60]  [1.x.61]   
*   This function assembles the system matrix, the preconditioner matrix, and the right hand side. The code is pretty standard.
* 

* 
* [1.x.62]
* 
*   [1.x.63]  [1.x.64]   
*   This function solves the linear system with MINRES with a block diagonal preconditioner and AMG for the two diagonal blocks as described in the introduction. The preconditioner applies a v cycle to the 0,0 block and a CG with the mass matrix for the 1,1 block (the Schur complement).
* 

* 
* [1.x.65]
* 
*  The InverseMatrix is used to solve for the mass matrix:
* 

* 
* [1.x.66]
* 
*  This constructs the block preconditioner based on the preconditioners for the individual blocks defined above.
* 

* 
* [1.x.67]
* 
*  With that, we can finally set up the linear solver and solve the system:
* 

* 
* [1.x.68]
* 
*  Like in  [2.x.47] , we subtract the mean pressure to allow error computations against our reference solution, which has a mean value of zero.
* 

* 
* [1.x.69]
* 
*   [1.x.70]  [1.x.71]   
*   The remainder of the code that deals with mesh refinement, output, and the main loop is pretty standard.
* 

* 
* [1.x.72]
* [1.x.73][1.x.74]
* 

* As expected from the discussion above, the number of iterations is independentof the number of processors and only very slightly dependent on  [2.x.48] :
*  [2.x.49] 
*  [2.x.50] 
* While the PETSc results show a constant number of iterations, the iterationsincrease when using Trilinos. This is likely because of the different settingsused for the AMG preconditioner. For performance reasons we do not allowcoarsening below a couple thousand unknowns. As the coarse solver is an exactsolve (we are using LU by default), a change in number of levels willinfluence the quality of a V-cycle. Therefore, a V-cycle is closer to an exactsolver for smaller problem sizes.
* [1.x.75][1.x.76][1.x.77]
* 

* [1.x.78][1.x.79]
* 

* Play with the smoothers, smoothing steps, and other properties for theTrilinos AMG to achieve an optimal preconditioner.
* [1.x.80][1.x.81]
* 

* This change requires changing the outer solver to GMRES or BiCGStab, becausethe system is no longer symmetric.
* You can prescribe the exact flow solution as  [2.x.51]  in the convective term  [2.x.52] . This should give the same solution as the original problem,if you set the right hand side to zero.
* [1.x.82][1.x.83]
* 

* So far, this tutorial program refines the mesh globally in each step.Replacing the code in  [2.x.53]  by something like
* [1.x.84]
* makes it simple to explore adaptive mesh refinement.
* 

* [1.x.85][1.x.86] [2.x.54] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-56_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] ,  [2.x.2] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28]
* [1.x.29]
*  [2.x.3] 
* [1.x.30][1.x.31][1.x.32]
* 

* [1.x.33][1.x.34]
* 

* The purpose of this tutorial is to create an efficient linear solver for theStokes equation and compare it to alternative approaches.  Here, we will useFGMRES with geometric multigrid as a preconditioner velocity block, and wewill show in the results section that this is a fundamentally better approachthan the linear solvers used in  [2.x.4]  (including the scheme described in"Possible Extensions").  Fundamentally, this is because only with multigrid itis possible to get  [2.x.5]  solve time, where  [2.x.6]  is the number of unknowns ofthe linear system. Using the Timer class, we collect some statistics tocompare setup times, solve times, and number of iterations. We also computeerrors to make sure that what we have implemented is correct.
* Let  [2.x.7] and  [2.x.8] . The Stokes equations read as follows in non-dimensionalized form:
* [1.x.35]
* 
* Note that we are using the deformation tensor instead of  [2.x.9]  (adetailed description of the difference between the two can be found in [2.x.10] , but in summary, the deformation tensor is more physical aswell as more expensive).
* [1.x.36][1.x.37]
* 

* The weak form ofthe discrete equations naturally leads to the following linear systemfor the nodal values of the velocity and pressure fields:[1.x.38]
* 
* Our goal is to compare several solution approaches.  While  [2.x.11] solves the linear system using a "Schur complement approach" in twoseparate steps, we instead attack theblock system at once using FMGRES with an efficientpreconditioner, in the spirit of the approach outlined in the "Results"section of  [2.x.12] . The idea is as follows: if we find a blockpreconditioner  [2.x.13]  such that the matrix
* [1.x.39]
* 
* is simple, then an iterative solver with that preconditioner willconverge in a few iterations. Notice that we are doing rightpreconditioning here.  Using the Schur complement  [2.x.14] ,we find that
* [1.x.40]
* 
* is a good choice. Let  [2.x.15]  be an approximation of  [2.x.16] and  [2.x.17]  of  [2.x.18] , we see[1.x.41]
* 
* Since  [2.x.19]  is aimed to be a preconditioner only, we shall usethe approximations on the right in the equation above.
* As discussed in  [2.x.20] ,  [2.x.21] , where  [2.x.22]  is the pressure mass matrix and is solved approximately by using CGwith ILU as a preconditioner, and  [2.x.23]  is obtained by one ofmultiple methods: solving a linear system with CG and ILU aspreconditioner, just using one application of an ILU, solving a linearsystem with CG and GMG (GeometricMultigrid as described in  [2.x.24] ) as a preconditioner, or just performing a single V-cycleof GMG.
* As a comparison, instead of FGMRES, we also use the direct solverUMFPACK on the whole system to compare our results with.  If you want to usea direct solver (like UMFPACK), the system needs to be invertible. To avoidthe one dimensional null space given by the constant pressures, we fix the first pressure unknown to zero. This is not necessary for the iterative solvers.
* 

* [1.x.42][1.x.43]
* 

* The test problem is a "Manufactured Solution" (see  [2.x.25]  fordetails), and we choose  [2.x.26]  and  [2.x.27] .We apply Dirichlet boundary conditions for the velocity on the wholeboundary of the domain  [2.x.28] .To enforce the boundary conditions we can just use our reference solution.
* If you look up in the deal.II manual what is needed to create a classderived from  [2.x.29] , you will find that thisclass has numerous  [2.x.30]  functions, including [2.x.31]   [2.x.32]   [2.x.33] etc., all of which can be overloaded.  Different parts of deal.IIwill require different ones of these particularfunctions. This can be confusing at first, but luckily the only thingyou actually have to implement is  [2.x.34]   The other virtualfunctions in the Function class have defaultimplementations inside that will call your implementation of  [2.x.35] by default.
* Notice that our reference solution fulfills  [2.x.36] . Inaddition, the pressure is chosen to have a mean value of zero.  Forthe "Method of Manufactured Solutions" of  [2.x.37] , we need to find  [2.x.38]  such that:
* [1.x.44]
* 
* Using the reference solution above, we obtain:
* [1.x.45]
* 
* [1.x.46][1.x.47]
* 

* Because we do not enforce the meanpressure to be zero for our numerical solution in the linear system,we need to post process the solution after solving. To do this we usethe  [2.x.39]  function to compute the mean valueof the pressure to subtract it from the pressure.
* 

* [1.x.48][1.x.49]
* 

* The way we implement geometric multigrid here only executes it on thevelocity variables (i.e., the  [2.x.40]  matrix described above) but not thepressure. One could implement this in different ways, including one inwhich one considers all coarse grid operations as acting on  [2.x.41]  block systems where we only consider the top leftblock. Alternatively, we can implement things by really onlyconsidering a linear system on the velocity part of the overall finiteelement discretization. The latter is the way we want to use here.
* To implement this, one would need to be able to ask questions such as"May I have just part of a DoFHandler?". This is not possible at thetime when this program was written, so in order to answer this requestfor our needs, we simply create a separate, second DoFHandler for just thevelocities. We then build linear systems for the multigridpreconditioner based on only this second DoFHandler, and simplytransfer the first block of (overall) vectors into correspondingvectors for the entire second DoFHandler. To make this work, we haveto assure that the [1.x.50] in which the (velocity) degrees of freedom areordered in the two DoFHandler objects is the same. This is in fact thecase by first distributing degrees of freedom on both, and then usingthe same sequence of DoFRenumbering operations on both.
* 

* [1.x.51][1.x.52]
* 

* The main difference between  [2.x.42]  and  [2.x.43]  is that we use blocksolvers instead of the Schur Complement approach used in [2.x.44] . Details of this approach can be found under the "Block Schurcomplement preconditioner" subsection of the "Possible Extensions"section of  [2.x.45] . For the preconditioner of the velocity block, weborrow a class from [1.x.53]called  [2.x.46]  that has the option to solve forthe inverse of  [2.x.47]  or just apply one preconditioner sweep for itinstead, which provides us with an expensive and cheap approach,respectively.
* 

*  [1.x.54] [1.x.55]
*   [1.x.56]  [1.x.57]
* 

* 
*  

* 
* [1.x.58]
* 
*  We need to include the following file to do timings:
* 

* 
* [1.x.59]
* 
*  This includes the files necessary for us to use geometric Multigrid
* 

* 
* [1.x.60]
* 
*  In order to make it easy to switch between the different solvers that are being used, we declare an enum that can be passed as an argument to the constructor of the main class.
* 

* 
* [1.x.61]
* 
*   [1.x.62]  [1.x.63]   
*   The class Solution is used to define the boundary conditions and to compute errors of the numerical solution. Note that we need to define the values and gradients in order to compute L2 and H1 errors. Here we decided to separate the implementations for 2d and 3d using template specialization.   
*   Note that the first dim components are the velocity components and the last is the pressure.
* 

* 
* [1.x.64]
* 
*  Note that for the gradient we need to return a Tensor<1,dim>
* 

* 
* [1.x.65]
* 
*  Implementation of  [2.x.48] . See the introduction for more information.
* 

* 
* [1.x.66]
* 
*   [1.x.67]  [1.x.68]
* 

* 
*  In the following, we will implement a preconditioner that expands on the ideas discussed in the Results section of  [2.x.49] . Specifically, we 1. use an upper block-triangular preconditioner because we want to use right preconditioning. 2. optionally allow using an inner solver for the velocity block instead of a single preconditioner application. 3. do not use InverseMatrix but explicitly call SolverCG. This approach is also used in the ASPECT code (see https://aspect.geodynamics.org) that solves the Stokes equations in the context of simulating convection in the earth mantle, and which has been used to solve problems on many thousands of processors.   
*   The bool flag  [2.x.50]  in the constructor allows us to either apply the preconditioner for the velocity block once or use an inner iterative solver for a more accurate approximation instead.   
*   Notice how we keep track of the sum of the inner iterations (preconditioner applications).
* 

* 
* [1.x.69]
* 
*  First solve with the approximation for S
* 

* 
* [1.x.70]
* 
*  Second, apply the top right block (B^T)
* 

* 
* [1.x.71]
* 
*  Finally, either solve with the top left block or just apply one preconditioner sweep
* 

* 
* [1.x.72]
* 
*   [1.x.73]  [1.x.74]   
*   This is the main class of the problem.
* 

* 
* [1.x.75]
* 
*  Finite element for the velocity only:
* 

* 
* [1.x.76]
* 
*  Finite element for the whole system:
* 

* 
* [1.x.77]
* 
*   [1.x.78]  [1.x.79]
* 

* 
*  This function sets up the DoFHandler, matrices, vectors, and Multigrid structures (if needed).
* 

* 
* [1.x.80]
* 
*  The main DoFHandler only needs active DoFs, so we are not calling distribute_mg_dofs() here
* 

* 
* [1.x.81]
* 
*  This block structure separates the dim velocity components from the pressure component (used for reordering). Note that we have 2 instead of dim+1 blocks like in  [2.x.51] , because our FESystem is nested and the dim velocity components appear as one block.
* 

* 
* [1.x.82]
* 
*  Velocities start at component 0:
* 

* 
* [1.x.83]
* 
*  ILU behaves better if we apply a reordering to reduce fillin. There is no advantage in doing this for the other solvers.
* 

* 
* [1.x.84]
* 
*  This ensures that all velocities DoFs are enumerated before the pressure unknowns. This allows us to use blocks for vectors and matrices and allows us to get the same DoF numbering for dof_handler and velocity_dof_handler.
* 

* 
* [1.x.85]
* 
*  This distributes the active dofs and multigrid dofs for the velocity space in a separate DoFHandler as described in the introduction.
* 

* 
* [1.x.86]
* 
*  The following block of code initializes the MGConstrainedDofs (using the boundary conditions for the velocity), and the sparsity patterns and matrices for each level. The resize() function of MGLevelObject<T> will destroy all existing contained objects.
* 

* 
* [1.x.87]
* 
*  The following makes use of a component mask for interpolation of the boundary values for the velocity only, which is further explained in the vector valued dealii  [2.x.52]  tutorial.
* 

* 
* [1.x.88]
* 
*  As discussed in the introduction, we need to fix one degree of freedom of the pressure variable to ensure solvability of the problem. We do this here by marking the first pressure dof, which has index n_u as a constrained dof.
* 

* 
* [1.x.89]
* 
*   [1.x.90]  [1.x.91]
* 

* 
*  In this function, the system matrix is assembled. We assemble the pressure mass matrix in the (1,1) block (if needed) and move it out of this location at the end of this function.
* 

* 
* [1.x.92]
* 
*  If true, we will assemble the pressure mass matrix in the (1,1) block:
* 

* 
* [1.x.93]
* 
*   [1.x.94]  [1.x.95]
* 

* 
*  Here, like in  [2.x.53] , we have a function that assembles the level and interface matrices necessary for the multigrid preconditioner.
* 

* 
* [1.x.96]
* 
*  This iterator goes over all cells (not just active)
* 

* 
* [1.x.97]
* 
*   [1.x.98]  [1.x.99]
* 

* 
*  This function sets up things differently based on if you want to use ILU or GMG as a preconditioner.  Both methods share the same solver (FGMRES) but require a different preconditioner to be initialized. Here we time not only the entire solve function, but we separately time the setup of the preconditioner as well as the solve itself.
* 

* 
* [1.x.100]
* 
*  Here we must make sure to solve for the residual with "good enough" accuracy
* 

* 
* [1.x.101]
* 
*  This is used to pass whether or not we want to solve for A inside the preconditioner.  One could change this to false to see if there is still convergence and if so does the program then run faster or slower
* 

* 
* [1.x.102]
* 
*  Transfer operators between levels
* 

* 
* [1.x.103]
* 
*  Setup coarse grid solver
* 

* 
* [1.x.104]
* 
*  Multigrid, when used as a preconditioner for CG, needs to be a symmetric operator, so the smoother must be symmetric
* 

* 
* [1.x.105]
* 
*  Now, we are ready to set up the V-cycle operator and the multilevel preconditioner.
* 

* 
* [1.x.106]
* 
*   [1.x.107]  [1.x.108]
* 

* 
*  This function computes the L2 and H1 errors of the solution. For this, we need to make sure the pressure has mean zero.
* 

* 
* [1.x.109]
* 
*  Compute the mean pressure  [2.x.54]  and then subtract it from each pressure coefficient. This will result in a pressure with mean value zero. Here we make use of the fact that the pressure is component  [2.x.55]  and that the finite element space is nodal.
* 

* 
* [1.x.110]
* 
*   [1.x.111]  [1.x.112]
* 

* 
*  This function generates graphical output like it is done in  [2.x.56] .
* 

* 
* [1.x.113]
* 
*   [1.x.114]  [1.x.115]
* 

* 
*  The last step in the Stokes class is, as usual, the function that generates the initial grid and calls the other functions in the respective order.
* 

* 
* [1.x.116]
* 
*   [1.x.117]  [1.x.118]
* 

* 
* [1.x.119]
* 
*  options for SolverType: UMFPACK FGMRES_ILU FGMRES_GMG
* 

* 
* [1.x.120]
* [1.x.121][1.x.122]
* 

* [1.x.123][1.x.124]
* 

* We first run the code and confirm that the finite element solution convergeswith the correct rates as predicted by the error analysis of mixed finiteelement problems. Given sufficiently smooth exact solutions  [2.x.57]  and  [2.x.58] ,the errors of the Taylor-Hood element  [2.x.59]  should be
* [1.x.125]
* see for example Ern/Guermond "Theory and Practice of Finite Elements", Section4.2.5 p195. This is indeed what we observe, using the  [2.x.60] element as an example (this is what is done in the code, but is easilychanged in  [2.x.61] ):
*  [2.x.62] 
* [1.x.126][1.x.127]
* 

* Let us compare the direct solver approach using UMFPACK to the twomethods in which we choose  [2.x.63]  and [2.x.64]  by solving linear systems with  [2.x.65]  usingCG. The preconditioner for CG is then either ILU or GMG.The following table summarizes solver iterations, timings, and virtualmemory (VM) peak usage:
*  [2.x.66] 
* As can be seen from the table:
* 1. UMFPACK uses large amounts of memory, especially in 3d. Also, UMFPACKtimings do not scale favorably with problem size.
* 2. Because we are using inner solvers for  [2.x.67]  and  [2.x.68] , ILU and GMG require thesame number of outer iterations.
* 3. The number of (inner) iterations for  [2.x.69]  increases for ILU with refinement, leadingto worse than linear scaling in solve time. In contrast, the number of inneriterations for  [2.x.70]  stays constant with GMG leading to nearly perfect scaling insolve time.
* 4. GMG needs slightly more memory than ILU to store the level and interfacematrices.
* [1.x.128][1.x.129]
* 

* [1.x.130][1.x.131]
* 

* Experiment with higher order stable FE pairs and check that you observe thecorrect convergence rates.
* [1.x.132][1.x.133]
* 

* The introduction also outlined another option to precondition theoverall system, namely one in which we do not choose  [2.x.71]  as in the table above, but in which [2.x.72]  is only a single preconditioner application withGMG or ILU, respectively.
* This is in fact implemented in the code: Currently, the boolean [2.x.73]  is set to  [2.x.74]  Theoption mentioned above is obtained by setting it to  [2.x.75] 
* What you will find is that the number of FGMRES iterations staysconstant under refinement if you use GMG this way. This means that theMultigrid is optimal and independent of  [2.x.76] .
* 

* [1.x.134][1.x.135] [2.x.77] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-57_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] ,  [2.x.2] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33]
*  [2.x.3] 
* [1.x.34]
*  [2.x.4] 
* [1.x.35][1.x.36][1.x.37]
* 

* [1.x.38][1.x.39]
* 

* In this tutorial we show how to solve the incompressible NavierStokes equations (NSE) with Newton's method. The flow we consider hereis assumed to be steady. In a domain  [2.x.5] ,  [2.x.6] , with a piecewise smooth boundary [2.x.7] , and a given force field  [2.x.8] , we seeka velocity field  [2.x.9]  and a pressure field  [2.x.10] satisfying[1.x.40]
* 
* Unlike the Stokes equations as discussed in  [2.x.11] , the NSE are anonlinear system of equations because of the convective term  [2.x.12] . The first step of computing a numerical solutionis to linearize the system and this will be done using Newton's method. Atime-dependent problem is discussed in  [2.x.13] , where the system is linearizedusing the solution from the last time step and no nonlinearsolve is necessary.
* [1.x.41][1.x.42]
* 

* We define a nonlinear function whose root is a solution to the NSE by[1.x.43]
* 
* Assuming the initial guess is good enough toguarantee the convergence of Newton's iteration and denoting [2.x.14] , Newton's iteration on a vector functioncan be defined as[1.x.44]
* 
* where  [2.x.15]  is the approximate solution in step  [2.x.16] , [2.x.17]  represents the solution from the previous step, and  [2.x.18]  is the Jacobian matrix evaluated at [2.x.19] .A similar iteration can be found in  [2.x.20] .
* The Newton iteration formula implies the newsolution is obtained by adding an update term to the old solution. Insteadof evaluating the Jacobian matrix and taking its inverse, we considerthe update term as a whole, that is[1.x.45]
* 
* where  [2.x.21] .
* We can find the update term by solving the system[1.x.46]
* 
* Here, the left of the previous equation represents thedirectional gradient of  [2.x.22]  along  [2.x.23]  at  [2.x.24] . By definition, the directional gradient is given by[1.x.47]
* 
* Therefore, we arrive at the linearized system:[1.x.48]
* 
* where  [2.x.25]  and  [2.x.26]  are the solutions from theprevious iteration. Additionally, theright hand side of the second equation is not zero since the discretesolution is not exactly divergence free (divergence free for the continuoussolution). The right hand side here acts as a correction which leads thediscrete solution of the velocity to be divergence free along Newton'siteration. In this linear system, the only unknowns are theupdate terms  [2.x.27]  and  [2.x.28] , and we can use asimilar strategy to the one used in  [2.x.29]  (and derive the weak form in thesame way).
* Now, Newton's iteration can be used to solve for the update terms:
*  [2.x.30]    [2.x.31] Initialization: Initial guess  [2.x.32]  and  [2.x.33] , tolerance  [2.x.34] ; [2.x.35]    [2.x.36] Linear solve to compute update term  [2.x.37]  and       [2.x.38] ; [2.x.39]    [2.x.40] Update the approximation:       [2.x.41]  and       [2.x.42] ; [2.x.43]    [2.x.44] Check residual norm:  [2.x.45] :       [2.x.46]          [2.x.47] If  [2.x.48] , STOP. [2.x.49]          [2.x.50] If  [2.x.51] , back to step 2. [2.x.52]        [2.x.53]  [2.x.54]  [2.x.55] 
* [1.x.49][1.x.50]
* 

* The initial guess needs to be close enough to the solution for Newton's methodto converge; hence, finding a good starting value is crucial to the nonlinearsolver.
* When the viscosity  [2.x.56]  is large, a good initial guess can be obtainedby solving the Stokes equation with viscosity  [2.x.57] . While problem dependent,this works for  [2.x.58]  for the test problem considered here.
* However, the convective term  [2.x.59]  will bedominant if the viscosity is small, like  [2.x.60]  in test case 2.  In thissituation, we use a continuation method to set up a series of auxiliary NSEs withviscosity approaching the one in the target NSE. Correspondingly, we create asequence  [2.x.61]  with  [2.x.62] , and accept that the solutions totwo NSE with viscosity  [2.x.63]  and  [2.x.64]  are close if  [2.x.65]  is small.  Then we use the solution to the NSE with viscosity [2.x.66]  as the initial guess of the NSE with  [2.x.67] . This can be thought ofas a staircase from the Stokes equations to the NSE we want to solve.
* That is, we first solve a Stokes problem[1.x.51]
* 
* to get the initial guess for[1.x.52]
* 
* which also acts as the initial guess of the continuation method.Here  [2.x.68]  is relatively large so that the solution to the Stokes problem with viscosity  [2.x.69] can be used as an initial guess for the NSE in Newton's iteration.
* Then the solution to[1.x.53]
* 
* acts as the initial guess for[1.x.54]
* 
* This process is repeated with a sequence of viscosities  [2.x.70]  that isdetermined experimentally so that the final solution can used as a startingguess for the Newton iteration.
* [1.x.55][1.x.56]
* 

* At each step of Newton's iteration, the problem results in solving asaddle point systems of the form[1.x.57]
* 
* This system matrix has the same block structure as the one in  [2.x.71] . However,the matrix  [2.x.72]  at the top left corner is not symmetric because of the nonlinear term.Instead of solving the above system, we can solve the equivalent system[1.x.58]
* 
* with a parameter  [2.x.73]  and an invertible matrix  [2.x.74] . Here [2.x.75]  is the Augmented Lagrangian term; see [1] for details.
* Denoting the system matrix of the new system by  [2.x.76]  and the right-handside by  [2.x.77] , we solve it iteratively with right preconditioning [2.x.78]  as  [2.x.79] , where[1.x.59]
* 
* with  [2.x.80]  and  [2.x.81]  is thecorresponding Schur complement  [2.x.82] . Welet  [2.x.83]  where  [2.x.84]  is the pressure mass matrix, then [2.x.85]  can be approximated by[1.x.60]
* 
* See [1] for details.
* We decompose  [2.x.86]  as[1.x.61]
* 
* Here two inexact solvers will be needed for  [2.x.87]  and [2.x.88] , respectively (see [1]). Since the pressure massmatrix is symmetric and positive definite,CG with ILU as a preconditioner is appropriate to use for  [2.x.89] . For simplicity, we usethe direct solver UMFPACK for  [2.x.90] . The last ingredient is a sparsematrix-vector product with  [2.x.91] . Instead of computing the matrix productin the augmented Lagrangian term in  [2.x.92] , we assemble Grad-Div stabilization [2.x.93] , as explained in [2].
* [1.x.62][1.x.63]
* 

* We use the lid driven cavity flow as our test case; see [3] for details.The computational domain is the unit square and the right-hand side is [2.x.94] . The boundary condition is[1.x.64]
* 
* When solving this problem, the error consists of the nonlinear error (fromNewton's iteration) and the discretization error (dependent on mesh size). Thenonlinear part decreases with each Newton iteration and the discretization errorreduces with mesh refinement. In this example, the solution from the coarsemesh is transferred to successively finer meshes and used as an initialguess. Therefore, the nonlinear error is always brought below the tolerance ofNewton's iteration and the discretization error is reduced with each meshrefinement.
* Inside the loop, we involve three solvers: one for  [2.x.95] ,one for  [2.x.96]  and one for  [2.x.97] . The first twosolvers are invoked in the preconditioner and the outer solver gives usthe update term. Overall convergence is controlled by the nonlinear residual;as Newton's method does not require an exact Jacobian, we employ FGMRES with arelative tolerance of only 1e-4 for the outer linear solver. In fact,we use the truncated Newton solve for this system.As described in  [2.x.98] , the inner linear solves are also not requiredto be done very accurately. Here we use CG with a relativetolerance of 1e-6 for the pressure mass matrix. As expected, we still see convergenceof the nonlinear residual down to 1e-14. Also, we use a simple linesearch algorithm for globalization of the Newton method.
* The cavity reference values for  [2.x.99]  and  [2.x.100]  arefrom [4] and [5], respectively, where  [2.x.101]  is the Reynolds number andcan be located at [8]. Here the viscosity is defined by  [2.x.102] .Even though we can still find a solution for  [2.x.103]  and thereferences contain results for comparison, we limit our discussion here to [2.x.104] . This is because the solution is no longer stationarystarting around  [2.x.105]  but instead becomes periodic, see [7] fordetails.
* [1.x.65][1.x.66]
*  [2.x.106] 
*    [2.x.107]   An Augmented Lagrangian-Based Approach to the Oseen Problem, M. Benzi and M. Olshanskii, SIAM J. SCI. COMPUT. 2006   [2.x.108]   Efficient augmented Lagrangian-type preconditioning for the Oseen problem using Grad-Div stabilization, Timo Heister and Gerd Rapin   [2.x.109]   http://www.cfd-online.com/Wiki/Lid-driven_cavity_problem   [2.x.110]   High-Re solution for incompressible flow using the Navier-Stokes Equations and a Multigrid Method, U. Ghia, K. N. Ghia, and C. T. Shin   [2.x.111]   Numerical solutions of 2-D steady incompressible driven cavity flow at high Reynolds numbers, E. Erturk, T.C. Corke and C. Gokcol   [2.x.112]  Implicit Weighted ENO Schemes for the Three-Dimensional Incompressible Navier-Stokes Equations, Yang et al, 1998   [2.x.113]  The 2D lid-driven cavity problem revisited, C. Bruneau and M. Saad, 2006   [2.x.114]  https://en.wikipedia.org/wiki/Reynolds_number [2.x.115] 
* 

*  [1.x.67] [1.x.68]
*   [1.x.69]  [1.x.70]
* 

* 
*  As usual, we start by including some well-known files:
* 

* 
* [1.x.71]
* 
*  To transfer solutions between meshes, this file is included:
* 

* 
* [1.x.72]
* 
*  This file includes UMFPACK: the direct solver:
* 

* 
* [1.x.73]
* 
*  And the one for ILU preconditioner:
* 

* 
* [1.x.74]
* 
*   [1.x.75]  [1.x.76]
* 

* 
*  This class manages the matrices and vectors described in the introduction: in particular, we store a BlockVector for the current solution, current Newton update, and the line search update.  We also store two AffineConstraints objects: one which enforces the Dirichlet boundary conditions and one that sets all boundary values to zero. The first constrains the solution vector while the second constraints the updates (i.e., we never update boundary values, so we force the relevant update vector values to be zero).
* 

* 
* [1.x.77]
* 
*   [1.x.78]  [1.x.79]
* 

* 
*  In this problem we set the velocity along the upper surface of the cavity to be one and zero on the other three walls. The right hand side function is zero so we do not need to set the right hand side function in this tutorial. The number of components of the boundary function is  [2.x.116] . We will ultimately use  [2.x.117]  to set boundary values, which requires the boundary value functions to have the same number of components as the solution, even if all are not used. Put another way: to make this function happy we define boundary values for the pressure even though we will never actually use them.
* 

* 
* [1.x.80]
* 
*   [1.x.81]  [1.x.82]   
*   As discussed in the introduction, the preconditioner in Krylov iterative methods is implemented as a matrix-vector product operator. In practice, the Schur complement preconditioner is decomposed as a product of three matrices (as presented in the first section). The  [2.x.118]  in the first factor involves a solve for the linear system  [2.x.119] . Here we solve this system via a direct solver for simplicity. The computation involved in the second factor is a simple matrix-vector multiplication. The Schur complement  [2.x.120]  can be well approximated by the pressure mass matrix and its inverse can be obtained through an inexact solver. Because the pressure mass matrix is symmetric and positive definite, we can use CG to solve the corresponding linear system.
* 

* 
* [1.x.83]
* 
*  We can notice that the initialization of the inverse of the matrix at the top left corner is completed in the constructor. If so, every application of the preconditioner then no longer requires the computation of the matrix factors.
* 

* 
*  

* 
* [1.x.84]
* 
*   [1.x.85]  [1.x.86]
*  [1.x.87]  [1.x.88]   
*   The constructor of this class looks very similar to the one in  [2.x.121] . The only difference is the viscosity and the Augmented Lagrangian coefficient  [2.x.122] .
* 

* 
* [1.x.89]
* 
*   [1.x.90]  [1.x.91]   
*   This function initializes the DoFHandler enumerating the degrees of freedom and constraints on the current mesh.
* 

* 
* [1.x.92]
* 
*  The first step is to associate DoFs with a given mesh.
* 

* 
* [1.x.93]
* 
*  We renumber the components to have all velocity DoFs come before the pressure DoFs to be able to split the solution vector in two blocks which are separately accessed in the block preconditioner.
* 

* 
* [1.x.94]
* 
*  In Newton's scheme, we first apply the boundary condition on the solution obtained from the initial step. To make sure the boundary conditions remain satisfied during Newton's iteration, zero boundary conditions are used for the update  [2.x.123] . Therefore we set up two different constraint objects.
* 

* 
* [1.x.95]
* 
*   [1.x.96]  [1.x.97]   
*   On each mesh the SparsityPattern and the size of the linear system are different. This function initializes them after mesh refinement.
* 

* 
* [1.x.98]
* 
*   [1.x.99]  [1.x.100]   
*   This function builds the system matrix and right hand side that we currently work on. The  [2.x.124]  argument is used to determine which set of constraints we apply (nonzero for the initial step and zero for the others). The  [2.x.125]  argument determines whether to assemble the whole system or only the right hand side vector, respectively.
* 

* 
* [1.x.101]
* 
*  For the linearized system, we create temporary storage for present velocity and gradient, and present pressure. In practice, they are all obtained through their shape functions at quadrature points.
* 

* 
*  

* 
* [1.x.102]
* 
*  The assembly is similar to  [2.x.126] . An additional term with gamma as a coefficient is the Augmented Lagrangian (AL), which is assembled via grad-div stabilization.  As we discussed in the introduction, the bottom right block of the system matrix should be zero. Since the pressure mass matrix is used while creating the preconditioner, we assemble it here and then move it into a separate SparseMatrix at the end (same as in  [2.x.127] ).
* 

* 
* [1.x.103]
* 
*  Finally we move pressure mass matrix into a separate matrix:
* 

* 
* [1.x.104]
* 
*  Note that settings this pressure block to zero is not identical to not assembling anything in this block, because this operation here will (incorrectly) delete diagonal entries that come in from hanging node constraints for pressure DoFs. This means that our whole system matrix will have rows that are completely zero. Luckily, FGMRES handles these rows without any problem.
* 

* 
* [1.x.105]
* 
*   [1.x.106]  [1.x.107]   
*   In this function, we use FGMRES together with the block preconditioner, which is defined at the beginning of the program, to solve the linear system. What we obtain at this step is the solution vector. If this is the initial step, the solution vector gives us an initial guess for the Navier Stokes equations. For the initial step, nonzero constraints are applied in order to make sure boundary conditions are satisfied. In the following steps, we will solve for the Newton update so zero constraints are used.
* 

* 
* [1.x.108]
* 
*   [1.x.109]  [1.x.110]   
*   After finding a good initial guess on the coarse mesh, we hope to decrease the error through refining the mesh. Here we do adaptive refinement similar to  [2.x.128]  except that we use the Kelly estimator on the velocity only. We also need to transfer the current solution to the next mesh using the SolutionTransfer class.
* 

* 
* [1.x.111]
* 
*  First the DoFHandler is set up and constraints are generated. Then we create a temporary BlockVector  [2.x.129] , whose size is according with the solution on the new mesh.
* 

* 
* [1.x.112]
* 
*  Transfer solution from coarse to fine mesh and apply boundary value constraints to the new transferred solution. Note that present_solution is still a vector corresponding to the old mesh.
* 

* 
* [1.x.113]
* 
*  Finally set up matrix and vectors and set the present_solution to the interpolated data.
* 

* 
* [1.x.114]
* 
*   [1.x.115]  [1.x.116]   
*   This function implements the Newton iteration with given tolerance, maximum number of iterations, and the number of mesh refinements to do.   
*   The argument  [2.x.130]  tells us whether  [2.x.131]  is necessary, and which part, system matrix or right hand side vector, should be assembled. If we do a line search, the right hand side is already assembled while checking the residual norm in the last iteration. Therefore, we just need to assemble the system matrix at the current iteration. The last argument  [2.x.132]  determines whether or not graphical output should be produced.
* 

* 
* [1.x.117]
* 
*  To make sure our solution is getting close to the exact solution, we let the solution be updated with a weight  [2.x.133]  such that the new residual is smaller than the one of last step, which is done in the following loop. This is the same line search algorithm used in  [2.x.134] .
* 

* 
* [1.x.118]
* 
*   [1.x.119]  [1.x.120]   
*   This function will provide us with an initial guess by using a continuation method as we discussed in the introduction. The Reynolds number is increased  [2.x.135] by-step until we reach the target value. By experiment, the solution to Stokes is good enough to be the initial guess of NSE with Reynolds number 1000 so we start there.  To make sure the solution from previous problem is close enough to the next one, the step size must be small enough.
* 

* 
* [1.x.121]
* 
*   [1.x.122]  [1.x.123]   
*   This function is the same as in  [2.x.136]  except that we choose a name for the output file that also contains the Reynolds number (i.e., the inverse of the viscosity in the current context).
* 

* 
* [1.x.124]
* 
*   [1.x.125]  [1.x.126]   
*   In our test case, we do not know the analytical solution. This function outputs the velocity components along  [2.x.137]  and  [2.x.138]  so they can be compared with data from the literature.
* 

* 
* [1.x.127]
* 
*   [1.x.128]  [1.x.129]   
*   This is the last step of this program. In this part, we generate the grid and run the other functions respectively. The max refinement can be set by the argument.
* 

* 
* [1.x.130]
* 
*  If the viscosity is smaller than  [2.x.139] , we have to first search for an initial guess via a continuation method. What we should notice is the search is always on the initial mesh, that is the  [2.x.140]  mesh in this program. After that, we just do the same as we did when viscosity is larger than  [2.x.141] : run Newton's iteration, refine the mesh, transfer solutions, and repeat.
* 

* 
* [1.x.131]
* 
*  When the viscosity is larger than 1/1000, the solution to Stokes equations is good enough as an initial guess. If so, we do not need to search for the initial guess using a continuation method. Newton's iteration can be started directly.
* 

* 
*  

* 
* [1.x.132]
* [1.x.133][1.x.134]
* 

* Now we use the method we discussed above to solve Navier Stokes equations withviscosity  [2.x.142]  and  [2.x.143] .
* [1.x.135][1.x.136]
* 

* In the first test case the viscosity is set to be  [2.x.144] . As we discussed in theintroduction, the initial guess is the solution to the corresponding Stokesproblem. In the following table, the residuals at each Newton's iteration onevery mesh is shown. The data in the table shows that Newton's iterationconverges quadratically.
*  [2.x.145] 
* 

* 
* 

* 
* 

* The following figures show the sequence of generated grids. For the caseof  [2.x.146] , the initial guess is obtained by solving Stokes on an [2.x.147]  mesh, and the mesh is refined adaptively. Between meshes, thesolution from the coarse mesh is interpolated to the fine mesh to be used as aninitial guess.
*  [2.x.148] 
* This picture is the graphical streamline result of lid-driven cavity with [2.x.149] . [2.x.150] 
* Then the solution is compared with a reference solutionfrom [4] and the reference solution data can be found in the file "ref_2d_ghia_u.txt".
*  [2.x.151] 
* [1.x.137][1.x.138]
* 

* Newton's iteration requires a good initial guess. However, the nonlinear termdominates when the Reynolds number is large, so that the solution to the Stokesequations may be far away from the exact solution. If the Stokes solution actsas the initial guess, the convergence will be lost. The following pictureshows that the nonlinear iteration gets stuck and the residual no longer decreasesin further iterations.
*  [2.x.152] 
* The initial guess, therefore, has to be obtained via a continuation methodwhich has been discussed in the introduction. Here the step size in the continuation method, that is  [2.x.153] , is 2000 and the initialmesh is of size  [2.x.154] . After obtaining an initial guess, the mesh isrefined as in the previous test case. The following picture shows that at eachrefinement Newton's iteration has quadratic convergence. 52 steps of Newton'siterations are executed for solving this test case.
*  [2.x.155] 
* We also show the residual from each step of Newton's iteration on everymesh. The quadratic convergence is clearly visible in the table.
*  [2.x.156] 
* 

* 
* 

* 
* 

* The sequence of generated grids looks like this: [2.x.157] We compare our solution with reference solution from [5]. [2.x.158] The following picture presents the graphical result. [2.x.159] 
* Furthermore, the error consists of the nonlinear error,which decreases as we perform Newton iterations, and the discretization error,which depends on the mesh size. That is why we have to refine themesh and repeat Newton's iteration on the next finer mesh. From the table above, we cansee that the residual (nonlinear error) is below 1e-12 on each mesh, but thefollowing picture shows us the difference between solutions on subsequently finermeshes.
*  [2.x.160] 
* [1.x.139]
* [1.x.140][1.x.141]
* 

* [1.x.142][1.x.143]
* 

* It is easy to compare the currently implemented linear solver to just usingUMFPACK for the whole linear system. You need to remove the nullspacecontaining the constant pressures and it is done in  [2.x.161] . More interestingis the comparison to other state of the art preconditioners like PCD. It turnsout that the preconditioner here is very competitive, as can be seen in thepaper [2].
* The following table shows the timing results between our iterative approach(FGMRES) compared to a direct solver (UMFPACK) for the whole systemwith viscosity set to 1/400. Even though we use the same direct solver forthe velocity block in the iterative solver, it is considerably faster andconsumes less memory. This will be even more pronounced in 3d.
*  [2.x.162] 
* 

* [1.x.144][1.x.145]
* 

* The code is set up to also run in 3d. Of course the reference values aredifferent, see [6] for example. High resolution computations are not doablewith this example as is, because a direct solver for the velocity block doesnot work well in 3d. Rather, a parallel solver based on algebraic or geometricmultigrid is needed. See below.
* [1.x.146][1.x.147]
* 

* For larger computations, especially in 3d, it is necessary to implement MPIparallel solvers and preconditioners. A good starting point would be  [2.x.163] ,which uses algebraic multigrid for the velocity block for the Stokesequations. Another option would be to take a look at the list of codesin the [1.x.148], which already contains parallel Navier-Stokes solvers.
* 

* [1.x.149][1.x.150] [2.x.164] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-58_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] ,  [2.x.2] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27]
*  [2.x.3] 
* [1.x.28][1.x.29]
* [1.x.30][1.x.31][1.x.32]
* 

* The [1.x.33] for a function  [2.x.4]  and a potential  [2.x.5]  is a model often used inquantum mechanics and nonlinear optics. If one measures in appropriatequantities (so that  [2.x.6] ), then it reads as follows:
* [1.x.34]
* If there is no potential, i.e.  [2.x.7] , then it can be usedto describe the propagation of light in optical fibers. If  [2.x.8] , the equation is also sometimes called the [1.x.35] and can be used to model the time dependent behavior of[1.x.36].
* For this particular tutorial program, the physical interpretation ofthe equation is not of much concern to us. Rather, we want to use itas a model that allows us to explain two aspects:
* 
*  - It is a [1.x.37] for  [2.x.9] . We have previously seen complex-valued equations in  [2.x.10] ,  but there have opted to split the equations into real and imaginary  parts and consequently ended up solving a system of two real-valued  equations. In contrast, the goal here is to show how to solve  problems in which we keep everything as complex numbers.
* 
*  - The equation is a nice model problem to explain how [1.x.38] work. This is because it has terms with  fundamentally different character: on the one hand,  [2.x.11]  is a regular spatial operator in the way we have seen  many times before; on the other hand,  [2.x.12]  has no spatial or temporal derivatives, i.e., it is a purely  local operator. It turns out that we have efficient methods for each  of these terms (in particular, we have analytic solutions for the  latter), and that we may be better off treating these terms  differently and separately. We will explain this in more detail  below.
* 

* 
* [1.x.39][1.x.40]
* 

* At first glance, the equations appear to be parabolic and similar tothe heat equation (see  [2.x.13] ) as there is only a single timederivative and two spatial derivatives. But this is misleading.Indeed, that this is not the correct interpretation ismore easily seen if we assume for a moment that the potential  [2.x.14] and  [2.x.15] . Then we have the equation
* [1.x.41]
* If we separate the solution into real and imaginary parts,  [2.x.16] ,with  [2.x.17] ,then we can split the one equation into its real and imaginary partsin the same way as we did in  [2.x.18] :
* [1.x.42]
* Not surprisingly, the factor  [2.x.19]  in front of the time derivativecouples the real and imaginary parts of the equation. If we want tounderstand this equation further, take the time derivative of one ofthe equations, say
* [1.x.43]
* (where we have assumed that, at least in some formal sense, we cancommute the spatial and temporal derivatives), and then insert theother equation into it:
* [1.x.44]
* This equation is hyperbolic and similar in character to the waveequation. (This will also be obvious if you look at the videoin the "Results" section of this program.) Furthermore, we couldhave arrived at the same equation for  [2.x.20]  as well.Consequently, a better assumption for the NLSE is to think ofit as a hyperbolic, wave-propagation equation than as a diffusionequation such as the heat equation. (You may wonder whether it iscorrect that the operator  [2.x.21]  appears with a positive signwhereas in the wave equation,  [2.x.22]  has a negative sign. This isindeed correct: After multiplying by a test function and integratingby parts, we want to come out with a positive (semi-)definiteform. So, from  [2.x.23]  we obtain  [2.x.24] . Likewise,after integrating by parts twice, we obtain from  [2.x.25]  theform  [2.x.26] . In both cases do we get the desired positivesign.)
* The real NLSE, of course, also has the terms  [2.x.27]  and [2.x.28] . However, these are of lower order in the spatialderivatives, and while they are obviously important, they do notchange the character of the equation.
* In any case, the purpose of this discussion is to figure outwhat time stepping scheme might be appropriate for the equation. Theconclusions is that, as a hyperbolic-kind of equation, we need tochoose a time step that satisfies a CFL-type condition. If we were touse an explicit method (which we will not), we would have to investigatethe eigenvalues of the matrix that corresponds to the spatialoperator. If you followed the discussions of the video lectures( [2.x.29] then you will remember that the pattern is that one needs to make surethat  [2.x.30]  where  [2.x.31]  is the time step,  [2.x.32]  the mesh width,and  [2.x.33]  are the orders of temporal and spatial derivatives.Whether you take the original equation ( [2.x.34] ) or the reformulationfor only the real or imaginary part, the outcome is that we would need tochoose  [2.x.35]  if we were to use an explicit time steppingmethod. This is not feasible for the same reasons as in  [2.x.36]  forthe heat equation: It would yield impractically small time stepsfor even only modestly refined meshes. Rather, we have to use animplicit time stepping method and can then choose a more balanced [2.x.37] . Indeed, we will use the implicit Crank-Nicolsonmethod as we have already done in  [2.x.38]  before for the regularwave equation.
* 

* [1.x.45][1.x.46]
* 

*  [2.x.39] 
* If one thought of the NLSE as an ordinary differential equation inwhich the right hand side happens to have spatial derivatives, i.e.,write it as
* [1.x.47]
* one may be tempted to "formally solve" it by integrating both sidesover a time interval  [2.x.40]  and obtain
* [1.x.48]
* Of course, it's not that simple: the  [2.x.41]  in the integrand isstill changing over time in accordance with the differential equation,so we cannot just evaluate the integral (or approximate it easily viaquadrature) because we don't know  [2.x.42] .But we can write this with separate contributions asfollows, and this will allow us to deal with different terms separately:
* [1.x.49]
* The way this equation can now be read is as follows: For each time interval [2.x.43] , the change  [2.x.44]  in thesolution consists of three contributions:
* 
*  - The contribution of the Laplace operator.
* 
*  - The contribution of the potential  [2.x.45] .
* 
*  - The contribution of the "phase" term  [2.x.46] .
* [1.x.50] is now an approximation technique thatallows us to treat each of these contributions separately. (If wewant: In practice, we will treat the first two together, and the lastone separate. But that is a detail, conceptually we could treat all ofthem differently.) To this end, let us introduce three separate "solutions":
* [1.x.51]
* 
* These three "solutions" can be thought of as satisfying the followingdifferential equations:
* [1.x.52]
* In other words, they are all trajectories  [2.x.47]  that start at [2.x.48]  and integrate up the effects of exactly one of the threeterms. The increments resulting from each of these terms over our timeinterval are then  [2.x.49] , [2.x.50] , and [2.x.51] .
* It is now reasonable to assume (this is an approximation!) that thechange due to all three of the effects in question is well approximatedby the sum of the three separate increments:
* [1.x.53]
* This intuition is indeed correct, though the approximation is notexact: the difference between the exact left hand side and the term [2.x.52]  (i.e., the difference between the [1.x.54] incrementfor the exact solution  [2.x.53]  when moving from  [2.x.54]  to  [2.x.55] ,and the increment composed of the three parts on the right hand side),is proportional to  [2.x.56] . In other words, thisapproach introduces an error of size  [2.x.57] . Nothing wehave done so far has discretized anything in time or space, so the[1.x.55] error is going to be  [2.x.58]  plus whatevererror we commit when approximating the integrals (the temporaldiscretization error) plus whatever error we commit when approximatingthe spatial dependencies of  [2.x.59]  (the spatial error).
* Before we continue with discussions about operator splitting, let ustalk about why one would even want to go this way? The answer issimple: For some of the separate equations for the  [2.x.60] , wemay have ways to solve them more efficiently than if we throweverything together and try to solve it at once. For example, andparticularly pertinent in the current case: The equation for [2.x.61] , i.e.,
* [1.x.56]
* or equivalently,
* [1.x.57]
* can be solved exactly: the equation is solved by
* [1.x.58]
* This is easy to see if (i) you plug this solution into thedifferential equation, and (ii) realize that the magnitude [2.x.62]  is constant, i.e., the term  [2.x.63]  in theexponent is in fact equal to  [2.x.64] . In other words, thesolution of the ODE for  [2.x.65]  only changes its [1.x.59],but the [1.x.60] of the complex-valued function  [2.x.66] remains constant. This makes computing  [2.x.67]  particularly convenient:we don't actually need to solve any ODE, we can write the solutiondown by hand. Using the operator splitting approach, none of themethods to compute  [2.x.68]  therefore have to deal with the nonlinearterm and all of the associated unpleasantries: we can get away withsolving only [1.x.61] problems, as long as we allow ourselves theluxury of using an operator splitting approach.
* Secondly, one often uses operator splitting if the different physicaleffects described by the different terms have different timescales. Imagine, for example, a case where we really did have somesort of diffusion equation. Diffusion acts slowly, but if  [2.x.69]  islarge, then the "phase rotation" by the term  [2.x.70]  acts quickly. If we treatedeverything together, this would imply having to take rather small timesteps. But with operator splitting, we can take large time steps [2.x.71]  for the diffusion, and (assuming we didn'thave an analytic solution) use an ODE solver with many small timesteps to integrate the "phase rotation" equation for  [2.x.72]  from [2.x.73]  to  [2.x.74] . In other words, operator splitting allows us todecouple slow and fast time scales and treat them differently, withmethods adjusted to each case.
* 

* [1.x.62][1.x.63]
* 

* While the method above allows to compute the three contributions [2.x.75]  in parallel, if we want, the method can be made slightlymore accurate and easy to implement if we don't let the trajectoriesfor the  [2.x.76]  start all at  [2.x.77] , but instead let thetrajectory for  [2.x.78]  start at the [1.x.64] of thetrajectory for  [2.x.79] , namely  [2.x.80] ; similarly,we will start the trajectory for  [2.x.81]  start at the end pointof the trajectory for  [2.x.82] , namely  [2.x.83] . Thismethod is then called "Lie splitting" and has the same order of erroras the method above, i.e., the splitting error is  [2.x.84] .
* This variation of operator splitting can be written asfollows (carefully compare the initial conditions to the ones above):
* [1.x.65]
* (Obviously, while the formulas above imply that we should solve theseproblems in this particular order, it is equally valid to first solvefor trajectory 3, then 2, then 1, or any other permutation.)
* The integrated forms of these equations are then
* [1.x.66]
* From a practical perspective, this has the advantage that we needto keep around fewer solution vectors: Once  [2.x.85]  has beencomputed, we don't need  [2.x.86]  any more; once  [2.x.87] has been computed, we don't need  [2.x.88]  any more. And once [2.x.89]  has been computed, we can just call it [2.x.90]  because, if you insert the first into the second, andthen into the third equation, you see that the right hand side of [2.x.91]  now contains the contributions of all three physicaleffects:
* [1.x.67]
* (Compare this again with the "exact" computation of  [2.x.92] :It only differs in how we approximate  [2.x.93]  in each of the three integrals.)In other words, Lie splitting is a lot simpler to implement that theoriginal method outlined above because data handling is so muchsimpler.
* 

* [1.x.68][1.x.69]
* 

* As mentioned above, Lie splitting is only  [2.x.94] accurate. This is acceptable if we were to use a first order timediscretization, for example using the explicit or implicit Eulermethods to solve the differential equations for  [2.x.95] . This isbecause these time integration methods introduce an error proportionalto  [2.x.96]  themselves, and so the splitting error is proportionalto an error that we would introduce anyway, and does not diminish theoverall convergence order.
* But we typically want to use something higher order
* 
*  -  say, a[1.x.70]or[1.x.71]method
* 
*  -  since these are often not more expensive than asimple Euler method. It would be a shame if we were to use a timestepping method that is  [2.x.97] , but then lose theaccuracy again through the operator splitting.
* This is where the [1.x.72] method comes in. It is easier to explain if we had onlytwo parts, and so let us combine the effects of the Laplace operatorand of the potential into one, and the phase rotation into a secondeffect. (Indeed, this is what we will do in the code since solving theequation with the Laplace equation with or without the potential coststhe same
* 
*  -  so we merge these two steps.) The Lie splitting methodfrom above would then do the following: It computes solutions of thefollowing two ODEs,
* [1.x.73]
* and then uses the approximation  [2.x.98] . In other words, we first make one full time stepfor physical effect one, then one full time step for physical effecttwo. The solution at the end of the time step is simply the sum of theincrements due to each of these physical effects separately.
* In contrast,[1.x.74](one of the titans of numerical analysis starting in the mid-20thcentury) figured out that it is more accurate to first doone half-step for one physical effect, then a full time step for theother physical effect, and then another half step for the first. Whichone is which does not matter, but because it is so simple to do thephase rotation, we will use this effect for the half steps and thenonly need to do one spatial solve with the Laplace operator pluspotential. This operator splitting method is now  [2.x.99]  accurate. Written in formulas, this yields the followingsequence of steps:
* [1.x.75]
* As before, the first and third step can be computed exactly for thisparticular equation, yielding
* [1.x.76]
* 
* This is then how we are going to implement things in this program:In each time step, we execute three steps, namely
* 
*  - Update the solution value at each node by analytically integrating  the phase rotation equation by one half time step;
* 
*  - Solving the space-time equation that corresponds to the full step  for  [2.x.100] , namely   [2.x.101] ,  with initial conditions equal to the solution of the first half step  above.
* 
*  - Update the solution value at each node by analytically integrating  the phase rotation equation by another half time step.
* This structure will be reflected in an obvious way in the main timeloop of the program.
* 

* 
* [1.x.77][1.x.78]
* 

* From the discussion above, it should have become clear that the onlypartial differential equation we have to solve in each time step is
* [1.x.79]
* This equation is linear. Furthermore, we only have to solve it from [2.x.102]  to  [2.x.103] , i.e., for exactly one time step.
* To do this, we will apply the second order accurate Crank-Nicolsonscheme that we have already used in some of the other time dependentcodes (specifically:  [2.x.104]  and  [2.x.105] ). It reads as follows:
* [1.x.80]
* Here, the "previous" solution  [2.x.106]  (or the "initialcondition" for this part of the time step) is the output of thefirst phase rotation half-step; the output of the current step willbe denoted by  [2.x.107] .  [2.x.108]  isthe length of the time step. (One could argue whether  [2.x.109] and  [2.x.110]  live at time step  [2.x.111]  or  [2.x.112]  and what their upperindices should be. This is a philosophical discussion without practicalimpact, and one might think of  [2.x.113]  as something like [2.x.114] , and  [2.x.115]  as [2.x.116]  if that helps clarify things
* 
*  -  though, again [2.x.117]  is not to be understood as "one third time step after [2.x.118] " but more like "we've already done one third of the work necessaryfor time step  [2.x.119] ".)
* If we multiply the whole equation with  [2.x.120]  and sort terms withthe unknown  [2.x.121]  to the left and those with the known [2.x.122]  to the right, then we obtain the following (spatial)partial differential equation that needs to be solved in each timestep:
* [1.x.81]
* 
* 

* 
* [1.x.82][1.x.83]
* 

* As mentioned above, the previous tutorial program dealing withcomplex-valued solutions (namely,  [2.x.123] ) separated real and imaginaryparts of the solution. It thus reduced everything to realarithmetic. In contrast, we here want to keep thingscomplex-valued.
* The first part of this is that we need to define the discretizedsolution as  [2.x.124]  where the  [2.x.125]  are the usual shape functions (which arereal valued) but the expansion coefficients  [2.x.126]  at time step [2.x.127]  are now complex-valued. This is easily done in deal.II: We justhave to use  [2.x.128]  instead of Vector<double> tostore these coefficients.
* Of more interest is how to build and solve the linearsystem. Obviously, this will only be necessary for the second step ofthe Strang splitting discussed above, with the time discretization ofthe previous subsection. We obtain the fully discrete version throughstraightforward substitution of  [2.x.129]  by  [2.x.130]  andmultiplication by a test function:
* [1.x.84]
* or written in a more compact way:
* [1.x.85]
* Here, the matrices are defined in their obvious ways:
* [1.x.86]
* Note that all matrices individually are in fact symmetric,real-valued, and at least positive semidefinite, though the same isobviously not true forthe system matrix  [2.x.131] and the corresponding matrix [2.x.132] on the right hand side.
* 

* [1.x.87][1.x.88]
* 

*  [2.x.133] 
* The only remaining important question about the solution procedure ishow to solve the complex-valued linear system
* [1.x.89]
* with the matrix  [2.x.134]  and a right hand side that is easily computed as the product ofa known matrix and the previous part-step's solution.As usual, this comes down to the question of what properties thematrix  [2.x.135]  has. If it is symmetric and positive definite, then we canfor example use the Conjugate Gradient method.
* Unfortunately, the matrix's only useful property is that it is complexsymmetric, i.e.,  [2.x.136] , as is easy to see by recalling that [2.x.137]  are all symmetric. It is not, however,[1.x.90],which would require that  [2.x.138]  where the bar indicates complexconjugation.
* Complex symmetry can be exploited for iterative solvers as a quickliterature search indicates. We will here not try to become toosophisticated (and indeed leave this to the [1.x.91] section below) andinstead simply go with the good old standby for problems withoutproperties: A direct solver. That's not optimal, especially for largeproblems, but it shall suffice for the purposes of a tutorial program.Fortunately, the SparseDirectUMFPACK class allows solving complex-valuedproblems.
* 

* [1.x.92][1.x.93]
* 

* Initial conditions for the NLSE are typically chosen to representparticular physical situations. This is beyond the scope of thisprogram, but suffice it to say that these initial conditions are(i) often superpositions of the wave functions of particles locatedat different points, and that (ii) because  [2.x.139] corresponds to a particle density function, the integral[1.x.94]corresponds to the number of particles in the system. (Clearly, ifone were to be physically correct,  [2.x.140]  better be a constant ifthe system is closed, or  [2.x.141]  if one has absorbingboundary conditions.) The important point is that one should chooseinitial conditions so that[1.x.95]makes sense.
* What we will use here, primarily because it makes for good graphics,is the following:[1.x.96]where  [2.x.142]  is the distance from the (fixed)locations  [2.x.143] , and [2.x.144]  are chosen so that each of the Gaussians that we areadding up adds an integer number of particles to  [2.x.145] . We achievethis by making sure that[1.x.97]is a positive integer. In other words, we need to choose  [2.x.146] as an integer multiple of[1.x.98]assuming for the moment that  [2.x.147] 
* 
*  -  which isof course not the case, but we'll ignore the small difference inintegral.
* Thus, we choose  [2.x.148]  for all, and [2.x.149] . This  [2.x.150]  is small enough that the difference between theexact (infinite) integral and the integral over  [2.x.151]  should not betoo concerning.We choose the four points  [2.x.152]  as  [2.x.153] 
* 
*  -  also far enough away from the boundary of  [2.x.154]  to keepourselves on the safe side.
* For simplicity, we pose the problem on the square  [2.x.155] . Forboundary conditions, we will use time-independent Neumann conditions of theform[1.x.99]This is not a realistic choice of boundary conditions but sufficientfor what we want to demonstrate here. We will comment further on thisin the [1.x.100] section below.
* Finally, we choose  [2.x.156] , and the potential as[1.x.101]Using a large potential makes sure that the wave function  [2.x.157]  remainssmall outside the circle of radius 0.7. All of the Gaussians that makeup the initial conditions are within this circle, and the solution willmostly oscillate within it, with a small amount of energy radiating intothe outside. The use of a large potential also makes sure that the nonphysicalboundary condition does not have too large an effect.
* 

*  [1.x.102] [1.x.103]
*   [1.x.104]  [1.x.105] The program starts with the usual include files, all of which you should have seen before by now:
* 

* 
* [1.x.106]
* 
*  Then the usual placing of all content of this program into a namespace and the importation of the deal.II namespace into the one we will work in:
* 

* 
* [1.x.107]
* 
*   [1.x.108]  [1.x.109]   
*   Then the main class. It looks very much like the corresponding classes in  [2.x.158]  or  [2.x.159] , with the only exception that the matrices and vectors and everything else related to the linear system are now storing elements of type  [2.x.160]  instead of just `double`.
* 

* 
* [1.x.110]
* 
*   [1.x.111]  [1.x.112]
* 

* 
*  Before we go on filling in the details of the main class, let us define the equation data corresponding to the problem, i.e. initial values, as well as a right hand side class. (We will reuse the initial conditions also for the boundary values, which we simply keep constant.) We do so using classes derived from the Function class template that has been used many times before, so the following should not look surprising. The only point of interest is that we here have a complex-valued problem, so we have to provide the second template argument of the Function class (which would otherwise default to `double`). Furthermore, the return type of the `value()` functions is then of course also complex.   
*   What precisely these functions return has been discussed at the end of the Introduction section.
* 

* 
* [1.x.113]
* 
*   [1.x.114]  [1.x.115]
* 

* 
*  We start by specifying the implementation of the constructor of the class. There is nothing of surprise to see here except perhaps that we choose quadratic ( [2.x.161] ) Lagrange elements
* 
*  -  the solution is expected to be smooth, so we choose a higher polynomial degree than the bare minimum.
* 

* 
* [1.x.116]
* 
*   [1.x.117]  [1.x.118]
* 

* 
*  The next function is the one that sets up the mesh, DoFHandler, and matrices and vectors at the beginning of the program, i.e. before the first time step. The first few lines are pretty much standard if you've read through the tutorial programs at least up to  [2.x.162] :
* 

* 
* [1.x.119]
* 
*  Next, we assemble the relevant matrices. The way we have written the Crank-Nicolson discretization of the spatial step of the Strang splitting (i.e., the second of the three partial steps in each time step), we were led to the linear system  [2.x.163] . In other words, there are two matrices in play here
* 
*  -  one for the left and one for the right hand side. We build these matrices separately. (One could avoid building the right hand side matrix and instead just form theaction* of the matrix on  [2.x.164]  in each time step. This may or may not be more efficient, but efficiency is not foremost on our minds for this program.)
* 

* 
* [1.x.120]
* 
*   [1.x.121]  [1.x.122]
* 

* 
*  Having set up all data structures above, we are now in a position to implement the partial steps that form the Strang splitting scheme. We start with the half-step to advance the phase, and that is used as the first and last part of each time step.   
*   To this end, recall that for the first half step, we needed to compute  [2.x.165] . Here,  [2.x.166]  and  [2.x.167]  are functions of space and correspond to the output of the previous complete time step and the result of the first of the three part steps, respectively. A corresponding solution must be computed for the third of the part steps, i.e.  [2.x.168] , where  [2.x.169]  is the result of the time step as a whole, and its input  [2.x.170]  is the result of the spatial step of the Strang splitting.   
*   An important realization is that while  [2.x.171]  may be a finite element function (i.e., is piecewise polynomial), this may not necessarily be the case for the "rotated" function in which we have updated the phase using the exponential factor (recall that the amplitude of that function remains constant as part of that step). In other words, we couldcompute*  [2.x.172]  at every point  [2.x.173] , but we can't represent it on a mesh because it is not a piecewise polynomial function. The best we can do in a discrete setting is to compute a projection or interpolation. In other words, we can compute  [2.x.174]  where  [2.x.175]  is a projection or interpolation operator. The situation is particularly simple if we choose the interpolation: Then, all we need to compute is the value of the right hand sideat the node points* and use these as nodal values for the vector  [2.x.176]  of degrees of freedom. This is easily done because evaluating the right hand side at node points for a Lagrange finite element as used here requires us to only look at a single (complex-valued) entry of the node vector. In other words, what we need to do is to compute  [2.x.177]  where  [2.x.178]  loops over all of the entries of our solution vector. This is what the function below does
* 
*  -  in fact, it doesn't even use separate vectors for  [2.x.179]  and  [2.x.180] , but just updates the same vector as appropriate.
* 

* 
* [1.x.123]
* 
*  The next step is to solve for the linear system in each time step, i.e., the second half step of the Strang splitting we use. Recall that it had the form  [2.x.181]  where  [2.x.182]  and  [2.x.183]  are the matrices we assembled earlier.   
*   The way we solve this here is using a direct solver. We first form the right hand side  [2.x.184]  using the  [2.x.185]  function and put the result into the `system_rhs` variable. We then call  [2.x.186]  which takes as argument the matrix  [2.x.187]  and the right hand side vector and returns the solution in the same vector `system_rhs`. The final step is then to put the solution so computed back into the `solution` variable.
* 

* 
* [1.x.124]
* 
*   [1.x.125]  [1.x.126]
* 

* 
*  The last of the helper functions and classes we ought to discuss are the ones that create graphical output. The result of running the half and full steps for the local and spatial parts of the Strang splitting is that we have updated the `solution` vector  [2.x.188]  to the correct value at the end of each time step. Its entries contain complex numbers for the solution at the nodes of the finite element mesh.   
*   Complex numbers are not easily visualized. We can output their real and imaginary parts, i.e., the fields  [2.x.189]  and  [2.x.190] , and that is exactly what the DataOut class does when one attaches as complex-valued vector via  [2.x.191]  and then calls  [2.x.192]  That is indeed what we do below.
* 

* 
*  But oftentimes we are not particularly interested in real and imaginary parts of the solution vector, but instead in derived quantities such as the magnitude  [2.x.193]  and phase angle  [2.x.194]  of the solution. In the context of quantum systems such as here, the magnitude itself is not so interesting, but instead it is the "amplitude",  [2.x.195]  that is a physical property: it corresponds to the probability density of finding a particle in a particular place of state. The way to put computed quantities into output files for visualization
* 
*  -  as used in numerous previous tutorial programs
* 
*  -  is to use the facilities of the DataPostprocessor and derived classes. Specifically, both the amplitude of a complex number and its phase angles are scalar quantities, and so the DataPostprocessorScalar class is the right tool to base what we want to do on.   
*   Consequently, what we do here is to implement two classes `ComplexAmplitude` and `ComplexPhase` that compute for each point at which DataOut decides to generate output, the amplitudes  [2.x.196]  and phases  [2.x.197]  of the solution for visualization. There is a fair amount of boiler-plate code below, with the only interesting parts of the first of these two classes being how its `evaluate_vector_field()` function computes the `computed_quantities` object.   
*   (There is also the rather awkward fact that the [1.x.127] function does not compute what one would naively imagine, namely  [2.x.198] , but returns  [2.x.199]  instead. It's certainly quite confusing to have a standard function mis-named in such a way...)
* 

* 
* [1.x.128]
* 
*  The second of these postprocessor classes computes the phase angle of the complex-valued solution at each point. In other words, if we represent  [2.x.200] , then this class computes  [2.x.201] . The function [1.x.129] does this for us, and returns the angle as a real number between  [2.x.202]  and  [2.x.203] .     
*   For reasons that we will explain in detail in the results section, we do not actually output this value at each location where output is generated. Rather, we take the maximum over all evaluation points of the phase and then fill each evaluation point's output field with this maximum
* 
*  -  in essence, we output the phase angle as a piecewise constant field, where each cell has its own constant value. The reasons for this will become clear once you read through the discussion further down below.
* 

* 
* [1.x.130]
* 
*  Having so implemented these post-processors, we create output as we always do. As in many other time-dependent tutorial programs, we attach flags to DataOut that indicate the number of the time step and the current simulation time.
* 

* 
* [1.x.131]
* 
*   [1.x.132]  [1.x.133]
* 

* 
*  The remaining step is how we set up the overall logic for this program. It's really relatively simple: Set up the data structures; interpolate the initial conditions onto finite element space; then iterate over all time steps, and on each time step perform the three parts of the Strang splitting method. Every tenth time step, we generate graphical output. That's it.
* 

* 
* [1.x.134]
* 
*   [1.x.135]  [1.x.136]
* 

* 
*  The rest is again boiler plate and exactly as in almost all of the previous tutorial programs:
* 

* 
* [1.x.137]
* [1.x.138][1.x.139]
* 

* Running the code results in screen output like the following:```Number of active cells: 4096Number of degrees of freedom: 16641
* Time step 1 at t=0Time step 2 at t=0.00390625Time step 3 at t=0.0078125Time step 4 at t=0.0117188[...]```Running the program also yields a good number of output files that we willvisualize in the following.
* 

* [1.x.140][1.x.141]
* 

* The `output_results()` function of this program generates output files thatconsist of a number of variables: The solution (split into its real and imaginaryparts), the amplitude, and the phase. If we visualize these four fields, we getimages like the following after a few time steps (at time  [2.x.204] , to beprecise:
*  [2.x.205] 
* While the real and imaginary parts of the solution shown above are notparticularly interesting (because, from a physical perspective, theglobal offset of the phase and therefore the balance between real andimaginary components, is meaningless), it is much more interesting tovisualize the amplitude  [2.x.206]  and phase [2.x.207]  of the solution and, in particular,their evolution. This leads to pictures like the following:
* The phase picture shown here clearly has some flaws:
* 
*  - First, phase is a "cyclic quantity", but the color scale uses a  fundamentally different color for values close to  [2.x.208]  than  for values close to  [2.x.209] . This is a nuisance
* 
*  -  what we need  is a "cyclic color map" that uses the same colors for the two  extremes of the range of the phase. Such color maps exist,  see [1.x.142] or  [1.x.143], for example. The problem is that the  author's favorite  one of the two big visualization packages, VisIt, does not have any  of these color maps built in. In an act of desperation, I therefore  had to resort to using Paraview given that it has several of the  color maps mentioned in the post above implemented. The picture  below uses the `nic_Edge` map in which both of the extreme values are shown  as black.
* 
*  - There is a problem on cells in which the phase wraps around. If  at some evaluation point of the cell the phase value is close to   [2.x.210]  and at another evaluation point it is close to  [2.x.211] , then  what we would really like to happen is for the entire cell to have a  color close to the extremes. But, instead, visualization programs  produce a linear interpolation in which the values within the cell,  i.e., between the evaluation points, is linearly interpolated between  these two values, covering essentially the entire range of possible  phase values and, consequently, cycling through the entire  rainbow of colors from dark red to dark green over the course of  one cell. The solution to this problem is to just output  the phase value on each cell as a piecewise constant. Because  averaging values close to the  [2.x.212]  and  [2.x.213]  is going to  result in an average that has nothing to do with the actual phase  angle, the `ComplexPhase` class just uses themaximal* phase  angle encountered on each cell.
* With these modifications, the phase plot now looks as follows:
*  [2.x.214] 
* Finally, we can generate a movie out of this. (To be precise, the videouses two more global refinement cycles and a time step half the sizeof what is used in the program above.) The author of these linesmade the movie with VisIt,because that's what he's more familiar with, and using a hacked color mapthat is also cyclic
* 
*  -  though this color map lacks all of the skill employed bythe people who wrote the posts mentioned in the links above. Itdoes, however, show the character of the solution as a wave equationif you look at the shaded part of the domain outside the circle ofradius 0.7 in which the potential is zero
* 
*  -  you can see how every timeone of the bumps (showing the amplitude  [2.x.215] )bumps into the area where the potential is large: a wave travelsoutbound from there. Take a look at the video:
* [1.x.144]
* 
* So why did I end up shading the area where the potential  [2.x.216]  islarge? In that outside region, the solution is relatively small. It is alsorelatively smooth. As a consequence, to some approximate degree, theequation in that region simplifies to[1.x.145]or maybe easier to read:[1.x.146]To the degree to which this approximation is valid (which, among other things,eliminates the traveling waves you can see in the video), this equation hasa solution[1.x.147]Because  [2.x.217]  is large, this means that the phaserotates quite rapidly*.If you focus on the semi-transparent outer part of the domain, you cansee that. If one colors this region in the same way as the inner part ofthe domain, this rapidly flashing outer part may be psychedelic, but is alsodistracting of what's happening on the inside; it's also quite hard toactually see the radiating waves that are easy to see at the beginningof the video.
* 

* [1.x.148][1.x.149][1.x.150]
* 

* [1.x.151][1.x.152]
* 

* The solver chosen here is just too simple. It is also not efficient.What we do here is give the matrix to a sparse direct solver in everytime step and let it find the solution of the linear system. But weknow that we could do far better:
* 
*  - First, we should make use of the fact that the matrix doesn't  actually change from time step to time step. This is an artifact  of the fact that we here have constant boundary values and that  we don't change the time step size
* 
*  -  two assumptions that might  not be true in actual applications. But at least in cases where this  does happen to be the case, it would make sense to only factorize  the matrix once (i.e., compute  [2.x.218]  and  [2.x.219]  factors once) and then  use these factors for all following time steps until the matrix   [2.x.220]  changes and requires a new factorization. The interface of the  SparseDirectUMFPACK class allows for this.
* 
*  - Ultimately, however, sparse direct solvers are only efficient for  relatively small problems, say up to a few 100,000 unknowns. Beyond  this, one needs iterative solvers such as the Conjugate Gradient method (for  symmetric and positive definite problems) or GMRES. We have used many  of these in other tutorial programs. In all cases, they need to be  accompanied by good preconditioners. For the current case, one  could in principle use GMRES
* 
*  -  a method that does not require  any specific properties of the matrix
* 
*  -  but would be better  advised to implement an iterative scheme that exploits the one  structural feature we know is true for this problem: That the matrix  is complex-symmetric (albeit not Hermitian).
* 

* [1.x.153][1.x.154]
* 

* In order to be usable for actual, realistic problems, solvers for thenonlinear Schr&ouml;dinger equation need to utilize boundary conditionsthat make sense for the problem at hand. We have here restricted ourselvesto simple Neumann boundary conditions
* 
*  -  but these do not actually makesense for the problem. Indeed, the equations are generally posed on aninfinite domain. But, since we can't compute on infinite domains, we needto truncate it somewhere and instead pose boundary conditions that makesense for this artificially small domain. The approach widely used is touse the [1.x.155] method that corresponds to a particularkind of attenuation. It is, in a different context, also used in [2.x.221] .
* 

* [1.x.156][1.x.157]
* 

* Finally, we know from experience and many other tutorial programs thatit is worthwhile to use adaptively refined meshes, rather than the uniformmeshes used here. It would, in fact, not be very difficult to add thishere: It just requires periodic remeshing and transfer of the solutionfrom one mesh to the next.  [2.x.222]  will be a good guide for how thiscould be implemented.
* 

* [1.x.158][1.x.159] [2.x.223] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-59_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15]
*  [2.x.2] 
* [1.x.16]
* [1.x.17][1.x.18][1.x.19]
* 

* Matrix-free operator evaluation enables very efficient implementations ofdiscretization with high-order polynomial bases due to a method called sumfactorization. This concept has been introduced in the  [2.x.3]  and  [2.x.4] tutorial programs. In this tutorial program, we extend those concepts todiscontinuous Galerkin (DG) schemes that include face integrals, a class ofmethods where high orders are particularly widespread.
* The underlying idea of the matrix-free evaluation is the same as forcontinuous elements: The matrix-vector product that appears in an iterativesolver or multigrid smoother is not implemented by a classical sparse matrixkernel, but instead applied implicitly by the evaluation of the underlyingintegrals on the fly. For tensor product shape functions that are integratedwith a tensor product quadrature rule, this evaluation is particularlyefficient by using the sum-factorization technique, which decomposes theinitially  [2.x.5]  operations for interpolation involving  [2.x.6]  vectorentries with associated shape functions at degree  [2.x.7]  in  [2.x.8]  dimensions to [2.x.9]  quadrature points into  [2.x.10]  one-dimensional operations of cost [2.x.11]  each. In 3D, this reduces the order of complexity by two powersin  [2.x.12] . When measured as the complexity per degree of freedom, the complexityis  [2.x.13]  in the polynomial degree. Due to the presence of faceintegrals in DG, and due to the fact that operations on quadrature pointsinvolve more memory transfer, which both scale as  [2.x.14] , theobserved complexity is often constant for moderate  [2.x.15] . This means thata high order method can be evaluated with the same throughput in terms ofdegrees of freedom per second as a low-order method.
* More information on the algorithms are available in the preprint [2.x.16] [1.x.20] by Martin Kronbichler andKatharina Kormann, arXiv:1711.03590.
* [1.x.21][1.x.22]
* 

* For this tutorial program, we exemplify the matrix-free DG framework for theinterior penalty discretization of the Laplacian, i.e., the same scheme as theone used for the  [2.x.17]  tutorial program. The discretization of the Laplacianis given by the following weak form
* [1.x.23]
* where  [2.x.18]  denotes the directed jump of the quantity  [2.x.19]  from thetwo associated cells  [2.x.20]  and  [2.x.21] , and  [2.x.22] is the average from both sides.
* The terms in the equation represent the cell integral after integration byparts, the primal consistency term that arises at the element interfaces dueto integration by parts and insertion of an average flux, the adjointconsistency term that is added for restoring symmetry of the underlyingmatrix, and a penalty term with factor  [2.x.23] , whose magnitude is equal thelength of the cells in direction normal to face multiplied by  [2.x.24] , see [2.x.25] . The penalty term is chosen such that an inverse estimate holds andthe final weak form is coercive, i.e., positive definite in the discretesetting. The adjoint consistency term and the penalty term involve the jump [2.x.26]  at the element interfaces, which disappears for the analyticsolution  [2.x.27] . Thus, these terms are consistent with the original PDE, ensuringthat the method can retain optimal orders of convergence.
* In the implementation below, we implement the weak form above by moving thenormal vector  [2.x.28]  from the jump terms to the derivatives to form a[1.x.24] derivative of the form  [2.x.29] . Thismakes the implementation on quadrature points slightly more efficient becausewe only need to work with scalar terms rather than tensors, and ismathematically equivalent.
* For boundary conditions, we use the so-called mirror principle that defines[1.x.25] exterior values  [2.x.30]  by extrapolation from the interiorsolution  [2.x.31]  combined with the given boundary data, setting  [2.x.32]  and  [2.x.33] on Dirichlet boundaries and  [2.x.34]  and  [2.x.35]  on Neumann boundaries, for givenDirichlet values  [2.x.36]  and Neumann values  [2.x.37] . Theseexpressions are then inserted in the above weak form. Contributions involvingthe known quantities  [2.x.38]  and  [2.x.39]  are eventually moved to theright hand side, whereas the unknown value  [2.x.40]  is retained on the left handside and contributes to the matrix terms similarly as interior faces. Uponthese manipulations, the same weak form as in  [2.x.41]  is obtained.
* [1.x.26][1.x.27]
* 

* The matrix-free framework of deal.II provides the necessary infrastructure toimplement the action of the discretized equation above. As opposed to the [2.x.42]  that we used in  [2.x.43]  and  [2.x.44] , we now build acode in terms of  [2.x.45]  that takes three function pointers, onefor the cell integrals, one for the inner face integrals, and one for theboundary face integrals (in analogy to the design of MeshWorker used in the [2.x.46]  tutorial program). In each of these three functions, we then implementthe respective terms on the quadrature points. For interpolation between thevector entries and the values and gradients on quadrature points, we use theclass FEEvaluation for cell contributions and FEFaceEvaluation for facecontributions. The basic usage of these functions has been discussedextensively in the  [2.x.47]  tutorial program.
* In  [2.x.48]  all interior faces are visited exactly once, so onemust make sure to compute the contributions from both the test functions [2.x.49]  and  [2.x.50] . Given the fact that the test functions on both sides areindeed independent, the weak form above effectively means that we submit thesame contribution to both an FEFaceEvaluation object called `phi_inner` and`phi_outer` for testing with the normal derivative of the test function, andvalues with opposite sign for testing with the values of the test function,because the latter involves opposite signs due to the jump term. For facesbetween cells of different refinement level, the integration is done from therefined side, and FEFaceEvaluation automatically performs interpolation to asubface on the coarse side. Thus, a hanging node never appears explicitly in auser implementation of a weak form.
* The fact that each face is visited exactly once also applies to those faces atsubdomain boundaries between different processors when parallelized with MPI,where one cell belongs to one processor and one to the other. The setup in [2.x.51]  splits the faces between the two sides, and eventuallyonly reports the faces actually handled locally in [2.x.52]  and  [2.x.53] respectively. Note that, in analogy to the cell integrals discussed in [2.x.54] , deal.II applies vectorization over several faces to use SIMD, workingon something we call a [1.x.28] with a single instruction. Theface batches are independent from the cell batches, even though the time atwhich face integrals are processed is kept close to the time when the cellintegrals of the respective cells are processed, in order to increase the datalocality.
* Another thing that is new in this program is the fact that we no longer splitthe vector access like  [2.x.55]  or [2.x.56]  from the evaluation and integrationsteps, but call combined functions  [2.x.57]  and [2.x.58]  respectively. This is useful for faceintegrals because, depending on what gets evaluated on the faces, not allvector entries of a cell must be touched in the first place. Think for exampleof the case of the nodal element FE_DGQ with node points on the elementsurface: If we are interested in the shape function values on a face, only [2.x.59]  degrees of freedom contribute to them in a non-trivial way (ina more technical way of speaking, only  [2.x.60]  shape functions have anonzero support on the face and return true for [2.x.61]  When compared to the  [2.x.62]  degreesof freedom of a cell, this is one power less.
* Now of course we are not interested in only the function values, but also thederivatives on the cell. Fortunately, there is an element in deal.II thatextends this property of reduced access also for derivatives on faces, theFE_DGQHermite element.
* [1.x.29][1.x.30]
* 

* The element FE_DGQHermite belongs to the family of FE_DGQ elements, i.e., itsshape functions are a tensor product of 1D polynomials and the element isfully discontinuous. As opposed to the nodal character in the usual FE_DGQelement, the FE_DGQHermite element is a mixture of nodal contributions andderivative contributions based on a Hermite-like concept. The underlyingpolynomial class is  [2.x.63]  and can besummarized as follows: For cubic polynomials, we use two polynomials torepresent the function value and first derivative at the left end of the unitinterval,  [2.x.64] , and two polynomials to represent the function value and firstderivative and the right end of the unit interval,  [2.x.65] . At the oppositeends, both the value and first derivative of the shape functions are zero,ensuring that only two out of the four basis functions contribute to valuesand derivative on the respective end. However, we deviate from the classicalHermite interpolation in not strictly assigning one degree of freedom for thevalue and one for the first derivative, but rather allow the first derivativeto be a linear combination of the first and the second shape function. This isdone to improve the conditioning of the interpolation. Also, when going todegrees beyond three, we add node points in the element interior in aLagrange-like fashion, combined with double zeros in the points  [2.x.66]  and [2.x.67] . The position of these extra nodes is determined by the zeros of someJacobi polynomials as explained in the description of the class [2.x.68] 
* Using this element, we only need to access  [2.x.69]  degrees of freedomfor computing both values and derivatives on a face. The check whether theHermite property is fulfilled is done transparently inside [2.x.70]  and  [2.x.71] that check the type of the basis and reduce the access to data ifpossible. Obviously, this would not be possible if we had separated [2.x.72]  from  [2.x.73]  becausethe amount of entries we need to read depends on the type of the derivative(only values, first derivative, etc.) and thus must be given to`read_dof_values()`.
* This optimization is not only useful for computing the face integrals, butalso for the MPI ghost layer exchange: In a naive exchange, we would need tosend all degrees of freedom of a cell to another processor if the otherprocessor is responsible for computing the face's contribution. Since we knowthat only some of the degrees of freedom in the evaluation withFEFaceEvaluation are touched, it is natural to only exchange the relevantones. The  [2.x.74]  function has support for a selected data exchangewhen combined with  [2.x.75]  To make this happen, weneed to tell the loop what kind of evaluation on faces we are going to do,using an argument of type  [2.x.76]  as can be seen in theimplementation of  [2.x.77]  below. The way data is exchangedin that case is as follows: The ghost layer data in the vector still pretendsto represent all degrees of freedom, such that FEFaceEvaluation can continueto read the values as if the cell were a locally owned one. The data exchangeroutines take care of the task for packing and unpacking the data into thisformat. While this sounds pretty complicated, we will show in the resultssection below that this really pays off by comparing the performance to abaseline code that does not specify the data access on faces.
* [1.x.31][1.x.32]
* 

* In the tradition of the  [2.x.78]  program, we again solve a Poisson problem witha geometric multigrid preconditioner inside a conjugate gradientsolver. Instead of computing the diagonal and use the basicPreconditionChebyshev as a smoother, we choose a different strategy in thistutorial program. We implement a block-Jacobi preconditioner, where a blockrefers to all degrees of freedom on a cell. Rather than building the full cellmatrix and applying its LU factorization (or inverse) in the preconditioner&mdash; an operation that would be heavily memory bandwidth bound and thuspretty slow &mdash; we approximate the inverse of the block by a specialtechnique called fast diagonalization method.
* The idea of the method is to take use of the structure of the cell matrix. Incase of the Laplacian with constant coefficients discretized on a Cartesianmesh, the cell matrix  [2.x.79]  can be written as
* [1.x.33]
* in 2D and
* [1.x.34]
* in 3D. The matrices  [2.x.80]  and  [2.x.81]  denote the 1D Laplace matrix (includingthe cell and face term associated to the current cell values  [2.x.82]  and [2.x.83] ) and  [2.x.84]  and  [2.x.85]  are the mass matrices. Note that this simpletensor product structure is lost once there are non-constant coefficients onthe cell or the geometry is not constant any more. We mention that a similarsetup could also be used to replace the computed integrals with this finaltensor product form of the matrices, which would cut the operations for theoperator evaluation into less than half. However, given the fact that thisonly holds for Cartesian cells and constant coefficients, which is a prettynarrow case, we refrain from pursuing this idea.
* Interestingly, the exact inverse of the matrix  [2.x.86]  can be found through tensorproducts due to a method introduced by [1.x.35] from 1964,
* [1.x.36]
* where  [2.x.87]  is the matrix of eigenvectors to the generalized eigenvalue problemin the given tensor direction  [2.x.88] :
* [1.x.37]
* and  [2.x.89]  is the diagonal matrix representing the generalizedeigenvalues  [2.x.90] . Note that the vectors  [2.x.91]  are such that theysimultaneously diagonalize  [2.x.92]  and  [2.x.93] , i.e.  [2.x.94]  and  [2.x.95] .
* The deal.II library implements a class using this concept, calledTensorProductMatrixSymmetricSum.
* For the sake of this program, we stick with constant coefficients andCartesian meshes, even though an approximate version based on tensor productswould still be possible for a more general mesh, and the operator evaluationitself is of course generic. Also, we do not bother with adaptive meshes wherethe multigrid algorithm would need to get access to flux matrices over theedges of different refinement, as explained in  [2.x.96] . One thing we do,however, is to still wrap our block-Jacobi preconditioner insidePreconditionChebyshev. That class relieves us from finding an appropriaterelaxation parameter (which would be around 0.7 in 2D and 0.5 in 3D for theblock-Jacobi smoother), and often increases smoothing efficiency a bit overplain Jacobi smoothing in that it enables lower the time to solution whensetting the degree of the Chebyshev polynomial to one or two.
* Note that the block-Jacobi smoother has an additional benefit: The fastdiagonalization method can also be interpreted as a change from theHermite-like polynomials underlying FE_DGQHermite to a basis where the cellLaplacian is diagonal. Thus, it cancels the effect of the basis, and we getthe same iteration counts irrespective of whether we use FE_DGQHermite orFE_DGQ. This is in contrast to using the PreconditionChebyshev class with onlythe diagonal (a point-Jacobi scheme), where FE_DGQ and FE_DGQHermite do indeedbehave differently and FE_DGQ needs 2-5 less iterations than FE_DGQHermite,despite the modification made to the Hermite-like shape functions to ensure agood conditioning.
* 

*  [1.x.38] [1.x.39]
*  The include files are essentially the same as in  [2.x.97] , with the exception of the finite element class FE_DGQHermite instead of FE_Q. All functionality for matrix-free computations on face integrals is already contained in `fe_evaluation.h`.
* 

* 
* [1.x.40]
* 
*  As in  [2.x.98] , we collect the dimension and polynomial degree as constants here at the top of the program for simplicity. As opposed to  [2.x.99] , we choose a really high order method this time with degree 8 where any implementation not using sum factorization would become prohibitively slow compared to the implementation with MatrixFree which provides an efficiency that is essentially the same as at degrees two or three. Furthermore, all classes in this tutorial program are templated, so it would be easy to select the degree at run time from an input file or a command-line argument by adding instantiations of the appropriate degrees in the `main()` function.
* 

* 
*  

* 
* [1.x.41]
* 
*   [1.x.42]  [1.x.43]
* 

* 
*  In analogy to  [2.x.100] , we define an analytic solution that we try to reproduce with our discretization. Since the aim of this tutorial is to show matrix-free methods, we choose one of the simplest possibilities, namely a cosine function whose derivatives are simple enough for us to compute analytically. Further down, the wave number 2.4 we select here will be matched with the domain extent in  [2.x.101] -direction that is 2.5, such that we obtain a periodic solution at  [2.x.102]  including  [2.x.103]  or three full wave revolutions in the cosine. The first function defines the solution and its gradient for expressing the analytic solution for the Dirichlet and Neumann boundary conditions, respectively. Furthermore, a class representing the negative Laplacian of the solution is used to represent the right hand side (forcing) function that we use to match the given analytic solution in the discretized version (manufactured solution).
* 

* 
*  

* 
* [1.x.44]
* 
*   [1.x.45]  [1.x.46]
* 

* 
*  The `LaplaceOperator` class is similar to the respective class in  [2.x.104] . A significant difference is that we do not derive the class from  [2.x.105]  because we want to present some additional features of  [2.x.106]  that are not available in the general-purpose class  [2.x.107]  We derive the class from the Subscriptor class to be able to use the operator within the Chebyshev preconditioner because that preconditioner stores the underlying matrix via a SmartPointer.   
*   Given that we implement a complete matrix interface by hand, we need to add an `initialize()` function, an `m()` function, a `vmult()` function, and a `Tvmult()` function that were previously provided by  [2.x.108]  Our LaplaceOperator also contains a member function `get_penalty_factor()` that centralizes the selection of the penalty parameter in the symmetric interior penalty method according to  [2.x.109] .
* 

* 
*  

* 
* [1.x.47]
* 
*  The `%PreconditionBlockJacobi` class defines our custom preconditioner for this problem. As opposed to  [2.x.110]  which was based on the matrix diagonal, we here compute an approximate inversion of the diagonal blocks in the discontinuous Galerkin method by using the so-called fast diagonalization method discussed in the introduction.
* 

* 
*  

* 
* [1.x.48]
* 
*  This free-standing function is used in both the `LaplaceOperator` and `%PreconditionBlockJacobi` classes to adjust the ghost range. This function is necessary because some of the vectors that the `vmult()` functions are supplied with are not initialized properly with  [2.x.111]  that includes the correct layout of ghost entries, but instead comes from the MGTransferMatrixFree class that has no notion on the ghost selection of the matrix-free classes. To avoid index confusion, we must adjust the ghost range before actually doing something with these vectors. Since the vectors are kept around in the multigrid smoother and transfer classes, a vector whose ghost range has once been adjusted will remain in this state throughout the lifetime of the object, so we can use a shortcut at the start of the function to see whether the partitioner object of the distributed vector, which is stored as a shared pointer, is the same as the layout expected by MatrixFree, which is stored in a data structure accessed by  [2.x.112]  where the 0 indicates the DoFHandler number from which this was extracted; we only use a single DoFHandler in MatrixFree, so the only valid number is 0 here.
* 

* 
*  

* 
* [1.x.49]
* 
*  The next five functions to clear and initialize the `LaplaceOperator` class, to return the shared pointer holding the MatrixFree data container, as well as the correct initialization of the vector and operator sizes are the same as in  [2.x.113]  or rather  [2.x.114] 
* 

* 
* [1.x.50]
* 
*  This function implements the action of the LaplaceOperator on a vector `src` and stores the result in the vector `dst`. When compared to  [2.x.115] , there are four new features present in this call.   
*   The first new feature is the `adjust_ghost_range_if_necessary` function mentioned above that is needed to fit the vectors to the layout expected by FEEvaluation and FEFaceEvaluation in the cell and face functions.   
*   The second new feature is the fact that we do not implement a `vmult_add()` function as we did in  [2.x.116]  (through the virtual function  [2.x.117]  but directly implement a `vmult()` functionality. Since both cell and face integrals will sum into the destination vector, we must of course zero the vector somewhere. For DG elements, we are given two options &ndash; one is to use  [2.x.118]  instead of  [2.x.119]  in the `apply_cell` function below. This works because the loop layout in MatrixFree is such that cell integrals always touch a given vector entry before the face integrals. However, this really only works for fully discontinuous bases where every cell has its own degrees of freedom, without any sharing with neighboring results. An alternative setup, the one chosen here, is to let the  [2.x.120]  take care of zeroing the vector. This can be thought of as simply calling `dst = 0;` somewhere in the code. The implementation is more involved for supported vectors such as  [2.x.121]  because we aim to not zero the whole vector at once. Doing the zero operation on a small enough pieces of a few thousands of vector entries has the advantage that the vector entries that get zeroed remain in caches before they are accessed again in  [2.x.122]  and  [2.x.123]  Since matrix-free operator evaluation is really fast, just zeroing a large vector can amount to up to a 25% of the operator evaluation time, and we obviously want to avoid this cost. This option of zeroing the vector is also available for  [2.x.124]  and for continuous bases, even though it was not used in the  [2.x.125]  or  [2.x.126]  tutorial programs.   
*   The third new feature is the way we provide the functions to compute on cells, inner faces, and boundary faces: The class MatrixFree has a function called `loop` that takes three function pointers to the three cases, allowing to separate the implementations of different things. As explained in  [2.x.127] , these function pointers can be  [2.x.128]  objects or member functions of a class. In this case, we use pointers to member functions.   
*   The final new feature are the last two arguments of type  [2.x.129]  that can be given to  [2.x.130]  This class passes the type of data access for face integrals to the MPI data exchange routines  [2.x.131]  and  [2.x.132]  of the parallel vectors. The purpose is to not send all degrees of freedom of a neighboring element, but to reduce the amount of data to what is really needed for the computations at hand. The data exchange is a real bottleneck in particular for high-degree DG methods, therefore a more restrictive way of exchange is often beneficial. The enum field  [2.x.133]  can take the value `none`, which means that no face integrals at all are done, which would be analogous to  [2.x.134]  the value `values` meaning that only shape function values (but no derivatives) are used on faces, and the value `gradients` when also first derivatives on faces are accessed besides the values. A value `unspecified` means that all degrees of freedom will be exchanged for the faces that are located at the processor boundaries and designated to be worked on at the local processor.   
*   To see how the data can be reduced, think of the case of the nodal element FE_DGQ with node points on the element surface, where only  [2.x.135]  degrees of freedom contribute to the values on a face for polynomial degree  [2.x.136]  in  [2.x.137]  space dimensions, out of the  [2.x.138]  degrees of freedom of a cell. A similar reduction is also possible for the interior penalty method that evaluates values and first derivatives on the faces. When using a Hermite-like basis in 1D, only up to two basis functions contribute to the value and derivative. The class FE_DGQHermite implements a tensor product of this concept, as discussed in the introduction. Thus, only  [2.x.139]  degrees of freedom must be exchanged for each face, which is a clear win once  [2.x.140]  gets larger than four or five. Note that this reduced exchange of FE_DGQHermite is valid also on meshes with curved boundaries, as the derivatives are taken on the reference element, whereas the geometry only mixes them on the inside. Thus, this is different from the attempt to obtain  [2.x.141]  continuity with continuous Hermite-type shape functions where the non-Cartesian case changes the picture significantly. Obviously, on non-Cartesian meshes the derivatives also include tangential derivatives of shape functions beyond the normal derivative, but those only need the function values on the element surface, too. Should the element not provide any compression, the loop automatically exchanges all entries for the affected cells.
* 

* 
*  

* 
* [1.x.51]
* 
*  Since the Laplacian is symmetric, the `Tvmult()` (needed by the multigrid smoother interfaces) operation is simply forwarded to the `vmult()` case.
* 

* 
*  

* 
* [1.x.52]
* 
*  The cell operation is very similar to  [2.x.142] . We do not use a coefficient here, though. The second difference is that we replaced the two steps of  [2.x.143]  followed by  [2.x.144]  by a single function call  [2.x.145]  which internally calls the sequence of the two individual methods. Likewise,  [2.x.146]  implements the sequence of  [2.x.147]  followed by  [2.x.148]  In this case, these new functions merely save two lines of code. However, we use them for the analogy with FEFaceEvaluation where they are more important as explained below.
* 

* 
*  

* 
* [1.x.53]
* 
*  The face operation implements the terms of the interior penalty method in analogy to  [2.x.149] , as explained in the introduction. We need two evaluator objects for this task, one for handling the solution that comes from the cell on one of the two sides of an interior face, and one for handling the solution from the other side. The evaluators for face integrals are called FEFaceEvaluation and take a boolean argument in the second slot of the constructor to indicate which of the two sides the evaluator should belong two. In FEFaceEvaluation and MatrixFree, we call one of the two sides the `interior` one and the other the `exterior` one. The name `exterior` refers to the fact that the evaluator from both sides will return the same normal vector. For the `interior` side, the normal vector points outwards, whereas it points inwards on the other side, and is opposed to the outer normal vector of that cell. Apart from the new class name, we again get a range of items to work with in analogy to what was discussed in  [2.x.150] , but for the interior faces in this case. Note that the data structure of MatrixFree forms batches of faces that are analogous to the batches of cells for the cell integrals. All faces within a batch involve different cell numbers but have the face number within the reference cell, have the same refinement configuration (no refinement or the same subface), and the same orientation, to keep SIMD operations simple and efficient.   
*   Note that there is no implied meaning in interior versus exterior except the logic decision of the orientation of the normal, which is pretty random internally. One can in no way rely on a certain pattern of assigning interior versus exterior flags, as the decision is made for the sake of access regularity and uniformity in the MatrixFree setup routines. Since most sane DG methods are conservative, i.e., fluxes look the same from both sides of an interface, the mathematics are unaltered if the interior/exterior flags are switched and normal vectors get the opposite sign.
* 

* 
*  

* 
* [1.x.54]
* 
*  On a given batch of faces, we first update the pointers to the current face and then access the vector. As mentioned above, we combine the vector access with the evaluation. In the case of face integrals, the data access into the vector can be reduced for the special case of an FE_DGQHermite basis as explained for the data exchange above: Since only  [2.x.151]  out of the  [2.x.152]  cell degrees of freedom get multiplied by a non-zero value or derivative of a shape function, this structure can be utilized for the evaluation, significantly reducing the data access. The reduction of the data access is not only beneficial because it reduces the data in flight and thus helps caching, but also because the data access to faces is often more irregular than for cell integrals when gathering values from cells that are farther apart in the index list of cells.
* 

* 
* [1.x.55]
* 
*  The next two statements compute the penalty parameter for the interior penalty method. As explained in the introduction, we would like to have a scaling like  [2.x.153]  of the length  [2.x.154]  normal to the face. For a general non-Cartesian mesh, this length must be computed by the product of the inverse Jacobian times the normal vector in real coordinates. From this vector of `dim` components, we must finally pick the component that is oriented normal to the reference cell. In the geometry data stored in MatrixFree, a permutation of the components in the Jacobian is applied such that this latter direction is always the last component `dim-1` (this is beneficial because reference-cell derivative sorting can be made agnostic of the direction of the face). This means that we can simply access the last entry `dim-1` and must not look up the local face number in `data.get_face_info(face).interior_face_no` and `data.get_face_info(face).exterior_face_no`. Finally, we must also take the absolute value of these factors as the normal could point into either positive or negative direction.
* 

* 
* [1.x.56]
* 
*  In the loop over the quadrature points, we eventually compute all contributions to the interior penalty scheme. According to the formulas in the introduction, the value of the test function gets multiplied by the difference of the jump in the solution times the penalty parameter and the average of the normal derivative in real space. Since the two evaluators for interior and exterior sides get different signs due to the jump, we pass the result with a different sign here. The normal derivative of the test function gets multiplied by the negative jump in the solution between the interior and exterior side. This term, coined adjoint consistency term, must also include the factor of  [2.x.155]  in the code in accordance with its relation to the primal consistency term that gets the factor of one half due to the average in the test function slot.
* 

* 
* [1.x.57]
* 
*  Once we are done with the loop over quadrature points, we can do the sum factorization operations for the integration loops on faces and sum the results into the result vector, using the `integrate_scatter` function. The name `scatter` reflects the distribution of the vector data into scattered positions in the vector using the same pattern as in `gather_evaluate`. Like before, the combined integrate + write operation allows us to reduce the data access.
* 

* 
* [1.x.58]
* 
*  The boundary face function follows by and large the interior face function. The only difference is the fact that we do not have a separate FEFaceEvaluation object that provides us with exterior values  [2.x.156] , but we must define them from the boundary conditions and interior values  [2.x.157] . As explained in the introduction, we use  [2.x.158]  and  [2.x.159]  on Dirichlet boundaries and  [2.x.160]  and  [2.x.161]  on Neumann boundaries. Since this operation implements the homogeneous part, i.e., the matrix-vector product, we must neglect the boundary functions  [2.x.162]  and  [2.x.163]  here, and added them to the right hand side in  [2.x.164]  Note that due to extension of the solution  [2.x.165]  to the exterior via  [2.x.166] , we can keep all factors  [2.x.167]  the same as in the inner face function, see also the discussion in  [2.x.168] .   
*   There is one catch at this point: The implementation below uses a boolean variable `is_dirichlet` to switch between the Dirichlet and the Neumann cases. However, we solve a problem where we also want to impose periodic boundary conditions on some boundaries, namely along those in the  [2.x.169]  direction. One might wonder how those conditions should be handled here. The answer is that MatrixFree automatically treats periodic boundaries as what they are technically, namely an inner face where the solution values of two adjacent cells meet and must be treated by proper numerical fluxes. Thus, all the faces on the periodic boundaries will appear in the `apply_face()` function and not in this one.
* 

* 
*  

* 
* [1.x.59]
* 
*  Next we turn to the preconditioner initialization. As explained in the introduction, we want to construct an (approximate) inverse of the cell matrices from a product of 1D mass and Laplace matrices. Our first task is to compute the 1D matrices, which we do by first creating a 1D finite element. Instead of anticipating FE_DGQHermite<1> here, we get the finite element's name from DoFHandler, replace the  [2.x.170]  argument (2 or 3) by 1 to create a 1D name, and construct the 1D element by using FETools.
* 

* 
*  

* 
* [1.x.60]
* 
*  As for computing the 1D matrices on the unit element, we simply write down what a typical assembly procedure over rows and columns of the matrix as well as the quadrature points would do. We select the same Laplace matrices once and for all using the coefficients 0.5 for interior faces (but possibly scaled differently in different directions as a result of the mesh). Thus, we make a slight mistake at the Dirichlet boundary (where the correct factor would be 1 for the derivative terms and 2 for the penalty term, see  [2.x.171] ) or at the Neumann boundary where the factor should be zero. Since we only use this class as a smoother inside a multigrid scheme, this error is not going to have any significant effect and merely affects smoothing quality.
* 

* 
* [1.x.61]
* 
*  The left and right boundary terms assembled by the next two statements appear to have somewhat arbitrary signs, but those are correct as can be verified by looking at  [2.x.172]  and inserting the value
* 
*  -  and 1 for the normal vector in the 1D case.
* 

* 
* [1.x.62]
* 
*  Next, we go through the cells and pass the scaled matrices to TensorProductMatrixSymmetricSum to actually compute the generalized eigenvalue problem for representing the inverse: Since the matrix approximation is constructed as  [2.x.173]  and the weights are constant for each element, we can apply all weights on the Laplace matrix and simply keep the mass matrices unscaled. In the loop over cells, we want to make use of the geometry compression provided by the MatrixFree class and check if the current geometry is the same as on the last cell batch, in which case there is nothing to do. This compression can be accessed by  [2.x.174]  once `reinit()` has been called.     
*   Once we have accessed the inverse Jacobian through the FEEvaluation access function (we take the one for the zeroth quadrature point as they should be the same on all quadrature points for a Cartesian cell), we check that it is diagonal and then extract the determinant of the original Jacobian, i.e., the inverse of the determinant of the inverse Jacobian, and set the weight as  [2.x.175]  according to the 1D Laplacian times  [2.x.176]  copies of the mass matrix.
* 

* 
* [1.x.63]
* 
*  Once we know the factor by which we should scale the Laplace matrix, we apply this weight to the unscaled DG Laplace matrix and send the array to the class TensorProductMatrixSymmetricSum for computing the generalized eigenvalue problem mentioned in the introduction.
* 

* 
*  

* 
* [1.x.64]
* 
*  The vmult function for the approximate block-Jacobi preconditioner is very simple in the DG context: We simply need to read the values of the current cell batch, apply the inverse for the given entry in the array of tensor product matrix, and write the result back. In this loop, we overwrite the content in `dst` rather than first setting the entries to zero. This is legitimate for a DG method because every cell has independent degrees of freedom. Furthermore, we manually write out the loop over all cell batches, rather than going through  [2.x.177]  We do this because we know that we are not going to need data exchange over the MPI network here as all computations are done on the cells held locally on each processor.
* 

* 
*  

* 
* [1.x.65]
* 
*  The definition of the LaplaceProblem class is very similar to  [2.x.178] . One difference is the fact that we add the element degree as a template argument to the class, which would allow us to more easily include more than one degree in the same program by creating different instances in the `main()` function. The second difference is the selection of the element, FE_DGQHermite, which is specialized for this kind of equations.
* 

* 
*  

* 
* [1.x.66]
* 
*  The setup function differs in two aspects from  [2.x.179] . The first is that we do not need to interpolate any constraints for the discontinuous ansatz space, and simply pass a dummy AffineConstraints object into  [2.x.180]  The second change arises because we need to tell MatrixFree to also initialize the data structures for faces. We do this by setting update flags for the inner and boundary faces, respectively. On the boundary faces, we need both the function values, their gradients, JxW values (for integration), the normal vectors, and quadrature points (for the evaluation of the boundary conditions), whereas we only need shape function values, gradients, JxW values, and normal vectors for interior faces. The face data structures in MatrixFree are always built as soon as one of `mapping_update_flags_inner_faces` or `mapping_update_flags_boundary_faces` are different from the default value `update_default` of UpdateFlags.
* 

* 
*  

* 
* [1.x.67]
* 
*  The computation of the right hand side is a bit more complicated than in  [2.x.181] . The cell term now consists of the negative Laplacian of the analytical solution, `RightHandSide`, for which we need to first split up the Point of VectorizedArray fields, i.e., a batch of points, into a single point by evaluating all lanes in the VectorizedArray separately. Remember that the number of lanes depends on the hardware; it could be 1 for systems that do not offer vectorization (or where deal.II does not have intrinsics), but it could also be 8 or 16 on AVX-512 of recent Intel architectures.
* 

* 
* [1.x.68]
* 
*  Secondly, we also need to apply the Dirichlet and Neumann boundary conditions. This function is the missing part of to the function  [2.x.182]  function once the exterior solution values  [2.x.183]  and  [2.x.184]  on Dirichlet boundaries and  [2.x.185]  and  [2.x.186]  on Neumann boundaries are inserted and expanded in terms of the boundary functions  [2.x.187]  and  [2.x.188] . One thing to remember is that we move the boundary conditions to the right hand side, so the sign is the opposite from what we imposed on the solution part.     
*   We could have issued both the cell and the boundary part through a  [2.x.189]  part, but we choose to manually write the full loop over all faces to learn how the index layout of face indices is set up in MatrixFree: Both the inner faces and the boundary faces share the index range, and all batches of inner faces have lower numbers than the batches of boundary cells. A single index for both variants allows us to easily use the same data structure FEFaceEvaluation for both cases that attaches to the same data field, just at different positions. The number of inner face batches (where a batch is due to the combination of several faces into one for vectorization) is given by  [2.x.190]  whereas the number of boundary face batches is given by  [2.x.191] 
* 

* 
* [1.x.69]
* 
*  The MatrixFree class lets us query the boundary_id of the current face batch. Remember that MatrixFree sets up the batches for vectorization such that all faces within a batch have the same properties, which includes their `boundary_id`. Thus, we can query that id here for the current face index `face` and either impose the Dirichlet case (where we add something to the function value) or the Neumann case (where we add something to the normal derivative).
* 

* 
* [1.x.70]
* 
*  Since we have manually run the loop over cells rather than using  [2.x.192]  we must not forget to perform the data exchange with MPI
* 
*  - or actually, we would not need that for DG elements here because each cell carries its own degrees of freedom and cell and boundary integrals only evaluate quantities on the locally owned cells. The coupling to neighboring subdomain only comes in by the inner face integrals, which we have not done here. That said, it does not hurt to call this function here, so we do it as a reminder of what happens inside  [2.x.193] 
* 

* 
* [1.x.71]
* 
*  The `solve()` function is copied almost verbatim from  [2.x.194] . We set up the same multigrid ingredients, namely the level transfer, a smoother, and a coarse grid solver. The only difference is the fact that we do not use the diagonal of the Laplacian for the preconditioner of the Chebyshev iteration used for smoothing, but instead our newly resolved class `%PreconditionBlockJacobi`. The mechanisms are the same, though.
* 

* 
* [1.x.72]
* 
*  Since we have solved a problem with analytic solution, we want to verify the correctness of our implementation by computing the L2 error of the numerical result against the analytic solution.
* 

* 
*  

* 
* [1.x.73]
* 
*  The `run()` function sets up the initial grid and then runs the multigrid program in the usual way. As a domain, we choose a rectangle with periodic boundary conditions in the  [2.x.195] -direction, a Dirichlet condition on the front face in  [2.x.196]  direction (i.e., the face with index number 2, with boundary id equal to 0), and Neumann conditions on the back face as well as the two faces in  [2.x.197]  direction for the 3D case (with boundary id equal to 1). The extent of the domain is a bit different in the  [2.x.198]  direction (where we want to achieve a periodic solution given the definition of `Solution`) as compared to the  [2.x.199]  and  [2.x.200]  directions.
* 

* 
*  

* 
* [1.x.74]
* 
*  There is nothing unexpected in the `main()` function. We call `MPI_Init()` through the `MPI_InitFinalize` class, pass on the two parameters on the dimension and the degree set at the top of the file, and run the Laplace problem.
* 

* 
*  

* 
* [1.x.75]
* [1.x.76][1.x.77]
* 

* [1.x.78][1.x.79]
* 

* Like in  [2.x.201] , we evaluate the multigrid solver in terms of run time.  Intwo space dimensions with elements of degree 8, a possible output could lookas follows:
* [1.x.80]
* 
* Like in  [2.x.202] , the number of CG iterations remains constant with increasingproblem size. The iteration counts are a bit higher, which is because we use alower degree of the Chebyshev polynomial (2 vs 5 in  [2.x.203] ) and because theinterior penalty discretization has a somewhat larger spread ineigenvalues. Nonetheless, 13 iterations to reduce the residual by 12 orders ofmagnitude, or almost a factor of 9 per iteration, indicates an overall veryefficient method. In particular, we can solve a system with 21 million degreesof freedom in 5 seconds when using 12 cores, which is a very goodefficiency. Of course, in 2D we are well inside the regime of roundoff for apolynomial degree of 8 &ndash; as a matter of fact, around 83k DoFs or 0.025swould have been enough to fully converge this (simple) analytic solutionhere.
* Not much changes if we run the program in three spatial dimensions, except forthe fact that we now use do something more useful with the higher polynomialdegree and increasing mesh sizes, as the roundoff errors are only obtained atthe finest mesh. Still, it is remarkable that we can solve a 3D Laplaceproblem with a wave of three periods to roundoff accuracy on a twelve-coremachine pretty easily
* 
*  - using about 3.5 GB of memory in total for the secondto largest case with 24m DoFs, taking not more than eight seconds. The largestcase uses 30GB of memory with 191m DoFs.
* [1.x.81]
* 
* [1.x.82][1.x.83]
* 

* In the introduction and in-code comments, it was mentioned several times thathigh orders are treated very efficiently with the FEEvaluation andFEFaceEvaluation evaluators. Now, we want to substantiate these claims bylooking at the throughput of the 3D multigrid solver for various polynomialdegrees. We collect the times as follows: We first run a solver at problemsize close to ten million, indicated in the first four table rows, and recordthe timings. Then, we normalize the throughput by recording the number ofmillion degrees of freedom solved per second (MDoFs/s) to be able to comparethe efficiency of the different degrees, which is computed by dividing thenumber of degrees of freedom by the solver time.
*  [2.x.204] 
* We clearly see how the efficiency per DoF initially improves until it reachesa maximum for the polynomial degree  [2.x.205] . This effect is surprising, not onlybecause higher polynomial degrees often yield a vastly better solution, butespecially also when having matrix-based schemes in mind where the densercoupling at higher degree leads to a monotonously decreasing throughput (and adrastic one in 3D, with  [2.x.206]  being more than ten times slower than [2.x.207] !). For higher degrees, the throughput decreases a bit, which is both dueto an increase in the number of iterations (going from 12 at  [2.x.208]  to 19at  [2.x.209] ) and due to the  [2.x.210]  complexity of operatorevaluation. Nonetheless, efficiency as the time to solution would be stillbetter for higher polynomial degrees because they have better convergence rates (at leastfor problems as simple as this one): For  [2.x.211] , we reach roundoff accuracyalready with 1 million DoFs (solver time less than a second), whereas for  [2.x.212] we need 24 million DoFs and 8 seconds. For  [2.x.213] , the error is around [2.x.214]  with 57m DoFs and thus still far away from roundoff, despite taking 16seconds.
* Note that the above numbers are a bit pessimistic because they include thetime it takes the Chebyshev smoother to compute an eigenvalue estimate, whichis around 10 percent of the solver time. If the system is solved several times(as e.g. common in fluid dynamics), this eigenvalue cost is only paid once andfaster times become available.
* [1.x.84][1.x.85]
* 

* Finally, we take a look at some of the special ingredients presented in thistutorial program, namely the FE_DGQHermite basis in particular and thespecification of  [2.x.215]  In the following table, thethird row shows the optimized solver above, the fourth row shows the timingswith only the  [2.x.216]  set to `unspecified` rather thanthe optimal `gradients`, and the last one with replacing FE_DGQHermite by thebasic FE_DGQ elements where both the MPI exchange are more expensive and theoperations done by  [2.x.217]  and [2.x.218] 
*  [2.x.219] 
* The data in the table shows that not using  [2.x.220] increases costs by around 10% for higher polynomial degrees. For lowerdegrees, the difference is obviously less pronounced because thevolume-to-surface ratio is more beneficial and less data needs to beexchanged. The difference is larger when looking at the matrix-vector productonly, rather than the full multigrid solver shown here, with around 20% worsetimings just because of the MPI communication.
* For  [2.x.221]  and  [2.x.222] , the Hermite-like basis functions do obviously not reallypay off (indeed, for  [2.x.223]  the polynomials are exactly the same as for FE_DGQ)and the results are similar as with the FE_DGQ basis. However, for degreesstarting at three, we see an increasing advantage for FE_DGQHermite, showingthe effectiveness of these basis functions.
* [1.x.86][1.x.87]
* 

* As mentioned in the introduction, the fast diagonalization method is tied to aCartesian mesh with constant coefficients. If we wanted to solvevariable-coefficient problems, we would need to invest a bit more time in thedesign of the smoother parameters by selecting proper generalizations (e.g.,approximating the inverse on the nearest box-shaped element).
* Another way of extending the program would be to include support for adaptivemeshes, for which interface operations at edges of different refinementlevel become necessary, as discussed in  [2.x.224] .
* 

* [1.x.88][1.x.89] [2.x.225] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-60_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21]
*  [2.x.2] 
* [1.x.22]
*  [2.x.3] 
* 

* [1.x.23][1.x.24]
* 

* [1.x.25][1.x.26]
* 

* 
* In this tutorial we consider the case of two domains,  [2.x.4]  in [2.x.5]  and  [2.x.6]  in  [2.x.7] , where  [2.x.8]  isembedded in  [2.x.9]  ( [2.x.10] ). We want to solve a partialdifferential equation on  [2.x.11] , enforcing some conditions on the solution ofthe problemon the embedded domain*  [2.x.12] .
* There are two interesting scenarios:
* 
*  - the geometrical dimension `dim` of the embedded domain  [2.x.13]  is the same ofthe domain  [2.x.14]  (`spacedim`), that is, the spacedim-dimensional measure of [2.x.15]  is not zero, or
* 
*  - the embedded domain  [2.x.16]  has an intrinsic dimension `dim` which is smallerthan that of  [2.x.17]  (`spacedim`), thus its spacedim-dimensional measure iszero; for example it is a curve embedded in a two dimensional domain, or asurface embedded in a three-dimensional domain.
* In both cases define the restriction operator  [2.x.18]  as the operator that,given a continuous function on  [2.x.19] , returns its (continuous) restriction on [2.x.20] , i.e.,
* [1.x.27]
* It is well known that the operator  [2.x.21]  can be extended to a continuousoperator on  [2.x.22] , mapping functions in  [2.x.23]  to functions in [2.x.24]  when the intrinsic dimension of  [2.x.25]  is the same of  [2.x.26] .
* The same is true, with a less regular range space (namely  [2.x.27] ),when the dimension of  [2.x.28]  is one less with respect to  [2.x.29] , and [2.x.30]  does not have a boundary. In this second case, the operator  [2.x.31]  isalso known as thetrace* operator, and it is well defined for Lipschitzco-dimension one curves and surfaces  [2.x.32]  embedded in  [2.x.33]  (read  [1.x.28]for further details on the trace operator).
* The co-dimension two case is a little more complicated, and in general it is notpossible to construct a continuous trace operator, not even from  [2.x.34]  to [2.x.35] , when the dimension of  [2.x.36]  is zero or one respectively in twoand three dimensions.
* In this tutorial program we're not interested in further details on  [2.x.37] : wetake the extension  [2.x.38]  for granted, assuming that the dimension of theembedded domain (`dim`) is always smaller by one or equal with respect to thedimension of the embedding domain  [2.x.39]  (`spacedim`).
* We are going to solve the following differential problem: given a sufficientlyregular function  [2.x.40]  on  [2.x.41] , find the solution  [2.x.42]  to
* [1.x.29]
* 
* This is a constrained problem, where we are looking for a harmonic function  [2.x.43] that satisfies homogeneous boundary conditions on  [2.x.44] , subject tothe constraint  [2.x.45]  using a Lagrange multiplier.
* This problem has a physical interpretation: harmonic functions, i.e., functionsthat satisfy the Laplace equation, can be thought of as the displacements of amembrane whose boundary values are prescribed. The current situation thencorresponds to finding the shape of a membrane for which not only thedisplacement at the boundary, but also on  [2.x.46]  is prescribed. For example,if  [2.x.47]  is a closed curve in 2d space, then that would model a soap filmthat is held in place by a wire loop along  [2.x.48]  as well as a secondloop along  [2.x.49] . In cases where  [2.x.50]  is a whole area, you can think ofthis as a membrane that is stretched over an obstacle where  [2.x.51]  is thecontact area. (If the contact area is not known we have a different problem
* 
*  - called the "obstacle problem"
* 
*  -  which is modeled in  [2.x.52] .)
* As a first example we study the zero Dirichlet boundary condition on [2.x.53] . The same equations apply if we apply zero Neumann boundaryconditions on  [2.x.54]  or a mix of the two.
* The variational formulation can be derived by introducing two infinitedimensional spaces  [2.x.55]  and  [2.x.56] , respectively for the solution [2.x.57]  and for the Lagrange multiplier  [2.x.58] .
* Multiplying the first equation by  [2.x.59]  and the second by  [2.x.60] , integrating by parts when possible, and exploiting the boundaryconditions on  [2.x.61] , we obtain the following variational problem:
* Given a sufficiently regular function  [2.x.62]  on  [2.x.63] , find the solution  [2.x.64]  to[1.x.30]
* 
* where  [2.x.65]  and  [2.x.66]  represent,respectively,  [2.x.67]  scalar products in  [2.x.68]  and in  [2.x.69] .
* Inspection of the variational formulation tells us that the space  [2.x.70] can be taken to be  [2.x.71] . The space  [2.x.72] , in the co-dimensionzero case, should be taken as  [2.x.73] , while in the co-dimension one caseshould be taken as  [2.x.74] .
* The function  [2.x.75]  should therefore be either in  [2.x.76]  (for theco-dimension zero case) or  [2.x.77]  (for the co-dimension one case).This leaves us with a Lagrange multiplier  [2.x.78]  in  [2.x.79] , which iseither  [2.x.80]  or  [2.x.81] .
* There are two options for the discretization of the problem above. One could choosematching discretizations, where the Triangulation for  [2.x.82]  is aligned with theTriangulation for  [2.x.83] , or one could choose to discretize the two domains ina completely independent way.
* The first option is clearly more indicated for the simple problem weproposed above: it is sufficient to use a single Triangulation for  [2.x.84]  andthen impose certain constraints depending  [2.x.85] . An example of this approachis studied in  [2.x.86] , where the solution has to stay above an obstacle and thisis achieved imposing constraints on  [2.x.87] .
* To solve more complex problems, for example one where the domain  [2.x.88]  is timedependent, the second option could be a more viable solution. Handlingnon aligned meshes is complex by itself: to illustrate how is done we study asimple problem.
* The technique we describe here is presented in the literature using one of many names:the [1.x.31], the [1.x.32], the[1.x.33], and others. The main principle isthat the discretization of the two grids and of the two finite element spacesare kept completely independent. This technique is particularly efficient forthe simulation of fluid-structure interaction problems, where the configurationof the embedded structure is part of the problem itself, and one solves a(possibly non-linear) elastic problem to determine the (time dependent)configuration of  [2.x.89] , and a (possibly non-linear) flow problem in  [2.x.90] , plus coupling conditions on the interface between the fluidand the solid.
* In this tutorial program we keep things a little simpler, and we assume that theconfiguration of the embedded domain is given in one of two possible ways:
* 
*  - as a deformation mapping  [2.x.91] ,defined on a continuous finite dimensional space on  [2.x.92]  and representing,for any point  [2.x.93] , its coordinate  [2.x.94]  in  [2.x.95] ;
* 
*  - as a displacement mapping  [2.x.96]  for  [2.x.97] ,representing for any point  [2.x.98]  the displacement vector applied in order todeform  [2.x.99]  to its actual configuration  [2.x.100] .
* We define the embedded reference domain  [2.x.101]  `embedded_grid`: onthis triangulation we construct a finite dimensional space (`embedded_configuration_dh`)to describe either the deformation or the displacement through a FiniteElementsystem of FE_Q objects (`embedded_configuration_fe`). This finite dimensionalspace is used only to interpolate a user supplied function(`embedded_configuration_function`) representing either  [2.x.102]  (if theparameter `use_displacement` is set to  [2.x.103]  or  [2.x.104]  (if theparameter `use_displacement` is set to  [2.x.105] 
* The Lagrange multiplier  [2.x.106]  and the user supplied function  [2.x.107]  aredefined through another finite dimensional space `embedded_dh`, and throughanother FiniteElement `embedded_fe`, using the same reference domain. Inorder to take into account the deformation of the domain, either a MappingFEFieldor a MappingQEulerian object are initialized with the `embedded_configuration`vector.
* In the embedding space, a standard finite dimensional space `space_dh` isconstructed on the embedding grid `space_grid`, using theFiniteElement `space_fe`, following almost verbatim the approach taken in  [2.x.108] .
* We represent the discretizations of the spaces  [2.x.109]  and  [2.x.110]  with[1.x.34]and[1.x.35]respectively, where  [2.x.111]  is the dimension of `space_dh`, and  [2.x.112] the dimension of `embedded_dh`.
* Once all the finite dimensional spaces are defined, the variational formulationof the problem above leaves us with the following finite dimensional systemof equations:
* [1.x.36]
* where
* [1.x.37]
* 
* While the matrix  [2.x.113]  is the standard stiffness matrix for the Poisson problem on [2.x.114] , and the vector  [2.x.115]  is a standard right-hand-side vector for a finiteelement problem with forcing term  [2.x.116]  on  [2.x.117] , (see, for example,  [2.x.118] ),the matrix  [2.x.119]  or its transpose  [2.x.120]  are non-standard since they coupleinformation on two non-matching grids.
* In particular, the integral that appears in the computation of a single entry of [2.x.121] , is computed on  [2.x.122] . As usual in finite elements we split thisintegral into contributions from all cells of the triangulation used todiscretize  [2.x.123] , we transform the integral on  [2.x.124]  to an integral on thereference element  [2.x.125] , where  [2.x.126]  is the mapping from  [2.x.127]  to  [2.x.128] ,and compute the integral on  [2.x.129]  using a quadrature formula:
* [1.x.38]
* Computing this sum is non-trivial because we have to evaluate  [2.x.130] . In general, if  [2.x.131]  and  [2.x.132]  are not aligned, the point [2.x.133]  is completely arbitrary with respect to  [2.x.134] , and unlesswe figure out a way to interpolate all basis functions of  [2.x.135]  on anarbitrary point on  [2.x.136] , we cannot compute the integral needed for an entryof the matrix  [2.x.137] .
* To evaluate  [2.x.138]  the following steps needs to betaken (as shown in the picture below):
* 
*  - For a given cell  [2.x.139]  in  [2.x.140]  compute the real point  [2.x.141] , where  [2.x.142]  is one of the quadrature points used for the integral on  [2.x.143] .
* 
*  - Find the cell of  [2.x.144]  in which  [2.x.145]  lies. We shall call this element  [2.x.146] .
* 
*  - To evaluate the basis function use the inverse of the mapping  [2.x.147]  thattransforms the reference element  [2.x.148]  into the element  [2.x.149] :  [2.x.150] .
*  [2.x.151] 
* The three steps above can be computed by calling, in turn,
* 
*  -  [2.x.152]  followed by
* 
*  -  [2.x.153]  We then
* 
*  - construct a custom Quadrature formula, containing the point in the reference cell and then
* 
*  - construct an FEValues object, with the given quadrature formula, and initialized with the cell obtained in the first step.
* This is what the deal.II function  [2.x.154]  does whenevaluating a finite element field (not just a single shape function) at anarbitrary point; but this would be inefficient in this case.
* A better solution is to use a convenient wrapper to perform the first threesteps on a collection of points:  [2.x.155]  If one isactually interested in computing the full coupling matrix, then it is possibleto call the method  [2.x.156]  that performs theabove steps in an efficient way, reusing all possible data structures, andgathering expensive steps together. This is the function we'll be using later inthis tutorial.
* We solve the final saddle point problem by an iterative solver, applied to theSchur complement  [2.x.157]  (whose construction is described, for example, in  [2.x.158] ),and we construct  [2.x.159]  using LinearOperator classes.
* 

* [1.x.39][1.x.40]
* 

* The problem we solve here is identical to  [2.x.160] , with the difference that weimpose some constraints on an embedded domain  [2.x.161] . The tutorial is writtenin a dimension independent way, and in the results section we show how to varyboth `dim` and `spacedim`.
* The tutorial is compiled for `dim` equal to one and `spacedim` equal to two. Ifyou want to run the program in embedding dimension `spacedim` equal to three,you will most likely want to change the reference domain for  [2.x.162]  to be, forexample, something you read from file, or a closed sphere that you later deformto something more interesting.
* In the default scenario,  [2.x.163]  has co-dimension one, and this tutorialprogram implements the Fictitious Boundary Method. As it turns out, the sametechniques are used in the Variational Immersed Finite Element Method, andthe coupling operator  [2.x.164]  defined above is the same in almost all of thesenon-matching methods.
* The embedded domain is assumed to be included in  [2.x.165] , which we take as theunit square  [2.x.166] . The definition of the fictitious domain  [2.x.167]  can bemodified through the parameter file, and can be given as a mapping from thereference interval  [2.x.168]  to a curve in  [2.x.169] .
* If the curve is closed, then the results will be similar to running the sameproblem on a grid whose boundary is  [2.x.170] . The program will happily run alsowith a non-closed  [2.x.171] , although in those cases the mathematicalformulation of the problem is more difficult, since  [2.x.172]  will have aboundary by itself that has co-dimension two with respect to the domain [2.x.173] .
* 

* [1.x.41][1.x.42]
* 

*  [2.x.174]  [2.x.175]  Glowinski, R., T.-W. Pan, T.I. Hesla, and D.D. Joseph. 1999. “A Distributed  Lagrange Multiplier/fictitious Domain Method for Particulate Flows.”  International Journal of Multiphase Flow 25 (5). Pergamon: 755–94.
*  [2.x.176]  Boffi, D., L. Gastaldi, L. Heltai, and C.S. Peskin. 2008. “On the  Hyper-Elastic Formulation of the Immersed Boundary Method.” Computer Methods  in Applied Mechanics and Engineering 197 (25–28).
*  [2.x.177]  Heltai, L., and F. Costanzo. 2012. “Variational Implementation of Immersed  Finite Element Methods.” Computer Methods in Applied Mechanics and Engineering  229–232. [2.x.178] 
* 

*  [1.x.43] [1.x.44]
*   [1.x.45]  [1.x.46] Most of these have been introduced elsewhere, we'll comment only on the new ones.
* 

* 
*  

* 
* [1.x.47]
* 
*  The parameter acceptor class is the first novelty of this tutorial program: in general parameter files are used to steer the execution of a program at run time. While even a simple approach saves compile time, as the same executable can be run with different parameter settings, it can become difficult to handle hundreds of parameters simultaneously while maintaining compatibility between different programs. This is where the class ParameterAcceptor proves useful.
* 

* 
*  This class is used to define a public interface for classes that want to use a single global ParameterHandler to handle parameters. The class provides a static ParameterHandler member, namely  [2.x.179]  and implements the "Command design pattern" (see, for example, E. Gamma, R. Helm, R. Johnson, J. Vlissides, Design Patterns: Elements of Reusable Object-Oriented Software, Addison-Wesley Professional, 1994. https://goo.gl/FNYByc).
* 

* 
*  ParameterAcceptor provides a global subscription mechanism. Whenever an object of a class derived from ParameterAcceptor is constructed, a pointer to that object-of-derived-type is registered, together with a section entry in the parameter file. Such registry is traversed upon invocation of the single function  [2.x.180]  which in turn makes sure that all classes stored in the global registry declare the parameters they will be using, and after having declared them, it reads the content of `file.prm` to parse the actual parameters.
* 

* 
*  If you call the method  [2.x.181]  for each of the parameters you want to use in your code, there is nothing else you need to do. If you are using an already existing class that provides the two functions `declare_parameters` and `parse_parameters`, you can still use ParameterAcceptor, by encapsulating the existing class into a ParameterAcceptorProxy class.
* 

* 
*  In this example, we'll use both strategies, using ParameterAcceptorProxy for deal.II classes, and deriving our own parameter classes directly from ParameterAcceptor.
* 

* 
* [1.x.48]
* 
*  The other new include file is the one that contains the  [2.x.182]  class. The structure of deal.II, as many modern numerical libraries, is organized following a Directed Acyclic Graph (DAG). A DAG is a directed graph with topological ordering: each node structurally represents an object, and is connected to non-root nodes by one (or more) oriented edges, from the parent to the child. The most significant example of this structure is the Triangulation and its  [2.x.183]  structure. From a Triangulation (the main node), we can access each cell (children nodes of the triangulation). From the cells themselves we can access over all vertices of the cell. In this simple example, the DAG structure can be represented as three node types (the triangulation, the cell iterator, and the vertex) connected by oriented edges from the triangulation to the cell iterators, and from the cell iterator to the vertices. This has several advantages, but it intrinsically creates “asymmetries”, making certain operations fast and their inverse very slow: finding the vertices of a cell has low computational cost, and can be done by simply traversing the DAG, while finding all the cells that share a vertex requires a non-trivial computation unless a new DAG data structure is added that represents the inverse search.
* 

* 
*  Since inverse operations are usually not needed in a finite element code, these are implemented in GridTools without the use of extra data structures related to the Triangulation which would make them much faster. One such data structure, for example, is a map from the vertices of a Triangulation to all cells that share those vertices, which would reduce the computations needed to answer to the previous question.
* 

* 
*  Some methods, for example  [2.x.184]  make heavy usage of these non-standard operations. If you need to call these methods more than once, it becomes convenient to store those data structures somewhere.  [2.x.185]  does exactly this, giving you access to previously computed objects, or computing them on the fly (and then storing them inside the class for later use), and making sure that whenever the Triangulation is updated, also the relevant data structures are recomputed.
* 

* 
* [1.x.49]
* 
*  In this example, we will be using a reference domain to describe an embedded Triangulation, deformed through a finite element vector field.
* 

* 
*  The next two include files contain the definition of two classes that can be used in these cases. MappingQEulerian allows one to describe a domain through adisplacement* field, based on a FESystem[FE_Q(p)^spacedim] finite element space. The second is a little more generic, and allows you to use arbitrary vector FiniteElement spaces, as long as they provide acontinuous*
 description of your domain. In this case, the description is done through the actualdeformation* field, rather than adisplacement* field.
* 

* 
*  Which one is used depends on how the user wants to specify the reference domain, and/or the actual configuration. We'll provide both options, and experiment a little in the results section of this tutorial program.
* 

* 
* [1.x.50]
* 
*  The parsed function class is another new entry. It allows one to create a Function object, starting from a string in a parameter file which is parsed into an object that you can use anywhere deal.II accepts a Function (for example, for interpolation, boundary conditions, etc.).
* 

* 
* [1.x.51]
* 
*  This is the last new entry for this tutorial program. The namespace NonMatching contains a few methods that are useful when performing computations on non-matching grids, or on curves that are not aligned with the underlying mesh.
* 

* 
*  We'll discuss its use in detail later on in the `setup_coupling` method.
* 

* 
* [1.x.52]
* 
*   [1.x.53]  [1.x.54]   
*   In the DistributedLagrangeProblem, we need two parameters describing the dimensions of the domain  [2.x.186]  (`dim`) and of the domain  [2.x.187]  (`spacedim`).   
*   These will be used to initialize a Triangulation<dim,spacedim> (for  [2.x.188] ) and a Triangulation<spacedim,spacedim> (for  [2.x.189] ).   
*   A novelty with respect to other tutorial programs is the heavy use of  [2.x.190]  These behave like classical pointers, with the advantage of doing automatic house-keeping: the contained object is automatically destroyed as soon as the unique_ptr goes out of scope, even if it is inside a container or there's an exception. Moreover it does not allow for duplicate pointers, which prevents ownership problems. We do this, because we want to be able to i) construct the problem, ii) read the parameters, and iii) initialize all objects according to what is specified in a parameter file.   
*   We construct the parameters of our problem in the internal class `Parameters`, derived from ParameterAcceptor. The `DistributedLagrangeProblem` class takes a const reference to a `Parameters` object, so that it is not possible to modify the parameters from within the DistributedLagrangeProblem class itself.   
*   We could have initialized the parameters first, and then pass the parameters to the DistributedLagrangeProblem assuming all entries are set to the desired values, but this has two disadvantages:
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - We should not make assumptions on how the user initializes a class that is not under our direct control. If the user fails to initialize the class, we should notice and throw an exception;
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - Not all objects that need to read parameters from a parameter file may be available when we construct the Parameters; this is often the case for complex programs, with multiple physics, or where we reuse existing code in some external classes. We simulate this by keeping some "complex" objects, like ParsedFunction objects, inside the `DistributedLagrangeProblem` instead of inside the `Parameters`.   
*   Here we assume that upon construction, the classes that build up our problem are not usable yet. Parsing the parameter file is what ensures we have all ingredients to build up our classes, and we design them so that if parsing fails, or is not executed, the run is aborted.
* 

* 
*  

* 
* [1.x.55]
* 
*  The `Parameters` class is derived from ParameterAcceptor. This allows us to use the  [2.x.191]  method in its constructor.     
*   The members of this function are all non-const, but the `DistributedLagrangeProblem` class takes a const reference to a `Parameters` object: this ensures that parameters are not modified from within the `DistributedLagrangeProblem` class.
* 

* 
* [1.x.56]
* 
*  The parameters now described can all be set externally using a parameter file: if no parameter file is present when running the executable, the program will create a "parameters.prm" file with the default values defined here, and then abort to give the user a chance to modify the parameters.prm file.
* 

* 
*  Initial refinement for the embedding grid, corresponding to the domain  [2.x.192] .
* 

* 
* [1.x.57]
* 
*  The interaction between the embedded grid  [2.x.193]  and the embedding grid  [2.x.194]  is handled through the computation of  [2.x.195] , which involves all cells of  [2.x.196]  overlapping with parts of  [2.x.197] : a higher refinement of such cells might improve quality of our computations. For this reason we define `delta_refinement`: if it is greater than zero, then we mark each cell of the space grid that contains a vertex of the embedded grid and its neighbors, execute the refinement, and repeat this process `delta_refinement` times.
* 

* 
* [1.x.58]
* 
*  Starting refinement of the embedded grid, corresponding to the domain  [2.x.198] .
* 

* 
* [1.x.59]
* 
*  The list of boundary ids where we impose homogeneous Dirichlet boundary conditions. On the remaining boundary ids (if any), we impose homogeneous Neumann boundary conditions. As a default problem we have zero Dirichlet boundary conditions on  [2.x.199] 
* 

* 
* [1.x.60]
* 
*  FiniteElement degree of the embedding space:  [2.x.200] 
* 

* 
* [1.x.61]
* 
*  FiniteElement degree of the embedded space:  [2.x.201] 
* 

* 
* [1.x.62]
* 
*  FiniteElement degree of the space used to describe the deformation of the embedded domain
* 

* 
* [1.x.63]
* 
*  Order of the quadrature formula used to integrate the coupling
* 

* 
* [1.x.64]
* 
*  If set to true, then the embedded configuration function is interpreted as a displacement function
* 

* 
* [1.x.65]
* 
*  Level of verbosity to use in the output
* 

* 
* [1.x.66]
* 
*  A flag to keep track if we were initialized or not
* 

* 
* [1.x.67]
* 
*  Entry point for the DistributedLagrangeProblem
* 

* 
* [1.x.68]
* 
*  Object containing the actual parameters
* 

* 
* [1.x.69]
* 
*  The following functions are similar to all other tutorial programs, with the exception that we now need to set up things for two different families of objects, namely the ones related to theembedding* grids, and the ones related to theembedded* one.
* 

* 
*  

* 
* [1.x.70]
* 
*  The only unconventional function we have here is the `setup_coupling()` method, used to generate the sparsity patter for the coupling matrix  [2.x.202] .
* 

* 
*  

* 
* [1.x.71]
* 
*  first we gather all the objects related to the embedding space geometry
* 

* 
*  

* 
* [1.x.72]
* 
*  Then the ones related to the embedded grid, with the DoFHandler associated to the Lagrange multiplier `lambda`
* 

* 
*  

* 
* [1.x.73]
* 
*  And finally, everything that is needed todeform* the embedded triangulation
* 

* 
* [1.x.74]
* 
*  The ParameterAcceptorProxy class is a "transparent" wrapper derived from both ParameterAcceptor and the type passed as its template parameter. At construction, the arguments are split into two parts: the first argument is an  [2.x.203]  forwarded to the ParameterAcceptor class, and containing the name of the section that should be used for this class, while all the remaining arguments are forwarded to the constructor of the templated type, in this case, to the  [2.x.204]  constructor.     
*   This class allows you to use existing classes in conjunction with the ParameterAcceptor registration mechanism, provided that those classes have the members `declare_parameters()` and `parse_parameters()`.     
*   This is the case here, making it fairly easy to exploit the  [2.x.205]  class: instead of requiring users to create new Function objects in their code for the RHS, boundary functions, etc., (like it is done in most of the other tutorials), here we allow the user to use deal.II interface to muParser (http://muparser.beltoforion.de), where the specification of the function is not done at compile time, but at run time, using a string that is parsed into an actual Function object.     
*   In this case, the `embedded_configuration_function` is a vector valued Function that can be interpreted as either adeformation* or adisplacement* according to the boolean value of `parameters.use_displacement`. The number of components is specified later on in the construction.
* 

* 
*  

* 
* [1.x.75]
* 
*  We do the same thing to specify the value of the function  [2.x.206] , which is what we want our solution to be in the embedded space. In this case the Function is a scalar one.
* 

* 
* [1.x.76]
* 
*  Similarly to what we have done with the  [2.x.207]  class, we repeat the same for the ReductionControl class, allowing us to specify all possible stopping criteria for the Schur complement iterative solver we'll use later on.
* 

* 
* [1.x.77]
* 
*  Next we gather all SparsityPattern, SparseMatrix, and Vector objects we'll need
* 

* 
* [1.x.78]
* 
*  The TimerOutput class is used to provide some statistics on the performance of our program.
* 

* 
* [1.x.79]
* 
*   [1.x.80]  [1.x.81]   
*   At construction time, we initialize also the ParameterAcceptor class, with the section name we want our problem to use when parsing the parameter file.   
*   Parameter files can be organized into section/subsection/etc.: this has the advantage that defined objects share parameters when sharing the same section/subsection/etc. ParameterAcceptor allows to specify the section name using Unix conventions on paths. If the section name starts with a slash ("/"), then the section is interpreted as anabsolute path*, ParameterAcceptor enters a subsection for each directory in the path, using the last name it encountered as the landing subsection for the current class.   
*   For example, if you construct your class using `ParameterAcceptor("/first/second/third/My Class")`, the parameters will be organized as follows:   
*    [2.x.208]    
*   Internally, thecurrent path* stored in ParameterAcceptor is now considered to be "/first/second/third/", i.e. when you specify an absolute path, ParameterAcceptorchanges* the current section to the current path, i.e. to the path of the section name until thelast* "/".   
*   You can now construct another class derived from ParameterAcceptor using a relative path (e.g., `ParameterAcceptor("My Other Class")`) instead of the absolute one (e.g. `ParameterAcceptor("/first/second/third/My Other Class")`), obtaining:  [2.x.209]    
*   If the section nameends* with a slash then subsequent classes will interpret this as a full path: for example, similar to the one above, if we have two classes, one initialized with `ParameterAcceptor("/first/second/third/My Class/")` and the other with `ParameterAcceptor("My Other Class")`, then the resulting parameter file will look like:   
*    [2.x.210]    
*   We are going to exploit this, by making our `Parameters` theparent* of all subsequently constructed classes. Since most of the other classes are members of `DistributedLagrangeProblem` this allows, for example, to construct two `DistributedLagrangeProblem` for two different dimensions, without having conflicts in the parameters for the two problems.
* 

* 
* [1.x.85]
* 
*  The  [2.x.211]  function does a few things:
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - enters the subsection specified at construction time to ParameterAcceptor
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - calls the  [2.x.212]  function
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - calls any signal you may have attached to  [2.x.213] 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - leaves the subsection     
*   In turn,  [2.x.214] 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - declares an entry in the parameter handler for the given variable;
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - takes the current value of the variable
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - transforms it to a string, used as the default value for the parameter file
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - attaches anaction* to  [2.x.215]  that monitors when a file is parsed, or when an entry is set, and when this happens, it updates the value of the variable passed to `add_parameter()` by setting it to whatever was specified in the input file (of course, after the input file has been parsed and the text representation converted to the type of the variable).
* 

* 
* [1.x.86]
* 
*  Once the parameter file has been parsed, then the parameters are good to go. Set the internal variable `initialized` to true.
* 

* 
* [1.x.87]
* 
*  The constructor is pretty standard, with the exception of the `ParameterAcceptorProxy` objects, as explained earlier.
* 

* 
* [1.x.88]
* 
*  Here is a way to set default values for a ParameterAcceptor class that was constructed using ParameterAcceptorProxy.     
*   In this case, we set the default deformation of the embedded grid to be a circle with radius  [2.x.216]  and center  [2.x.217] , we set the default value for the embedded_value_function to be the constant one, and specify some sensible values for the SolverControl object.     
*   It is fundamental for  [2.x.218]  to be embedded: from the definition of  [2.x.219]  is clear that, if  [2.x.220] , certain rows of the matrix  [2.x.221]  will be zero. This would be a problem, as the Schur complement method requires  [2.x.222]  to have full column rank.
* 

* 
* [1.x.89]
* 
*   [1.x.90]  [1.x.91]   
*   The function  [2.x.223]  is used to set up the finite element spaces. Notice how  [2.x.224]  is used to create objects wrapped inside  [2.x.225]  objects.
* 

* 
* [1.x.92]
* 
*  Initializing  [2.x.226] : constructing the Triangulation and wrapping it into a  [2.x.227]  object
* 

* 
* [1.x.93]
* 
*  Next, we actually create the triangulation using  [2.x.228]  The last argument is set to true: this activates colorization (i.e., assigning different boundary indicators to different parts of the boundary), which we use to assign the Dirichlet and Neumann conditions.
* 

* 
* [1.x.94]
* 
*  Once we constructed a Triangulation, we refine it globally according to the specifications in the parameter file, and construct a  [2.x.229]  with it.
* 

* 
* [1.x.95]
* 
*  The same is done with the embedded grid. Since the embedded grid is deformed, we first need to setup the deformation mapping. We do so in the following few lines:
* 

* 
* [1.x.96]
* 
*  Once we have defined a finite dimensional space for the deformation, we interpolate the `embedded_configuration_function` defined in the parameter file:
* 

* 
* [1.x.97]
* 
*  Now we can interpret it according to what the user has specified in the parameter file: as a displacement, in which case we construct a mapping thatdisplaces* the position of each support point of our configuration finite element space by the specified amount on the corresponding configuration vector, or as an absolution position.     
*   In the first case, the class MappingQEulerian offers its services, while in the second one, we'll use the class MappingFEField. They are in fact very similar. MappingQEulerian will only work for systems of FE_Q finite element spaces, where the displacement vector is stored in the first `spacedim` components of the FESystem, and the degree given as a parameter at construction time, must match the degree of the first `spacedim` components.     
*   The class MappingFEField is slightly more general, in that it allows you to select arbitrary FiniteElement types when constructing your approximation. Naturally some choices may (or may not) make sense, according to the type of FiniteElement you choose. MappingFEField implements the pure iso-parametric concept, and can be used, for example, to implement iso-geometric analysis codes in deal.II, by combining it with the FE_Bernstein finite element class. In this example, we'll use the two interchangeably, by taking into account the fact that one configuration will be a `displacement`, while the other will be an absolute `deformation` field.
* 

* 
*  

* 
* [1.x.98]
* 
*  In this tutorial program we not only refine  [2.x.230]  globally, but also allow a local refinement depending on the position of  [2.x.231] , according to the value of `parameters.delta_refinement`, that we use to decide how many rounds of local refinement we should do on  [2.x.232] , corresponding to the position of  [2.x.233] .     
*   With the mapping in place, it is now possible to query what is the location of all support points associated with the `embedded_dh`, by calling the method  [2.x.234]      
*   This method has two variants. One that doesnot* take a Mapping, and one that takes a Mapping. If you use the second type, like we are doing in this case, the support points are computed through the specified mapping, which can manipulate them accordingly.     
*   This is precisely what the `embedded_mapping` is there for.
* 

* 
* [1.x.99]
* 
*  Once we have the support points of the embedded finite element space, we would like to identify what cells of the embedding space contain what support point, to get a chance at refining the embedding grid where it is necessary, i.e., where the embedded grid is. This can be done manually, by looping over each support point, and then calling the method  [2.x.235]  for each cell of the embedding space, until we find one that returns points in the unit reference cell, or it can be done in a more intelligent way.     
*   The  [2.x.236]  is a possible option that performs the above task in a cheaper way, by first identifying the closest vertex of the embedding Triangulation to the target point, and then by calling  [2.x.237]  only for those cells that share the found vertex.     
*   In fact, there are algorithms in the GridTools namespace that exploit a  [2.x.238]  object, and possibly a KDTree object to speed up these operations as much as possible.     
*   The simplest way to exploit the maximum speed is by calling a specialized method,  [2.x.239]  that will store a lot of useful information and data structures during the first point search, and then reuse all of this for subsequent points.     
*    [2.x.240]  returns a tuple where the first element is a vector of cells containing the input points, in this case support_points. For refinement, this is the only information we need, and this is exactly what happens now.     
*   When we need to assemble a coupling matrix, however, we'll also need the reference location of each point to evaluate the basis functions of the embedding space. The other elements of the tuple returned by  [2.x.241]  allow you to reconstruct, for each point, what cell contains it, and what is the location in the reference cell of the given point. Since this information is better grouped into cells, then this is what the algorithm returns: a tuple, containing a vector of all cells that have at least one point in them, together with a list of all reference points and their corresponding index in the original vector.     
*   In the following loop, we will be ignoring all returned objects except the first, identifying all cells contain at least one support point of the embedded space. This allows for a simple adaptive refinement strategy: refining these cells and their neighbors.     
*   Notice that we need to do some sanity checks, in the sense that we want to have an embedding grid which is well refined around the embedded grid, but where two consecutive support points lie either in the same cell, or in neighbor embedding cells.     
*   This is only possible if we ensure that the smallest cell size of the embedding grid is nonetheless bigger than the largest cell size of the embedded grid. Since users can modify both levels of refinements, as well as the amount of local refinement they want around the embedded grid, we make sure that the resulting meshes satisfy our requirements, and if this is not the case, we bail out with an exception.
* 

* 
* [1.x.100]
* 
*  In order to construct a well posed coupling interpolation operator  [2.x.242] , there are some constraints on the relative dimension of the grids between the embedding and the embedded domains. The coupling operator  [2.x.243]  and the spaces  [2.x.244]  and  [2.x.245]  have to satisfy an inf-sup condition in order for the problem to have a solution. It turns out that the non-matching  [2.x.246]  projection satisfies such inf-sup, provided that the spaces  [2.x.247]  and  [2.x.248]  are compatible between each other (for example, provided that they are chosen to be the ones described in the introduction).     
*   However, thediscrete* inf-sup condition must also hold. No complications arise here, but it turns out that the discrete inf-sup constant deteriorates when the non-matching grids have local diameters that are too far away from each other. In particular, it turns out that if you choose an embedding grid which isfiner* with respect to the embedded grid, the inf-sup constant deteriorates much more than if you let the embedded grid be finer.     
*   In order to avoid issues, in this tutorial we will throw an exception if the parameters chosen by the user are such that the maximal diameter of the embedded grid is greater than the minimal diameter of the embedding grid.     
*   This choice guarantees that almost every cell of the embedded grid spans no more than two cells of the embedding grid, with some rare exceptions, that are negligible in terms of the resulting inf-sup.
* 

* 
* [1.x.101]
* 
*   [2.x.249]  has been refined and we can now set up its DoFs
* 

* 
* [1.x.102]
* 
*  We now set up the DoFs of  [2.x.250]  and  [2.x.251] : since they are fundamentally independent (except for the fact that  [2.x.252] 's mesh is more refined "around"  [2.x.253] ) the procedure is standard.
* 

* 
* [1.x.103]
* 
*  By definition the stiffness matrix involves only  [2.x.254] 's DoFs
* 

* 
* [1.x.104]
* 
*  By definition the rhs of the system we're solving involves only a zero vector and  [2.x.255] , which is computed using only  [2.x.256] 's DoFs
* 

* 
* [1.x.105]
* 
*  Creating the coupling sparsity pattern is a complex operation, but it can be easily done using the  [2.x.257]  which requires the two DoFHandler objects, the quadrature points for the coupling, a DynamicSparsityPattern (which then needs to be copied into the sparsity one, as usual), the component mask for the embedding and embedded Triangulation (which we leave empty) and the mappings for both the embedding and the embedded Triangulation.
* 

* 
* [1.x.106]
* 
*   [1.x.107]  [1.x.108]   
*   The following function creates the matrices: as noted before computing the stiffness matrix and the rhs is a standard procedure.
* 

* 
* [1.x.109]
* 
*  Embedding stiffness matrix  [2.x.258] , and the right hand side  [2.x.259] .
* 

* 
* [1.x.110]
* 
*  To compute the coupling matrix we use the  [2.x.260]  tool, which works similarly to  [2.x.261] 
* 

* 
* [1.x.111]
* 
*   [1.x.112]  [1.x.113]   
*   All parts have been assembled: we solve the system using the Schur complement method
* 

* 
* [1.x.114]
* 
*  Start by creating the inverse stiffness matrix
* 

* 
* [1.x.115]
* 
*  Initializing the operators, as described in the introduction
* 

* 
* [1.x.116]
* 
*  Using the Schur complement method
* 

* 
* [1.x.117]
* 
*  The following function simply generates standard result output on two separate files, one for each mesh.
* 

* 
* [1.x.118]
* 
*  The only difference between the two output routines is that in the second case, we want to output the data on the current configuration, and not on the reference one. This is possible by passing the actual embedded_mapping to the  [2.x.262]  function. The mapping will take care of outputting the result on the actual deformed configuration.
* 

* 
*  

* 
* [1.x.119]
* 
*  Similar to all other tutorial programs, the `run()` function simply calls all other methods in the correct order. Nothing special to note, except that we check if parsing was done before we actually attempt to run our program.
* 

* 
* [1.x.120]
* 
*  Differently to what happens in other tutorial programs, here we use ParameterAcceptor style of initialization, i.e., all objects are first constructed, and then a single call to the static method  [2.x.263]  is issued to fill all parameters of the classes that are derived from ParameterAcceptor.       
*   We check if the user has specified a parameter file name to use when the program was launched. If so, try to read that parameter file, otherwise, try to read the file "parameters.prm".       
*   If the parameter file that was specified (implicitly or explicitly) does not exist,  [2.x.264]  will create one for you, and exit the program.
* 

* 
*  

* 
* [1.x.121]
* [1.x.122][1.x.123]
* 

* The directory in which this program is run does not contain a parameter file bydefault. On the other hand, this program wants to read its parameters from afile called parameters.prm
* 
*  -  and so, when you execute it the first time, youwill get an exception that no such file can be found:
* [1.x.124]
* 
* However, as the error message already states, the code that triggers theexception will also generate a parameters.prm file that simply contains thedefault values for all parameters this program cares about. By inspection of theparameter file, we see the following:
* [1.x.125]
* 
* If you now run the program, you will get a file called `used_parameters.prm`,containing a shorter version of the above parameters (without comments anddocumentation), documenting all parameters that were used to run your program:
* [1.x.126]
* 
* The rationale behind creating first `parameters.prm` file (the first time theprogram is run) and then a `used_parameters.prm` (every other times you run theprogram), is because you may want to leave most parameters to their defaultvalues, and only modify a handful of them.
* For example, you could use the following (perfectly valid) parameter file withthis tutorial program:
* [1.x.127]
* 
* and you would obtain exactly the same results as in test case 1 below.
* [1.x.128][1.x.129]
* 

* For the default problem the value of  [2.x.265]  on  [2.x.266]  is set to the constant  [2.x.267] :this is like imposing a constant Dirichlet boundary condition on  [2.x.268] , seenas boundary of the portion of  [2.x.269]  inside  [2.x.270] . Similarly on  [2.x.271]  we have zero Dirichlet boundary conditions.
* 

*  [2.x.272] 
* The output of the program will look like the following:
* [1.x.130]
* 
* You may notice that, in terms of CPU time, assembling the coupling system istwice as expensive as assembling the standard Poisson system, even though thematrix is smaller. This is due to the non-matching nature of the discretization.Whether this is acceptable or not, depends on the applications.
* If the problem was set in a three-dimensional setting, and the immersed mesh wastime dependent, it would be much more expensive to recreate the mesh at eachstep rather than use the technique we present here. Moreover, you may be able tocreate a very fast and optimized solver on a uniformly refined square or cubicgrid, and embed the domain where you want to perform your computation using thetechnique presented here. This would require you to only have a surfacerepresentatio of your domain (a much cheaper and easier mesh to produce).
* To play around a little bit, we are going to complicate a little the fictitiousdomain as well as the boundary conditions we impose on it.
* [1.x.131][1.x.132]
* 

* If we use the following parameter file:
* [1.x.133]
* 
* We get a "flowery" looking domain, where we impose a linear boundary condition [2.x.273] . This test shows that the method is actually quite accurate inrecovering an exactly linear function from its boundary conditions, and eventhough the meshes are not aligned, we obtain a pretty good result.
* Replacing  [2.x.274]  with  [2.x.275] , i.e., modifying the parameter filesuch that we have
* [1.x.134]
* produces the saddle on the right.
*  [2.x.276] 
* [1.x.135][1.x.136][1.x.137]
* 

* [1.x.138][1.x.139]
* 

* While the current tutorial program is written for `spacedim` equal to two, thereare only minor changes you have to do in order for the program to run indifferent combinations of dimensions.
* If you want to run with `spacedim` equal to three and `dim` equal to two, thenyou will almost certainly want to perform the following changes:
* 
*  - use a different reference domain for the embedded grid, maybe reading it from  a file. It is not possible to construct a smooth closed surface with one  single parametrization of a square domain, therefore you'll most likely want  to use a reference domain that is topologically equivalent to a the boundary  of a sphere.
* 
*  - use a displacement instead of the deformation to map  [2.x.277]  into  [2.x.278] 
* [1.x.140][1.x.141]
* 

* We have seen in other tutorials (for example in  [2.x.279]  and  [2.x.280] ) how to readgrids from input files. A nice generalization for this tutorial program would beto allow the user to select a grid to read from the parameter file itself,instead of hardcoding the mesh type in the tutorial program itself.
* [1.x.142][1.x.143]
* 

* At the moment, we have no preconditioner on the Schur complement. This is ok fortwo dimensional problems, where a few hundred iterations bring the residual downto the machine precision, but it's not going to work in three dimensions.
* It is not obvious what a good preconditioner would be here. The physical problemwe are solving with the Schur complement, is to associate to the Dirichlet data [2.x.281] , the value of the Lagrange multiplier  [2.x.282] .  [2.x.283]  can beinterpreted as thejump* in the normal gradient that needs to be imposed on  [2.x.284] across  [2.x.285] , in order to obtain the Dirichlet data  [2.x.286] .
* So  [2.x.287]  is some sort of Neumann to Dirichlet map, and we would like to have agood approximation for the Dirichlet to Neumann map. A possibility would be touse a Boundary Element approximation of the problem on  [2.x.288] , and construct arough approximation of the hyper-singular operator for the Poisson problemassociated to  [2.x.289] , which is precisely a Dirichlet to Neumann map.
* [1.x.144][1.x.145]
* 

* The simple code proposed here can serve as a starting point for morecomplex problems which, to be solved, need to be run on parallelcode, possibly using distributed meshes (see  [2.x.290] ,  [2.x.291] , and thedocumentation for  [2.x.292]  and [2.x.293] 
* When using non-matching grids in parallel a problem arises: to compute thematrix  [2.x.294]  a process needs information about both meshes on the same portion ofreal space but, when working with distributed meshes, this information may notbe available, because the locally owned part of the  [2.x.295]  triangulationstored on a given processor may not be physically co-located with the locallyowned part of the  [2.x.296]  triangulation stored on the same processor.
* Various strategies can be implemented to tackle this problem:
* 
*  - distribute the two meshes so that this constraint is satisfied;
* 
*  - use communication for the parts of real space where the constraint is not  satisfied;
* 
*  - use a distributed triangulation for the embedding space, and a shared  triangulation for the emdedded configuration.
* The latter strategy is clearly the easiest to implement, as most of thefunctions used in this tutorial program will work unchanged also in the parallelcase. Of course one could use the reversal strategy (that is, have a distributedembedded Triangulation and a shared embedding Triangulation).
* However, this strategy is most likely going to be more expensive, since bydefinition the embedding grid is larger than the embedded grid, and it makesmore sense to distribute the largest of the two grids, maintaining the smallestone shared among all processors.
* 

* [1.x.146][1.x.147] [2.x.297] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-6_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28]
* [1.x.29][1.x.30][1.x.31]
* 

*  [2.x.2] 
* This program is finally about one of the main features of deal.II:the use of adaptively (locally) refined meshes. The program is stillbased on  [2.x.3]  and  [2.x.4] , and, as you will see, it does not actuallytake very much code to enable adaptivity. Indeed, while we do a greatdeal of explaining, adaptive meshes can be added to an existing programwith barely a dozen lines of additional code. The program shows whatthese lines are, as well as another important ingredient of adaptivemesh refinement (AMR): a criterion that can be used to determine whetherit is necessary to refine a cell because the error is large on it,whether the cell can be coarsened because the error is particularlysmall on it, or whether we should just leave the cell as it is. Wewill discuss all of these issues in the following.
* 

* [1.x.32][1.x.33]
* 

* There are a number of ways how one can adaptively refine meshes. Thebasic structure of the overall algorithm is always the same and consistsof a loop over the following steps:
* 
*  - Solve the PDE on the current mesh;
* 
*  - Estimate the error on each cell using some criterion that is indicative  of the error;
* 
*  - Mark those cells that have large errors for refinement, mark those that have  particularly small errors for coarsening, and leave the rest alone;
* 
*  - Refine and coarsen the cells so marked to obtain a new mesh;
* 
*  - Repeat the steps above on the new mesh until the overall error is  sufficiently small.
* For reasons that are probably lost to history (maybe that these functionsused to be implemented in FORTRAN, a language that does not care aboutwhether something is spelled in lower or UPPER case letters, with programmersoften choosing upper case letters habitually), the loop above is oftenreferenced in publications about mesh adaptivity as theSOLVE-ESTIMATE-MARK-REFINE loop (with this spelling).
* Beyond this structure, however, there are a variety of ways to achievethis. Fundamentally, they differ in how exactly one generates one meshfrom the previous one.
* If one were to use triangles (which deal.II does not do), then there aretwo essential possibilities:
* 
*  - Longest-edge refinement: In this strategy, a triangle marked for refinement  is cut into two by introducing one new edge from the midpoint of the longest  edge to the opposite vertex. Of course, the midpoint from the longest edge  has to somehow be balanced byalso* refining the cell on the other side of  that edge (if there is one). If the edge in question is also the longest  edge of the neighboring cell, then we can just run a new edge through the  neighbor to the opposite vertex; otherwise a slightly more involved  construction is necessary that adds more new vertices on at least one  other edge of the neighboring cell, and then may propagate to the neighbors  of the neighbor until the algorithm terminates. This is hard to describe  in words, and because deal.II does not use triangles not worth the time here.  But if you're curious, you can always watch video lecture 15 at the link  shown at the top of this introduction.
* 
*  - Red-green refinement: An alternative is what is called "red-green refinement".  This strategy is even more difficult to describe (but also discussed in the  video lecture) and has the advantage that the refinement does not propagate  beyond the immediate neighbors of the cell that we want to refine. It is,  however, substantially more difficult to implement.
* There are other variations of these approaches, but the important point isthat they always generate a mesh where the lines where two cells touchare entire edges of both adjacent cells. With a bit of work, this strategyis readily adapted to three-dimensional meshes made from tetrahedra.
* Neither of these methods works for quadrilaterals in 2d and hexahedra in 3d,or at least not easily. The reason is that the transition elements createdout of the quadrilateral neighbors of a quadrilateral cell that is to be refinedwould be triangles, and we don't want this. Consequently,the approach to adaptivity chosen in deal.II is to use grids in whichneighboring cells may differ in refinement level by one. This thenresults in nodes on the interfaces of cells which belong to oneside, but are unbalanced on the other. The common term for these is&ldquo;hanging nodes&rdquo;, and these meshes then look like this in a verysimple situation:
*  [2.x.5] 
* A more complicated two-dimensional mesh would look like this (and isdiscussed in the "Results" section below):
* <img src="https://www.dealii.org/images/steps/developer/step_6_grid_5_ladutenko.svg"     alt="Fifth adaptively refined Ladutenko grid: the cells are clustered          along the inner circle."     width="300" height="300">
* Finally, a three-dimensional mesh (from  [2.x.6] ) with such hanging nodes is shown here:
* <img src="https://www.dealii.org/images/steps/developer/ [2.x.7] .3d.mesh.png" alt=""     width="300" height="300">
* The first and third mesh are of course based on a square and a cube, but as thesecond mesh shows, this is not necessary. The important point is simply that wecan refine a mesh independently of its neighbors (subject to the constraintthat a cell can be only refined once more than its neighbors), but that we endup with these &ldquo;hanging nodes&rdquo; if we do this.
* 

* [1.x.34][1.x.35]
* 

* Now that you have seen what these adaptively refined meshes look like,you should ask [1.x.36] we would want to do this. After all, we know fromtheory that if we refine the mesh globally, the error will go down to zeroas
* [1.x.37]
* where  [2.x.8]  is some constant independent of  [2.x.9]  and  [2.x.10] , [2.x.11]  is the polynomial degree of the finite element in use, and [2.x.12]  is the diameter of the largest cell. So if the[1.x.38] cell is important, then why would we want to makethe mesh fine in some parts of the domain but not all?
* The answer lies in the observation that the formula above is notoptimal. In fact, some more work shows that the followingis a better estimate (which you should compare to the square ofthe estimate above):
* [1.x.39]
* (Because  [2.x.13] , this formula immediately implies theprevious one if you just pull the mesh size out of the sum.)What this formula suggests is that it is not necessary to makethe [1.x.40] cell small, but that the cells really onlyneed to be small [1.x.41]!In other words: The mesh really only has to be fine where thesolution has large variations, as indicated by the  [2.x.14] st derivative.This makes intuitive sense: if, for example, we use a linear element [2.x.15] , then places where the solution is nearly linear (as indicatedby  [2.x.16]  being small) will be well resolved even if the meshis coarse. Only those places where the second derivative is largewill be poorly resolved by large elements, and consequentlythat's where we should make the mesh small.
* Of course, this [1.x.42] is not very usefulin practice since we don't know the exact solution  [2.x.17]  of theproblem, and consequently, we cannot compute  [2.x.18] .But, and that is the approach commonly taken, we can computenumerical approximations of  [2.x.19]  based only onthe discrete solution  [2.x.20]  that we have computed before. Wewill discuss this in slightly more detail below. This will thenhelp us determine which cells have a large  [2.x.21] st derivative,and these are then candidates for refining the mesh.
* 

* [1.x.43][1.x.44]
* 

* The methods using triangular meshes mentioned above go to greatlengths to make sure that each vertex is a vertex of all adjacentcells
* 
*  -  i.e., that there are no hanging nodes. This thenautomatically makes sure that we can define shape functions in such away that they are globally continuous (if we use the common  [2.x.22] Lagrange finite element methods we have been using so far in thetutorial programs, as represented by the FE_Q class).
* On the other hand, if we define shape functions on meshes with hangingnodes, we may end up with shape functions that are not continuous. Tosee this, think about the situation above where the top right cell isnot refined, and consider for a moment the use of a bilinear finiteelement. In that case, the shape functions associated with the hangingnodes are defined in the obvious way on the two small cells adjacentto each of the hanging nodes. But how do we extend them to the bigadjacent cells? Clearly, the function's extension to the big cellcannot be bilinear because then it needs to be linear along each edgeof the large cell, and that means that it needs to be zero on theentire edge because it needs to be zero on the two vertices of thelarge cell on that edge. But it is not zero at the hanging node itselfwhen seen from the small cells' side
* 
*  -  so it is not continuous. Thefollowing three figures show three of the shape functions along theedges in question that turn out to not be continuous when defined inthe usual way simply based on the cells they are adjacent to:
*  [2.x.23] 
* 

* But we do want the finite element solution to be continuous so that wehave a &ldquo;conforming finite element method&rdquo; where thediscrete finite element space is a proper subset of the  [2.x.24]  functionspace in which we seek the solution of the Laplace equation.To guarantee that the global solution is continuous at these nodes as well, wehave to state some additional constraints on the values of the solution atthese nodes. The trick is to realize that while the shape functions shownabove are discontinuous (and consequently an [1.x.45] linear combinationof them is also discontinuous), that linear combinations in which the shapefunctions are added up as  [2.x.25] can be continuous [1.x.46].In other words, the coefficients  [2.x.26]  can not be chosen arbitrarilybut have to satisfy certain constraints so that the function  [2.x.27]  is in factcontinuous.What these constraints have to look is relatively easy tounderstand conceptually, but the implementation in software iscomplicated and takes several thousand lines of code. On the otherhand, in user code, it is only about half a dozen lines you have toadd when dealing with hanging nodes.
* In the program below, we will show how we can get theseconstraints from deal.II, and how to use them in the solution of thelinear system of equations. Before going over the details of the programbelow, you may want to take a look at the  [2.x.28]  documentationmodule that explains how these constraints can be computed and what classes indeal.II work on them.
* 

* [1.x.47][1.x.48]
* 

* The practice of hanging node constraints is rather simpler than thetheory we have outlined above. In reality, you will really only have toadd about half a dozen lines of additional code to a program like  [2.x.29] to make it work with adaptive meshes that have hanging nodes. Theinteresting part about this is that it is entirely independent of theequation you are solving: The algebraic nature of these constraints has nothingto do with the equation and only depends on the choice of finite element.As a consequence, the code to deal with these constraints is entirelycontained in the deal.II library itself, and you do not need to worryabout the details.
* The steps you need to make this work are essentially like this:
* 
*  - You have to create an AffineConstraints object, which (as the name  suggests) will store all constraints on the finite element space. In  the current context, these are the constraints due to our desire to  keep the solution space continuous even in the presence of hanging  nodes. (Below we will also briefly mention that we will also put  boundary values into this same object, but that is a separate matter.)
* 
*  - You have to fill this object using the function   [2.x.30]  to ensure continuity of  the elements of the finite element space.
* 
*  - You have to use this object when you copy the local contributions to  the matrix and right hand side into the global objects, by using   [2.x.31]  Up until  now, we have done this ourselves, but now with constraints, this  is where the magic happens and we apply the constraints to the  linear system. What this function does is make sure that the  degrees of freedom located at hanging nodes are not, in fact,  really free. Rather, they are factually eliminated from the  linear system by setting their rows and columns to zero and putting  something on the diagonal to ensure the matrix remains invertible.  The matrix resulting from this process remains symmetric and  positive definite for the Laplace equation we solve here, so we can  continue to use the Conjugate Gradient method for it.
* 
*  - You then solve the linear system as usual, but at the end of this  step, you need to make sure that the degrees of "freedom" located  on hanging nodes get their correct (constrained) value so that the  solution you then visualize or evaluate in other ways is in  fact continuous. This is done by calling   [2.x.32]  immediately after solving.
* These four steps are really all that is necessary
* 
*  -  it's that simplefrom a user perspective. The fact that, in the function calls mentionedabove, you will run through several thousand lines of not-so-trivialcode is entirely immaterial to this: In user code, there are reallyonly four additional steps.
* 

* [1.x.49][1.x.50]
* 

* The next question, now that we know how to [1.x.51] with meshes thathave these hanging nodes is how we [1.x.52] them.
* A simple way has already been shown in  [2.x.33] : If you [1.x.53] whereit is necessary to refine the mesh, then you can create one by hand. Butin reality, we don't know this: We don't know the solution of the PDEup front (because, if we did, we wouldn't have to use the finite elementmethod), and consequently we do not know where it is necessary toadd local mesh refinement to better resolve areas where the solutionhas strong variations. But the discussion above shows that maybe wecan get away with using the discrete solution  [2.x.34]  on one mesh toestimate the derivatives  [2.x.35] , and then use this to determinewhich cells are too large and which already small enough. We can thengenerate a new mesh from the current one using local mesh refinement.If necessary, this step is then repeated until we are happy with ournumerical solution
* 
*  -  or, more commonly, until we run out of computationalresources or patience.
* So that's exactly what we will do.The locally refined grids are produced using an [1.x.54]which estimates the energy error for numerical solutions of the Laplaceoperator. Since it was developed by Kelly andco-workers, we often refer to it as the &ldquo;Kelly refinementindicator&rdquo; in the library, documentation, and mailing list. Theclass that implements it is calledKellyErrorEstimator, and there is a great deal of information tobe found in the documentation of that class that need not be repeatedhere. The summary, however, is that the class computes a vector withas many entries as there are  [2.x.36]  "active cells", andwhere each entry contains an estimate of the error on that cell.This estimate is then used to refine the cells of the mesh: thosecells that have a large error will be marked for refinement, thosethat have a particularly small estimate will be marked forcoarsening. We don't have to do this by hand: The functions innamespace GridRefinement will do all of this for us once we haveobtained the vector of error estimates.
* It is worth noting that while the Kelly error estimator was developedfor Laplace's equation, it has proven to be a suitable tool to generatelocally refined meshes for a wide range of equations, not even restrictedto elliptic only problems. Although it will create non-optimal meshes for otherequations, it is often a good way to quickly produce meshes that arewell adapted to the features of solutions, such as regions of greatvariation or discontinuities.
* 

* 
* [1.x.55][1.x.56]
* 

* It turns out that one can see Dirichlet boundary conditions as just anotherconstraint on the degrees of freedom. It's a particularly simple one,indeed: If  [2.x.37]  is a degree of freedom on the boundary, with position [2.x.38] , then imposing the boundary condition  [2.x.39]  on  [2.x.40] simply yields the constraint  [2.x.41] .
* The AffineConstraints class can handle such constraints as well, which makes itconvenient to let the same object we use for hanging node constraintsalso deal with these Dirichlet boundary conditions.This way, we don't need to apply the boundary conditions after assembly(like we did in the earlier steps).All that is necessary is that we call the variant of [2.x.42]  that returns its informationin an AffineConstraints object, rather than the  [2.x.43]  we have usedin previous tutorial programs.
* 

* [1.x.57] [1.x.58]
* 

* 
* Since the concepts used for locally refined grids are so important,we do not show much other material in this example. The mostimportant exception is that we show how to use biquadratic elementsinstead of the bilinear ones which we have used in all previousexamples. In fact, the use of higher order elements is accomplished byonly replacing three lines of the program, namely the initialization ofthe  [2.x.44]  member variable in the constructor of the mainclass of this program, and the use of an appropriate quadrature formulain two places. The rest of the program is unchanged.
* The only other new thing is a method to catch exceptions in the [2.x.45]  function in order to output some information in case theprogram crashes for some reason. This is discussed below in more detail.
* 

*  [1.x.59] [1.x.60]
*   [1.x.61]  [1.x.62]
* 

* 
*  The first few files have already been covered in previous examples and will thus not be further commented on.
* 

* 
* [1.x.63]
* 
*  From the following include file we will import the declaration of H1-conforming finite element shape functions. This family of finite elements is called  [2.x.46] , and was used in all examples before already to define the usual bi- or tri-linear elements, but we will now use it for bi-quadratic elements:
* 

* 
* [1.x.64]
* 
*  We will not read the grid from a file as in the previous example, but generate it using a function of the library. However, we will want to write out the locally refined grids (just the grid, not the solution) in each step, so we need the following include file instead of  [2.x.47] :
* 

* 
* [1.x.65]
* 
*  When using locally refined grids, we will get so-called <code>hanging nodes</code>. However, the standard finite element methods assumes that the discrete solution spaces be continuous, so we need to make sure that the degrees of freedom on hanging nodes conform to some constraints such that the global solution is continuous. We are also going to store the boundary conditions in this object. The following file contains a class which is used to handle these constraints:
* 

* 
* [1.x.66]
* 
*  In order to refine our grids locally, we need a function from the library that decides which cells to flag for refinement or coarsening based on the error indicators we have computed. This function is defined here:
* 

* 
* [1.x.67]
* 
*  Finally, we need a simple way to actually compute the refinement indicators based on some error estimate. While in general, adaptivity is very problem-specific, the error indicator in the following file often yields quite nicely adapted grids for a wide class of problems.
* 

* 
* [1.x.68]
* 
*  Finally, this is as in previous programs:
* 

* 
* [1.x.69]
* 
*   [1.x.70]  [1.x.71]
* 

* 
*  The main class is again almost unchanged. Two additions, however, are made: we have added the  [2.x.48]  function, which is used to adaptively refine the grid (instead of the global refinement in the previous examples), and a variable which will hold the constraints.
* 

* 
* [1.x.72]
* 
*  This is the new variable in the main class. We need an object which holds a list of constraints to hold the hanging nodes and the boundary conditions.
* 

* 
* [1.x.73]
* 
*   [1.x.74]  [1.x.75]
* 

* 
*  The implementation of nonconstant coefficients is copied verbatim from  [2.x.49] :
* 

* 
* [1.x.76]
* 
*   [1.x.77]  [1.x.78]
* 

* 
*   [1.x.79]  [1.x.80]
* 

* 
*  The constructor of this class is mostly the same as before, but this time we want to use the quadratic element. To do so, we only have to replace the constructor argument (which was  [2.x.50]  in all previous examples) by the desired polynomial degree (here  [2.x.51] ):
* 

* 
* [1.x.81]
* 
*   [1.x.82]  [1.x.83]
* 

* 
*  The next function sets up all the variables that describe the linear finite element problem, such as the DoFHandler, matrices, and vectors. The difference to what we did in  [2.x.52]  is only that we now also have to take care of hanging node constraints. These constraints are handled almost exclusively by the library, i.e. you only need to know that they exist and how to get them, but you do not have to know how they are formed or what exactly is done with them.
* 

* 
*  At the beginning of the function, you find all the things that are the same as in  [2.x.53] : setting up the degrees of freedom (this time we have quadratic elements, but there is no difference from a user code perspective to the linear
* 
*  -  or any other degree, for that matter
* 
*  -  case), generating the sparsity pattern, and initializing the solution and right hand side vectors. Note that the sparsity pattern will have significantly more entries per row now, since there are now 9 degrees of freedom per cell (rather than only four), that can couple with each other.
* 

* 
* [1.x.84]
* 
*  We may now populate the AffineConstraints object with the hanging node constraints. Since we will call this function in a loop we first clear the current set of constraints from the last system and then compute new ones:
* 

* 
* [1.x.85]
* 
*  Now we are ready to interpolate the boundary values with indicator 0 (the whole boundary) and store the resulting constraints in our  [2.x.54]  object. Note that we do not to apply the boundary conditions after assembly, like we did in earlier steps: instead we put all constraints on our function space in the AffineConstraints object. We can add constraints to the AffineConstraints object in either order: if two constraints conflict then the constraint matrix either abort or throw an exception via the Assert macro.
* 

* 
* [1.x.86]
* 
*  After all constraints have been added, they need to be sorted and rearranged to perform some actions more efficiently. This postprocessing is done using the  [2.x.55]  function, after which no further constraints may be added any more:
* 

* 
* [1.x.87]
* 
*  Now we first build our compressed sparsity pattern like we did in the previous examples. Nevertheless, we do not copy it to the final sparsity pattern immediately.  Note that we call a variant of make_sparsity_pattern that takes the AffineConstraints object as the third argument. We are letting the routine know that we will never write into the locations given by  [2.x.56]  by setting the argument  [2.x.57]  to false (in other words, that we will never write into entries of the matrix that correspond to constrained degrees of freedom). If we were to condense the constraints after assembling, we would have to pass  [2.x.58]  instead because then we would first write into these locations only to later set them to zero again during condensation.
* 

* 
* [1.x.88]
* 
*  Now all non-zero entries of the matrix are known (i.e. those from regularly assembling the matrix and those that were introduced by eliminating constraints). We may copy our intermediate object to the sparsity pattern:
* 

* 
* [1.x.89]
* 
*  We may now, finally, initialize the sparse matrix:
* 

* 
* [1.x.90]
* 
*   [1.x.91]  [1.x.92]
* 

* 
*  Next, we have to assemble the matrix. However, to copy the local matrix and vector on each cell into the global system, we are no longer using a hand-written loop. Instead, we use  [2.x.59]  that internally executes this loop while performing Gaussian elimination on rows and columns corresponding to constrained degrees on freedom.
* 

* 
*  The rest of the code that forms the local contributions remains unchanged. It is worth noting, however, that under the hood several things are different than before. First, the variable  [2.x.60]  and return value of  [2.x.61]  now are 9 each, where they were 4 before. Introducing such variables as abbreviations is a good strategy to make code work with different elements without having to change too much code. Secondly, the  [2.x.62]  object of course needs to do other things as well, since the shape functions are now quadratic, rather than linear, in each coordinate variable. Again, however, this is something that is completely handled by the library.
* 

* 
* [1.x.93]
* 
*  Finally, transfer the contributions from  [2.x.63]  and  [2.x.64]  into the global objects.
* 

* 
* [1.x.94]
* 
*  Now we are done assembling the linear system. The constraint matrix took care of applying the boundary conditions and also eliminated hanging node constraints. The constrained nodes are still in the linear system (there is a nonzero entry, chosen in a way that the matrix is well conditioned, on the diagonal of the matrix and all other entries for this line are set to zero) but the computed values are invalid (i.e., the corresponding entries in  [2.x.65]  are currently meaningless). We compute the correct values for these nodes at the end of the  [2.x.66]  function.
* 

* 
* [1.x.95]
* 
*   [1.x.96]  [1.x.97]
* 

* 
*  We continue with gradual improvements. The function that solves the linear system again uses the SSOR preconditioner, and is again unchanged except that we have to incorporate hanging node constraints. As mentioned above, the degrees of freedom from the AffineConstraints object corresponding to hanging node constraints and boundary values have been removed from the linear system by giving the rows and columns of the matrix a special treatment. This way, the values for these degrees of freedom have wrong, but well-defined values after solving the linear system. What we then have to do is to use the constraints to assign to them the values that they should have. This process, called  [2.x.67]  constraints, computes the values of constrained nodes from the values of the unconstrained ones, and requires only a single additional function call that you find at the end of this function:
* 

* 
*  

* 
* [1.x.98]
* 
*   [1.x.99]  [1.x.100]
* 

* 
*  We use a sophisticated error estimation scheme to refine the mesh instead of global refinement. We will use the KellyErrorEstimator class which implements an error estimator for the Laplace equation; it can in principle handle variable coefficients, but we will not use these advanced features, but rather use its most simple form since we are not interested in quantitative results but only in a quick way to generate locally refined grids.
* 

* 
*  Although the error estimator derived by Kelly et al. was originally developed for the Laplace equation, we have found that it is also well suited to quickly generate locally refined grids for a wide class of problems. This error estimator uses the solution gradient's jump at cell faces (which is a measure for the second derivatives) and scales it by the size of the cell. It is therefore a measure for the local smoothness of the solution at the place of each cell and it is thus understandable that it yields reasonable grids also for hyperbolic transport problems or the wave equation as well, although these grids are certainly suboptimal compared to approaches specially tailored to the problem. This error estimator may therefore be understood as a quick way to test an adaptive program.
* 

* 
*  The way the estimator works is to take a  [2.x.68]  object describing the degrees of freedom and a vector of values for each degree of freedom as input and compute a single indicator value for each active cell of the triangulation (i.e. one value for each of the active cells). To do so, it needs two additional pieces of information: a face quadrature formula, i.e., a quadrature formula on  [2.x.69]  dimensional objects. We use a 3-point Gauss rule again, a choice that is consistent and appropriate with the bi-quadratic finite element shape functions in this program. (What constitutes a suitable quadrature rule here of course depends on knowledge of the way the error estimator evaluates the solution field. As said above, the jump of the gradient is integrated over each face, which would be a quadratic function on each face for the quadratic elements in use in this example. In fact, however, it is the square of the jump of the gradient, as explained in the documentation of that class, and that is a quartic function, for which a 3 point Gauss formula is sufficient since it integrates polynomials up to order 5 exactly.)
* 

* 
*  Secondly, the function wants a list of boundary indicators for those boundaries where we have imposed Neumann values of the kind  [2.x.70] , along with a function  [2.x.71]  for each such boundary. This information is represented by a map from boundary indicators to function objects describing the Neumann boundary values. In the present example program, we do not use Neumann boundary values, so this map is empty, and in fact constructed using the default constructor of the map in the place where the function call expects the respective function argument.
* 

* 
*  The output is a vector of values for all active cells. While it may make sense to compute the [1.x.101] of a solution degree of freedom very accurately, it is usually not necessary to compute the [1.x.102] corresponding to the solution on a cell particularly accurately. We therefore typically use a vector of floats instead of a vector of doubles to represent error indicators.
* 

* 
* [1.x.103]
* 
*  The above function returned one error indicator value for each cell in the  [2.x.72]  array. Refinement is now done as follows: refine those 30 per cent of the cells with the highest error values, and coarsen the 3 per cent of cells with the lowest values.   
*   One can easily verify that if the second number were zero, this would approximately result in a doubling of cells in each step in two space dimensions, since for each of the 30 per cent of cells, four new would be replaced, while the remaining 70 per cent of cells remain untouched. In practice, some more cells are usually produced since it is disallowed that a cell is refined twice while the neighbor cell is not refined; in that case, the neighbor cell would be refined as well.   
*   In many applications, the number of cells to be coarsened would be set to something larger than only three per cent. A non-zero value is useful especially if for some reason the initial (coarse) grid is already rather refined. In that case, it might be necessary to refine it in some regions, while coarsening in some other regions is useful. In our case here, the initial grid is very coarse, so coarsening is only necessary in a few regions where over-refinement may have taken place. Thus a small, non-zero value is appropriate here.   
*   The following function now takes these refinement indicators and flags some cells of the triangulation for refinement or coarsening using the method described above. It is from a class that implements several different algorithms to refine a triangulation based on cell-wise error indicators.
* 

* 
* [1.x.104]
* 
*  After the previous function has exited, some cells are flagged for refinement, and some other for coarsening. The refinement or coarsening itself is not performed by now, however, since there are cases where further modifications of these flags is useful. Here, we don't want to do any such thing, so we can tell the triangulation to perform the actions for which the cells are flagged:
* 

* 
* [1.x.105]
* 
*   [1.x.106]  [1.x.107]
* 

* 
*  At the end of computations on each grid, and just before we continue the next cycle with mesh refinement, we want to output the results from this cycle.
* 

* 
*  We have already seen in  [2.x.73]  how this can be achieved for the mesh itself. Here, we change a few things:  [2.x.74]   [2.x.75] We use two different formats: gnuplot and VTU. [2.x.76]   [2.x.77] We embed the cycle number in the output file name. [2.x.78]   [2.x.79] For gnuplot output, we set up a  [2.x.80]  object to provide a few extra visualization arguments so that edges appear curved. This is explained in further detail in  [2.x.81] . [2.x.82]   [2.x.83] 
* 

* 
* [1.x.108]
* 
*   [1.x.109]  [1.x.110]
* 

* 
*  The final function before  [2.x.84]  is again the main driver of the class,  [2.x.85] . It is similar to the one of  [2.x.86] , except that we generate a file in the program again instead of reading it from disk, in that we adaptively instead of globally refine the mesh, and that we output the solution on the final mesh in the present function.
* 

* 
*  The first block in the main loop of the function deals with mesh generation. If this is the first cycle of the program, instead of reading the grid from a file on disk as in the previous example, we now again create it using a library function. The domain is again a circle with center at the origin and a radius of one (these are the two hidden arguments to the function, which have default values).
* 

* 
*  You will notice by looking at the coarse grid that it is of inferior quality than the one which we read from the file in the previous example: the cells are less equally formed. However, using the library function this program works in any space dimension, which was not the case before.
* 

* 
*  In case we find that this is not the first cycle, we want to refine the grid. Unlike the global refinement employed in the last example program, we now use the adaptive procedure described above.
* 

* 
*  The rest of the loop looks as before:
* 

* 
* [1.x.111]
* 
*   [1.x.112]  [1.x.113]
* 

* 
*  The main function is unaltered in its functionality from the previous example, but we have taken a step of additional caution. Sometimes, something goes wrong (such as insufficient disk space upon writing an output file, not enough memory when trying to allocate a vector or a matrix, or if we can't read from or write to a file for whatever reason), and in these cases the library will throw exceptions. Since these are run-time problems, not programming errors that can be fixed once and for all, this kind of exceptions is not switched off in optimized mode, in contrast to the  [2.x.87]  macro which we have used to test against programming errors. If uncaught, these exceptions propagate the call tree up to the  [2.x.88]  function, and if they are not caught there either, the program is aborted. In many cases, like if there is not enough memory or disk space, we can't do anything but we can at least print some text trying to explain the reason why the program failed. A way to do so is shown in the following. It is certainly useful to write any larger program in this way, and you can do so by more or less copying this function except for the  [2.x.89]  block that actually encodes the functionality particular to the present application.
* 

* 
* [1.x.114]
* 
*  The general idea behind the layout of this function is as follows: let's try to run the program as we did before...
* 

* 
* [1.x.115]
* 
*  ...and if this should fail, try to gather as much information as possible. Specifically, if the exception that was thrown is an object of a class that is derived from the C++ standard class  [2.x.90]  member function to get a string which describes the reason why the exception was thrown.   
*   The deal.II exception classes are all derived from the standard class, and in particular, the  [2.x.91]  function will return approximately the same string as would be generated if the exception was thrown using the  [2.x.92]  macro. You have seen the output of such an exception in the previous example, and you then know that it contains the file and line number of where the exception occurred, and some other information. This is also what the following statements would print.   
*   Apart from this, there isn't much that we can do except exiting the program with an error code (this is what the  [2.x.93]  does):
* 

* 
* [1.x.116]
* 
*  If the exception that was thrown somewhere was not an object of a class derived from the standard  [2.x.94]  class, then we can't do anything at all. We then simply print an error message and exit.
* 

* 
* [1.x.117]
* 
*  If we got to this point, there was no exception which propagated up to the main function (there may have been exceptions, but they were caught somewhere in the program or the library). Therefore, the program performed as was expected and we can return without error.
* 

* 
* [1.x.118]
* [1.x.119][1.x.120]
* 

* 
* The output of the program looks as follows:
* [1.x.121]
* 
* 

* 
* As intended, the number of cells roughly doubles in each cycle. Thenumber of degrees is slightly more than four times the number ofcells; one would expect a factor of exactly four in two spatialdimensions on an infinite grid (since the spacing between the degreesof freedom is half the cell width: one additional degree of freedom oneach edge and one in the middle of each cell), but it is larger thanthat factor due to the finite size of the mesh and due to additionaldegrees of freedom which are introduced by hanging nodes and localrefinement.
* 

* 
* The program outputs the solution and mesh in each cycle of therefinement loop. The solution looks as follows:
*  [2.x.95] 
* It is interesting to follow how the program arrives at the final mesh:
*  [2.x.96] 
* 

* It is clearly visible that the region where the solution has a kink,i.e. the circle at radial distance 0.5 from the center, isrefined most. Furthermore, the central region where the solution isvery smooth and almost flat, is almost not refined at all, but thisresults from the fact that we did not take into account that thecoefficient is large there. The region outside is refined ratherarbitrarily, since the second derivative is constant there and refinementis therefore mostly based on the size of the cells and their deviationfrom the optimal square.
* 

* 
* [1.x.122][1.x.123][1.x.124]
* 

* [1.x.125][1.x.126]
* 

* 
* One thing that is always worth playing around with if one solvesproblems of appreciable size (much bigger than the one we have here)is to try different solvers or preconditioners. In the current case,the linear system is symmetric and positive definite, which makes theCG algorithm pretty much the canonical choice for solving. However,the SSOR preconditioner we use in the  [2.x.97]  function isup for grabs.
* In deal.II, it is relatively simple to change the preconditioner. Forexample, by changing the existing lines of code
* [1.x.127]
* into
* [1.x.128]
* we can try out different relaxation parameters for SSOR. By using
* [1.x.129]
* we can use Jacobi as a preconditioner. And by using
* [1.x.130]
* we can use a simple incomplete LU decomposition without any thresholding orstrengthening of the diagonal (to use this preconditioner, you have to alsoadd the header file  [2.x.98]  to the include listat the top of the file).
* Using these various different preconditioners, we can compare thenumber of CG iterations needed (available through the [2.x.99]  call, see [2.x.100] ) as well as CPU time needed (using the Timer class,discussed, for example, in  [2.x.101] ) and get thefollowing results (left: iterations; right: CPU time):
*  [2.x.102] 
* As we can see, all preconditioners behave pretty much the same on thissimple problem, with the number of iterations growing like  [2.x.103]  and because each iteration requires around  [2.x.104]  operations the total CPU time grows like  [2.x.105]  (for the few smallest meshes, the CPU time is so smallthat it doesn't record). Note that even though it is the simplestmethod, Jacobi is the fastest for this problem.
* The situation changes slightly when the finite element is not abi-quadratic one as set in the constructor of this program, but abi-linear one. If one makes this change, the results are as follows:
*  [2.x.106] 
* In other words, while the increase in iterations and CPU time is asbefore, Jacobi is now the method that requires the most iterations; itis still the fastest one, however, owing to the simplicity of theoperations it has to perform. This is not to say that Jacobiis actually a good preconditioner
* 
*  -  for problems of appreciable size, it isdefinitely not, and other methods will be substantially better
* 
*  -  but reallyonly that it is fast because its implementation is so simple that it cancompensate for a larger number of iterations.
* The message to take away from this is not that simplicity inpreconditioners is always best. While this may be true for the currentproblem, it definitely is not once we move to more complicatedproblems (elasticity or Stokes, for examples  [2.x.107]  or [2.x.108] ). Secondly, all of these preconditioners stilllead to an increase in the number of iterations as the number  [2.x.109]  ofdegrees of freedom grows, for example  [2.x.110] ; this, inturn, leads to a total growth in effort as  [2.x.111] since each iteration takes  [2.x.112]  work. This behavior isundesirable: we would really like to solve linear systems with  [2.x.113] unknowns in a total of  [2.x.114]  work; there is a classof preconditioners that can achieve this, namely geometric ( [2.x.115] , [2.x.116] ,  [2.x.117] )or algebraic multigrid ( [2.x.118] ,  [2.x.119] , and several others)preconditioners. They are, however, significantly more complex thanthe preconditioners outlined above.
* Finally, the last message to takehome is that when the data shown above was generated (in 2018), linearsystems with 100,000 unknowns areeasily solved on a desktop machine in about a second, makingthe solution of relatively simple 2d problems even to very highaccuracy not that big a task as it used to be even in thepast. At the time, the situation for 3d problems was entirely different,but even that has changed substantially in the intervening time
* 
*  -  thoughsolving problems in 3d to high accuracy remains a challenge.
* 

* [1.x.131][1.x.132]
* 

* If you look at the meshes above, you will see even though the domain is theunit disk, and the jump in the coefficient lies along a circle, the cellsthat make up the mesh do not track this geometry well. The reason, already hintedat in  [2.x.120] , is that in the absence of other information,the Triangulation class only sees a bunch ofcoarse grid cells but has, of course, no real idea what kind of geometry theymight represent when looked at together. For this reason, we need to tellthe Triangulation what to do when a cell is refined: where should the newvertices at the edge midpoints and the cell midpoint be located so that thechild cells better represent the desired geometry than the parent cell.
* To visualize what the triangulation actually knows about the geometry,it is not enough to just output the location of vertices and draw astraight line for each edge; instead, we have to output both interiorand boundary lines as multiple segments so that they lookcurved. We can do this by making one change to the gnuplot part of [2.x.121] :
* [1.x.133]
* 
* In the code above, we already do this for faces that sit at the boundary: thishappens automatically since we use  [2.x.122]  which attaches aSphericalManifold to the boundary of the domain. To make the mesh[1.x.134] also track a circular domain, we need to work a bit harder,though. First, recall that our coarse mesh consists of a central squarecell and four cells around it. Now first consider what would happen if wealso attached the SphericalManifold object not only to the four exterior facesbut also the four cells at the perimeter as well as all of their faces. We cando this by adding the following snippet (testing that the center of a cell islarger than a small multiple, say one tenth, of the cell diameter away fromcenter of the mesh only fails for the central square of the mesh):
* [1.x.135]
* 
* After a few global refinement steps, this would lead to a mesh of the followingkind:
* 

*    [2.x.123] 
* Creating good meshes, and in particular making them fit the geometry youwant, is a complex topic in itself. You can find much more on this in [2.x.124] ,  [2.x.125] , and  [2.x.126] , among other tutorial programs that coverthe issue.  [2.x.127]  shows another, less manual way to achieve a meshwell fit to the problem here.Information on curved domains can also be found in thedocumentation module on  [2.x.128]  "Manifold descriptions".
* Why does it make sense to choose a mesh that tracks the internalinterface? There are a number of reasons, but the most essential onecomes down to what we actually integrate in our bilinearform. Conceptually, we want to integrate the term  [2.x.129]  as thecontribution of cell  [2.x.130]  to the matrix entry  [2.x.131] . We can notcompute it exactly and have to resort to quadrature. We know thatquadrature is accurate if the integrand is smooth. That is becausequadrature in essence computes a polynomial approximation to theintegrand that coincides with the integrand in the quadrature points,and then computes the volume under this polynomial as an approximationto the volume under the original integrand. This polynomialinterpolant is accurate if the integrand is smooth on a cell, but itis usually rather inaccurate if the integrand is discontinuous on acell.
* Consequently, it is worthwhile to align cells in such a way that theinterfaces across which the coefficient is discontinuous are alignedwith cell interfaces. This way, the coefficient is constant on eachcell, following which the integrand will be smooth, and its polynomialapproximation and the quadrature approximation of the integral willboth be accurate. Note that such an alignment is common in manypractical cases, so deal.II provides a number of functions (such as [2.x.132]  "material_id") to help manage such a scenario.Refer to  [2.x.133]  and  [2.x.134]  for examples of how material ids can beapplied.
* Finally, let us consider the case of a coefficient that has a smoothand non-uniform distribution in space. We can repeat once again all ofthe above discussion on the representation of such a function with thequadrature. So, to simulate it accurately there are a few readilyavailable options: you could reduce the cell size, increase the orderof the polynomial used in the quadrature formula, select a moreappropriate quadrature formula, or perform a combination of thesesteps. The key is that providing the best fit of the coefficient'sspatial dependence with the quadrature polynomial will lead to a moreaccurate finite element solution of the PDE.
* As a final note: The discussion in the previous paragraphs shows, we herehave a very concrete way of stating what we think of a good mesh
* 
*  -  it shouldbe aligned with the jump in the coefficient. But one could also have askedthis kind of question in a more general setting: Given some equation witha smooth solution and smooth coefficients, can we say what a good meshwould look like? This is a question for which the answer is easier to statein intuitive terms than mathematically: A good mesh has cells that all,by and large, look like squares (or cubes, in 3d). A bad mesh would containcells that are very elongated in some directions or, more generally, for whichthere are cells that have both short and long edges. There are many waysin which one could assign a numerical quality index to each cell that measureswhether the cell is "good" or "bad"; some of these are often chosen becausethey are cheap and easy to compute, whereas others are based on what entersinto proofs of convergence. An example of the former would be the ratio ofthe longest to the shortest edge of a cell: In the ideal case, that ratiowould be one; bad cells have values much larger than one. An example of thelatter kind would consider the gradient (the "Jacobian") of the mappingfrom the reference cell  [2.x.135]  to the real cell  [2.x.136] ; thisgradient is a matrix, and a quantity that enters into error estimatesis the maximum over all points on the reference cell of the ratio of thelargest to the smallest eigenvalue of this matrix. It is again not difficultto see that this ratio is constant if the cell  [2.x.137]  is an affine image of [2.x.138] , and that it is one for squares and cubes.
* In practice, it might be interesting to visualize such quality measures.The function  [2.x.139]  provides oneway to get this kind of information. Even better, visualization toolssuch as VisIt often allow you to visualize this sort of informationfor a variety of measures from within the visualization softwareitself; in the case of VisIt, just add a "pseudo-color" plot and selectone of the mesh quality measures instead of the solution field.
* 

* [1.x.138][1.x.139]
* 

* From a mathematical perspective, solutions of the Laplace equation[1.x.140]on smoothly bounded, convex domains are known to be smooth themselves. The exact degreeof smoothness, i.e., the function space in which the solution lives, dependson how smooth exactly the boundary of the domain is, and how smooth the righthand side is. Some regularity of the solution may be lost at the boundary, butone generally has that the solution is twice more differentiable incompact subsets of the domain than the right hand side.If, in particular, the right hand side satisfies  [2.x.140] , then [2.x.141]  where  [2.x.142]  is any compact subset of  [2.x.143] ( [2.x.144]  is an open domain, so a compact subset needs to keep a positive distancefrom  [2.x.145] ).
* The situation we chose for the current example is different, however: we lookat an equation with a non-constant coefficient  [2.x.146] :[1.x.141]Here, if  [2.x.147]  is not smooth, then the solution will not be smooth either,regardless of  [2.x.148] . In particular, we expect that wherever  [2.x.149]  is discontinuousalong a line (or along a plane in 3d),the solution will have a kink. This is easy to see: if for example  [2.x.150] is continuous, then  [2.x.151]  needs to becontinuous. This means that  [2.x.152]  must be continuously differentiable(not have a kink). Consequently, if  [2.x.153]  has a discontinuity, then  [2.x.154] must have an opposite discontinuity so that the two exactly cancel and theirproduct yields a function without a discontinuity. But for  [2.x.155]  to havea discontinuity,  [2.x.156]  must have a kink. This is of course exactly what ishappening in the current example, and easy to observe in the pictures of thesolution.
* In general, if the coefficient  [2.x.157]  is discontinuous along a line in 2d,or a plane in 3d, then the solution may have a kink, but the gradient of thesolution will not go to infinity. That means, that the solution is at leaststill in the [1.x.142] [2.x.158]  (i.e., roughly speaking, in thespace of functions whose derivatives are bounded). On the other hand,we know that in the mostextreme cases
* 
*  -  i.e., where the domain has reentrant corners, theright hand side only satisfies  [2.x.159] , or the coefficient  [2.x.160]  is only in [2.x.161] 
* 
*  -  all we can expect is that  [2.x.162]  (i.e., the[1.x.143] of functions whose derivative is square integrable), a much larger space than [2.x.163] . It is not very difficult to create cases wherethe solution is in a space  [2.x.164]  where we can get  [2.x.165]  to become as smallas we want. Such cases are often used to test adaptive finite elementmethods because the mesh will have to resolve the singularity that causesthe solution to not be in  [2.x.166]  any more.
* The typical example one uses for this is called the [1.x.144](referring to  [2.x.167] ), which in the commonly used form has a coefficient [2.x.168]  that has different values in the four quadrants of the plane(or eight different values in the octants of  [2.x.169] ). The exact degreeof regularity (the  [2.x.170]  in the index of the Sobolev space above) depends on thevalues of  [2.x.171]  coming together at the origin, and by choosing thejumps large enough, the regularity of the solution can be made as close asdesired to  [2.x.172] .
* To implement something like this, one could replace the coefficientfunction by the following (shown here only for the 2d case):
* [1.x.145]
* (Adding the  [2.x.173]  at the end ensures that either an exceptionis thrown or that the program aborts if we ever get to that point
* 
*  -  which of course we shouldn't,but this is a good way to insure yourself: we all make mistakes bysometimes not thinking of all cases, for example by checkingfor  [2.x.174]  to be less than and greater than zero,rather than greater-or-equal to zero, and thereby forgettingsome cases that would otherwise lead to bugs that are awkwardto find. The  [2.x.175]  at the end is only there toavoid compiler warnings that the function does not end in a [2.x.176]  statement
* 
*  -  the compiler cannot see that thefunction would never actually get to that point because of thepreceding  [2.x.177]  statement.)
* By playing with such cases where four or more sectors cometogether and on which the coefficient has different values, one canconstruct cases where the solution has singularities at theorigin. One can also see how the meshes are refined in such cases.
* 

* [1.x.146][1.x.147] [2.x.178] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-61_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34][1.x.35][1.x.36][1.x.37][1.x.38][1.x.39]
*  [2.x.2] 
* [1.x.40]
* [1.x.41][1.x.42][1.x.43]
* 

* This tutorial program presents an implementation of the "weak Galerkin"finite element method for the Poisson equation. In some sense, the motivation forconsidering this method starts from the same point as in  [2.x.3] : We would like toconsider discontinuous shape functions, but then need to address the fact thatthe resulting problem has a much larger number of degrees of freedom compared tothe usual continuous Galerkin method (because, forexample, each vertex carries as many degrees of freedom as there are adjacent cells).We also have to address the fact that, unlike in the continuousGalerkin method, [1.x.44] degree of freedomon one cell couples with all of the degrees of freedom on each of its face neighborcells. Consequently, the matrix one gets from the "traditional" discontinuousGalerkin methods are both large and relatively dense.
* Both the hybridized discontinuous Galerkin method (HDG) in  [2.x.4]  and the weakGalerkin (WG) method in this tutorial address the issue of coupling by introducingadditional degrees of freedom whose shape functions only live on a face betweencells (i.e., on the "skeleton" of the mesh), and which therefore "insulate" thedegrees of freedom on the adjacent cells from each other: cell degrees of freedomonly couple with other cell degrees of freedom on the same cell, as well as facedegrees of freedom, but not with cell degrees of freedom on neighboring cells.Consequently, the coupling of shape functions for these cell degrees of freedomindeed couple on exactly one cell and the degrees of freedom defined on itsfaces.
* For a given equation, say the second order Poisson equation,the difference between the HDG and the WG method is how precisely one formulatesthe problem that connects all of these different shape functions. (Indeed,for some WG and HDG formulation, it is possible to show that they are equivalent.)The HDG does things by reformulating second order problems in terms of a system of firstorder equations and then conceptually considers the face degrees of freedomto be "fluxes" of this first order system. In contrast, the WG method keeps thingsin second order form and considers the face degrees of freedom as of the sametype as the primary solution variable, just restricted to the lower-dimensionalfaces. For the purposes of the equation, one then needs to somehow "extend"these shape functions into the interior of the cell when defining what it meansto apply a differential operator to them. Compared to the HDG, the methodhas the advantage that it does not lead to a proliferation of unknowns dueto rewriting the equation as a first-order system, but it is also not quiteas easy to implement. However, as we will see in the following, thisadditional effort is not prohibitive.
* 

* [1.x.45][1.x.46]
* 

* Weak Galerkin Finite Element Methods (WGFEMs) use discrete weak functionsto approximate scalar unknowns, and discrete weak gradients toapproximate classical gradients.The method was originally introduced by Junping Wang and Xiu Yein the paper[1.x.47][1.x.48].Compared to the continuous Galerkin method,the weak Galerkin method satisfies important physical properties, namelylocal mass conservation and bulk normal flux continuity.It results in a SPD linear system, and optimal convergence rates canbe obtained with mesh refinement.
* 

* [1.x.49][1.x.50]
* This program solves the Poisson equationusing the weak Galerkin finite element method:
* [1.x.51]
* where  [2.x.5]  is a bounded domain.In the context of the flow of a fluid through a porous medium, [2.x.6]  is the pressure,  [2.x.7]  is a permeability tensor, [2.x.8]  is the source term, and [2.x.9]  represent Dirichlet and Neumann boundary conditions.We can introduce a flux,  [2.x.10] , that correspondsto the Darcy velocity (in the way we did in  [2.x.11] ) and this variable willbe important in the considerations below.
* In this program, we will consider a test case where the exact pressureis  [2.x.12]  on the unit square domain,with homogeneous Dirichelet boundary conditions and  [2.x.13]  the identity matrix.Then we will calculate  [2.x.14]  errors of pressure, velocity, and flux.
* 

* [1.x.52][1.x.53]
* 

* The Poisson equation above has a solution  [2.x.15]  that needs to satisfy the weakformulation of the problem,[1.x.54]
* for all test functions  [2.x.16] , where[1.x.55]
* and[1.x.56]
* Here, we have integrated by parts in the bilinear form, and we are evaluatingthe gradient of  [2.x.17]  in the interior and the values of  [2.x.18]  on the boundaryof the domain. All of this is well defined because we assume that the solutionis in  [2.x.19]  for which taking the gradient and evaluating boundary valuesare valid operations.
* The idea of the weak Galerkin method is now to approximate the exact  [2.x.20] solution with a [1.x.57]  [2.x.21] . This function may only bediscontinuous along interfaces between cells, and because we will want toevaluate this function also along interfaces, we have toprescribe not only what values it is supposed to have in the cell interiorsbut also its values along interfaces. We do this by saying that  [2.x.22]  isactually a tuple,  [2.x.23] , though it's really justa single function that is either equal to  [2.x.24]  or  [2.x.25] ,depending on whether it is evaluated at a point  [2.x.26]  that lies in the cellinterior or on cell interfaces.
* We would then like to simply stick this approximation into the bilinearform above. This works for the case where we have to evaluate thetest function  [2.x.27]  on the boundary (where we would simply take its interfacepart  [2.x.28] ) but we have to be careful with the gradient becausethat is only defined in cell interiors. Consequently,the weak Galerkin scheme for the Poisson equation is defined by[1.x.58]
* for all discrete test functions  [2.x.29] , where[1.x.59]
* and[1.x.60]
* The key point is that here, we have replaced the gradient  [2.x.30]  by the[1.x.61] operator [2.x.31]  that makes sense for our peculiarly defined approximation  [2.x.32] .
* The question is then how that operator works. For this, let us first say how wethink of the discrete approximation  [2.x.33]  of the pressure. As mentioned above,the "function"  [2.x.34]  actually consists of two parts: the values  [2.x.35]  inthe interior of cells, and  [2.x.36]  on the interfaces. We have to definediscrete (finite-dimensional) function spaces for both of these; in thisprogram, we will use FE_DGQ for  [2.x.37]  as the space in the interior ofcells (defined on each cell, but in general discontinuous along interfaces),and FE_FaceQ for  [2.x.38]  as the space on the interfaces.
* Then let us consider just a single cell (because the integrals above are alldefined cell-wise, and because the weak discrete gradient is defined cell-by-cell).The restriction of  [2.x.39]  to cell  [2.x.40] ,  [2.x.41]  then consistsof the pair  [2.x.42] . In essence, we canthink of  [2.x.43]  of some function defined on  [2.x.44]  that approximatesthe gradient; in particular, if  [2.x.45]  was the restriction of a differentiablefunction (to the interior and boundary of  [2.x.46] 
* 
*  -  which would make it continuousbetween the interior and boundary), then [2.x.47]  would simply be the exact gradient  [2.x.48] . But, since [2.x.49]  is not continuous between interior and boundary of  [2.x.50] , we need a moregeneral definition; furthermore, we can not deal with arbitrary functions, andso require that  [2.x.51]  is also in a finite element space (which, sincethe gradient is a vector, has to be vector-valued, and because the weak gradientis defined on each cell separately, will also be discontinuous between cells).
* The way this is done is to define this weak gradient operator  [2.x.52]  (where  [2.x.53]  is thevector-valued Raviart-Thomas space of order  [2.x.54]  on cell  [2.x.55] ) in the following way:[1.x.62]
* for all test functions  [2.x.56] .This is, in essence, simply an application of the integration-by-partsformula. In other words, for a given  [2.x.57] ,we need to think of  [2.x.58]  as thatRaviart-Thomas function of degree  [2.x.59]  for which the left hand side and right hand sideare equal for all test functions.
* A key point to make is then the following: While the usual gradient  [2.x.60]  isalocal* operator that computes derivatives based simply on the value ofa function at a point and its (infinitesimal) neighborhood, the weak discrete gradient [2.x.61]  does not have this property: It depends on the values of the functionit is applied to on the entire cell, including the cell's boundary. Both are,however, linear operators as is clear from the definition of  [2.x.62] above, and that will allow us to represent  [2.x.63]  via a matrixin the discussion below.
*  [2.x.64]  It may be worth pointing out that while the weak discrete  gradient is an element of the Raviart-Thomas space  [2.x.65]  on each  cell  [2.x.66] , it is discontinuous between cells. On the other hand, the  Raviart-Thomas space  [2.x.67]  defined on the entire  mesh and implemented by the FE_RaviartThomas class represents  functions that have continuous normal components at interfaces  between cells. This means that [1.x.63],  [2.x.68]   is not in  [2.x.69] , even though it is on every cell  [2.x.70]  in  [2.x.71] .  Rather, it is in a "broken" Raviart-Thomas space that below we will  represent by the symbol  [2.x.72] . (The term "broken" here refers to  the process of "breaking something apart", and not to the synonym to  the expression "not functional".) One might therefore (rightfully) argue that  the notation used in the weak Galerkin literature is a bit misleading,  but as so often it all depends on the context in which a certain  notation is used
* 
*  -  in the current context, references to the  Raviart-Thomas space or element are always understood to be to the  "broken" spaces.
*  [2.x.73]  deal.II happens to have an implementation of this broken Raviart-Thomas  space: The FE_DGRT class. As a consequence, in this tutorial we will simply  always use the FE_DGRT class, even though in all of those places where  we have to compute cell-local matrices and vectors, it makes no difference.
* 

* [1.x.64][1.x.65]
* 

* Since  [2.x.74]  is an element of a finite element space, we can expand it in a basisas we always do, i.e., we can write[1.x.66]
* Here, since  [2.x.75]  has two components (the interior and the interface components),the same must hold true for the basis functions  [2.x.76] , which wecan write as  [2.x.77] . If you'vefollowed the descriptions in  [2.x.78] ,  [2.x.79] , and the [2.x.80]  "documentation module on vector-valued problems",it will be no surprise that for some values of  [2.x.81] ,  [2.x.82]  will bezero, whereas for other values of  [2.x.83] ,  [2.x.84]  will be zero
* 
*  -  i.e.,shape functions will be of either one or the other kind. That is not important,here, however. What is important is that we need to wonder how we can represent [2.x.85]  because that is clearly what will appear in theproblem when we want to implement the bilinear form[1.x.67]
* 
* The key point is that  [2.x.86]  is known to be a member of the"broken" Raviart-Thomas space  [2.x.87] . What this means is that we canrepresent (on each cell  [2.x.88]  separately)[1.x.68]
* where the functions  [2.x.89] , and where  [2.x.90]  is a matrix ofdimension
* [1.x.69]
* (That the weak discrete gradient can be represented as a matrix should not comeas a surprise: It is a linear operator from one finite dimensionalspace to another finite dimensional space. If one chooses basesfor both of these spaces, then [1.x.70] canof course be written as a matrix mapping the vector of expansion coefficientswith regards to the basis of the domain space of the operator, tothe vector of expansion coefficients with regards to the basis in the imagespace.)
* Using this expansion, we can easily use the definition of the weakdiscrete gradient above to define what the matrix is going to be:[1.x.71]
* for all test functions  [2.x.91] .
* This clearly leads to a linear system of the form[1.x.72]
* with[1.x.73]
* and consequently[1.x.74]
* (In this last step, we have assumed that the indices  [2.x.92]  only rangeover those degrees of freedom active on cell  [2.x.93] ,thereby ensuring that the mass matrix on the space  [2.x.94]  is invertible.)Equivalently, using the symmetry of the matrix  [2.x.95] , we have that[1.x.75]
* Also worth pointing out is that thematrices  [2.x.96]  and  [2.x.97]  are of course not square but rectangular.
* 

* [1.x.76][1.x.77]
* 

* Having explained how the weak discrete gradient is defined, we can nowcome back to the question of how the linear system for the equation in questionshould be assembled. Specifically, using the definition of the bilinearform  [2.x.98]  shown above, we then need to compute the elements of thelocal contribution to the global matrix,[1.x.78]
* As explained above, we can expand  [2.x.99]  in terms of theRaviart-Thomas basis on each cell, and similarly for  [2.x.100] :[1.x.79]
* By re-arranging sums, this yields the following expression:[1.x.80]
* So, if we have the matrix  [2.x.101]  for each cell  [2.x.102] , then we can easily computethe contribution  [2.x.103]  for cell  [2.x.104]  to the matrix  [2.x.105]  as follows:[1.x.81]
* Here,[1.x.82]
* which is really just the mass matrix on cell  [2.x.106]  using the Raviart-Thomasbasis and weighting by the permeability tensor  [2.x.107] . The derivationhere then shows that the weak Galerkin method really just requires usto compute these  [2.x.108]  and  [2.x.109]  matrices on each cell  [2.x.110] , and then [2.x.111] , which is easily computed. The code to be shownbelow does exactly this.
* Having so computed the contribution  [2.x.112]  of cell  [2.x.113]  to the globalmatrix, all we have to do is to "distribute" these local contributionsinto the global matrix. How this is done is first shown in  [2.x.114]  and [2.x.115] . In the current program, this will be facilitated by calling [2.x.116] 
* A linear system of course also needs a right hand side. There is no difficultyassociated with computing the right hand side here other than the factthat we only need to use the cell-interior part  [2.x.117]  foreach shape function  [2.x.118] .
* 

* [1.x.83][1.x.84][1.x.85]
* 

* The discussions in the previous sections have given us a linearsystem that we can solve for the numerical pressure  [2.x.119] . We can usethis to compute an approximation to the variable  [2.x.120] that corresponds to the velocity with which the medium flows in a porousmedium if this is the model we are trying to solve. This kind ofstep
* 
*  -  computing a derived quantity from the solution of the discreteproblem
* 
*  -  is typically called "post-processing".
* Here, instead of using the exact gradient of  [2.x.121] , let us insteaduse the discrete weak gradient of  [2.x.122]  to calculate the velocity on each element.As discussed above,on each element the gradient of the numerical pressure  [2.x.123]  can beapproximated by discrete weak gradients   [2.x.124] :[1.x.86]
* 
* On cell  [2.x.125] ,the numerical velocity  [2.x.126]  can be written as
* [1.x.87]
* where  [2.x.127]  is the expansion matrix from above, and [2.x.128]  is the basis function of the  [2.x.129]  space on a cell.
* Unfortunately,  [2.x.130]  may not be in the  [2.x.131]  space(unless, of course, if  [2.x.132]  is constant times the identity matrix).So, in order to represent it in a finite element program, we need toproject it back into a finite dimensional space we can work with. Here,we will use the  [2.x.133] -projection to project it back to the (broken)  [2.x.134] space.
* We define the projection as [2.x.135]  on each cell  [2.x.136] .For any  [2.x.137] , [2.x.138] So, rather than the formula shown above, the numerical velocity on cell  [2.x.139] instead becomes[1.x.88]
* and we have the following system to solve for the coefficients  [2.x.140] :[1.x.89]
* In the implementation below, the matrix with elements [2.x.141] is called  [2.x.142] ,whereas the matrix with elements [2.x.143] is called  [2.x.144] .
* Then the elementwise velocity is[1.x.90]
* where  [2.x.145]  is called`cell_velocity` in the code.
* Using this velocity obtained by "postprocessing" the solution, we candefine the  [2.x.146] -errors of pressure, velocity, and fluxby the following formulas:
* [1.x.91]
* where  [2.x.147]  is the area of the element, [2.x.148]  are faces of the element, [2.x.149]  are unit normal vectors of each face. The last of thesenorms measures the accuracy of the normal component of the velocityvectors over the interfaces between the cells of the mesh. The scalingfactor  [2.x.150]  is chosen so as to scale out the difference inthe length (or area) of the collection of interfaces as the mesh sizechanges.
* The first of these errors above is easily computed using [2.x.151]  The others require a bit more workand are implemented in the code below.
* 

*  [1.x.92] [1.x.93]
*   [1.x.94]  [1.x.95] This program is based on  [2.x.152] ,  [2.x.153]  and  [2.x.154] , so most of the following header files are familiar. We need the following, of which only the one that imports the FE_DGRaviartThomas class (namely, `deal.II/fe/fe_dg_vector.h`) is really new; the FE_DGRaviartThomas implements the "broken" Raviart-Thomas space discussed in the introduction:
* 

* 
* [1.x.96]
* 
*  Our first step, as always, is to put everything related to this tutorial program into its own namespace:
* 

* 
* [1.x.97]
* 
*   [1.x.98]  [1.x.99]
* 

* 
*  This is the main class of this program. We will solve for the numerical pressure in the interior and on faces using the weak Galerkin (WG) method, and calculate the  [2.x.155]  error of pressure. In the post-processing step, we will also calculate  [2.x.156] -errors of the velocity and flux.   
*   The structure of the class is not fundamentally different from that of previous tutorial programs, so there is little need to comment on the details with one exception: The class has a member variable `fe_dgrt` that corresponds to the "broken" Raviart-Thomas space mentioned in the introduction. There is a matching `dof_handler_dgrt` that represents a global enumeration of a finite element field created from this element, and a vector `darcy_velocity` that holds nodal values for this field. We will use these three variables after solving for the pressure to compute a postprocessed velocity field for which we can then evaluate the error and which we can output for visualization.
* 

* 
* [1.x.100]
* 
*   [1.x.101]  [1.x.102]
* 

* 
*  Next, we define the coefficient matrix  [2.x.157]  (here, the identity matrix), Dirichlet boundary conditions, the right-hand side  [2.x.158] , and the exact solution that corresponds to these choices for  [2.x.159]  and  [2.x.160] , namely  [2.x.161] .
* 

* 
* [1.x.103]
* 
*  The class that implements the exact pressure solution has an oddity in that we implement it as a vector-valued one with two components. (We say that it has two components in the constructor where we call the constructor of the base Function class.) In the `value()` function, we do not test for the value of the `component` argument, which implies that we return the same value for both components of the vector-valued function. We do this because we describe the finite element in use in this program as a vector-valued system that contains the interior and the interface pressures, and when we compute errors, we will want to use the same pressure solution to test both of these components.
* 

* 
* [1.x.104]
* 
*   [1.x.105]  [1.x.106]
* 

* 
*   [1.x.107]  [1.x.108]
* 

* 
*  In this constructor, we create a finite element space for vector valued functions, which will here include the ones used for the interior and interface pressures,  [2.x.162]  and  [2.x.163] .
* 

* 
* [1.x.109]
* 
*   [1.x.110]  [1.x.111]
* 

* 
*  We generate a mesh on the unit square domain and refine it.
* 

* 
* [1.x.112]
* 
*   [1.x.113]  [1.x.114]
* 

* 
*  After we have created the mesh above, we distribute degrees of freedom and resize matrices and vectors. The only piece of interest in this function is how we interpolate the boundary values for the pressure. Since the pressure consists of interior and interface components, we need to make sure that we only interpolate onto that component of the vector-valued solution space that corresponds to the interface pressures (as these are the only ones that are defined on the boundary of the domain). We do this via a component mask object for only the interface pressures.
* 

* 
* [1.x.115]
* 
*  In the bilinear form, there is no integration term over faces between two neighboring cells, so we can just use  [2.x.164]  to calculate the sparse matrix.
* 

* 
* [1.x.116]
* 
*   [1.x.117]  [1.x.118]
* 

* 
*  This function is more interesting. As detailed in the introduction, the assembly of the linear system requires us to evaluate the weak gradient of the shape functions, which is an element in the Raviart-Thomas space. As a consequence, we need to define a Raviart-Thomas finite element object, and have FEValues objects that evaluate it at quadrature points. We then need to compute the matrix  [2.x.165]  on every cell  [2.x.166] , for which we need the matrices  [2.x.167]  and  [2.x.168]  mentioned in the introduction.   
*   A point that may not be obvious is that in all previous tutorial programs, we have always called  [2.x.169]  with a cell iterator from a DoFHandler. This is so that one can call functions such as  [2.x.170]  that extract the values of a finite element function (represented by a vector of DoF values) on the quadrature points of a cell. For this operation to work, one needs to know which vector elements correspond to the degrees of freedom on a given cell
* 
*  -  i.e., exactly the kind of information and operation provided by the DoFHandler class.   
*   We could create a DoFHandler object for the "broken" Raviart-Thomas space (using the FE_DGRT class), but we really don't want to here: At least in the current function, we have no need for any globally defined degrees of freedom associated with this broken space, but really only need to reference the shape functions of such a space on the current cell. As a consequence, we use the fact that one can call  [2.x.171]  also with cell iterators into Triangulation objects (rather than DoFHandler objects). In this case, FEValues can of course only provide us with information that only references information about cells, rather than degrees of freedom enumerated on these cells. So we can't use  [2.x.172]  but we can use  [2.x.173]  to obtain the values of shape functions at quadrature points on the current cell. It is this kind of functionality we will make use of below. The variable that will give us this information about the Raviart-Thomas functions below is then the `fe_values_rt` (and corresponding `fe_face_values_rt`) object.   
*   Given this introduction, the following declarations should be pretty obvious:
* 

* 
* [1.x.119]
* 
*  Next, let us declare the various cell matrices discussed in the introduction:
* 

* 
* [1.x.120]
* 
*  We need  [2.x.174]  to access the  [2.x.175]  and  [2.x.176]  component of the shape functions.
* 

* 
* [1.x.121]
* 
*  This finally gets us in position to loop over all cells. On each cell, we will first calculate the various cell matrices used to construct the local matrix
* 
*  -  as they depend on the cell in question, they need to be re-computed on each cell. We need shape functions for the Raviart-Thomas space as well, for which we need to create first an iterator to the cell of the triangulation, which we can obtain by assignment from the cell pointing into the DoFHandler.
* 

* 
* [1.x.122]
* 
*  The first cell matrix we will compute is the mass matrix for the Raviart-Thomas space.  Hence, we need to loop over all the quadrature points for the velocity FEValues object.
* 

* 
* [1.x.123]
* 
*  Next we take the inverse of this matrix by using  [2.x.177]  It will be used to calculate the coefficient matrix  [2.x.178]  later. It is worth recalling later that `cell_matrix_M` actually contains theinverse*
 of  [2.x.179]  after this call.
* 

* 
* [1.x.124]
* 
*  From the introduction, we know that the right hand side  [2.x.180]  of the equation that defines  [2.x.181]  is the difference between a face integral and a cell integral. Here, we approximate the negative of the contribution in the interior. Each component of this matrix is the integral of a product between a basis function of the polynomial space and the divergence of a basis function of the Raviart-Thomas space. These basis functions are defined in the interior.
* 

* 
* [1.x.125]
* 
*  Next, we approximate the integral on faces by quadrature. Each component is the integral of a product between a basis function of the polynomial space and the dot product of a basis function of the Raviart-Thomas space and the normal vector. So we loop over all the faces of the element and obtain the normal vector.
* 

* 
* [1.x.126]
* 
*   [2.x.182]  is then the matrix product between the transpose of  [2.x.183]  and the inverse of the mass matrix (where this inverse is stored in  [2.x.184] 
* 

* 
* [1.x.127]
* 
*  Finally we can compute the local matrix  [2.x.185] .  Element  [2.x.186]  is given by  [2.x.187] . We have calculated the coefficients  [2.x.188]  in the previous step, and so obtain the following after suitably re-arranging the loops:
* 

* 
* [1.x.128]
* 
*  Next, we calculate the right hand side,  [2.x.189] :
* 

* 
* [1.x.129]
* 
*  The last step is to distribute components of the local matrix into the system matrix and transfer components of the cell right hand side into the system right hand side:
* 

* 
* [1.x.130]
* 
*   [1.x.131]  [1.x.132]
* 

* 
*  This step is rather trivial and the same as in many previous tutorial programs:
* 

* 
* [1.x.133]
* 
*   [1.x.134]  [1.x.135]
* 

* 
*  In this function, compute the velocity field from the pressure solution previously computed. The velocity is defined as  [2.x.190] , which requires us to compute many of the same terms as in the assembly of the system matrix. There are also the matrices  [2.x.191]  we need to assemble (see the introduction) but they really just follow the same kind of pattern.   
*   Computing the same matrices here as we have already done in the `assemble_system()` function is of course wasteful in terms of CPU time. Likewise, we copy some of the code from there to this function, and this is also generally a poor idea. A better implementation might provide for a function that encapsulates this duplicated code. One could also think of using the classic trade-off between computing efficiency and memory efficiency to only compute the  [2.x.192]  matrices once per cell during the assembly, storing them somewhere on the side, and re-using them here. (This is what  [2.x.193]  does, for example, where the `assemble_system()` function takes an argument that determines whether the local matrices are recomputed, and a similar approach
* 

* 
* 
*  -  maybe with storing local matrices elsewhere
* 
*  -  could be adapted for the current program.)
* 

* 
* [1.x.136]
* 
*  In the introduction, we explained how to calculate the numerical velocity on the cell. We need the pressure solution values on each cell, coefficients of the Gram matrix and coefficients of the  [2.x.194]  projection. We have already calculated the global solution, so we will extract the cell solution from the global solution. The coefficients of the Gram matrix have been calculated when we assembled the system matrix for the pressures. We will do the same way here. For the coefficients of the projection, we do matrix multiplication, i.e., the inverse of the Gram matrix times the matrix with  [2.x.195]  as components. Then, we multiply all these coefficients and call them beta. The numerical velocity is the product of beta and the basis functions of the Raviart-Thomas space.
* 

* 
* [1.x.137]
* 
*  The component of this  [2.x.196]  is the integral of  [2.x.197] .  [2.x.198]  is the Gram matrix.
* 

* 
* [1.x.138]
* 
*  To compute the matrix  [2.x.199]  mentioned in the introduction, we then need to evaluate  [2.x.200]  as explained in the introduction:
* 

* 
* [1.x.139]
* 
*  Then we also need, again, to compute the matrix  [2.x.201]  that is used to evaluate the weak discrete gradient. This is the exact same code as used in the assembly of the system matrix, so we just copy it from there:
* 

* 
* [1.x.140]
* 
*  Finally, we need to extract the pressure unknowns that correspond to the current cell:
* 

* 
* [1.x.141]
* 
*  We are now in a position to compute the local velocity unknowns (with respect to the Raviart-Thomas space we are projecting the term  [2.x.202]  into):
* 

* 
* [1.x.142]
* 
*  We compute Darcy velocity. This is same as cell_velocity but used to graph Darcy velocity.
* 

* 
* [1.x.143]
* 
*   [1.x.144]  [1.x.145]
* 

* 
*  This part is to calculate the  [2.x.203]  error of the pressure.  We define a vector that holds the norm of the error on each cell. Next, we use  [2.x.204]  to compute the error in the  [2.x.205]  norm on each cell. However, we really only care about the error in the interior component of the solution vector (we can't even evaluate the interface pressures at the quadrature points because these are all located in the interior of cells) and consequently have to use a weight function that ensures that the interface component of the solution variable is ignored. This is done by using the ComponentSelectFunction whose arguments indicate which component we want to select (component zero, i.e., the interior pressures) and how many components there are in total (two).
* 

* 
* [1.x.146]
* 
*   [1.x.147]  [1.x.148]
* 

* 
*  In this function, we evaluate  [2.x.206]  errors for the velocity on each cell, and  [2.x.207]  errors for the flux on faces. The function relies on the `compute_postprocessed_velocity()` function having previous computed, which computes the velocity field based on the pressure solution that has previously been computed.   
*   We are going to evaluate velocities on each cell and calculate the difference between numerical and exact velocities.
* 

* 
* [1.x.149]
* 
*  Having previously computed the postprocessed velocity, we here only have to extract the corresponding values on each cell and face and compare it to the exact values.
* 

* 
* [1.x.150]
* 
*  First compute the  [2.x.208]  error between the postprocessed velocity field and the exact one:
* 

* 
* [1.x.151]
* 
*  For reconstructing the flux we need the size of cells and faces. Since fluxes are calculated on faces, we have the loop over all four faces of each cell. To calculate the face velocity, we extract values at the quadrature points from the `darcy_velocity` which we have computed previously. Then, we calculate the squared velocity error in normal direction. Finally, we calculate the  [2.x.209]  flux error on the cell by appropriately scaling with face and cell areas and add it to the global error.
* 

* 
* [1.x.152]
* 
*  After adding up errors over all cells and faces, we take the square root and get the  [2.x.210]  errors of velocity and flux. These we output to screen.
* 

* 
* [1.x.153]
* 
*   [1.x.154]  [1.x.155]
* 

* 
*  We have two sets of results to output: the interior solution and the skeleton solution. We use  [2.x.211]  to visualize interior results. The graphical output for the skeleton results is done by using the DataOutFaces class.   
*   In both of the output files, both the interior and the face variables are stored. For the interface output, the output file simply contains the interpolation of the interior pressures onto the faces, but because it is undefined which of the two interior pressure variables you get from the two adjacent cells, it is best to ignore the interior pressure in the interface output file. Conversely, for the cell interior output file, it is of course impossible to show any interface pressures  [2.x.212] , because these are only available on interfaces and not cell interiors. Consequently, you will see them shown as an invalid value (such as an infinity).   
*   For the cell interior output, we also want to output the velocity variables. This is a bit tricky since it lives on the same mesh but uses a different DoFHandler object (the pressure variables live on the `dof_handler` object, the Darcy velocity on the `dof_handler_dgrt` object). Fortunately, there are variations of the  [2.x.213]  function that allow specifying which DoFHandler a vector corresponds to, and consequently we can visualize the data from both DoFHandler objects within the same file.
* 

* 
* [1.x.156]
* 
*  First attach the pressure solution to the DataOut object:
* 

* 
* [1.x.157]
* 
*  Then do the same with the Darcy velocity field, and continue with writing everything out into a file.
* 

* 
* [1.x.158]
* 
*   [1.x.159]  [1.x.160]
* 

* 
*  This is the final function of the main class. It calls the other functions of our class.
* 

* 
* [1.x.161]
* 
*   [1.x.162]  [1.x.163]
* 

* 
*  This is the main function. We can change the dimension here to run in 3d.
* 

* 
* [1.x.164]
* [1.x.165][1.x.166]
* 

* We run the program with a right hand side that will produce thesolution  [2.x.214]  and with homogeneous Dirichletboundary conditions in the domain  [2.x.215] . In addition, wechoose the coefficient matrix in the differential operator [2.x.216]  as the identity matrix. We test this setup using [2.x.217] ,  [2.x.218]  and [2.x.219]  element combinations, which one canselect by using the appropriate constructor argument for the`WGDarcyEquation` object in `main()`. We will then visualize pressurevalues in interiors of cells and on faces. We want to see that thepressure maximum is around 1 and the minimum is around 0. With meshrefinement, the convergence rates of pressure, velocity and fluxshould then be around 1 for  [2.x.220]  , 2 for [2.x.221] , and 3 for [2.x.222] .
* 

* [1.x.167][1.x.168][1.x.169]
* 

* The following figures show interior pressures and face pressures using the [2.x.223]  element. The mesh is refined 2 times (top)and 4 times (bottom), respectively. (This number can be adjusted in the`make_grid()` function.) When the mesh is coarse, one can seethe face pressures  [2.x.224]  neatly between the values of the interiorpressures  [2.x.225]  on the two adjacent cells.
*  [2.x.226] 
* From the figures, we can see that with the mesh refinement, the maximum andminimum pressure values are approaching the values we expect.Since the mesh is a rectangular mesh and numbers of cells in each direction is even, wehave symmetric solutions. From the 3d figures on the right,we can see that on  [2.x.227] , the pressure is a constantin the interior of the cell, as expected.
* [1.x.170][1.x.171][1.x.172]
* 

* We run the code with differently refined meshes (chosen in the `make_grid()` function)and get the following convergence rates of pressure,velocity, and flux (as defined in the introduction).
*  [2.x.228] 
* We can see that the convergence rates of  [2.x.229]  are around 1.This, of course, matches our theoretical expectations.
* 

* [1.x.173][1.x.174][1.x.175]
* 

* We can repeat the experiment from above using the next higher polynomialdegree:The following figures are interior pressures and face pressures implemented using [2.x.230] . The mesh is refined 4 times.  Compared to theprevious figures using [2.x.231] , on each cell, the solution is no longer constanton each cell, as we now use bilinear polynomials to do the approximation.Consequently, there are 4 pressure values in one interior, 2 pressure values oneach face.
*  [2.x.232] 
* Compared to the corresponding image for the  [2.x.233] combination, the solution is now substantially more accurate and, inparticular so close to being continuous at the interfaces that we canno longer distinguish the interface pressures  [2.x.234]  from theinterior pressures  [2.x.235]  on the adjacent cells.
* [1.x.176][1.x.177][1.x.178]
* 

* The following are the convergence rates of pressure, velocity, and fluxwe obtain from using the  [2.x.236]  element combination:
*  [2.x.237] 
* The convergence rates of  [2.x.238]  are around 2, as expected.
* 

* 
* [1.x.179][1.x.180][1.x.181]
* 

* Let us go one polynomial degree higher.The following are interior pressures and face pressures implemented using [2.x.239] , with mesh size  [2.x.240]  (i.e., 5 global meshrefinement steps). In the program, we use`data_out_face.build_patches(fe.degree)` when generating graphical output(see the documentation of  [2.x.241]  which here implies thatwe divide each 2d cell interior into 4 subcells in order to provide a bettervisualization of the quadratic polynomials. [2.x.242] 
* 

* [1.x.182][1.x.183][1.x.184]
* 

* As before, we can generate convergence data for the [2.x.243]  errors of pressure, velocity, and fluxusing the  [2.x.244]  combination:
*  [2.x.245] 
* Once more, the convergence rates of  [2.x.246]  isas expected, with values around 3.
* 

* [1.x.185][1.x.186] [2.x.247] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-62_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] ,  [2.x.2] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34][1.x.35][1.x.36][1.x.37][1.x.38]
*  [2.x.3] 
* [1.x.39] [2.x.4] 
* 

* 
*  [2.x.5]  As a prerequisite of this program, you need to have HDF5, complex PETSc,and the p4est libraries installed. The installation of deal.IItogether with these additional libraries is described in the [1.x.40] file.
* [1.x.41][1.x.42]
* A phononic crystal is a periodic nanostructure that modifies the motion ofmechanical vibrations or [phonons](https://en.wikipedia.org/wiki/Phonon).Phononic structures can be used to disperse, route and confine mechanical vibrations.These structures have potential applications in[quantum information](https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.86.1391)and have been used to study[macroscopic quantum phenomena](https://science.sciencemag.org/content/358/6360/203).Phononic crystals are usually fabricated in[cleanrooms](https://en.wikipedia.org/wiki/Cleanroom).
* In this tutorial we show how to a design a[phononic superlattice cavity](https://doi.org/10.1103/PhysRevA.94.033813)which is a particular type of phononic crystal that can be used to confinemechanical vibrations. A phononic superlattice cavity is formed by two[Distributed Bragg Reflector](https://en.wikipedia.org/wiki/Distributed_Bragg_reflector),mirrors and a  [2.x.6]  cavity where  [2.x.7]  is the acousticwavelength. Acoustic DBRs are  periodic structures where a set of bilayerstacks with contrasting physical properties (sound velocity index) isrepeated  [2.x.8]  times.Superlattice cavities are usually grown on a[Gallium Arsenide](https://en.wikipedia.org/wiki/Gallium_arsenide)wafer by[Molecular Beam Epitaxy](https://en.wikipedia.org/wiki/Molecular-beam_epitaxy).The bilayers correspond to GaAs/AlAs mirror pairs.As shown below, the thickness of the mirror layers (brown and green) is [2.x.9]  and the thickness of the cavity (blue) is  [2.x.10] .
*  [2.x.11] 
* In this tutorial we calculate the[band gap](https://en.wikipedia.org/wiki/Band_gap) and themechanical resonance of a phononic superlattice cavity but the code presented herecan be easily used to design and calculate other types of[phononic crystals](https://science.sciencemag.org/content/358/6360/203).
* The device is a waveguide in which the wave goes from left to right.The simulations of this tutorial are done in 2D, but the code is dimensionindependent and can be easily used with 3D simulations.The waveguide width is equal to the  [2.x.12]  dimension of the domain and thewaveguide length is equal to the  [2.x.13]  dimension of the domain.There are two regimes that depend on the waveguide width:
* 
*  - Single mode: In this case the width of the structure is much  smaller than the wavelength.  This case can be solved either with FEM (the approach that we take here) or with  a simple semi-analytical  [1D transfer matrix formalism](https://en.wikipedia.org/wiki/Transfer_matrix).
* 
*  - Multimode: In this case the width of the structure is larger than the wavelength.  This case can be solved using FEM  or with a [scattering matrix formalism](https://doi.org/10.1103/PhysRevA.94.033813).  Although we do not study this case in this tutorial, it is very easy to reach the multimode  regime by increasing the parameter waveguide width (`dimension_y` in the jupyter notebook).
* The simulations of this tutorial are performed in the frequency domain.To calculate the transmission spectrum, we use a[procedure](https://meep.readthedocs.io/en/latest/Python_Tutorials/Resonant_Modes_and_Transmission_in_a_Waveguide_Cavity/)that is commonly used in time domain [FDTD](https://en.wikipedia.org/wiki/Finite-difference_time-domain_method)simulations. A pulse at a certain frequency is generated on the left side of thestructure and the transmitted energy is measured on the right side of the structure.The simulation is run twice. First, we run the simulation with the phononicstructure and measure the transmitted energy:
*  [2.x.14] 
* Then, we run the simulation without the phononic structure and measure the transmittedenergy. We use the simulation without the structure for the calibration:
*  [2.x.15] 
* The transmission coefficient corresponds to the energy of the first simulationdivided by the calibration energy.We repeat this procedure for each frequency step.
* 

* [1.x.43][1.x.44]
* What we want to simulate here is the transmission of elasticwaves. Consequently, the right description of the problem uses theelastic equations, which in the time domain are given by[1.x.45]where the stiffness tensor  [2.x.16]  depends on the spatial coordinates andthe strain is the symmetrized gradient of the displacement, given by[1.x.46]
* [A perfectly matched layer (PML)](https://en.wikipedia.org/wiki/Perfectly_matched_layer)can be used to truncate the solution at the boundaries.A PML is a transformation that results in a complex coordinatestretching.
* Instead of a time domain approach, this tutorial program converts theequations above into the frequency domain by performing a Fouriertransform with regard to the time variable.The elastic equations in the frequency domain then read as follows[1.x.47]
* where the coefficients  [2.x.17]  account for the absorption.There are 3  [2.x.18]  coefficients in 3D and 2 in 2D.The imaginary par of  [2.x.19]  is equal to zero outside the PML.The PMLs are reflectionless only for the exact wave equations.When the set of equations is discretized the PML is no longer reflectionless.The reflections can be made arbitrarily small as long as themedium is slowly varying, see[the adiabatic theorem](https://doi.org/10.1103/PhysRevE.66.066608).In the code a quadratic turn-on of the PML has been used.A linear and cubic turn-on is also[known to work](https://doi.org/10.1364/OE.16.011376).These equations can be expanded into[1.x.48][1.x.49]where summation over repeated indices (here  [2.x.20] , as well as  [2.x.21]  and  [2.x.22] ) is as always implied.Note that the strain is no longer symmetric after applying the complex coordinatestretching of the PML.This set of equations can be written as[1.x.50]
* The same as the strain, the stress tensor is not symmetric inside the PML ( [2.x.23] ).Indeed the fields inside the PML are not physical.It is useful to introduce the tensors  [2.x.24]  and  [2.x.25] .[1.x.51]
* We can multiply by  [2.x.26]  and integrate over the domain  [2.x.27]  and integrate by parts.[1.x.52]
* It is this set of equations we want to solve for a set of frequencies  [2.x.28]  in order to compute thetransmission coefficient as function of frequency.The linear system becomes[1.x.53]
* 
* [1.x.54][1.x.55]
* In this tutorial we use a python[jupyter notebook](https://github.com/dealii/dealii/blob/master/examples/ [2.x.29] / [2.x.30] .ipynb)to set up the parameters and run the simulation.First we create a HDF5 file where we store the parameters and the results ofthe simulation.
* Each of the simulations (displacement and calibration) is stored in a separate HDF5 group:
* [1.x.56]
* 
* 

*  [1.x.57] [1.x.58]
*   [1.x.59]  [1.x.60]
* 

* 
*  Most of the include files we need for this program have already been discussed in previous programs, in particular in  [2.x.31] .
* 

* 
* [1.x.61]
* 
*  The following header provides the Tensor class that we use to represent the material properties.
* 

* 
* [1.x.62]
* 
*  The following header is necessary for the HDF5 interface of deal.II.
* 

* 
* [1.x.63]
* 
*  This header is required for the function  [2.x.32]  that we use to evaluate the result of the simulation.
* 

* 
* [1.x.64]
* 
*  We need these headers for the function  [2.x.33]  that we use in the function  [2.x.34] 
* 

* 
* [1.x.65]
* 
*   [1.x.66]  [1.x.67] The following classes are used to store the parameters of the simulation.
* 

* 
*   [1.x.68]  [1.x.69] This class is used to define the force pulse on the left side of the structure:
* 

* 
* [1.x.70]
* 
*  The variable `data` is the  [2.x.35]  in which all the simulation results will be stored. Note that the variables  [2.x.36]   [2.x.37]   [2.x.38]  and  [2.x.39]  point to the same group of the HDF5 file. When a  [2.x.40]  is copied, it will point to the same group of the HDF5 file.
* 

* 
* [1.x.71]
* 
*  The simulation parameters are stored in `data` as HDF5 attributes. The following attributes are defined in the jupyter notebook, stored in `data` as HDF5 attributes and then read by the constructor.
* 

* 
* [1.x.72]
* 
*  In this particular simulation the force has only a  [2.x.41]  component,  [2.x.42] .
* 

* 
* [1.x.73]
* 
*   [1.x.74]  [1.x.75] This class is used to define the shape of the Perfectly Matches Layer (PML) to absorb waves traveling towards the boundary:
* 

* 
* [1.x.76]
* 
*   [2.x.43]  in which all the simulation results will be stored.
* 

* 
* [1.x.77]
* 
*  The same as before, the following attributes are defined in the jupyter notebook, stored in `data` as HDF5 attributes and then read by the constructor.
* 

* 
* [1.x.78]
* 
*   [1.x.79]  [1.x.80] This class is used to define the mass density.
* 

* 
* [1.x.81]
* 
*   [2.x.44]  in which all the simulation results will be stored.
* 

* 
* [1.x.82]
* 
*  The same as before, the following attributes are defined in the jupyter notebook, stored in `data` as HDF5 attributes and then read by the constructor.
* 

* 
* [1.x.83]
* 
*   [1.x.84]  [1.x.85] This class contains all the parameters that will be used in the simulation.
* 

* 
* [1.x.86]
* 
*   [2.x.45]  in which all the simulation results will be stored.
* 

* 
* [1.x.87]
* 
*  The same as before, the following attributes are defined in the jupyter notebook, stored in `data` as HDF5 attributes and then read by the constructor.
* 

* 
* [1.x.88]
* 
*   [1.x.89]  [1.x.90] The calculation of the mass and stiffness matrices is very expensive. These matrices are the same for all the frequency steps. The right hand side vector is also the same for all the frequency steps. We use this class to store these objects and re-use them at each frequency step. Note that here we don't store the assembled mass and stiffness matrices and right hand sides, but instead the data for a single cell. `QuadratureCache` class is very similar to the `PointHistory` class that has been used in  [2.x.46] .
* 

* 
* [1.x.91]
* 
*  We store the mass and stiffness matrices in the variables mass_coefficient and stiffness_coefficient. We store as well the right_hand_side and JxW values which are going to be the same for all the frequency steps.
* 

* 
* [1.x.92]
* 
*   [1.x.93]  [1.x.94]
* 

* 
*  This function returns the stiffness tensor of the material. For the sake of simplicity we consider the stiffness to be isotropic and homogeneous; only the density  [2.x.47]  depends on the position. As we have previously shown in  [2.x.48] , if the stiffness is isotropic and homogeneous, the stiffness coefficients  [2.x.49]  can be expressed as a function of the two coefficients  [2.x.50]  and  [2.x.51] . The coefficient tensor reduces to [1.x.95]
* 

* 
* [1.x.96]
* 
*   [1.x.97]  [1.x.98]
* 

* 
*  Next let's declare the main class of this program. Its structure is very similar to the  [2.x.52]  tutorial program. The main differences are:
* 

* 
* 
*  - The sweep over the frequency values.
* 

* 
* 
*  - We save the stiffness and mass matrices in `quadrature_cache` and use them for each frequency step.
* 

* 
* 
*  - We store the measured energy by the probe for each frequency step in the HDF5 file.
* 

* 
* [1.x.99]
* 
*  This is called before every frequency step to set up a pristine state for the cache variables.
* 

* 
* [1.x.100]
* 
*  This function loops over the frequency vector and runs the simulation for each frequency step.
* 

* 
* [1.x.101]
* 
*  The parameters are stored in this variable.
* 

* 
* [1.x.102]
* 
*  We store the mass and stiffness matrices for each cell this vector.
* 

* 
* [1.x.103]
* 
*  This vector contains the range of frequencies that we are going to simulate.
* 

* 
* [1.x.104]
* 
*  This vector contains the coordinates  [2.x.53]  of the points of the measurement probe.
* 

* 
* [1.x.105]
* 
*  HDF5 datasets to store the frequency and `probe_positions` vectors.
* 

* 
* [1.x.106]
* 
*  HDF5 dataset that stores the values of the energy measured by the probe.
* 

* 
* [1.x.107]
* 
*   [1.x.108]  [1.x.109]
* 

* 
*   [1.x.110]  [1.x.111]
* 

* 
*  The constructor reads all the parameters from the  [2.x.54]  `data` using the  [2.x.55]  function.
* 

* 
* [1.x.112]
* 
*  This function defines the spatial shape of the force vector pulse which takes the form of a Gaussian function

* 
* [1.x.113]
*  where  [2.x.56]  is the maximum amplitude that takes the force and  [2.x.57]  and  [2.x.58]  are the standard deviations for the  [2.x.59]  and  [2.x.60]  components. Note that the pulse has been cropped to  [2.x.61]  and  [2.x.62] .
* 

* 
* [1.x.114]
* 
*   [1.x.115]  [1.x.116]
* 

* 
*  As before, the constructor reads all the parameters from the  [2.x.63]  `data` using the  [2.x.64]  function. As we have discussed, a quadratic turn-on of the PML has been defined in the jupyter notebook. It is possible to use a linear, cubic or another power degree by changing the parameter `pml_coeff_degree`. The parameters `pml_x` and `pml_y` can be used to turn on and off the `x` and `y` PMLs.
* 

* 
* [1.x.117]
* 
*  The PML coefficient for the `x` component takes the form  [2.x.65] 
* 

* 
* [1.x.118]
* 
*   [1.x.119]  [1.x.120]
* 

* 
*  This class is used to define the mass density. As we have explaine before, a phononic superlattice cavity is formed by two [Distributed Reflector](https://en.wikipedia.org/wiki/Band_gap), mirrors and a  [2.x.66]  cavity where  [2.x.67]  is the acoustic wavelength. Acoustic DBRs are periodic structures where a set of bilayer stacks with contrasting physical properties (sound velocity index) is repeated  [2.x.68]  times. The change of in the wave velocity is generated by alternating layers with different density.
* 

* 
* [1.x.121]
* 
*  In order to increase the precision we use [subpixel smoothing](https://meep.readthedocs.io/en/latest/Subpixel_Smoothing/).
* 

* 
* [1.x.122]
* 
*  The speed of sound is defined by [1.x.123] where  [2.x.69]  is the effective elastic constant and  [2.x.70]  the density. Here we consider the case in which the waveguide width is much smaller than the wavelength. In this case it can be shown that for the two dimensional case [1.x.124] and for the three dimensional case  [2.x.71]  is equal to the Young's modulus. [1.x.125]
* 

* 
* [1.x.126]
* 
*  The density  [2.x.72]  takes the following form <img alt="Phononic superlattice cavity" src="https://www.dealii.org/images/steps/developer/ [2.x.73] .04.svg" height="200" /> where the brown color represents material_a and the green color represents material_b.
* 

* 
* [1.x.127]
* 
*  Here we define the [subpixel smoothing](https://meep.readthedocs.io/en/latest/Subpixel_Smoothing/) which improves the precision of the simulation.
* 

* 
* [1.x.128]
* 
*  then the cavity
* 

* 
* [1.x.129]
* 
*  the material_a layers
* 

* 
* [1.x.130]
* 
*  the material_b layers
* 

* 
* [1.x.131]
* 
*  and finally the default is material_a.
* 

* 
* [1.x.132]
* 
*   [1.x.133]  [1.x.134]
* 

* 
*  The constructor reads all the parameters from the  [2.x.74]  `data` using the  [2.x.75]  function.
* 

* 
* [1.x.135]
* 
*   [1.x.136]  [1.x.137]
* 

* 
*  We need to reserve enough space for the mass and stiffness matrices and the right hand side vector.
* 

* 
* [1.x.138]
* 
*   [1.x.139]  [1.x.140]
* 

* 
*   [1.x.141]  [1.x.142]
* 

* 
*  This is very similar to the constructor of  [2.x.76] . In addition we create the HDF5 datasets `frequency_dataset`, `position_dataset` and `displacement`. Note the use of the `template` keyword for the creation of the HDF5 datasets. It is a C++ requirement to use the `template` keyword in order to treat `create_dataset` as a dependent template name.
* 

* 
* [1.x.143]
* 
*   [1.x.144]  [1.x.145]
* 

* 
*  There is nothing new in this function, the only difference with  [2.x.77]  is that we don't have to apply boundary conditions because we use the PMLs to truncate the domain.
* 

* 
* [1.x.146]
* 
*   [1.x.147]  [1.x.148]
* 

* 
*  This function is also very similar to  [2.x.78] , though there are notable differences. We assemble the system for each frequency/omega step. In the first step we set `calculate_quadrature_data = True` and we calculate the mass and stiffness matrices and the right hand side vector. In the subsequent steps we will use that data to accelerate the calculation.
* 

* 
* [1.x.149]
* 
*  Here we store the value of the right hand side, rho and the PML.
* 

* 
* [1.x.150]
* 
*  We calculate the stiffness tensor for the  [2.x.79]  and  [2.x.80]  that have been defined in the jupyter notebook. Note that contrary to  [2.x.81]  the stiffness is constant among for the whole domain.
* 

* 
* [1.x.151]
* 
*  We use the same method of  [2.x.82]  for vector-valued problems.
* 

* 
* [1.x.152]
* 
*  We have to calculate the values of the right hand side, rho and the PML only if we are going to calculate the mass and the stiffness matrices. Otherwise we can skip this calculation which considerably reduces the total calculation time.
* 

* 
* [1.x.153]
* 
*  We have done this in  [2.x.83] . Get a pointer to the quadrature cache data local to the present cell, and, as a defensive measure, make sure that this pointer is within the bounds of the global array:
* 

* 
* [1.x.154]
* 
*  The quadrature_data variable is used to store the mass and stiffness matrices, the right hand side vector and the value of `JxW`.
* 

* 
* [1.x.155]
* 
*  Below we declare the force vector and the parameters of the PML  [2.x.84]  and  [2.x.85] .
* 

* 
* [1.x.156]
* 
*  The following block is calculated only in the first frequency step.
* 

* 
* [1.x.157]
* 
*  Store the value of `JxW`.
* 

* 
* [1.x.158]
* 
*  Convert vectors to tensors and calculate xi
* 

* 
* [1.x.159]
* 
*  Here we calculate the  [2.x.86]  and  [2.x.87]  tensors.
* 

* 
* [1.x.160]
* 
*  calculate the values of the mass matrix.
* 

* 
* [1.x.161]
* 
*  Loop over the  [2.x.88]  indices of the stiffness tensor.
* 

* 
* [1.x.162]
* 
*  Here we calculate the stiffness matrix. Note that the stiffness matrix is not symmetric because of the PMLs. We use the gradient function (see the [documentation](https://www.dealii.org/current/doxygen/deal.II/group__vector__valued.html)) which is a  [2.x.89] . The matrix  [2.x.90]  consists of entries [1.x.163] Note the position of the indices  [2.x.91]  and  [2.x.92]  and the notation that we use in this tutorial:  [2.x.93] . As the stiffness tensor is not symmetric, it is very easy to make a mistake.
* 

* 
* [1.x.164]
* 
*  We save the value of the stiffness matrix in quadrature_data
* 

* 
* [1.x.165]
* 
*  and the value of the right hand side in quadrature_data.
* 

* 
* [1.x.166]
* 
*  We loop again over the degrees of freedom of the cells to calculate the system matrix. These loops are really quick because we have already calculated the stiffness and mass matrices, only the value of  [2.x.94]  changes.
* 

* 
* [1.x.167]
* 
*   [1.x.168]  [1.x.169]
* 

* 
*  This is even more simple than in  [2.x.95] . We use the parallel direct solver MUMPS which requires less options than an iterative solver. The drawback is that it does not scale very well. It is not straightforward to solve the Helmholtz equation with an iterative solver. The shifted Laplacian multigrid method is a well known approach to precondition this system, but this is beyond the scope of this tutorial.
* 

* 
* [1.x.170]
* 
*   [1.x.171]  [1.x.172]
* 

* 
*  We use this function to calculate the values of the position vector.
* 

* 
* [1.x.173]
* 
*  Because of the way the operator + and
* 
*  - are overloaded to subtract two points, the following has to be done: `Point_b<dim> + (-Point_a<dim>)`
* 

* 
* [1.x.174]
* 
*   [1.x.175]  [1.x.176]
* 

* 
*  This function stores in the HDF5 file the measured energy by the probe.
* 

* 
* [1.x.177]
* 
*  We store the displacement in the  [2.x.96]  direction; the displacement in the  [2.x.97]  direction is negligible.
* 

* 
* [1.x.178]
* 
*  The vector coordinates contains the coordinates in the HDF5 file of the points of the probe that are located in locally owned cells. The vector displacement_data contains the value of the displacement at these points.
* 

* 
* [1.x.179]
* 
*  Then we can store the values of the displacement in the points of the probe in `displacement_data`.
* 

* 
* [1.x.180]
* 
*  We write the displacement data in the HDF5 file. The call  [2.x.98]  is MPI collective which means that all the processes have to participate.
* 

* 
* [1.x.181]
* 
*  Therefore even if the process has no data to write it has to participate in the collective call. For this we can use  [2.x.99]  Note that we have to specify the data type, in this case  [2.x.100] 
* 

* 
* [1.x.182]
* 
*  If the variable `save_vtu_files` in the input file equals `True` then all the data will be saved as vtu. The procedure to write `vtu` files has been described in  [2.x.101] .
* 

* 
* [1.x.183]
* 
*  And on the cells that we are not interested in, set the respective value to a bogus value in order to make sure that if we were somehow wrong about our assumption we would find out by looking at the graphical output:
* 

* 
* [1.x.184]
* 
*   [1.x.185]  [1.x.186]
* 

* 
*  This function writes the datasets that have not already been written.
* 

* 
* [1.x.187]
* 
*  The vectors `frequency` and `position` are the same for all the processes. Therefore any of the processes can write the corresponding `datasets`. Because the call  [2.x.102]  is MPI collective, the rest of the processes will have to call  [2.x.103] 
* 

* 
* [1.x.188]
* 
*   [1.x.189]  [1.x.190]
* 

* 
*  We use this function at the beginning of our computations to set up initial values of the cache variables. This function has been described in  [2.x.104] . There are no differences with the function of  [2.x.105] .
* 

* 
* [1.x.191]
* 
*   [1.x.192]  [1.x.193]
* 

* 
*  For clarity we divide the function `run` of  [2.x.106]  into the functions `run` and `frequency_sweep`. In the function `frequency_sweep` we place the iteration over the frequency vector.
* 

* 
* [1.x.194]
* 
*  Write the simulation parameters only once
* 

* 
* [1.x.195]
* 
*  We calculate the frequency and omega values for this particular step.
* 

* 
* [1.x.196]
* 
*  In the first frequency step we calculate the mass and stiffness matrices and the right hand side. In the subsequent frequency steps we will use those values. This improves considerably the calculation time.
* 

* 
* [1.x.197]
* 
*   [1.x.198]  [1.x.199]
* 

* 
*  This function is very similar to the one in  [2.x.107] .
* 

* 
* [1.x.200]
* 
*   [1.x.201]  [1.x.202]
* 

* 
*  The main function is very similar to the one in  [2.x.108] .
* 

* 
* [1.x.203]
* 
*  Each of the simulations (displacement and calibration) is stored in a separate HDF5 group:
* 

* 
* [1.x.204]
* 
*  For each of these two group names, we now create the group and put attributes into these groups. Specifically, these are:
* 

* 
* 
*  - The dimensions of the waveguide (in  [2.x.109]  and  [2.x.110]  directions)
* 

* 
* 
*  - The position of the probe (in  [2.x.111]  and  [2.x.112]  directions)
* 

* 
* 
*  - The number of points in the probe
* 

* 
* 
*  - The global refinement level
* 

* 
* 
*  - The cavity resonance frequency
* 

* 
* 
*  - The number of mirror pairs
* 

* 
* 
*  - The material properties
* 

* 
* 
*  - The force parameters
* 

* 
* 
*  - The PML parameters
* 

* 
* 
*  - The frequency parameters
* 

* 
*  

* 
* [1.x.205]
* 
*  Displacement simulation. The parameters are read from the displacement HDF5 group and the results are saved in the same HDF5 group.
* 

* 
* [1.x.206]
* 
*  Calibration simulation. The parameters are read from the calibration HDF5 group and the results are saved in the same HDF5 group.
* 

* 
* [1.x.207]
* [1.x.208][1.x.209]
* 

* [1.x.210][1.x.211]
* 

* The results are analyzed in the[jupyter notebook](https://github.com/dealii/dealii/blob/master/examples/ [2.x.113] / [2.x.114] .ipynb)with the following code
* [1.x.212]
* 
* A phononic cavity is characterized by the[resonance frequency](https://en.wikipedia.org/wiki/Resonance) and the[the quality factor](https://en.wikipedia.org/wiki/Q_factor).The quality factor is equal to the ratio between the stored energy in the resonator and the energydissipated energy per cycle, which is approximately equivalent to the ratio between theresonance frequency and the[full width at half maximum (FWHM)](https://en.wikipedia.org/wiki/Full_width_at_half_maximum).The FWHM is equal to the bandwidth over which the power of vibration is greater than half thepower at the resonant frequency.[1.x.213]
* The square of the amplitude of the mechanical resonance  [2.x.115]  as a function of the frequencyhas a gaussian shape[1.x.214]where  [2.x.116]  is the resonance frequency and  [2.x.117]  is the dissipation rate.We used the previous equation in the jupyter notebook to fit the mechanical resonance.
* Given the values we have chosen for the parameters, one could estimate the resonance frequencyanalytically. Indeed, this is then confirmed by what we get in this program:the phononic superlattice cavity exhibits a mechanical resonance at 20GHz and a quality factor of 5046.The following images show the transmission amplitude and phase as a function of frequency in thevicinity of the resonance frequency:
*  [2.x.118]  [2.x.119] 
* The images above suggest that the periodic structure has its intended effect: It really only lets waves of a veryspecific frequency pass through, whereas all other waves are reflected. This is of course precisely what one buildsthese sorts of devices for.But it is not quite this easy. In practice, there is really only a "band gap", i.e., the device blocks waves other thanthe desired one at 20GHz only within a certain frequency range. Indeed, to find out how large this "gap" is withinwhich waves are blocked, we can extend the frequency range to 16 GHz through the appropriate parameters in theinput file. We then obtain the following image:
*  [2.x.120] 
* What this image suggests is that in the range of around 18 to around 22 GHz, really only the waves with a frequencyof 20 GHz are allowed to pass through, but beyond this range, there are plenty of other frequencies that can passthrough the device.
* [1.x.215][1.x.216]
* 

* We can inspect the mode profile with Paraview or VisIt.As we have discussed, at resonance all the mechanicalenergy is transmitted and the amplitude of motion is amplified inside the cavity.It can be observed that the PMLs are quite effective to truncate the solution.The following image shows the mode profile at resonance:
*  [2.x.121] 
* On the other hand,  out of resonance all the mechanical energy isreflected. The following image shows the profile at 19.75 GHz.Note the interference between the force pulse and the reflected waveat the position  [2.x.122] .
*  [2.x.123] 
* [1.x.217][1.x.218]
* 

* Phononic superlattice cavities find application in[quantum optomechanics](https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.86.1391).Here we have presented the simulation of a 2D superlattice cavity,but this code can be used as well to simulate "real world" 3D devices such as[micropillar superlattice cavities](https://journals.aps.org/prb/abstract/10.1103/PhysRevB.99.060101),which are promising candidates to study macroscopic quantum phenomena.The 20GHz mode of a micropillar superlattice cavity is essentially a mechanical harmonic oscillator that is very well isolatedfrom the environment. If the device is cooled down to 20mK in a dilution fridge, the mode would then become amacroscopic quantum harmonic oscillator.
* 

* [1.x.219][1.x.220]
* 

* Instead of setting the parameters in the C++ file we could set the parametersusing a python script and save them in the HDF5 file that we will use forthe simulations. Then the deal.II program will read the parameters from theHDF5 file.
* [1.x.221]
* 
* In order to read the HDF5 parameters we have to use the [2.x.124]  flag.
* [1.x.222]
* 
* 

* [1.x.223][1.x.224] [2.x.125] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-63_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33]
*  [2.x.2] 
* [1.x.34]
*  [2.x.3] 
* [1.x.35][1.x.36][1.x.37]
* 

* This program solves an advection-diffusion problem using a geometric multigrid(GMG) preconditioner. The basics of this preconditioner are discussed in  [2.x.4] ;here we discuss the necessary changes needed for a non-symmetricPDE. Additionally, we introduce the idea of block smoothing (as compared topoint smoothing in  [2.x.5] ), and examine the effects of DoF renumbering foradditive and multiplicative smoothers.
* [1.x.38][1.x.39]
* The advection-diffusion equation is given by
* [1.x.40]
* where  [2.x.6] ,  [2.x.7]  is the [1.x.41], and  [2.x.8]  is a source. A few notes:
* 1. If  [2.x.9] , this is the Laplace equation solved in  [2.x.10] (and many other places).
* 2. If  [2.x.11]  then this is the stationary advection equation solved in [2.x.12] .
* 3. One can define a dimensionless number for this problem, called the[1.x.42]:  [2.x.13] , where  [2.x.14]  is the length scale of the domain. Itcharacterizes the kind of equation we areconsidering: If  [2.x.15] , we say the problem is[1.x.43], else if  [2.x.16]  we will say the problem is[1.x.44].
* For the discussion in this tutorial we will be concerned withadvection-dominated flow. This is the complicated case: We know thatfor diffusion-dominated problems, the standard Galerkin method worksjust fine, and we also know that simple multigrid methods such asthose defined in  [2.x.17]  are very efficient. On the other hand, foradvection-dominated problems, the standard Galerkin approach leads tooscillatory and unstable discretizations, and simple solvers are oftennot very efficient. This tutorial program is therefore intended toaddress both of these issues.
* 

* [1.x.45][1.x.46]
* 

* Using the standard Galerkin finite element method, for suitable testfunctions  [2.x.18] , a discrete weak form of the PDE would read
* [1.x.47]
* where
* [1.x.48]
* 
* Unfortunately, one typically gets oscillatory solutions with thisapproach. Indeed, the following error estimate can be shown for thisformulation:
* [1.x.49]
* The infimum on the right can be estimated as follows if the exactsolution is sufficiently smooth:
* [1.x.50]
* where  [2.x.19]  is the polynomial degree of the finite elements used. As aconsequence, we obtain the estimate
* [1.x.51]
* In other words, the numerical solution will converge. On the other hand,given the definition of  [2.x.20]  above, we have to expect poornumerical solutions with a large error when  [2.x.21] , i.e., if the problem has only a smallamount of diffusion.
* To combat this, we will consider the new weak form
* [1.x.52]
* where the sum is done over all cells  [2.x.22]  with the inner product takenfor each cell, and  [2.x.23]  is a cell-wise constantstabilization parameter defined in [2.x.24] .
* Essentially, adding in thediscrete strong form residual enhances the coercivity of the bilinearform  [2.x.25]  which increases the stability of the discretesolution. This method is commonly referred to as [1.x.53] or [1.x.54] (streamline upwind/Petrov-Galerkin).
* 

* [1.x.55][1.x.56]
* 

* One of the goals of this tutorial is to expand from using a simple(point-wise) Gauss-Seidel (SOR) smoother that is used in  [2.x.26] (class PreconditionSOR) on each level of the multigrid hierarchy.The term "point-wise" is traditionally used in solvers to indicate that onesolves at one "grid point" at a time; for scalar problems, this meansto use a solver that updates one unknown of the linearsystem at a time, keeping all of the others fixed; one would theniterate over all unknowns in the problem and, once done, start over againfrom the first unknown until these "sweeps" converge. Jacobi,Gauss-Seidel, and SOR iterations can all be interpreted in this way.In the context of multigrid, one does not think of these methods as"solvers", but as "smoothers". As such, one is not interested inactually solving the linear system. It is enough to remove the high-frequencypart of the residual for the multigrid method to work, because that allowsrestricting the solution to a coarser mesh.  Therefore, one only does a few,fixed number of "sweeps" over all unknowns. In the code in thistutorial this is controlled by the "Smoothing steps" parameter.
* But these methods are known to converge rather slowly when used assolvers. While as multigrid smoothers, they are surprisingly good,they can also be improved upon. In particular, we consider"cell-based" smoothers here as well. These methods solve for allunknowns on a cell at once, keeping all other unknowns fixed; theythen move on to the next cell, and so on and so forth. One can thinkof them as "block" versions of Jacobi, Gauss-Seidel, or SOR, butbecause degrees of freedom are shared among multiple cells, theseblocks overlap and the methods are in factbest be explained within the framework of additive and multiplicativeSchwarz methods.
* In contrast to  [2.x.27] , our test problem contains an advectiveterm. Especially with a small diffusion constant  [2.x.28] , information istransported along streamlines in the given advection direction. This meansthat smoothers are likely to be more effective if they allow information totravel in downstream direction within a single smootherapplication. If we want to solve one unknown (or block of unknowns) ata time in the order in which these unknowns (or blocks) areenumerated, then this information propagation propertyrequires reordering degrees of freedom or cells (for the cell-based smoothers)accordingly so that the ones further upstream are treated earlier(have lower indices) and those further downstream are treated later(have larger indices). The influence of the ordering will be visiblein the results section.
* Let us now briefly define the smoothers used in this tutorial.For a more detailed introduction, we refer to [2.x.29]  and the books  [2.x.30]  and  [2.x.31] .A Schwarzpreconditioner requires a decomposition
* [1.x.57]
* of our finite element space  [2.x.32] . Each subproblem  [2.x.33]  also has a Ritzprojection  [2.x.34]  based on the bilinear form [2.x.35] . This projection induces a local operator  [2.x.36]  for eachsubproblem  [2.x.37] . If  [2.x.38]  is the orthogonal projector onto [2.x.39] , one can show  [2.x.40] .
* With this we can define an [1.x.58] for theoperator  [2.x.41]  as
* [1.x.59]
* In other words, we project our solution into each subproblem, apply theinverse of the subproblem  [2.x.42] , and sum the contributions up over all  [2.x.43] .
* Note that one can interpret the point-wise (one unknown at a time)Jacobi method as an additiveSchwarz method by defining a subproblem  [2.x.44]  for each degree offreedom. Then,  [2.x.45]  becomes a multiplication with the inverse of adiagonal entry of  [2.x.46] .
* For the "Block Jacobi" method used in this tutorial, we define a subproblem [2.x.47]  for each cell of the mesh on the current level. Note that we use acontinuous finite element, so these blocks are overlapping, as degrees offreedom on an interface between two cells belong to both subproblems. Thelogic for the Schwarz operator operating on the subproblems (in deal.II theyare called "blocks") is implemented in the class RelaxationBlock. The "BlockJacobi" method is implemented in the class RelaxationBlockJacobi. Manyaspects of the class (for example how the blocks are defined and how to invertthe local subproblems  [2.x.48] ) can be configured in the smoother data, see [2.x.49]  and  [2.x.50]  for details.
* So far, we discussed additive smoothers where the updates can be appliedindependently and there is no information flowing within a single smootherapplication. A [1.x.60] addresses thisand is defined by
* [1.x.61]
* In contrast to above, the updates on the subproblems  [2.x.51]  are appliedsequentially. This means that the update obtained when inverting thesubproblem  [2.x.52]  is immediately used in  [2.x.53] . This becomesvisible when writing out the project:
* [1.x.62]
* 
* When defining the sub-spaces  [2.x.54]  as whole blocks of degrees offreedom, this method is implemented in the class RelaxationBlockSOR and used when youselect "Block SOR" in this tutorial. The class RelaxationBlockSOR is alsoderived from RelaxationBlock. As such, both additive and multiplicativeSchwarz methods are implemented in a unified framework.
* Finally, let us note that the standard Gauss-Seidel (or SOR) method can beseen as a multiplicative Schwarz method with a subproblem for each DoF.
* 

* [1.x.63][1.x.64]
* 

* We will be considering the following test problem:  [2.x.55] , i.e., a squarewith a circle of radius 0.3 centered at theorigin removed. In addition, we use  [2.x.56] ,  [2.x.57] ,  [2.x.58] , and Dirichlet boundary values
* [1.x.65]
* 
* The following figures depict the solutions with (left) and without(right) streamline diffusion. Without streamline diffusion we see largeoscillations around the boundary layer, demonstrating the instabilityof the standard Galerkin finite element method for this problem.
*  [2.x.59] 
* 

*  [1.x.66] [1.x.67]
*   [1.x.68]  [1.x.69]
* 

* 
*  Typical files needed for standard deal.II:
* 

* 
* [1.x.70]
* 
*  Include all relevant multilevel files:
* 

* 
* [1.x.71]
* 
*  C++:
* 

* 
* [1.x.72]
* 
*  We will be using  [2.x.60]  functionality for assembling matrices:
* 

* 
* [1.x.73]
* 
*   [1.x.74]  [1.x.75]
* 

* 
*  As always, we will be putting everything related to this program into a namespace of its own.
* 

* 
*  Since we will be using the MeshWorker framework, the first step is to define the following structures needed by the assemble_cell() function used by  [2.x.61]  `ScratchData` contains an FEValues object which is needed for assembling a cell's local contribution, while `CopyData` contains the output from a cell's local contribution and necessary information to copy that to the global system. (Their purpose is also explained in the documentation of the WorkStream class.)
* 

* 
* [1.x.76]
* 
*   [1.x.77]  [1.x.78]
* 

* 
*  The second step is to define the classes that deal with run-time parameters to be read from an input file.   
*   We will use ParameterHandler to pass in parameters at runtime. The structure `Settings` parses and stores the parameters to be queried throughout the program.
* 

* 
* [1.x.79]
* 
*   [1.x.80]  [1.x.81]   
*   The ordering in which cells and degrees of freedom are traversed will play a role in the speed of convergence for multiplicative methods. Here we define functions which return a specific ordering of cells to be used by the block smoothers.   
*   For each type of cell ordering, we define a function for the active mesh and one for a level mesh (i.e., for the cells at one level of a multigrid hierarchy). While the only reordering necessary for solving the system will be on the level meshes, we include the active reordering for visualization purposes in output_results().   
*   For the two downstream ordering functions, we first create an array with all of the relevant cells that we then sort in downstream direction using a "comparator" object. The output of the functions is then simply an array of the indices of the cells in the just computed order.
* 

* 
* [1.x.82]
* 
*  The functions that produce a random ordering are similar in spirit in that they first put information about all cells into an array. But then, instead of sorting them, they shuffle the elements randomly using the facilities C++ offers to generate random numbers. The way this is done is by iterating over all elements of the array, drawing a random number for another element before that, and then exchanging these elements. The result is a random shuffle of the elements of the array.
* 

* 
* [1.x.83]
* 
*   [1.x.84]  [1.x.85]
* 

* 
*  The problem solved in this tutorial is an adaptation of Ex. 3.1.3 found on pg. 118 of [1.x.86]. The main difference being that we add a hole in the center of our domain with zero Dirichlet boundary conditions.   
*   For a complete description, we need classes that implement the zero right-hand side first (we could of course have just used  [2.x.62] 
* 

* 
* [1.x.87]
* 
*  We also have Dirichlet boundary conditions. On a connected portion of the outer, square boundary we set the value to 1, and we set the value to 0 everywhere else (including the inner, circular boundary):
* 

* 
* [1.x.88]
* 
*  Set boundary to 1 if  [2.x.63] , or if  [2.x.64]  and  [2.x.65] .
* 

* 
* [1.x.89]
* 
*   [1.x.90]  [1.x.91]
* 

* 
*  The streamline diffusion method has a stabilization constant that we need to be able to compute. The choice of how this parameter is computed is taken from [1.x.92].
* 

* 
* [1.x.93]
* 
*   [1.x.94]  [1.x.95]
* 

* 
*  This is the main class of the program, and should look very similar to  [2.x.66] . The major difference is that, since we are defining our multigrid smoother at runtime, we choose to define a function `create_smoother()` and a class object `mg_smoother` which is a  [2.x.67]  to a smoother that is derived from MGSmoother. Note that for smoothers derived from RelaxationBlock, we must include a `smoother_data` object for each level. This will contain information about the cell ordering and the method of inverting cell matrices.
* 

* 
*  

* 
* [1.x.96]
* 
*   [1.x.97]  [1.x.98]
* 

* 
*  Here we first set up the DoFHandler, AffineConstraints, and SparsityPattern objects for both active and multigrid level meshes.   
*   We could renumber the active DoFs with the DoFRenumbering class, but the smoothers only act on multigrid levels and as such, this would not matter for the computations. Instead, we will renumber the DoFs on each multigrid level below.
* 

* 
* [1.x.99]
* 
*  Having enumerated the global degrees of freedom as well as (in the last line above) the level degrees of freedom, let us renumber the level degrees of freedom to get a better smoother as explained in the introduction.  The first block below renumbers DoFs on each level in downstream or upstream direction if needed. This is only necessary for point smoothers (SOR and Jacobi) as the block smoothers operate on cells (see `create_smoother()`). The blocks below then also implement random numbering.
* 

* 
* [1.x.100]
* 
*  The rest of the function just sets up data structures. The last lines of the code below is unlike the other GMG tutorials, as it sets up both the interface in and out matrices. We need this since our problem is non-symmetric.
* 

* 
* [1.x.101]
* 
*   [1.x.102]  [1.x.103]
* 

* 
*  Here we define the assembly of the linear system on each cell to be used by the mesh_loop() function below. This one function assembles the cell matrix for either an active or a level cell (whatever it is passed as its first argument), and only assembles a right-hand side if called with an active cell.
* 

* 
*  

* 
* [1.x.104]
* 
*  If we are using streamline diffusion we must add its contribution to both the cell matrix and the cell right-hand side. If we are not using streamline diffusion, setting  [2.x.68]  negates this contribution below and we are left with the standard, Galerkin finite element assembly.
* 

* 
* [1.x.105]
* 
*  The assembly of the local matrix has two parts. First the Galerkin contribution:
* 

* 
* [1.x.106]
* 
*  and then the streamline diffusion contribution:
* 

* 
* [1.x.107]
* 
*  The same applies to the right hand side. First the Galerkin contribution:
* 

* 
* [1.x.108]
* 
*  and then the streamline diffusion contribution:
* 

* 
* [1.x.109]
* 
*   [1.x.110]  [1.x.111]
* 

* 
*  Here we employ  [2.x.69]  to go over cells and assemble the system_matrix, system_rhs, and all mg_matrices for us.
* 

* 
*  

* 
* [1.x.112]
* 
*  Unlike the constraints for the active level, we choose to create constraint objects for each multigrid level local to this function since they are never needed elsewhere in the program.
* 

* 
* [1.x.113]
* 
*  If  [2.x.70]  is an `interface_out` dof pair, then  [2.x.71]  is an `interface_in` dof pair. Note: For `interface_in`, we load the transpose of the interface entries, i.e., the entry for dof pair  [2.x.72]  is stored in `interface_in(i,j)`. This is an optimization for the symmetric case which allows only one matrix to be used when setting the edge_matrices in solve(). Here, however, since our problem is non-symmetric, we must store both `interface_in` and `interface_out` matrices.
* 

* 
* [1.x.114]
* 
*   [1.x.115]  [1.x.116]
* 

* 
*  Next, we set up the smoother based on the settings in the `.prm` file. The two options that are of significance is the number of pre- and post-smoothing steps on each level of the multigrid v-cycle and the relaxation parameter.
* 

* 
*  Since multiplicative methods tend to be more powerful than additive method, fewer smoothing steps are required to see convergence independent of mesh size. The same holds for block smoothers over point smoothers. This is reflected in the choice for the number of smoothing steps for each type of smoother below.
* 

* 
*  The relaxation parameter for point smoothers is chosen based on trial and error, and reflects values necessary to keep the iteration counts in the GMRES solve constant (or as close as possible) as we refine the mesh. The two values given for both "Jacobi" and "SOR" in the `.prm` files are for degree 1 and degree 3 finite elements. If the user wants to change to another degree, they may need to adjust these numbers. For block smoothers, this parameter has a more straightforward interpretation, namely that for additive methods in 2D, a DoF can have a repeated contribution from up to 4 cells, therefore we must relax these methods by 0.25 to compensate. This is not an issue for multiplicative methods as each cell's inverse application carries new information to all its DoFs.
* 

* 
*  Finally, as mentioned above, the point smoothers only operate on DoFs, and the block smoothers on cells, so only the block smoothers need to be given information regarding cell orderings. DoF ordering for point smoothers has already been taken care of in `setup_system()`.
* 

* 
*  

* 
* [1.x.117]
* 
*   [1.x.118]  [1.x.119]
* 

* 
*  Before we can solve the system, we must first set up the multigrid preconditioner. This requires the setup of the transfer between levels, the coarse matrix solver, and the smoother. This setup follows almost identically to  [2.x.73] , the main difference being the various smoothers defined above and the fact that we need different interface edge matrices for in and out since our problem is non-symmetric. (In reality, for this tutorial these interface matrices are empty since we are only using global refinement, and thus have no refinement edges. However, we have still included both here since if one made the simple switch to an adaptively refined method, the program would still run correctly.)
* 

* 
*  The last thing to note is that since our problem is non-symmetric, we must use an appropriate Krylov subspace method. We choose here to use GMRES since it offers the guarantee of residual reduction in each iteration. The major disavantage of GMRES is that, for each iteration, the number of stored temporary vectors increases by one, and one also needs to compute a scalar product with all previously stored vectors. This is rather expensive. This requirement is relaxed by using the restarted GMRES method which puts a cap on the number of vectors we are required to store at any one time (here we restart after 50 temporary vectors, or 48 iterations). This then has the disadvantage that we lose information we have gathered throughout the iteration and therefore we could see slower convergence. As a consequence, where to restart is a question of balancing memory consumption, CPU effort, and convergence speed. However, the goal of this tutorial is to have very low iteration counts by using a powerful GMG preconditioner, so we have picked the restart length such that all of the results shown below converge prior to restart happening, and thus we have a standard GMRES method. If the user is interested, another suitable method offered in deal.II would be BiCGStab.
* 

* 
*  

* 
* [1.x.120]
* 
*   [1.x.121]  [1.x.122]
* 

* 
*  The final function of interest generates graphical output. Here we output the solution and cell ordering in a .vtu format.
* 

* 
*  At the top of the function, we generate an index for each cell to visualize the ordering used by the smoothers. Note that we do this only for the active cells instead of the levels, where the smoothers are actually used. For the point smoothers we renumber DoFs instead of cells, so this is only an approximation of what happens in reality. Finally, the random ordering is not the random ordering we actually use (see `create_smoother()` for that).   
*   The (integer) ordering of cells is then copied into a (floating point) vector for graphical output.
* 

* 
* [1.x.123]
* 
*  The remainder of the function is then straightforward, given previous tutorial programs:
* 

* 
* [1.x.124]
* 
*   [1.x.125]  [1.x.126]
* 

* 
*  As in most tutorials, this function creates/refines the mesh and calls the various functions defined above to set up, assemble, solve, and output the results.
* 

* 
*  In cycle zero, we generate the mesh for the on the square  [2.x.74]  with a hole of radius 3/10 units centered at the origin. For objects with `manifold_id` equal to one (namely, the faces adjacent to the hole), we assign a spherical manifold.
* 

* 
*  

* 
* [1.x.127]
* 
*   [1.x.128]  [1.x.129]
* 

* 
*  Finally, the main function is like most tutorials. The only interesting bit is that we require the user to pass a `.prm` file as a sole command line argument. If no parameter file is given, the program will output the contents of a sample parameter file with all default values to the screen that the user can then copy and paste into their own `.prm` file.
* 

* 
*  

* 
* [1.x.130]
* [1.x.131][1.x.132]
* 

* [1.x.133][1.x.134]
* 

* The major advantage for GMG is that it is an  [2.x.75]  method,that is, the complexity of the problem increases linearly with theproblem size. To show then that the linear solver presented in thistutorial is in fact  [2.x.76] , all one needs to do is show thatthe iteration counts for the GMRES solve stay roughly constant as werefine the mesh.
* Each of the following tables gives the GMRES iteration counts to reduce theinitial residual by a factor of  [2.x.77] . We selected a sufficient number of smoothing steps(based on the method) to get iteration numbers independent of mesh size. Ascan be seen from the tables below, the method is indeed  [2.x.78] .
* [1.x.135][1.x.136]
* 

* The point-wise smoothers ("Jacobi" and "SOR") get applied in the order theDoFs are numbered on each level. We can influence this using theDoFRenumbering namespace. The block smoothers are applied based on theordering we set in `setup_smoother()`. We can visualize this numbering. Thefollowing pictures show the cell numbering of the active cells in downstream,random, and upstream numbering (left to right):
*  [2.x.79] 
* Let us start with the additive smoothers. The following table showsthe number of iterations necessary to obtain convergence from GMRES:
*  [2.x.80] 
* We see that renumbering theDoFs/cells has no effect on convergence speed. This is because thesesmoothers compute operations on each DoF (point-smoother) or cell(block-smoother) independently and add up the results. Since we candefine these smoothers as an application of a sum of matrices, andmatrix addition is commutative, the order at which we sum thedifferent components will not affect the end result.
* On the other hand, the situation is different for multiplicative smoothers:
*  [2.x.81] 
* Here, we can speed upconvergence by renumbering the DoFs/cells in the advection direction,and similarly, we can slow down convergence if we do the renumberingin the opposite direction. This is because advection-dominatedproblems have a directional flow of information (in the advectiondirection) which, given the right renumbering of DoFs/cells,multiplicative methods are able to capture.
* This feature of multiplicative methods is, however, dependent on thevalue of  [2.x.82] . As we increase  [2.x.83]  and the problembecomes more diffusion-dominated, we have a more uniform propagationof information over the mesh and there is a diminished advantage forrenumbering in the advection direction. On the opposite end, in theextreme case of  [2.x.84]  (advection-only), we have a 1st-orderPDE and multiplicative methods with the right renumbering becomeeffective solvers: A correct downstream numbering may lead to methodsthat require only a single iteration because information can bepropagated from the inflow boundary downstream, with no informationtransport in the opposite direction. (Note, however, that in the caseof  [2.x.85] , special care must be taken for the boundaryconditions in this case).
* 

* [1.x.137][1.x.138]
* 

* We will limit the results to runs using the downstreamrenumbering. Here is a cross comparison of all four smoothers for both [2.x.86]  and  [2.x.87]  elements:
*  [2.x.88] 
* We see that for  [2.x.89] , both multiplicative smoothers require a smallercombination of smoothing steps and iteration counts than eitheradditive smoother. However, when we increase the degree to a  [2.x.90] element, there is a clear advantage for the block smoothers in termsof the number of smoothing steps and iterations required tosolve. Specifically, the block SOR smoother gives constant iterationcounts over the degree, and the block Jacobi smoother only sees abouta 38% increase in iterations compared to 75% and 183% for Jacobi andSOR respectively.
* [1.x.139][1.x.140]
* 

* Iteration counts do not tell the full story in the optimality of a onesmoother over another. Obviously we must examine the cost of aniteration. Block smoothers here are at a disadvantage as they arehaving to construct and invert a cell matrix for each cell. Here is acomparison of solve times for a  [2.x.91]  element with 74,496 DoFs:
*  [2.x.92] 
* The smoother that requires the most iterations (Jacobi) actually takesthe shortest time (roughly 2/3 the time of the next fastestmethod). This is because all that is required to apply a Jacobismoothing step is multiplication by a diagonal matrix which is verycheap. On the other hand, while SOR requires over 3x more iterations(each with 3x more smoothing steps) than block SOR, the times areroughly equivalent, implying that a smoothing step of block SOR isroughly 9x slower than a smoothing step of SOR. Lastly, block Jacobiis almost 6x more expensive than block SOR, which intuitively makessense from the fact that 1 step of each method has the same cost(inverting the cell matrices and either adding or multiply themtogether), and block Jacobi has 3 times the number of smoothing steps periteration with 2 times the iterations.
* 

* [1.x.141][1.x.142]
* 

* There are a few more important points to mention:
*  [2.x.93]  [2.x.94]  For a mesh distributed in parallel, multiplicative methods cannotbe executed over the entire domain. This is because they operate onecell at a time, and downstream cells can only be handled once upstreamcells have already been done. This is fine on a single processor: Theprocessor just goes through the list of cells one after theother. However, in parallel, it would imply that some processors areidle because upstream processors have not finished doing the work oncells upstream from the ones owned by the current processor. Once theupstream processors are done, the downstream ones can start, but bythat time the upstream processors have no work left. In other words,most of the time during these smoother steps, most processors are infact idle. This is not how one obtains good parallel scalability!
* One can use a hybrid method wherea multiplicative smoother is applied on each subdomain, but as youincrease the number of subdomains, the method approaches the behaviorof an additive method. This is a major disadvantage to these methods. [2.x.95] 
*  [2.x.96]  Current research into block smoothers suggest that soon we will beable to compute the inverse of the cell matrices much cheaper thanwhat is currently being done inside deal.II. This research is based onthe fast diagonalization method (dating back to the 1960s) and hasbeen used in the spectral community for around 20 years (see, e.g., [1.x.143]). There are currently efforts to generalize thesemethods to DG and make them more robust. Also, it seems that oneshould be able to take advantage of matrix-free implementations andthe fact that, in the interior of the domain, cell matrices tend tolook very similar, allowing fewer matrix inverse computations. [2.x.97]  [2.x.98] 
* Combining 1. and 2. gives a good reason for expecting that a methodlike block Jacobi could become very powerful in the future, eventhough currently for these examples it is quite slow.
* 

* [1.x.144][1.x.145]
* 

* [1.x.146][1.x.147]
* 

* Change the number of smoothing steps and the smoother relaxationparameter (set in  [2.x.99]  inside [2.x.100] , only necessary for point smoothers) sothat we maintain a constant number of iterations for a  [2.x.101]  element.
* [1.x.148][1.x.149]
* 

* Increase/decrease the parameter "Epsilon" in the `.prm` files of themultiplicative methods and observe for which values renumbering nolonger influences convergence speed.
* [1.x.150][1.x.151]
* 

* The code is set up to work correctly with an adaptively refined mesh (theinterface matrices are created and set). Devise a suitable refinementcriterium or try the KellyErrorEstimator class.
* 

* [1.x.152][1.x.153] [2.x.102] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-64_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] ,  [2.x.2] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15]
*  [2.x.3] 
* [1.x.16]
* 

* [1.x.17][1.x.18]
* 

* This example shows how to implement a matrix-free method on the GPU using CUDAfor the Helmholtz equation with variable coefficients on a hypercube. The linearsystem will be solved using the conjugate gradient method and is parallelized with MPI.
* In the last few years, heterogeneous computing in general and GPUs in particularhave gained a lot of popularity. This is because GPUs offer better computingcapabilities and memory bandwidth than CPUs for a given power budget.Among the architectures available in early 2019, GPUs are about 2x-3x as powerefficient than server CPUs with wide [1.x.19] for PDE-relatedtasks. GPUs are alsothe most popular architecture for machine learning. On the other hand,GPUs are not easy to program. This program explores the deal.IIcapabilities to see how efficiently such a program can be implemented.
* While we have tried for the interface of the matrix-free classes for the CPU andthe GPU to be as close as possible, there are a few differences. When usingthe matrix-free framework on a GPU, one must write some CUDA code. However, theamount is fairly small and the use of CUDA is limited to a few keywords.
* 

* [1.x.20][1.x.21]
* 

* In this example, we consider the Helmholtz problem [1.x.22]
* where  [2.x.4]  is a variable coefficient.
* We choose as domain  [2.x.5]  and  [2.x.6] . Since the coefficient is symmetric around the origin butthe domain is not, we will end up with a non-symmetric solution.
* If you've made it this far into the tutorial, you will know how theweak formulation of this problem looks like and how, in principle, oneassembles linear systems for it. Of course, in this program we will infact not actually form the matrix, but rather only represent itsaction when one multiplies with it.
* 

* [1.x.23][1.x.24]
* 

* GPUs (we will use the term "device" from now on to refer to the GPU) have their own memorythat is separate from the memory accessible to the CPU (we will use the term"host" from now on). A normal calculation on the device can be divided in threeseparate steps:
* 

* 
* 
*  - the data is moved from the host to the device,
* 

* 
* 
*  - the computation is done on the device,
* 

* 
* 
*  - the result is moved back from the device to the host
* The data movements can either be done explicitly by the user code or doneautomatically using UVM (Unified Virtual Memory). In deal.II, only the firstmethod is supported. While it means an extra burden for the user, thisallows forbetter control of data movement and more importantly it avoids to mistakenly runimportant kernels on the host instead of the device.
* The data movement in deal.II is done using  [2.x.7]  Thesevectors can be seen as buffers on the host that are used to either store datareceived from the device or to send data to the device. There are two types of vectorsthat can be used on the device:
* 
*  -  [2.x.8]  which is similar to the more commonVector<Number>, and
* 
*  -  [2.x.9]  [2.x.10]  which is a regular [2.x.11]  where we have specified which memoryspace to use.
* If no memory space is specified, the default is  [2.x.12] 
* Next, we show how to move data to/from the device using [2.x.13] 
* [1.x.25]
* Both of the vector classes used here only work on a single machine,i.e., one memory space on a host and one on a device.
* But there are cases where one wants to run a parallel computationbetween multiple MPI processes on a number of machines, each of whichis equipped with GPUs. In that case, one wants to use [2.x.14] which is similar but the `import()` stage may involve MPI communication:
* [1.x.26]
* The `relevant_rw_vector` is an object that stores a subset of allelements of the vector. Typically, these are the [2.x.15]  "locally relevant DoFs",which implies that they overlap between different MPIprocesses. Consequently, the elements stored in that vector on onemachine may not coincide with the ones stored by the GPU on thatmachine, requiring MPI communication to import them.
* In all of these cases, while importing a vector, values can either beinserted (using  [2.x.16]  or added to prior content ofthe vector (using  [2.x.17] 
* 

* [1.x.27][1.x.28]
* 

* The code necessary to evaluate the matrix-free operator on the device is verysimilar to the one on the host. However, there are a few differences, the mainones being that the `local_apply()` function in  [2.x.18]  and the loop overquadrature points both need to be encapsulated in their own functors.
* 

*  [1.x.29] [1.x.30]
*  First include the necessary files from the deal.II library known from the previous tutorials.
* 

* 
* [1.x.31]
* 
*  The following ones include the data structures for the implementation of matrix-free methods on GPU:
* 

* 
* [1.x.32]
* 
*  As usual, we enclose everything into a namespace of its own:
* 

* 
* [1.x.33]
* 
*   [1.x.34]  [1.x.35]
* 

* 
*  Next, we define a class that implements the varying coefficients we want to use in the Helmholtz operator. Later, we want to pass an object of this type to a  [2.x.19]  object that expects the class to have an `operator()` that fills the values provided in the constructor for a given cell. This operator needs to run on the device, so it needs to be marked as `__device__` for the compiler.
* 

* 
* [1.x.36]
* 
*  Since  [2.x.20]  doesn't know about the size of its arrays, we need to store the number of quadrature points and the numbers of degrees of freedom in this class to do necessary index conversions.
* 

* 
* [1.x.37]
* 
*  The following function implements this coefficient. Recall from the introduction that we have defined it as  [2.x.21] 
* 

* 
* [1.x.38]
* 
*   [1.x.39]  [1.x.40]
* 

* 
*  The class `HelmholtzOperatorQuad` implements the evaluation of the Helmholtz operator at each quadrature point. It uses a similar mechanism as the MatrixFree framework introduced in  [2.x.22] . In contrast to there, the actual quadrature point index is treated implicitly by converting the current thread index. As before, the functions of this class need to run on the device, so need to be marked as `__device__` for the compiler.
* 

* 
* [1.x.41]
* 
*  The Helmholtz problem we want to solve here reads in weak form as follows: [1.x.42]
*  If you have seen  [2.x.23] , then it will be obvious that the two terms on the left-hand side correspond to the two function calls here:
* 

* 
* [1.x.43]
* 
*   [1.x.44]  [1.x.45]
* 

* 
*  Finally, we need to define a class that implements the whole operator evaluation that corresponds to a matrix-vector product in matrix-based approaches.
* 

* 
* [1.x.46]
* 
*  Again, the  [2.x.24]  object doesn't know about the number of degrees of freedom and the number of quadrature points so we need to store these for index calculations in the call operator.
* 

* 
* [1.x.47]
* 
*  This is the call operator that performs the Helmholtz operator evaluation on a given cell similar to the MatrixFree framework on the CPU. In particular, we need access to both values and gradients of the source vector and we write value and gradient information to the destination vector.
* 

* 
* [1.x.48]
* 
*   [1.x.49]  [1.x.50]
* 

* 
*  The `HelmholtzOperator` class acts as wrapper for `LocalHelmholtzOperator` defining an interface that can be used with linear solvers like SolverCG. In particular, like every class that implements the interface of a linear operator, it needs to have a `vmult()` function that performs the action of the linear operator on a source vector.
* 

* 
* [1.x.51]
* 
*  The following is the implementation of the constructor of this class. In the first part, we initialize the `mf_data` member variable that is going to provide us with the necessary information when evaluating the operator.   
*   In the second half, we need to store the value of the coefficient for each quadrature point in every active, locally owned cell. We can ask the parallel triangulation for the number of active, locally owned cells but only have a DoFHandler object at hand. Since  [2.x.25]  returns a Triangulation object, not a  [2.x.26]  object, we have to downcast the return value. This is safe to do here because we know that the triangulation is a  [2.x.27]  object in fact.
* 

* 
* [1.x.52]
* 
*  The key step then is to use all of the previous classes to loop over all cells to perform the matrix-vector product. We implement this in the next function.   
*   When applying the Helmholtz operator, we have to be careful to handle boundary conditions correctly. Since the local operator doesn't know about constraints, we have to copy the correct values from the source to the destination vector afterwards.
* 

* 
* [1.x.53]
* 
*   [1.x.54]  [1.x.55]
* 

* 
*  This is the main class of this program. It defines the usual framework we use for tutorial programs. The only point worth commenting on is the `solve()` function and the choice of vector types.
* 

* 
* [1.x.56]
* 
*  Since all the operations in the `solve()` function are executed on the graphics card, it is necessary for the vectors used to store their values on the GPU as well.  [2.x.28]  can be told which memory space to use. There is also  [2.x.29]  that always uses GPU memory storage but doesn't work with MPI. It might be worth noticing that the communication between different MPI processes can be improved if the MPI implementation is CUDA-aware and the configure flag `DEAL_II_MPI_WITH_CUDA_SUPPORT` is enabled. (The value of this flag needs to be set at the time you call `cmake` when installing deal.II.)     
*   In addition, we also keep a solution vector with CPU storage such that we can view and display the solution as usual.
* 

* 
* [1.x.57]
* 
*  The implementation of all the remaining functions of this class apart from  [2.x.30]  doesn't contain anything new and we won't further comment much on the overall approach.
* 

* 
* [1.x.58]
* 
*  Unlike programs such as  [2.x.31]  or  [2.x.32] , we will not have to assemble the whole linear system but only the right hand side vector. This looks in essence like we did in  [2.x.33] , for example, but we have to pay attention to using the right constraints object when copying local contributions into the global vector. In particular, we need to make sure the entries that correspond to boundary nodes are properly zeroed out. This is necessary for CG to converge.  (Another solution would be to modify the `vmult()` function above in such a way that we pretend the source vector has zero entries by just not taking them into account in matrix-vector products. But the approach used here is simpler.)   
*   At the end of the function, we can't directly copy the values from the host to the device but need to use an intermediate object of type  [2.x.34]  to construct the correct communication pattern necessary.
* 

* 
* [1.x.59]
* 
*  This solve() function finally contains the calls to the new classes previously discussed. Here we don't use any preconditioner, i.e., precondition by the identity matrix, to focus just on the peculiarities of the  [2.x.35]  framework. Of course, in a real application the choice of a suitable preconditioner is crucial but we have at least the same restrictions as in  [2.x.36]  since matrix entries are computed on the fly and not stored.   
*   After solving the linear system in the first part of the function, we copy the solution from the device to the host to be able to view its values and display it in `output_results()`. This transfer works the same as at the end of the previous function.
* 

* 
* [1.x.60]
* 
*  The output results function is as usual since we have already copied the values back from the GPU to the CPU.   
*   While we're already doing something with the function, we might as well compute the  [2.x.37]  norm of the solution. We do this by calling  [2.x.38]  That function is meant to compute the error by evaluating the difference between the numerical solution (given by a vector of values for the degrees of freedom) and an object representing the exact solution. But we can easily compute the  [2.x.39]  norm of the solution by passing in a zero function instead. That is, instead of evaluating the error  [2.x.40] , we are just evaluating  [2.x.41]  instead.
* 

* 
* [1.x.61]
* 
*  There is nothing surprising in the `run()` function either. We simply compute the solution on a series of (globally) refined meshes.
* 

* 
* [1.x.62]
* 
*   [1.x.63]  [1.x.64]
* 

* 
*  Finally for the `main()` function.  By default, all the MPI ranks will try to access the device with number 0, which we assume to be the GPU device associated with the CPU on which a particular MPI rank runs. This works, but if we are running with MPI support it may be that multiple MPI processes are running on the same machine (for example, one per CPU core) and then they would all want to access the same GPU on that machine. If there is only one GPU in the machine, there is nothing we can do about it: All MPI ranks on that machine need to share it. But if there are more than one GPU, then it is better to address different graphic cards for different processes. The choice below is based on the MPI process id by assigning GPUs round robin to GPU ranks. (To work correctly, this scheme assumes that the MPI ranks on one machine are consecutive. If that were not the case, then the rank-GPU association may just not be optimal.) To make this work, MPI needs to be initialized before using this function.
* 

* 
* [1.x.65]
* [1.x.66][1.x.67]
* 

* Since the main purpose of this tutorial is to demonstrate how to use the [2.x.42]  interface, not to compute anything useful initself, we just show the expected output here:
* [1.x.68]
* 
* One can make two observations here: First, the norm of the numerical solutionconverges, presumably to the norm of the exact (but unknown)solution. And second, the number of iterations roughly doubles witheach refinement of the mesh. (This is in keeping with the expectationthat the number of CG iterations grows with the square root of thecondition number of the matrix; and that we know that the conditionnumber of the matrix of a second-order differential operation growslike  [2.x.43] .) This is of course rather inefficient, as anoptimal solver would have a number of iterations that is independentof the size of the problem. But having such a solver would requireusing a better preconditioner than the identity matrix we have used here.
* 

* [1.x.69][1.x.70][1.x.71]
* 

* Currently, this program uses no preconditioner at all. This is mainlysince constructing an efficient matrix-free preconditioner isnon-trivial.  However, simple choices just requiring the diagonal ofthe corresponding matrix are good candidates and these can be computedin a matrix-free way as well. Alternatively, and maybe even better,one could extend the tutorial to use multigrid with Chebyshevsmoothers similar to  [2.x.44] .
* 

* [1.x.72][1.x.73] [2.x.45] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-65_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21]
* 
*  [2.x.2] 
* [1.x.22]
* [1.x.23][1.x.24][1.x.25]
* 

* This tutorial program presents an advanced manifold class,TransfiniteInterpolationManifold, and how to work around its maindisadvantage, the relatively high cost.
* [1.x.26][1.x.27]
* 

* [1.x.28][1.x.29]
* 

* In many applications, the finite element mesh must be able to represent arelatively complex geometry. In the  [2.x.3] ,  [2.x.4] , and  [2.x.5]  tutorialprograms, some techniques to generate grids available within the deal.IIlibrary have been introduced. Given a base mesh, deal.II is then able tocreate a finer mesh by subdividing the cells into children, either uniformlyor only in selected parts of the computational domain. Besides the basicmeshing capabilities collected in the GridGenerator namespace, deal.II alsocomes with interfaces to read in meshes generated by (quad- and hex-only) meshgenerators using the functions of namespace GridIn, as for exampledemonstrated in  [2.x.6] . A fundamental limitation ofexternally generated meshes is that the information provided by the generatedcells in the mesh only consists of the position of the vertices and theirconnectivity, without the context of the underlying geometry that used to beavailable in the mesh generator that originally created this mesh. Thisbecomes problematic once the mesh is refined within deal.II and additionalpoints need to be placed. The  [2.x.7]  tutorial program shows how toovercome this limitation by using CAD surfaces in terms of the OpenCASCADElibrary, and  [2.x.8]  by providing the same kind of informationprogrammatically from within the source code.
* Within deal.II, the placement of new points during mesh refinement or for thedefinition of higher order mappings is controlled by manifold objects, see the [2.x.9]  "manifold module"for details.To give an example, consider the following situation of a two-dimensionalannulus (with pictures taken from the manifold module). If we start with aninitial mesh of 10 cells and refine the mesh three times globally withoutattaching any manifolds, we would obtain the following mesh:
*  [2.x.10] 
* The picture looks like this because, by default, deal.II only knowswhere to put the vertices of child cells by averaging the locations ofthe vertices of the parent cell. This yields a polygonal domain whosefaces are the edges of the original (coarse mesh) cells.Obviously, we must attach a curved description to the boundary faces of thetriangulation to reproduce the circular shape upon mesh refinement, like inthe following picture:
*  [2.x.11] 
* This is better: At least the inner and outer boundaries are nowapproaching real circles if we continue to refine the mesh.However, the mesh in this picture is still not optimal for an annulus in thesense that the [1.x.30] lines from one cell to the next have kinks at certain vertices,and one would rather like to use the following mesh:
*  [2.x.12] 
* In this last (optimal) case, which is also the default produced by [2.x.13]  the curved manifold description (in this case apolar manifold description) is applied not only to the boundary faces, but tothe whole domain. Whenever the triangulation requests a new point, e.g., themid point of the edges or the cells when it refines a cell into four children,it will place them along the respective mid points in the polar coordinatesystem. By contrast, the case above where only the boundary was subject to thepolar manifold, only mid points along the boundary would be placed along thecurved description, whereas mid points in the interior would be computed bysuitable averages of the surrounding points in the Cartesian coordinate system(see the  [2.x.14]  "manifold module" for more details).
* At this point, one might assume that curved volume descriptions are the way togo. This is generally not wrong, though it is sometimes not so easy todescribe how exactly this should work. Here are a couple of examples:
* 
*  - Imagine that the mesh above had actually been a disk, not just a ring.  In that case the polar manifold degenerates at the origin and  would not produce reasonable new points. In fact, defining a  manifold description for things that are supposed "to look round"  but might have points at or close to the origin is surprisingly very  difficult.
* 
*  - A similar thing happens at the origin  of the three-dimensional ball when one tries to attach a spherical manifold to  the whole volume &ndash; in this case, the computation of new manifold points  would abort with an exception.
* 
*  - CAD geometries often only describe the boundary of the domain, in a  similar way to how we only attached a manifold to the boundary in  the second picture above. Similarly,  [2.x.15]  only uses the CAD  geometry to generate a surface mesh (maybe because that is what is  needed to solve the problem in question), but if one wanted to solve  a problem in the water or the air around the ship described there,  we would need to have a volume mesh. The question is then how  exactly we should describe what is supposed to happen in the  interior of the domain.
* These simple examples make it clear thatfor many interesting cases we must step back from the desire to have ananalytic curved description for the full volume: There will need to be[1.x.31] kind of information that leads to curvature also in theinterior, but it must be possible to do this without actually writingdown an explicit formula that describes the kind of geometry.
* So what happens if we don't do anything at all in the interior andonly describe the surface as a manifold? Sometimes, as in the ringshown above, the result is not terrible. But sometimes it is. Consider thecase of a torus (e.g. generated with  [2.x.16]  with aTorusManifold object attached to the surface only, no additional manifolds onthe interior cells and faces, and with six cells in toroidal direction beforerefinement. If the mesh is refined once, we would obtain the following mesh,shown with the upper half of the mesh clipped away:
*  [2.x.17] 
* This is clearly sub-optimal. Indeed, if we had started with fewer thanthe six cells shown above in toroidal direction, the mapping actuallyinverts in some regionsbecause the new points placed along interior cells intersect with the boundaryas they are not following the circular shape along the toroidal direction. Thesimple case of a torus can still be fixed because we know that the toroidaldirection follows a cylindrical coordinate system, so attaching aTorusManifold to the surface combined with CylindricalManifold withappropriate periodicity in toroidal direction applied to all interior entitieswould produce a high-quality mesh as follows, now shown with two top cellshidden:
*  [2.x.18] 
* This mesh is pretty good, but obviously it is linked to a good description ofthe volume, which we lack in other cases. Actually, there is an imperfectionalso in this case, as we can see some unnatural kinks of two adjacent cells inthe interior of the domain which are hidden by the top two boundary cells, asopposed to the following setup (the default manifolds applied by [2.x.19]  and using the TransfiniteInterpolationManifold):
*  [2.x.20] 
* [1.x.32][1.x.33]
* 

* In order to find a better strategy, let us look at the two-dimensional diskagain (that is also the base entity rotated along the toroidal direction inthe torus). As we learned above, we can only apply the curved polardescription to the boundary (or a rim of cells sufficiently far away from theorigin) but must eventually transition to a straight description towards thedisk's center. If we use a flat manifold in the interior of the cells(i.e., one in which new vertices are created by averaging of theadjacent existing ones) and apolar manifold only for the boundary of the disk, we get the following meshupon four global refinements:
*  [2.x.21] 
* That's not a terrible mesh. At the same time,if you know that the original coarse mesh consisted of a single squarein the middle, with four caps around it, then it's not hard to seeevery refinement step that happened to this mesh to get the pictureabove.
* While the triangulation class of deal.II tries to propagate information fromthe boundary into the interior when creating new points, the reach of thisalgorithm is limited:
*  [2.x.22] 
* The picture above highlights those cells on the disk that are touching theboundary and where boundary information could in principle be taken intoaccount when only looking at a single cell at the time. Clearly, the areawhere some curvature can be taken into account gets more limited as the meshis refined, thus creating the seemingly irregular spots in the mesh: Whencomputing the center of any one of the boundary cells in the leftmost picture,the ideal position is the mid point between the outer circle and the cell inthe middle. This is exactly what is used for the first refinement step in theTriangulation class. However, for the second refinement all interior edges aswell as the interior cell layers can only add points according to a flatmanifold description.
* At this point, we realize what would be needed to create a better mesh: For[1.x.34] new points in [1.x.35] child cell that is created within the red shadedlayer on the leftmost picture, we want to compute the interpolation withrespect to the curvature in the area covered by the respective coarsecell. This is achieved by adding the class TransfiniteInterpolationManifold tothe highlighted cells of the coarse grid in the leftmost panel of the figureabove. This class adheres to the general manifold interfaces, i.e., given anyset of points within its domain of definition, it can compute weightedaverages conforming to the manifold (using a formula that will be given in aminute). These weighted averages are used whenever the mesh is refined, orwhen a higher order mapping (such as MappingQGeneric or MappingC1)is evaluated on a given cellsubject to this manifold. Using this manifold on the shaded cells of thecoarse grid of the disk (i.e., not only in the outer-most layer ofcells) produces the following mesh upon four globalsteps of refinement:
*  [2.x.23] 
* There are still some kinks in the lines of this mesh, but they arerestricted to the faces between coarse mesh cells, whereas the rest ofthe mesh is about as smooth as one would like. Indeed,given a straight-sided central cell, this representation is the best possibleone as all mesh cells follow a smooth transition from the straight sides inthe square block in the interior to the circular shape on the boundary. (Onecould possibly do a bit better by allowing some curvature also in the centralsquare block, that eventually vanishes as the center is approached.)
* 

* [1.x.36][1.x.37]
* 

* In the simple case of a disk with one curved and three straight edges, we canexplicitly write down how to achieve the blending of the shapes. For this, itis useful to map the physical cell, like the top one, back to the referencecoordinate system  [2.x.24]  where we compute averages betweencertain points. If we were to use a simple bilinear map spanned by fourvertices  [2.x.25] , the image of a point [2.x.26]  would be
* [1.x.38]
* 
* For the case of the curved surface, we want to modify this formula. For thetop cell of the coarse mesh of the disk, we can assume that the points [2.x.27]  and  [2.x.28]  sit along the straight line at the lower end andthe points  [2.x.29]  and  [2.x.30]  are connected by a quarter circle alongthe top. We would then map a point  [2.x.31]  as
* [1.x.39]
* where  [2.x.32]  is a curve that describes the  [2.x.33]  coordinates ofthe quarter circle in terms of an arclength parameter  [2.x.34] . Thisrepresents a linear interpolation between the straight lower edge and thecurved upper edge of the cell, and is the basis for the picture shown above.
* This formula is easily generalized to the case where all four edges aredescribed by a curve rather than a straight line. We call the four functions,parameterized by a single coordinate  [2.x.35]  or  [2.x.36]  in the horizontal andvertical directions,  [2.x.37]  for the left, right, lower, and upper edge of aquadrilateral, respectively. The interpolation then reads
* [1.x.40]
* 
* This formula assumes that the boundary curves match and coincide with thevertices  [2.x.38] , e.g.  [2.x.39]  or  [2.x.40] . The subtraction of the bilinearinterpolation in the second line of the formula makes sure that the prescribedcurves are followed exactly on the boundary: Along each of the four edges, weneed to subtract the contribution of the two adjacent edges evaluated in thecorners, which is then simply a vertex position. It is easy to checkthat the formula for the circle above is reproduced if three of the fourcurves  [2.x.41]  are straight and thus coincide with the bilinearinterpolation.
* This formula, called transfinite interpolation, was introduced in 1973 by [1.x.41]. Eventhough transfinite interpolation essentially only represents a linear blendingof the bounding curves, the interpolation exactly follows the boundary curvesfor each real number  [2.x.42]  or  [2.x.43] , i.e., it interpolatesin an infinite number of points, which was the original motivation to labelthis variant of interpolation a transfinite one by Gordon and Hall. Anotherinterpretation is that the transfinite interpolation interpolates from theleft and right and the top and bottom linearly, from which we need to subtractthe bilinear interpolation to ensure a unit weight in the interior of thedomain.
* The transfinite interpolation is easily generalized to three spatialdimensions. In that case, the interpolation allows to blend 6 differentsurface descriptions for any of the quads of a three-dimensional cell and 12edge descriptions for the lines of a cell. Again, to ensure a consistent map,it is necessary to subtract the contribution of edges and add the contributionof vertices again to make the curves follow the prescribed surface or edgedescription. In the three-dimensional case, it is also possible to use atransfinite interpolation from a curved edge both into the adjacent faces andthe adjacent cells.
* The interpolation of the transfinite interpolation in deal.II is general inthe sense that it can deal with arbitrary curves. It will evaluate the curvesin terms of their original coordinates of the  [2.x.44] -dimensional space but withone (or two, in the case of edges in 3D) coordinate held fixed at  [2.x.45]  or  [2.x.46]  to ensurethat any other manifold class, including CAD files if desired, can be appliedout of the box. Transfinite interpolation is a standard ingredient in meshgenerators, so the main strength of the integration of this feature within thedeal.II library is to enable it during adaptive refinement and coarsening ofthe mesh, and for creating higher-degree mappings that use manifolds to insertadditional points beyond the mesh vertices.
* As a final remark on transfinite interpolation, we mention that the meshrefinement strategies in deal.II in absence of a volume manifold descriptionare also based on the weights of the transfinite interpolation and optimal inthat sense. The difference is that the default algorithm sees only onecell at a time, and so will apply the optimal algorithm only on thosecells touching the curved manifolds. In contrast, using thetransfinite mapping on entire [1.x.42] of cells (originatingfrom one coarser cell) allows to use the transfinite interpolationmethod in a way that propagates information from the boundary to cellsfar away.
* 

* [1.x.43][1.x.44]
* 

* A mesh with a transfinite manifold description is typically set up in twosteps. The first step is to create a coarse mesh (or read it in from a file) and toattach a curved manifold to some of the mesh entities. For the above exampleof the disk, we attach a polar manifold to the faces along the outer circle(this is done automatically by  [2.x.47]  Before we startrefining the mesh, we then assign a TransfiniteInterpolationManifold to allinterior cells and edges of the mesh, which of course needs to be based onsome manifold id that we have assigned to those entities (everything exceptthe circle on the boundary). It does not matter whether we also assign aTransfiniteInterpolationManifold to the inner square of the disk or notbecause the transfinite interpolation on a coarse cell with straightedges (or flat faces in 3d) simply yields subdivided children withstraight edges (flat faces).
* Later, when the mesh is refined or when a higher-order mapping is set up basedon this mesh, the cells will query the underlying manifold object for newpoints. This process takes a set of surrounding points, for example the fourvertices of a two-dimensional cell, and a set of weights to each of thesepoints, for definition a new point. For the mid point of a cell, each of thefour vertices would get weight 0.25. For the transfinite interpolationmanifold, the process of building weighted sums requires some serious work. Byconstruction, we want to combine the points in terms of the referencecoordinates  [2.x.48]  and  [2.x.49]  (or  [2.x.50]  in 3D) of the surroundingpoints. However, the interface of the manifold classes in deal.II does not getthe reference coordinates of the surrounding points (as they are not storedglobally) but rather the physical coordinates only. Thus, the first step thetransfinite interpolation manifold has to do is to invert the mapping and findthe reference coordinates within one of the coarse cells of the transfiniteinterpolation (e.g. one of the four shaded coarse-grid cells of the disk meshabove). This inversion is done by a Newton iteration (or rather,finite-difference based Newton scheme combined with Broyden's method) andqueries the transfinite interpolation according to the formula above severaltimes. Each of these queries in turn might call an expensive manifold, e.g. aspherical description of a ball, and be expensive on its own. Since theManifold interface class of deal.II only provides a set of points, thetransfinite interpolation initially does not even know to which coarse gridcell the set of surrounding points belong to and needs to search among severalcells based on some heuristics. In terms of [1.x.45],one could describe theimplementation of the transfinite interpolation as an [1.x.46]-basedimplementation: Each cell of the initial coarse grid of the triangulationrepresents a chart with its own reference space, and the surrounding manifoldsprovide a way to transform from the chart space (i.e., the reference cell) tothe physical space. The collection of the charts of the coarse grid cells isan atlas, and as usual, the first thing one does when looking up something inan atlas is to find the right chart.
* Once the reference coordinates of the surrounding points have been found, anew point in the reference coordinate system is computed by a simple weightedsum. Finally, the reference point is inserted into the formula for thetransfinite interpolation, which gives the desired new point.
* In a number of cases, the curved manifold is not only used during meshrefinement, but also to ensure a curved representation of boundaries withinthe cells of the computational domain. This is a necessity to guaranteehigh-order convergence for high-order polynomials on complex geometriesanyway, but sometimes an accurate geometry is also desired with linear shapefunctions. This is often done by polynomial descriptions of the cells andcalled the isoparametric concept if the polynomial degree to represent thecurved mesh elements is the same as the degree of the polynomials for thenumerical solution. If the degree of the geometry is higher or lower than thesolution, one calls that a super- or sub-parametric geometry representation,respectively. In deal.II, the standard class for polynomial representation isMappingQGeneric. If, for example, this class is used with polynomial degree  [2.x.51]  in 3D, atotal of 125 (i.e.,  [2.x.52] ) points are needed for theinterpolation. Among these points, 8 are the cell's vertices and alreadyavailable from the mesh, but the other 117 need to be provided by themanifold. In case the transfinite interpolation manifold is used, we canimagine that going through the pull-back into reference coordinates of someyet to be determined coarse cell, followed by subsequent push-forward on eachof the 117 points, is a lot of work and can be very time consuming.
* What makes things worse is that the structure of many programs is suchthat themapping is queried several times independently for the same cell. Its primaryuse is in the assembly of the linear system, i.e., the computation of thesystem matrix and the right hand side, via the `mapping` argument of theFEValues object. However, also the interpolation of boundary values, thecomputation of numerical errors, writing the output, and evaluation of errorestimators must involve the same mapping to ensure a consistent interpretationof the solution vectors. Thus, even a linear stationary problem that is solvedonce will evaluate the points of the mapping several times. For the cubic casein 3D mentioned above, this means computing 117 points per cell by anexpensive algorithm many times. The situation is more pressing for nonlinearor time-dependent problems where those operations are done over and overagain.
* As the manifold description via a transfinite interpolation can easily behundreds of times more expensive than a similar query on a flat manifold, itmakes sense to compute the additional points only once and use them in allsubsequent calls. The deal.II library provides the class MappingQCache forexactly this purpose. The cache is typically not overly big compared to thememory consumed by a system matrix, as will become clear when looking at theresults of this tutorial program. The usage of MappingQCache is simple: Oncethe mesh has been set up (or changed during refinement), we call [2.x.53]  with the desired triangulation as well as adesired mapping as arguments. The initialization then goes through all cellsof the mesh and queries the given mapping for its additional points. Those getstored for an identifier of the cell so that they can later be returnedwhenever the mapping computes some quantities related to the cell (like theJacobians of the map between the reference and physical coordinates).
* As a final note, we mention that the TransfiniteInterpolationManifold alsomakes the refinement of the mesh more expensive. In this case, theMappingQCache does not help because it would compute points that cansubsequently not be re-used; there currently does not exist a moreefficient mechanism in deal.II. However, the mesh refinement contains manyother expensive steps as well, so it is not as big as an issue compared to therest of the computation. It also only happens at most once per timestep or nonlinear iteration.
* [1.x.47][1.x.48]
* 

* In this tutorial program, the usage of TransfiniteInterpolationManifold isexemplified in combination with MappingQCache. The test case is relativelysimple and takes up the solution stages involved in many typical programs,e.g., the  [2.x.54]  tutorial program. As a geometry, we select one prototype useof TransfiniteInterpolationManifold, namely a setup involving a spherical ballthat is in turn surrounded by a cube. Such a setup would be used, for example,for a spherical inclusion embedded in a background medium, and if thatinclusion has different material properties that require that theinterface between the two materials needs to be tracked by element interfaces. Avisualization of the grid is given here:
*  [2.x.55] 
* For this case, we want to attach a spherical description to the surface insidethe domain and use the transfinite interpolation to smoothly switch to thestraight lines of the outer cube and the cube at the center of the ball.
* Within the program, we will follow a typical flow in finite element programs,starting from the setup of DoFHandler and sparsity patterns, the assembly of alinear system for solving the Poisson equation with a jumping coefficient, itssolution with a simple iterative method, computation of some numerical errorwith  [2.x.56]  as well as an error estimator. Werecord timings for each section and run the code twice. In the first run, wehand a MappingQGeneric object to each stage of the program separately, wherepoints get re-computed over and over again. In the second run, we useMappingQCache instead.
* 

*  [1.x.49] [1.x.50]
*   [1.x.51]  [1.x.52]
* 

* 
*  The include files for this tutorial are essentially the same as in  [2.x.57] . Importantly, the TransfiniteInterpolationManifold class we will be using is provided by `deal.II/grid/manifold_lib.h`.
* 

* 
*  

* 
* [1.x.53]
* 
*  The only new include file is the one for the MappingQCache class.
* 

* 
* [1.x.54]
* 
*   [1.x.55]  [1.x.56]
* 

* 
*  In this tutorial program, we want to solve the Poisson equation with a coefficient that jumps along a sphere of radius 0.5, and using a constant right hand side of value  [2.x.58] . (This setup is similar to  [2.x.59]  and  [2.x.60] , but the concrete values for the coefficient and the right hand side are different.) Due to the jump in the coefficient, the analytical solution must have a kink where the coefficient switches from one value to the other. To keep things simple, we select an analytical solution that is quadratic in all components, i.e.,  [2.x.61]  in the ball of radius 0.5 and  [2.x.62]  in the outer part of the domain. This analytical solution is compatible with the right hand side in case the coefficient is 0.5 in the inner ball and 5 outside. It is also continuous along the circle of radius 0.5.
* 

* 
* [1.x.57]
* 
*   [1.x.58]  [1.x.59]   
*   The implementation of the Poisson problem is very similar to what we used in the  [2.x.63]  tutorial program. The two main differences are that we pass a mapping object to the various steps in the program in order to switch between two mapping representations as explained in the introduction, and the `timer` object (of type TimerOutput) that will be used for measuring the run times in the various cases. (The concept of mapping objects was first introduced in  [2.x.64]  and  [2.x.65] , in case you want to look up the use of these classes.)
* 

* 
* [1.x.60]
* 
*  In the constructor, we set up the timer object to record wall times but be quiet during the normal execution. We will query it for timing details in the  [2.x.66]  function. Furthermore, we select a relatively high polynomial degree of three for the finite element in use.
* 

* 
* [1.x.61]
* 
*   [1.x.62]  [1.x.63]   
*   The next function presents the typical usage of TransfiniteInterpolationManifold. The first step is to create the desired grid, which can be done by composition of two grids from GridGenerator. The inner ball mesh is simple enough: We run  [2.x.67]  centered at the origin with radius 0.5 (third function argument). The second mesh is more interesting and constructed as follows: We want to have a mesh that is spherical in the interior but flat on the outer surface. Furthermore, the mesh topology of the inner ball should be compatible with the outer grid in the sense that their vertices coincide so as to allow the two grid to be merged. The grid coming out of  [2.x.68]  fulfills the requirements on the inner side in case it is created with  [2.x.69]  coarse cells (6 coarse cells in 3D which we are going to use) &ndash; this is the same number of cells as there are boundary faces for the ball. For the outer surface, we use the fact that the 6 faces on the surface of the shell without a manifold attached would degenerate to the surface of a cube. What we are still missing is the radius of the outer shell boundary. Since we desire a cube of extent  [2.x.70]  and the 6-cell shell puts its 8 outer vertices at the 8 opposing diagonals, we must translate the points  [2.x.71]  into a radius: Clearly, the radius must be  [2.x.72]  in  [2.x.73]  dimensions, i.e.,  [2.x.74]  for the three-dimensional case we want to consider.   
*   Thus, we have a plan: After creating the inner triangulation for the ball and the one for the outer shell, we merge those two grids but remove all manifolds that the functions in GridGenerator may have set from the resulting triangulation, to ensure that we have full control over manifolds. In particular, we want additional points added on the boundary during refinement to follow a flat manifold description. To start the process of adding more appropriate manifold ids, we assign the manifold id 0 to all mesh entities (cells, faces, lines), which will later be associated with the TransfiniteInterpolationManifold. Then, we must identify the faces and lines that are along the sphere of radius 0.5 and mark them with a different manifold id, so as to then assign a SphericalManifold to those. We will choose the manifold id of 1. Since we have thrown away all manifolds that pre-existed after calling  [2.x.75]  we manually go through the cells of the mesh and all their faces. We have found a face on the sphere if all four vertices have a radius of 0.5, or, as we write in the program, have  [2.x.76] . Note that we call `cell->face(f)->set_all_manifold_ids(1)` to set the manifold id both on the faces and the surrounding lines. Furthermore, we want to distinguish the cells inside the ball and outside the ball by a material id for visualization, corresponding to the picture in the introduction.
* 

* 
* [1.x.64]
* 
*  With all cells, faces and lines marked appropriately, we can attach the Manifold objects to those numbers. The entities with manifold id 1 will get a spherical manifold, whereas the other entities, which have the manifold id 0, will be assigned the TransfiniteInterpolationManifold. As mentioned in the introduction, we must explicitly initialize the manifold with the current mesh using a call to  [2.x.77]  in order to pick up the coarse mesh cells and the manifolds attached to the boundaries of those cells. We also note that the manifold objects we create locally in this function are allowed to go out of scope (as they do at the end of the function scope), because the Triangulation object internally copies them.     
*   With all manifolds attached, we will finally go about and refine the mesh a few times to create a sufficiently large test case.
* 

* 
* [1.x.65]
* 
*   [1.x.66]  [1.x.67]   
*   The following function is well-known from other tutorials in that it enumerates the degrees of freedom, creates a constraint object and sets up a sparse matrix for the linear system. The only thing worth mentioning is the fact that the function receives a reference to a mapping object that we then pass to the  [2.x.78]  function to ensure that our boundary values are evaluated on the high-order mesh used for assembly. In the present example, it does not really matter because the outer surfaces are flat, but for curved outer cells this leads to more accurate approximation of the boundary values.
* 

* 
* [1.x.68]
* 
*   [1.x.69]  [1.x.70]   
*   The function that assembles the linear system is also well known from the previous tutorial programs. One thing to note is that we set the number of quadrature points to the polynomial degree plus two, not the degree plus one as in most other tutorials. This is because we expect some extra accuracy as the mapping also involves a degree one more than the polynomials for the solution.   
*   The only somewhat unusual code in the assembly is the way we compute the cell matrix. Rather than using three nested loop over the quadrature point index, the row, and the column of the matrix, we first collect the derivatives of the shape function, multiplied by the square root of the product of the coefficient and the integration factor `JxW` in a separate matrix `partial_matrix`. To compute the cell matrix, we then execute `cell_matrix = partial_matrix transpose(partial_matrix)` in the line `partial_matrix.mTmult(cell_matrix, partial_matrix);`. To understand why this works, we realize that the matrix-matrix multiplication performs a summation over the columns of `partial_matrix`. If we denote the coefficient by  [2.x.79] , the entries in the temporary matrix are  [2.x.80] . If we take the product of the [1.x.71]th row with the [1.x.72]th column of that matrix, we compute a nested sum involving  [2.x.81] , which is exactly the terms needed for the bilinear form of the Laplace equation.   
*   The reason for choosing this somewhat unusual scheme is due to the heavy work involved in computing the cell matrix for a relatively high polynomial degree in 3D. As we want to highlight the cost of the mapping in this tutorial program, we better do the assembly in an optimized way in order to not chase bottlenecks that have been solved by the community already. Matrix-matrix multiplication is one of the best optimized kernels in the HPC context, and the  [2.x.82]  function will call into those optimized BLAS functions. If the user has provided a good BLAS library when configuring deal.II (like OpenBLAS or Intel's MKL), the computation of the cell matrix will execute close to the processor's peak arithmetic performance. As a side note, we mention that despite an optimized matrix-matrix multiplication, the current strategy is sub-optimal in terms of complexity as the work to be done is proportional to  [2.x.83]  operations for degree  [2.x.84]  (this also applies to the usual evaluation with FEValues). One could compute the cell matrix with  [2.x.85]  operations by utilizing the tensor product structure of the shape functions, as is done by the matrix-free framework in deal.II. We refer to  [2.x.86]  and the documentation of the tensor-product-aware evaluators FEEvaluation for details on how an even more efficient cell matrix computation could be realized.
* 

* 
* [1.x.73]
* 
*   [1.x.74]  [1.x.75]   
*   For solving the linear system, we pick a simple Jacobi-preconditioned conjugate gradient solver, similar to the settings in the early tutorials.
* 

* 
* [1.x.76]
* 
*   [1.x.77]  [1.x.78]   
*   In the next function we do various post-processing steps with the solution, all of which involve the mapping in one way or the other.   
*   The first operation we do is to write the solution as well as the material ids to a VTU file. This is similar to what was done in many other tutorial programs. The new ingredient presented in this tutorial program is that we want to ensure that the data written to the file used for visualization is actually a faithful representation of what is used internally by deal.II. That is because most of the visualization data formats only represent cells by their vertex coordinates, but have no way of representing the curved boundaries that are used in deal.II when using higher order mappings
* 
*  -  in other words, what you see in the visualization tool is not actually what you are computing on. (The same, incidentally, is true when using higher order shape functions: Most visualization tools only render bilinear/trilinear representations. This is discussed in detail in  [2.x.87]    
*   So we need to ensure that a high-order representation is written to the file. We need to consider two particular topics. Firstly, we tell the DataOut object via the  [2.x.88]  that we intend to interpret the subdivisions of the elements as a high-order Lagrange polynomial rather than a collection of bilinear patches. Recent visualization programs, like ParaView version 5.5 or newer, can then render a high-order solution (see a [1.x.79] for more details). Secondly, we need to make sure that the mapping is passed to the  [2.x.89]  method. Finally, the DataOut class only prints curved faces for [1.x.80] cells by default, so we need to ensure that also inner cells are printed in a curved representation via the mapping.
* 

* 
* [1.x.81]
* 
*  The next operation in the postprocessing function is to compute the  [2.x.90]  and  [2.x.91]  errors against the analytical solution. As the analytical solution is a quadratic polynomial, we expect a very accurate result at this point. If we were solving on a simple mesh with planar faces and a coefficient whose jumps are aligned with the faces between cells, then we would expect the numerical result to coincide with the analytical solution up to roundoff accuracy. However, since we are using deformed cells following a sphere, which are only tracked by polynomials of degree 4 (one more than the degree for the finite elements), we will see that there is an error around  [2.x.92] . We could get more accuracy by increasing the polynomial degree or refining the mesh.
* 

* 
* [1.x.82]
* 
*  The final post-processing operation we do here is to compute an error estimate with the KellyErrorEstimator. We use the exact same settings as in the  [2.x.93]  tutorial program, except for the fact that we also hand in the mapping to ensure that errors are evaluated along the curved element, consistent with the remainder of the program. However, we do not really use the result here to drive a mesh adaptation step (that would refine the mesh around the material interface along the sphere), as the focus here is on the cost of this operation.
* 

* 
* [1.x.83]
* 
*   [1.x.84]  [1.x.85]   
*   Finally, we define the `run()` function that controls how we want to execute this program (which is called by the main() function in the usual way). We start by calling the `create_grid()` function that sets up our geometry with the appropriate manifolds. We then run two instances of a solver chain, starting from the setup of the equations, the assembly of the linear system, its solution with a simple iterative solver, and the postprocessing discussed above. The two instances differ in the way they use the mapping. The first uses a conventional MappingQGeneric mapping object which we initialize to a degree one more than we use for the finite element &ndash; after all, we expect the geometry representation to be the bottleneck as the analytic solution is only a quadratic polynomial. (In reality, things are interlinked to quite some extent because the evaluation of the polynomials in real coordinates involves the mapping of a higher-degree polynomials, which represent some smooth rational functions. As a consequence, higher-degree polynomials still pay off, so it does not make sense to increase the degree of the mapping further.) Once the first pass is completed, we let the timer print a summary of the compute times of the individual stages.
* 

* 
* [1.x.86]
* 
*  For the second instance, we instead set up the MappingQCache class. Its use is very simple: After constructing it (with the degree, given that we want it to show the correct degree functionality in other contexts), we fill the cache via the  [2.x.94]  function. At this stage, we specify which mapping we want to use (obviously, the same MappingQGeneric as previously in order to repeat the same computations) for the cache, and then run through the same functions again, now handing in the modified mapping. In the end, we again print the accumulated wall times since the reset to see how the times compare to the original setting.
* 

* 
* [1.x.87]
* 
* [1.x.88][1.x.89]
* 

* [1.x.90][1.x.91]
* 

* If we run the three-dimensional version of this program with polynomials ofdegree three, we get the following program output:
* [1.x.92]
* 
* Before discussing the timings, we look at the memory consumption for theMappingQCache object: Our program prints that it utilizes 23 MB ofmemory. If we relate this number to the memory consumption of a single(solution or right hand side) vector,which is 1.5 MB (namely, 181,609 elements times 8 bytes per entry indouble precision), or to the memory consumed by thesystem matrix and the sparsity pattern (which is 274 MB), we realize that it isnot an overly heavy data structure, given its benefits.
* With respect to the timers, we see a clear improvement in the overall run timeof the program by a factor of 2.7. If we disregard the iterative solver, whichis the same in both cases (and not optimal, given the simple preconditioner weuse, and the fact that sparse matrix-vector products waste operations forcubic polynomials), the advantage is a factor of almost 5. This is prettyimpressive for a linear stationary problem, and cost savings would indeed bemuch more prominent for time-dependent and nonlinear problems where assemblyis called several times. If we look into the individual components, we get aclearer picture of what is going on and why the cache is so efficient: In theMappingQGeneric case, essentially every operation that involves a mapping takeat least 5 seconds to run. The norm computation runs two [2.x.95]  functions, which each take almost 5seconds. (The computation of constraints is cheaper because it only evaluatesthe mapping in cells at the boundary for the interpolation of boundaryconditions.) If we compare these 5 seconds to the time it takes to fill theMappingQCache, which is 5.2 seconds (for all cells, not just the active ones),it becomes obvious that the computation of the mapping support pointsdominates over everything else in the MappingQGeneric case. Perhaps the moststriking result is the time for the error estimator, labeled "Compute errorestimator", where the MappingQGeneric implementation takes 17.3 seconds andthe MappingQCache variant less than 0.5 seconds. The reason why the former isso expensive (three times more expensive than the assembly, for instance) isthat the error estimation involves evaluation of quantities over faces, whereeach face in the mesh requests additional points of the mapping that in turngo through the very expensive TransfiniteInterpolationManifold class. As thereare six faces per cell, this happens much more often than in assembly. Again,MappingQCache nicely eliminates the repeated evaluation, aggregating all theexpensive steps involving the manifold in a single initialization call thatgets repeatedly used.
* 

* [1.x.93][1.x.94] [2.x.96] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-66_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] ,  [2.x.2] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33]
*  [2.x.3] 
* [1.x.34]
* 

* [1.x.35][1.x.36][1.x.37]
* 

* The aim of this tutorial program is to demonstrate how to solve a nonlinearproblem using Newton's method within the matrix-free framework. This tutorialcombines several techniques already introduced in  [2.x.4] ,  [2.x.5] ,  [2.x.6] , [2.x.7]  and others.
* 

* [1.x.38][1.x.39]
* On the unit circle  [2.x.8] we consider the following nonlinear elliptic boundary value problem subject to ahomogeneous Dirichlet boundary condition: Find a function [2.x.9]  such that it holds:
* [1.x.40]
* This problem is also called the [1.x.41] and is a typical examplefor problems from combustion theory, see for example [2.x.10] .
* 

* [1.x.42][1.x.43]
* As usual, we first derive the weak formulation for this problem by multiplyingwith a smooth test function  [2.x.11]  respecting theboundary condition and integrating over the domain  [2.x.12] . Integration byparts and putting the term from the right hand side to the left yields the weakformulation: Find a function  [2.x.13]  such that for alltest functions  [2.x.14]  it holds:
* [1.x.44]
* 
* Choosing the Lagrangian finite element space  [2.x.15] , which directly incorporatesthe homogeneous Dirichlet boundary condition, we can define a basis [2.x.16]  and thus it suffices to test only with thosebasis functions. So the discrete problem reads as follows: Find  [2.x.17] such that for all  [2.x.18]  it holds:
* [1.x.45]
* As each finite element function is a linear combination of the basis functions [2.x.19] , we can identify the finite element solution bya vector from  [2.x.20]  consisting of the unknown values in each degree offreedom (DOF). Thus, we define the nonlinear function [2.x.21]  representing the discrete nonlinearproblem.
* To solve this nonlinear problem we use Newton's method. So given aninitial guess  [2.x.22] , which already fulfills the Dirichlet boundarycondition, we determine a sequence of Newton steps  [2.x.23]  bysuccessively applying the following scheme:
* [1.x.46]
* So in each Newton step we have to solve a linear problem  [2.x.24] , where thesystem matrix  [2.x.25]  is represented by the Jacobian [2.x.26]  and the right hand side [2.x.27]  by the negative residual  [2.x.28] . The solution vector  [2.x.29]  is in thatcase the Newton update of the  [2.x.30] -th Newton step. Note, that we assume aninitial guess  [2.x.31] , which already fulfills the Dirichlet boundary conditionsof the problem formulation (in fact this could also be an inhomogeneousDirichlet boundary condition) and thus the Newton updates  [2.x.32]  satisfy ahomogeneous Dirichlet condition.
* Until now we only tested with the basis functions, however, we can alsorepresent any function of  [2.x.33]  as linear combination of basis functions. Moremathematically this means, that every element of  [2.x.34]  can beidentified with a vector  [2.x.35]  via the representation formula: [2.x.36] . So using this we can give an expression forthe discrete Jacobian and the residual:
* [1.x.47]
* Compared to  [2.x.37]  we could also have formed the Frech{\'e}t derivative of thenonlinear function corresponding to the strong formulation of the problem anddiscretized it afterwards. However, in the end we would get the same set ofdiscrete equations.
* 

* [1.x.48][1.x.49]
* Note, how the system matrix, actually the Jacobian, depends on the previousNewton step  [2.x.38] . Hence we need to tell the function that computesthe system matrix about the solution at the last Newton step. In animplementation with a classical  [2.x.39]  function wewould gather this information from the last Newton step during assembly by theuse of the member functions  [2.x.40]  and [2.x.41]  The  [2.x.42] function would then looks like:
* [1.x.50]
* 
* Since we want to solve this problem without storing a matrix, we need to tellthe matrix-free operator this information before we use it. Therefore in thederived class  [2.x.43]  we will implement a functioncalled  [2.x.44] , which will process the information ofthe last Newton step prior to the usage of the matrix-vector implementation.Furthermore we want to use a geometric multigrid (GMG) preconditioner for thelinear solver, so in order to apply the multilevel operators we need to pass thelast Newton step also to these operators. This is kind of a tricky task, sincethe vector containing the last Newton step has to be interpolated to all levelsof the triangulation. In the code this task will be done by the function [2.x.45]  Note, a fundamental difference tothe previous cases, where we set up and used a geometric multigridpreconditioner, is the fact, that we can reuse the MGTransferMatrixFree objectfor the computation of all Newton steps. So we can save some work here bydefining a class variable and using an already set up MGTransferMatrixFreeobject  [2.x.46]  that was initialized in the [2.x.47]  function.
* [1.x.51]
* 
* The function evaluating the nonlinearity works basically in the same way as thefunction  [2.x.48]  from  [2.x.49]  evaluating a coefficientfunction. The idea is to use an FEEvaluation object to evaluate the Newton stepand store the expression in a table for all cells and all quadrature points:
* [1.x.52]
* 
* 

* [1.x.53][1.x.54]
* As said in  [2.x.50]  the matrix-free method gets more efficient if we choose ahigher order finite element space. Since we want to solve the problem on the [2.x.51] -dimensional unit ball, it would be good to have an appropriate boundaryapproximation to overcome convergence issues. For this reason we use anisoparametric approach with the MappingQGeneric class to recover the smoothboundary as well as the mapping for inner cells. In addition, to get a goodtriangulation in total we make use of the TransfiniteInterpolationManifold.
* 

*  [1.x.55] [1.x.56]
*  First we include the typical headers of the deal.II library needed for this tutorial:
* 

* 
* [1.x.57]
* 
*  In particular, we need to include the headers for the matrix-free framework:
* 

* 
* [1.x.58]
* 
*  And since we want to use a geometric multigrid preconditioner, we need also the multilevel headers:
* 

* 
* [1.x.59]
* 
*  Finally some common C++ headers for in and output:
* 

* 
* [1.x.60]
* 
*   [1.x.61]  [1.x.62]
* 

* 
*  In the beginning we define the matrix-free operator for the Jacobian. As a guideline we follow the tutorials  [2.x.52]  and  [2.x.53] , where the precise interface of the  [2.x.54]  class was extensively documented.   
*   Since we want to use the Jacobian as system matrix and pass it to the linear solver as well as to the multilevel preconditioner classes, we derive the  [2.x.55]  class from the  [2.x.56]  class, such that we have already the right interface. The two functions we need to override from the base class are the  [2.x.57]  and the  [2.x.58]  function. To allow preconditioning with float precision we define the number type as template argument.   
*   As mentioned already in the introduction, we need to evaluate the Jacobian  [2.x.59]  at the last Newton step  [2.x.60]  for the computation of the Newton update  [2.x.61] . To get the information of the last Newton step  [2.x.62]  we do pretty much the same as in  [2.x.63] , where we stored the values of a coefficient function in a table  [2.x.64]  once before we use the matrix-free operator. Instead of a function  [2.x.65] , we here implement a function  [2.x.66] .   
*   As additional private member functions of the  [2.x.67]  we implement the  [2.x.68]  and the  [2.x.69]  function. The first one is the actual worker function for the matrix-vector application, which we pass to the  [2.x.70]  in the  [2.x.71]  function. The later one is the worker function to compute the diagonal, which we pass to the  [2.x.72]  function.   
*   For better readability of the source code we further define an alias for the FEEvaluation object.
* 

* 
* [1.x.63]
* 
*  The constructor of the  [2.x.73]  just calls the constructor of the base class  [2.x.74]  which is itself derived from the Subscriptor class.
* 

* 
* [1.x.64]
* 
*  The  [2.x.75]  function resets the table holding the values for the nonlinearity and call the  [2.x.76]  function of the base class.
* 

* 
* [1.x.65]
* 
*   [1.x.66]  [1.x.67]
* 

* 
*  The following  [2.x.77]  function is based on the  [2.x.78]  function from  [2.x.79] . However, it does not evaluate a function object, but evaluates a vector representing a finite element function, namely the last Newton step needed for the Jacobian. Therefore we set up a FEEvaluation object and evaluate the finite element function in the quadrature points with the  [2.x.80]  and  [2.x.81]  functions. We store the evaluated values of the finite element function directly in the  [2.x.82]  table.   
*   This will work well and in the  [2.x.83]  function we can use the values stored in the table to apply the matrix-vector product. However, we can also optimize the implementation of the Jacobian at this stage. We can directly evaluate the nonlinear function  [2.x.84]  and store these values in the table. This skips all evaluations of the nonlinearity in each call of the  [2.x.85]  function.
* 

* 
* [1.x.68]
* 
*   [1.x.69]  [1.x.70]
* 

* 
*  Now in the  [2.x.86]  function, which actually implements the cell wise action of the system matrix, we can use the information of the last Newton step stored in the table  [2.x.87] . The rest of this function is basically the same as in  [2.x.88] . We set up the FEEvaluation object, gather and evaluate the values and gradients of the input vector  [2.x.89] , submit the values and gradients according to the form of the Jacobian and finally call  [2.x.90]  to perform the cell integration and distribute the local contributions into the global vector  [2.x.91] .
* 

* 
* [1.x.71]
* 
*  Next we use  [2.x.92]  to perform the actual loop over all cells computing the cell contribution to the matrix-vector product.
* 

* 
* [1.x.72]
* 
*   [1.x.73]  [1.x.74]
* 

* 
*  The internal worker function  [2.x.93]  for the computation of the diagonal is similar to the above worker function  [2.x.94] . However, as major difference we do not read values from a input vector or distribute any local results to an output vector. Instead the only input argument is the used FEEvaluation object.
* 

* 
* [1.x.75]
* 
*  Finally we override the  [2.x.95]  function of the base class of the  [2.x.96] . Although the name of the function suggests just the computation of the diagonal, this function does a bit more. Because we only really need the inverse of the matrix diagonal elements for the Chebyshev smoother of the multigrid preconditioner, we compute the diagonal and store the inverse elements. Therefore we first initialize the  [2.x.97] . Then we compute the diagonal by passing the worker function  [2.x.98]  to the  [2.x.99]  function. In the end we loop over the diagonal and invert the elements by hand. Note, that during this loop we catch the constrained DOFs and set them manually to one.
* 

* 
* [1.x.76]
* 
*   [1.x.77]  [1.x.78]
* 

* 
*  After implementing the matrix-free operators we can now define the solver class for the [1.x.79]. This class is based on the common structure of all previous tutorial programs, in particular it is based on  [2.x.100] , solving also a nonlinear problem. Since we are using the matrix-free framework, we no longer need an assemble_system function any more, instead the information of the matrix is rebuilt in every call of the  [2.x.101]  function. However, for the application of the Newton scheme we need to assemble the right hand side of the linearized problems and compute the residuals. Therefore, we implement an additional function  [2.x.102] , which we later call in the  [2.x.103]  function. Finally, the typical  [2.x.104]  function here implements the Newton method, whereas the solution of the linearized system is computed in the function  [2.x.105] . As the MatrixFree framework handles the polynomial degree of the Lagrangian finite element method as a template parameter, we declare it also as a template parameter for the problem solver class.
* 

* 
* [1.x.80]
* 
*  For the parallel computation we define a  [2.x.106]  As the computational domain is a circle in 2D and a ball in 3D, we assign in addition to the SphericalManifold for boundary cells a TransfiniteInterpolationManifold object for the mapping of the inner cells, which takes care of the inner cells. In this example we use an isoparametric finite element approach and thus use the MappingQGeneric class. Note, that we could also create an instance of the MappingQ class and set the  [2.x.107]  flags in the contructor call to  [2.x.108] . For further details on the connection of MappingQ and MappingQGeneric you may read the detailed description of these classes.
* 

* 
* [1.x.81]
* 
*  As usual we then define the Lagrangian finite elements FE_Q and a DoFHandler.
* 

* 
* [1.x.82]
* 
*  For the linearized discrete system we define an AffineConstraints objects and the  [2.x.109] , which is in this example represented as a matrix-free operator.
* 

* 
* [1.x.83]
* 
*  The multilevel object is also based on the matrix-free operator for the Jacobian. Since we need to evaluate the Jacobian with the last Newton step, we also need to evaluate the level operator with the last Newton step for the preconditioner. Thus in addition to  [2.x.110] , we also need a MGLevelObject to store the interpolated solution vector on each level. As in  [2.x.111]  we use float precision for the preconditioner. Moreover, we define the MGTransferMatrixFree object as a class variable, since we need to set it up only once when the triangulation has changed and can then use it again in each Newton step.
* 

* 
* [1.x.84]
* 
*  Of course we also need vectors holding the  [2.x.112] , the  [2.x.113] . In that way we can always store the last Newton step in the solution vector and just add the update to get the next Newton step.
* 

* 
* [1.x.85]
* 
*  Finally we have a variable for the number of iterations of the linear solver.
* 

* 
* [1.x.86]
* 
*  For the output in programs running in parallel with MPI, we use the ConditionalOStream class to avoid multiple output of the same data by different MPI ranks.
* 

* 
* [1.x.87]
* 
*  Finally for the time measurement we use a TimerOutput object, which prints the elapsed CPU and wall times for each function in a nicely formatted table after the program has finished.
* 

* 
* [1.x.88]
* 
*  The constructor of the  [2.x.114]  initializes the class variables. In particular, we set up the multilevel support for the  [2.x.115]  set the mapping degree equal to the finite element degree, initialize the ConditionalOStream and tell the TimerOutput that we want to see the wall times only on demand.
* 

* 
* [1.x.89]
* 
*   [1.x.90]  [1.x.91]
* 

* 
*  As the computational domain we use the  [2.x.116] -dimensional unit ball. We follow the instructions for the TransfiniteInterpolationManifold class and also assign a SphericalManifold for the boundary. Finally, we refine the initial mesh 3
* 
*  -  [2.x.117]  times globally.
* 

* 
* [1.x.92]
* 
*   [1.x.93]  [1.x.94]
* 

* 
*  The  [2.x.118]  function is quasi identical to the one in  [2.x.119] . The only differences are obviously the time measurement with only one  [2.x.120]  instead of measuring each part individually, and more importantly the initialization of the MGLevelObject for the interpolated solution vector of the previous Newton step. Another important change is the setup of the MGTransferMatrixFree object, which we can reuse in each Newton step as the  [2.x.121]  will not be not changed.   
*   Note how we can use the same MatrixFree object twice, for the  [2.x.122]  and the multigrid preconditioner.
* 

* 
* [1.x.95]
* 
*   [1.x.96]  [1.x.97]
* 

* 
*  Next we implement a function which evaluates the nonlinear discrete residual for a given input vector ( [2.x.123] ). This function is then used for the assembly of the right hand side of the linearized system and later for the computation of the residual of the next Newton step to check if we already reached the error tolerance. As this function should not affect any class variable we define it as a constant function. Internally we exploit the fast finite element evaluation through the FEEvaluation class and the  [2.x.124]  similar to  [2.x.125] .   
*   First we create a pointer to the MatrixFree object, which is stored in the  [2.x.126] . Then we pass the worker function  [2.x.127]  for the cell wise evaluation of the residual together with the input and output vector to the  [2.x.128]  In addition, we enable the zero out of the output vector in the loop, which is more efficient than calling <code>dst = 0.0</code> separately before.   
*   Note that with this approach we do not have to take care about the MPI related data exchange, since all the bookkeeping is done by the  [2.x.129] 
* 

* 
* [1.x.98]
* 
*   [1.x.99]  [1.x.100]
* 

* 
*  This is the internal worker function for the evaluation of the residual. Essentially it has the same structure as the  [2.x.130]  function of the  [2.x.131]  and evaluates the residual for the input vector  [2.x.132]  on the given set of cells  [2.x.133] . The difference to the above mentioned  [2.x.134]  function is, that we split the  [2.x.135]  function into  [2.x.136]  and  [2.x.137]  since the input vector might have constrained DOFs.
* 

* 
* [1.x.101]
* 
*   [1.x.102]  [1.x.103]
* 

* 
*  Using the above function  [2.x.138]  to evaluate the nonlinear residual, the assembly of the right hand side of the linearized system becomes now a very easy task. We just call the  [2.x.139]  function and multiply the result with minus one.   
*   Experiences show that using the FEEvaluation class is much faster than a classical implementation with FEValues and co.
* 

* 
* [1.x.104]
* 
*   [1.x.105]  [1.x.106]
* 

* 
*  According to  [2.x.140]  the following function computes the norm of the nonlinear residual for the solution  [2.x.141]  with the help of the  [2.x.142]  function. The Newton step length  [2.x.143]  becomes important if we would use an adaptive version of the Newton method. Then for example we would compute the residual for different step lengths and compare the residuals. However, for our problem the full Newton step with  [2.x.144]  is the best we can do. An adaptive version of Newton's method becomes interesting if we have no good initial value. Note that in theory Newton's method converges with quadratic order, but only if we have an appropriate initial value. For unsuitable initial values the Newton method diverges even with quadratic order. A common way is then to use a damped version  [2.x.145]  until the Newton step is good enough and the full Newton step can be performed. This was also discussed in  [2.x.146] .
* 

* 
* [1.x.107]
* 
*   [1.x.108]  [1.x.109]
* 

* 
*  In order to compute the Newton updates in each Newton step we solve the linear system with the CG algorithm together with a geometric multigrid preconditioner. For this we first set up the PreconditionMG object with a Chebyshev smoother like we did in  [2.x.147] .
* 

* 
* [1.x.110]
* 
*  We remember that the Jacobian depends on the last Newton step stored in the solution vector. So we update the ghost values of the Newton step and pass it to the  [2.x.148]  to store the information.
* 

* 
* [1.x.111]
* 
*  Next we also have to pass the last Newton step to the multilevel operators. Therefore, we need to interpolate the Newton step to all levels of the triangulation. This is done with the  [2.x.149] 
* 

* 
* [1.x.112]
* 
*  Now we can set up the preconditioner. We define the smoother and pass the interpolated vectors of the Newton step to the multilevel operators.
* 

* 
* [1.x.113]
* 
*  Finally we set up the SolverControl and the SolverCG to solve the linearized problem for the current Newton update. An important fact of the implementation of SolverCG or also SolverGMRES is, that the vector holding the solution of the linear system (here  [2.x.150] ) can be used to pass a starting value. In order to start the iterative solver always with a zero vector we reset the  [2.x.151]  explicitly before calling  [2.x.152]  Afterwards we distribute the Dirichlet boundary conditions stored in  [2.x.153]  and store the number of iteration steps for the later output.
* 

* 
* [1.x.114]
* 
*  Then for bookkeeping we zero out the ghost values.
* 

* 
* [1.x.115]
* 
*   [1.x.116]  [1.x.117]
* 

* 
*  Now we implement the actual Newton solver for the nonlinear problem.
* 

* 
* [1.x.118]
* 
*  We define a maximal number of Newton steps and tolerances for the convergence criterion. Usually, with good starting values, the Newton method converges in three to six steps, so maximal ten steps should be totally sufficient. As tolerances we use  [2.x.154]  for the norm of the residual and  [2.x.155]  for the norm of the Newton update. This seems a bit over the top, but we will see that, for our example, we will achieve these tolerances after a few steps.
* 

* 
* [1.x.119]
* 
*  Now we start the actual Newton iteration.
* 

* 
* [1.x.120]
* 
*  We assemble the right hand side of the linearized problem and compute the Newton update.
* 

* 
* [1.x.121]
* 
*  Then we compute the errors, namely the norm of the Newton update and the residual. Note that at this point one could incorporate a step size control for the Newton method by varying the input parameter  [2.x.156]  for the compute_residual function. However, here we just use  [2.x.157]  equal to one for a plain Newton iteration.
* 

* 
* [1.x.122]
* 
*  Next we advance the Newton step by adding the Newton update to the current Newton step.
* 

* 
* [1.x.123]
* 
*  A short output will inform us on the current Newton step.
* 

* 
* [1.x.124]
* 
*  After each Newton step we check the convergence criteria. If at least one of those is fulfilled we are done and end the loop. If we haven't found a satisfying solution after the maximal amount of Newton iterations, we inform the user about this shortcoming.
* 

* 
* [1.x.125]
* 
*   [1.x.126]  [1.x.127]
* 

* 
*  The computation of the H1-seminorm of the solution can be done in the same way as in  [2.x.158] . We update the ghost values and use the function  [2.x.159]  In the end we gather all computations from all MPI ranks and return the norm.
* 

* 
* [1.x.128]
* 
*   [1.x.129]  [1.x.130]
* 

* 
*  We generate the graphical output files in vtu format together with a pvtu master file at once by calling the  [2.x.160]  function in the same way as in  [2.x.161] . In addition, as in  [2.x.162] , we query the  [2.x.163]  of each cell and write the distribution of the triangulation among the MPI ranks into the output file. Finally, we generate the patches of the solution by calling  [2.x.164]  However, since we have a computational domain with a curved boundary, we additionally pass the  [2.x.165]  and the finite element degree as number of subdivision. But this is still not enough for the correct representation of the solution, for example in ParaView, because we attached a TransfiniteInterpolationManifold to the inner cells, which results in curved cells in the interior. Therefore we pass as third argument the  [2.x.166]  option, such that also the inner cells use the corresponding manifold description to build the patches.   
*   Note that we could handle the higher order elements with the flag  [2.x.167]  However, due to the limited compatibility to previous version of ParaView and the missing support by VisIt, we left this option for a future version.
* 

* 
* [1.x.131]
* 
*   [1.x.132]  [1.x.133]
* 

* 
*  The last missing function of the solver class for the [1.x.134] is the run function. In the beginning we print information about the system specifications and the finite element space we use. The problem is solved several times on a successively refined mesh.
* 

* 
* [1.x.135]
* 
*  The first task in actually solving the problem is to generate or refine the triangulation.
* 

* 
* [1.x.136]
* 
*  Now we set up the system and solve the problem. These steps are accompanied by time measurement and textual output.
* 

* 
* [1.x.137]
* 
*  After the problem was solved we compute the norm of the solution and generate the graphical output files.
* 

* 
* [1.x.138]
* 
*  Finally after each cycle we print the timing information.
* 

* 
* [1.x.139]
* 
*   [1.x.140]  [1.x.141]
* 

* 
*  As typical for programs running in parallel with MPI we set up the MPI framework and disable shared-memory parallelization by limiting the number of threads to one. Finally to run the solver for the [1.x.142] we create an object of the  [2.x.168]  class and call the run function. Exemplarily we solve the problem once in 2D and once in 3D each with fourth-order Lagrangian finite elements.
* 

* 
* [1.x.143]
* [1.x.144][1.x.145]
* 

* The aim of this tutorial step was to demonstrate the solution of a nonlinearPDE with the matrix-free framework.
* 

* 
* [1.x.146][1.x.147]
* Running the program on two processes in release mode via
* [1.x.148]
* gives the following output on the console
* [1.x.149]
* 
* We show the solution for the two- and three-dimensional problem in thefollowing figure.
*  [2.x.169] 
* 

* 
* [1.x.150][1.x.151]
* In the program output above we find some interesting information about theNewton iterations. The terminal output in each refinement cycle presentsdetailed diagnostics of the Newton method, which show first of all the numberof Newton steps and for each step the norm of the residual  [2.x.170] ,the norm of the Newton update  [2.x.171] , and the number of CG iterations [2.x.172] .
* We observe that for all cases the Newton method converges in approximatelythree to four steps, which shows the quadratic convergence of the Newton methodwith a full step length  [2.x.173] . However, be aware that for a badly choseninitial guess  [2.x.174] , the Newton method will also diverge quadratically.Usually if you do not have an appropriate initial guess, you try a few dampedNewton steps with a reduced step length  [2.x.175]  until the Newton step isagain in the quadratic convergence domain. This damping and relaxation of theNewton step length truly requires a more sophisticated implementation of theNewton method, which we designate to you as a possible extension of thetutorial.
* Furthermore, we see that the number of CG iterations is approximately constantwith successive mesh refinements and an increasing number of DoFs. This is ofcourse due to the geometric multigrid preconditioner and similar to theobservations made in other tutorials that use this method, e.g.,  [2.x.176]  and [2.x.177] . Just to give an example, in the three-dimensional case after fiverefinements, we have approximately 14.7 million distributed DoFs withfourth-order Lagrangian finite elements, but the number of CG iterations isstill less than ten.
* In addition, there is one more very useful optimization that we applied andthat should be mentioned here. In the  [2.x.178]  function weexplicitly reset the vector holding the Newton update before passing it as theoutput vector to the solver. In that case we use a starting value of zero forthe CG method, which is more suitable than the previous Newton update, theactual content of the  [2.x.179]  before resetting, and thusreduces the number of CG iterations by a few steps.
* 

* 
* [1.x.152][1.x.153]
* A couple of possible extensions are available concerning minor updates fo thepresent code as well as a deeper numerical investigation of the Gelfand problem.
* [1.x.154][1.x.155]
* Beside a step size controlled version of the Newton iteration as mentionedalready in  [2.x.180] , one could also implement a more flexible stopping criterionfor the Newton iteration. For example one could replace the fixed tolerancesfor the residual  [2.x.181] and implement a mixed error control with a given absolute and relativetolerance, such that the Newton iteration exists with success as, e.g.,
* [1.x.156]
* For more advanced applications with many nonlinear systems to solve, forexample at each time step for a time-dependent problem, it turns out that it isnot necessary to set up and assemble the Jacobian anew at every single Newtonstep or even for each time step. Instead, the existing Jacobian from a previousstep can be used for the Newton iteration. The Jacobian is then only rebuiltif, for example, the Newton iteration converges too slowly. Such an idea yieldsa [1.x.157]. Admittedly, when using the matrix-free framework, the assembly ofthe Jacobian is omitted anyway, but with in this way one can try to optimizethe reassembly of the geometric multigrid preconditioner. Remember that eachtime the solution from the old Newton step must be distributed to all levelsand the mutligrid preconditioner must be reinitialized.
* [1.x.158][1.x.159]
* In the results section of  [2.x.182]  and others, the parallel scalability of thematrix-free framework on a large number of processors has already beendemonstrated very impressively. In the nonlinear case we consider here, we notethat one of the bottlenecks could become the transfer and evaluation of thematrix-free Jacobi operator and its multistage operators in the previous Newtonstep, since we need to transfer the old solution at all stages in each step. Afirst parallel scalability analysis in  [2.x.183]  shows quitegood strong scalability when the problem size is large enough. However, a moredetailed analysis needs to be performed for reliable results. Moreover, theproblem has been solved only with MPI so far, without using the possibilitiesof shared memory parallelization with threads. Therefore, for this example, youcould try hybrid parallelization with MPI and threads, such as described in [2.x.184] .
* [1.x.160][1.x.161]
* Analogously to  [2.x.185]  and the mentioned possible extension of  [2.x.186] , you canconvince yourself which method is faster.
* [1.x.162][1.x.163]
* One can consider the corresponding eigenvalue problem, which is called Bratuproblem. For example, if we define a fixed eigenvalue  [2.x.187] , we cancompute the corresponding discrete eigenfunction. You will notice that thenumber of Newton steps will increase with increasing  [2.x.188] . To reduce thenumber of Newton steps you can use the following trick: start from a certain [2.x.189] , compute the eigenfunction, increase  [2.x.190] , and then use the previous solution as an initial guess for theNewton iteration. In the end you can plot the  [2.x.191] -norm over theeigenvalue  [2.x.192] . What do you observe forfurther increasing  [2.x.193] ?
* 

* [1.x.164][1.x.165] [2.x.194] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-67_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] ,  [2.x.2] ,  [2.x.3] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28]
* 
*  [2.x.4] 
* [1.x.29]
* [1.x.30][1.x.31][1.x.32]
* 

* This tutorial program solves the Euler equations of fluid dynamics using anexplicit time integrator with the matrix-free framework applied to ahigh-order discontinuous Galerkin discretization in space. For details aboutthe Euler system and an alternative implicit approach, we also refer to the [2.x.5]  tutorial program. You might also want to look at  [2.x.6]  foran alternative approach to solving these equations.
* 

* [1.x.33][1.x.34]
* 

* The Euler equations are a conservation law, describing the motion of acompressible inviscid gas,[1.x.35]where the  [2.x.7]  components of the solution vector are  [2.x.8] . Here,  [2.x.9]  denotes the fluid density, [2.x.10]  the fluid velocity, and  [2.x.11]  theenergy density of the gas. The velocity is not directly solved for, but ratherthe variable  [2.x.12] , the linear momentum (since this is theconserved quantity).
* The Euler flux function, a  [2.x.13]  matrix, is defined as[1.x.36]with  [2.x.14]  the  [2.x.15]  identity matrix and  [2.x.16]  the outerproduct; its components denote the mass, momentum, and energy fluxes, respectively.The right hand side forcing is given by[1.x.37]where the vector  [2.x.17]  denotes the direction and magnitude ofgravity. It could, however, also denote any other external force per unit massthat is acting on the fluid. (Think, for example, of the electrostaticforces exerted by an external electric field on charged particles.)
* The three blocks of equations, the second involving  [2.x.18]  components, describethe conservation of mass, momentum, and energy. The pressure is not asolution variable but needs to be expressed through a "closure relationship"by the other variables; we here choose the relationship appropriatefor a gas with molecules composed of two atoms, which at moderatetemperatures is given by  [2.x.19]  with the constant  [2.x.20] .
* 

* [1.x.38][1.x.39]
* 

* For spatial discretization, we use a high-order discontinuous Galerkin (DG)discretization, using a solution expansion of the form[1.x.40]Here,  [2.x.21]  denotes the  [2.x.22] th basis function, writtenin vector form with separate shape functions for the different components andletting  [2.x.23]  go through the density, momentum, and energy variables,respectively. In this form, the space dependence is contained in the shapefunctions and the time dependence in the unknown coefficients  [2.x.24] . Asopposed to the continuous finite element method where some shape functionsspan across element boundaries, the shape functions are local to a singleelement in DG methods, with a discontinuity from one element to the next. Theconnection of the solution from one cell to its neighbors is insteadimposed by the numerical fluxesspecified below. This allows for some additional flexibility, for example tointroduce directionality in the numerical method by, e.g., upwinding.
* DG methods are popular methods for solving problems of transport characterbecause they combine low dispersion errors with controllable dissipation onbarely resolved scales. This makes them particularly attractive for simulationin the field of fluid dynamics where a wide range of active scales needs to berepresented and inadequately resolved features are prone to disturb theimportant well-resolved features. Furthermore, high-order DG methods arewell-suited for modern hardware with the right implementation. At the sametime, DG methods are no silver bullet. In particular when the solutiondevelops discontinuities (shocks), as is typical for the Euler equations insome flow regimes, high-order DG methods tend to oscillatory solutions, likeall high-order methods when not using flux- or slope-limiters. This is a consequence of [1.x.41]that states that any total variation limited (TVD) scheme that is linear (likea basic DG discretization) can at most be first-order accurate. Putdifferently, since DG methods aim for higher order accuracy, they cannot beTVD on solutions that develop shocks. Even though some communities claim thatthe numerical flux in DG methods can control dissipation, this is of limitedvalue unless [1.x.42] shocks in a problem align with cell boundaries. Anyshock that passes through the interior of cells will again produce oscillatorycomponents due to the high-order polynomials. In the finite element and DGcommunities, there exist a number of different approaches to deal with shocks,for example the introduction of artificial diffusion on troubled cells (usinga troubled-cell indicator based e.g. on a modal decomposition of thesolution), a switch to dissipative low-order finite volume methods on asubgrid, or the addition of some limiting procedures. Given the amplepossibilities in this context, combined with the considerable implementationeffort, we here refrain from the regime of the Euler equations with pronouncedshocks, and rather concentrate on the regime of subsonic flows with wave-likephenomena. For a method that works well with shocks (but is more expensive perunknown), we refer to the  [2.x.25]  tutorial program.
* For the derivation of the DG formulation, we multiply the Euler equations withtest functions  [2.x.26]  and integrate over an individual cell  [2.x.27] , whichgives[1.x.43]
* We then integrate the second term by parts, moving the divergencefrom the solution slot to the test function slot, and producing an integralover the element boundary:[1.x.44]In the surface integral, we have replaced the term  [2.x.28]  bythe term  [2.x.29] , the numerical flux. The role ofthe numerical flux is to connect the solution on neighboring elements andweakly impose continuity of the solution. This ensures that the globalcoupling of the PDE is reflected in the discretization, despite independentbasis functions on the cells. The connectivity to the neighbor is included bydefining the numerical flux as a function  [2.x.30]  of the solution from both sides of an interior face,  [2.x.31]  and  [2.x.32] . A basic property we require is that the numerical fluxneeds to be [1.x.45]. That is, we want all information (i.e.,mass, momentum, and energy) that leaves a cell overa face to enter the neighboring cell in its entirety and vice versa. This canbe expressed as  [2.x.33] , meaning that the numericalflux evaluates to the same result from either side. Combined with the factthat the numerical flux is multiplied by the unit outer normal vector on theface under consideration, which points in opposite direction from the twosides, we see that the conservation is fulfilled. An alternative point of viewof the numerical flux is as a single-valued intermediate state that links thesolution weakly from both sides.
* There is a large number of numerical flux functions available, also calledRiemann solvers. For the Euler equations, there exist so-called exact Riemannsolvers
* 
*  -  meaning that the states from both sides are combined in a way thatis consistent with the Euler equations along a discontinuity
* 
*  -  andapproximate Riemann solvers, which violate some physical properties and relyon other mechanisms to render the scheme accurate overall. Approximate Riemannsolvers have the advantage of beging cheaper to compute. Most flux functionshave their origin in the finite volume community, which are similar to DGmethods with polynomial degree 0 within the cells (called volumes). As thevolume integral of the Euler operator  [2.x.34]  would disappear forconstant solution and test functions, the numerical flux must fully representthe physical operator, explaining why there has been a large body of researchin that community. For DG methods, consistency is guaranteed by higher orderpolynomials within the cells, making the numerical flux less of an issue andusually affecting only the convergence rate, e.g., whether the solutionconverges as  [2.x.35] ,  [2.x.36]  or  [2.x.37]  in the  [2.x.38]  norm for polynomials of degree  [2.x.39] . The numericalflux can thus be seen as a mechanism to select more advantageousdissipation/dispersion properties or regarding the extremal eigenvalue of thediscretized and linearized operator, which affect the maximal admissible timestep size in explicit time integrators.
* In this tutorial program, we implement two variants of fluxes that can becontrolled via a switch in the program (of course, it would be easy to makethem a run time parameter controlled via an input file). The first flux isthe local Lax--Friedrichs flux[1.x.46]
* In the original definition of the Lax--Friedrichs flux, a factor  [2.x.40]  is used(corresponding to the maximal speed at which information is moving onthe two sides of the interface), statingthat the difference between the two states,  [2.x.41]  is penalizedby the largest eigenvalue in the Euler flux, which is  [2.x.42] ,where  [2.x.43]  is the speed of sound. In the implementationbelow, we modify the penalty term somewhat, given that the penalty is ofapproximate nature anyway. We use
* [1.x.47]
* The additional factor  [2.x.44]  reduces the penalty strength (which resultsin a reduced negative real part of the eigenvalues, and thus increases theadmissible time step size). Using the squares within the sums allows us toreduce the number of expensive square root operations, which is 4 for theoriginal Lax--Friedrichs definition, to a single one.This simplification leads to at most a factor of2 in the reduction of the parameter  [2.x.45] , since  [2.x.46] , with the last inequality followingfrom Young's inequality.
* The second numerical flux is one proposed by Harten, Lax and van Leer, calledthe HLL flux. It takes the different directions of propagation of the Eulerequations into account, depending on the speed of sound. It utilizes someintermediate states  [2.x.47]  and  [2.x.48]  to define the twobranches  [2.x.49]  and  [2.x.50] . From these branches, one then defines the flux[1.x.48]Regarding the definition of the intermediate state  [2.x.51]  and [2.x.52] , several variants have been proposed. The variant originallyproposed uses a density-averaged definition of the velocity,  [2.x.53] . Since we consider the Euler equations without shocks, wesimply use arithmetic means,  [2.x.54]  and  [2.x.55] , with  [2.x.56] , in this tutorial program, and leave othervariants to a possible extension. We also note that the HLL flux has beenextended in the literature to the so-called HLLC flux, where C stands for theability to represent contact discontinuities.
* At the boundaries with no neighboring state  [2.x.57]  available, it iscommon practice to deduce suitable exterior values from the boundaryconditions (see the general literature on DG methods for details). In thistutorial program, we consider three types of boundary conditions, namely[1.x.49] where all components are prescribed,[1.x.50][1.x.51], where we do not prescribe exteriorsolutions as the flow field is leaving the domain and use the interior valuesinstead; we still need to prescribe the energy as there is one incomingcharacteristic left in the Euler flux,[1.x.52]and [1.x.53] which describe a no-penetrationconfiguration:[1.x.54]
* The polynomial expansion of the solution is finally inserted to the weak formand test functions are replaced by the basis functions. This gives a discretein space, continuous in time nonlinear system with a finite number of unknowncoefficient values  [2.x.58] ,  [2.x.59] . Regarding the choice ofthe polynomial degree in the DG method, there is no consensus in literature asof 2019 as to what polynomial degrees are most efficient and the decision isproblem-dependent. Higher order polynomials ensure better convergence ratesand are thus superior for moderate to high accuracy requirements for[1.x.55] solutions. At the same time, the volume-to-surface ratioof where degrees of freedom are located,increases with higher degrees, and this makes the effect of the numerical fluxweaker, typically reducing dissipation. However, in most of the cases thesolution is not smooth, at least not compared to the resolution that can beafforded. This is true for example in incompressible fluid dynamics,compressible fluid dynamics, and the related topic of wave propagation. In thispre-asymptotic regime, the error is approximately proportional to thenumerical resolution, and other factors such as dispersion errors or thedissipative behavior become more important. Very high order methods are oftenruled out because they come with more restrictive CFL conditions measuredagainst the number of unknowns, and they are also not as flexible when itcomes to representing complex geometries. Therefore, polynomial degreesbetween two and six are most popular in practice, see e.g. the efficiencyevaluation in  [2.x.60]  and references cited therein.
* [1.x.56][1.x.57]
* 

* To discretize in time, we slightly rearrange the weak form and sum over allcells:[1.x.58]where  [2.x.61]  runs through all basis functions with from 1 to [2.x.62] .
* We now denote by  [2.x.63]  the mass matrix with entries  [2.x.64] , and by[1.x.59]the operator evaluating the right-hand side of the Euler operator, given afunction  [2.x.65]  associated with a global vector of unknownsand the finite element in use. This function  [2.x.66]  is explicitly time-dependent as thenumerical flux evaluated at the boundary will involve time-dependent data [2.x.67] ,  [2.x.68] , and  [2.x.69]  on someparts of the boundary, depending on the assignment of boundaryconditions. With this notation, we can write the discrete in space, continuousin time system compactly as[1.x.60]where we have taken the liberty to also denote the global solutionvector by  [2.x.70]  (in addition to the the corresponding finiteelement function). Equivalently, the system above has the form[1.x.61]
* For hyperbolic systems discretized by high-order discontinuous Galerkinmethods, explicit time integration of this system is very popular. This is dueto the fact that the mass matrix  [2.x.71]  is block-diagonal (with eachblock corresponding to only variables of the same kind defined on the samecell) and thus easily inverted. In each time step
* 
*  -  or stage of aRunge--Kutta scheme
* 
*  -  one only needs to evaluate the differential operatoronce using the given data and subsequently apply the inverse of the massmatrix. For implicit time stepping, on the other hand, one would first have tolinearize the equations and then iteratively solve the linear system, whichinvolves several residual evaluations and at least a dozen applications ofthe linearized operator, as has been demonstrated in the  [2.x.72]  tutorialprogram.
* Of course, the simplicity of explicit time stepping comes with a price, namelyconditional stability due to the so-called Courant--Friedrichs--Lewy (CFL)condition. It states that the time step cannot be larger than the fastestpropagation of information by the discretized differential operator. In moremodern terms, the speed of propagation corresponds to the largest eigenvaluein the discretized operator, and in turn depends on the mesh size, thepolynomial degree  [2.x.73]  and the physics of the Euler operator, i.e., theeigenvalues of the linearization of  [2.x.74]  with respect to [2.x.75] . In this program, we set the time step as follows:[1.x.62]
* with the maximum taken over all quadrature points and all cells. Thedimensionless number  [2.x.76]  denotes the Courant number and can bechosen up to a maximally stable number  [2.x.77] , whose valuedepends on the selected time stepping method and its stability properties. Thepower  [2.x.78]  used for the polynomial scaling is heuristic and representsthe closest fit for polynomial degrees between 1 and 8, see e.g. [2.x.79] . In the limit of higher degrees,  [2.x.80] , a scaling of [2.x.81]  is more accurate, related to the inverse estimates typically used forinterior penalty methods. Regarding the [1.x.63] mesh sizes  [2.x.82]  and [2.x.83]  used in the formula, we note that the convective transport isdirectional. Thus an appropriate scaling is to use the element length in thedirection of the velocity  [2.x.84] . The code below derives this scalingfrom the inverse of the Jacobian from the reference to real cell, i.e., weapproximate  [2.x.85] . The acoustic waves, instead, are isotropic in character, whichis why we use the smallest feature size, represented by the smallest singularvalue of  [2.x.86] , for the acoustic scaling  [2.x.87] . Finally, we need to add theconvective and acoustic limits, as the Euler equations can transportinformation with speed  [2.x.88] .
* In this tutorial program, we use a specific variant of [1.x.64], which in general use the following update procedurefrom the state  [2.x.89]  at time  [2.x.90]  to the new time  [2.x.91]  with [2.x.92] :[1.x.65]The vectors  [2.x.93] ,  [2.x.94] , in an  [2.x.95] -stage scheme areevaluations of the operator at some intermediate state and used to define theend-of-step value  [2.x.96]  via some linear combination. The scalarcoefficients in this scheme,  [2.x.97] ,  [2.x.98] , and  [2.x.99] , are defined such thatcertain conditions are satisfied for higher order schemes, the most basic onebeing  [2.x.100] . The parameters are typically collected inthe form of a so-called [1.x.66] that collects all of the coefficients that define thescheme. For a five-stage scheme, it would look like this:[1.x.67]
* In this tutorial program, we use a subset of explicit Runge--Kutta methods,so-called low-storage Runge--Kutta methods (LSRK), which assume additionalstructure in the coefficients. In the variant used by reference [2.x.101] , the assumption is to use Butcher tableaus ofthe form[1.x.68]With such a definition, the update to  [2.x.102]  shares the storage withthe information for the intermediate values  [2.x.103] . Starting with [2.x.104]  and  [2.x.105] , the updatein each of the  [2.x.106]  stages simplifies to[1.x.69]Besides the vector  [2.x.107]  that is successively updated, this schemeonly needs two auxiliary vectors, namely the vector  [2.x.108]  to hold theevaluation of the differential operator, and the vector  [2.x.109]  thatholds the right-hand side for the differential operator application. Insubsequent stages  [2.x.110] , the values  [2.x.111]  and  [2.x.112]  can usethe same storage.
* The main advantages of low-storage variants are the reduced memory consumptionon the one hand (if a very large number of unknowns must be fit in memory,holding all  [2.x.113]  to compute subsequent updates can be a limitalready for  [2.x.114]  in between five and eight
* 
*  -  recall that we are usingan explicit scheme, so we do not need to store any matrices that aretypically much larger than a few vectors), and the reduced memory access onthe other. In this program, we are particularly interested in the latteraspect. Since cost of operator evaluation is only a small multiple of the costof simply streaming the input and output vector from memory with the optimizedmatrix-free methods of deal.II, we must consider the cost of vector updates,and low-storage variants can deliver up to twice the throughput ofconventional explicit Runge--Kutta methods for this reason, see e.g. theanalysis in  [2.x.115] .
* Besides three variants for third, fourth and fifth order accuracy from thereference  [2.x.116] , we also use a fourth-order accuratevariant with seven stages that was optimized for acoustics setups from [2.x.117] . Acoustic problems are one of the interesting aspects ofthe subsonic regime of the Euler equations where compressibility leads to thetransmission of sound waves; often, one uses further simplifications of thelinearized Euler equations around a background state or the acoustic waveequation around a fixed frame.
* 

* [1.x.70][1.x.71]
* 

* The major ingredients used in this program are the fast matrix-free techniqueswe use to evaluate the operator  [2.x.118]  and the inverse mass matrix [2.x.119] . Actually, the term [1.x.72] is a slight misnomer,because we are working with a nonlinear operator and do not linearize theoperator that in turn could be represented by a matrix. However, fastevaluation of integrals has become popular as a replacement of sparsematrix-vector products, as shown in  [2.x.120]  and  [2.x.121] , and we have coinedthis infrastructure [1.x.73] in deal.II for thisreason. Furthermore, the inverse mass matrix is indeed applied in amatrix-free way, as detailed below.
* The matrix-free infrastructure allows us to quickly evaluate the integrals inthe weak forms. The ingredients are the fast interpolation from solutioncoefficients into values and derivatives at quadrature points, point-wiseoperations at quadrature points (where we implement the differential operatoras derived above), as well as multiplication by all test functions andsummation over quadrature points. The first and third component make use ofsum factorization and have been extensively discussed in the  [2.x.122]  tutorialprogram for the cell integrals and  [2.x.123]  for the face integrals. The onlydifference is that we now deal with a system of  [2.x.124]  components, rather thanthe scalar systems in previous tutorial programs. In the code, all thatchanges is a template argument of the FEEvaluation and FEFaceEvaluationclasses, the one to set the number of components. The access to the vector isthe same as before, all handled transparently by the evaluator. We also notethat the variant with a single evaluator chosen in the code below is not theonly choice
* 
*  -  we could also have used separate evalators for the separatecomponents  [2.x.125] ,  [2.x.126] , and  [2.x.127] ; given that we treat allcomponents similarly (also reflected in the way we state the equation as avector system), this would be more complicated here. As before, theFEEvaluation class provides explicit vectorization by combining the operationson several cells (and faces), involving data types calledVectorizedArray. Since the arithmetic operations are overloaded for this type,we do not have to bother with it all that much, except for the evaluation offunctions through the Function interface, where we need to provide particular[1.x.74] evaluations for several quadrature point locations at once.
* A more substantial change in this program is the operation at quadraturepoints: Here, the multi-component evaluators provide us with return types notdiscussed before. Whereas  [2.x.128]  would return a scalar(more precisely, a VectorizedArray type due to vectorization across cells) forthe Laplacian of  [2.x.129] , it now returns a type that is`Tensor<1,dim+2,VectorizedArray<Number>>`. Likewise, the gradient type is now`Tensor<1,dim+2,Tensor<1,dim,VectorizedArray<Number>>>`, where the outertensor collects the `dim+2` components of the Euler system, and the innertensor the partial derivatives in the various directions. For example, theflux  [2.x.130]  of the Euler system is of this type. In order to reduce the amount ofcode we have to write for spelling out these types, we use the C++ `auto`keyword where possible.
* From an implementation point of view, the nonlinearity is not a bigdifficulty: It is introduced naturally as we express the terms of the Eulerweak form, for example in the form of the momentum term  [2.x.131] . To obtain this expression, we first deduce the velocity [2.x.132]  from the momentum variable  [2.x.133] . Given that  [2.x.134]  is represented as a  [2.x.135] -degree polynomial, as is  [2.x.136] , thevelocity  [2.x.137]  is a rational expression in terms of the referencecoordinates  [2.x.138] . As we perform the multiplication  [2.x.139] , we obtain an expression that is theratio of two polynomials, with polynomial degree  [2.x.140]  in thenumerator and polynomial degree  [2.x.141]  in the denominator. Combined with thegradient of the test function, the integrand is of degree  [2.x.142]  in thenumerator and  [2.x.143]  in the denominator already for affine cells, i.e.,for parallelograms/ parallelepipeds.For curved cells, additional polynomial and rational expressionsappear when multiplying the integrand by the determinant of the Jacobian ofthe mapping. At this point, one usually needs to give up on insisting on exactintegration, and take whatever accuracy the Gaussian (more precisely,Gauss--Legrende) quadrature provides. The situation is then similar to the onefor the Laplace equation, where the integrand contains rational expressions onnon-affince cells and is also only integrated approximately. As these formulasonly integrate polynomials exactly, we have to live with the [1.x.75] in the form of an integration error.
* While inaccurate integration is usually tolerable for elliptic problems, forhyperbolic problems inexact integration causes some headache in the form of aneffect called [1.x.76]. The term comes from signal processing andexpresses the situation of inappropriate, too coarse sampling. In terms ofquadrature, the inappropriate sampling means that we use too few quadraturepoints compared to what would be required to accurately sample thevariable-coefficient integrand. It has been shown in the DG literature thataliasing errors can introduce unphysical oscillations in the numericalsolution for [1.x.77] resolved simulations. The fact that aliasing mostlyaffects coarse resolutions
* 
*  -  whereas finer meshes with the same schemework fine
* 
*  -  is not surprising because well-resolved simulationstend to be smooth on length-scales of a cell (i.e., they havesmall coefficients in the higher polynomial degrees that are missed bytoo few quadrature points, whereas the main solution contribution in the lowerpolynomial degrees is still well-captured
* 
*  -  this is simply a consequence of Taylor'stheorem). To address this topic, various approaches have been proposed in theDG literature. One technique is filtering which damps the solution componentspertaining to higher polynomial degrees. As the chosen nodal basis is nothierarchical, this would mean to transform from the nodal basis into ahierarchical one (e.g., a modal one based on Legendre polynomials) where thecontributions within a cell are split by polynomial degrees. In that basis,one could then multiply the solution coefficients associated with higherdegrees by a small number, keep the lower ones intact (to not destroy consistency), andthen transform back to the nodal basis. However, filters reduce the accuracy of themethod. Another, in some sense simpler, strategy is to use more quadraturepoints to capture non-linear terms more accurately. Using more than  [2.x.144] quadrature points per coordinate directions is sometimes calledover-integration or consistent integration. The latter name is most common inthe context of the incompressible Navier-Stokes equations, where the [2.x.145]  nonlinearity results in polynomial integrandsof degree  [2.x.146]  (when also considering the test function), which can beintegrated exactly with  [2.x.147]  quadraturepoints per direction as long as the element geometry is affine. In the contextof the Euler equations with non-polynomial integrands, the choice is lessclear. Depending on the variation in the various variables both [2.x.148]  or  [2.x.149]  points (integratingexactly polynomials of degree  [2.x.150]  or  [2.x.151] , respectively) are common.
* To reflect this variability in the choice of quadrature in the program, wekeep the number of quadrature points a variable to be specified just as thepolynomial degree, and note that one would make different choices dependingalso on the flow configuration. The default choice is  [2.x.152]  points
* 
*  -  a bitmore than the minimum possible of  [2.x.153]  points. The FEEvaluation andFEFaceEvaluation classes allow to seamlessly change the number of points by atemplate parameter, such that the program does not get more complicatedbecause of that.
* 

* [1.x.78][1.x.79]
* 

* The last ingredient is the evaluation of the inverse mass matrix  [2.x.154] . In DG methods with explicit time integration, mass matrices areblock-diagonal and thus easily inverted
* 
*  -  one only needs to invert thediagonal blocks. However, given the fact that matrix-free evaluation ofintegrals is closer in cost to the access of the vectors only, even theapplication of a block-diagonal matrix (e.g. via an array of LU factors) wouldbe several times more expensive than evaluation of  [2.x.155] simply because just storing and loading matrices of size`dofs_per_cell` times `dofs_per_cell` for higher order finite elementsrepeatedly is expensive. As this isclearly undesirable, part of the community has moved to bases where the massmatrix is diagonal, for example the [1.x.80]-orthogonal Legendre basis usinghierarchical polynomials or Lagrange polynomials on the points of the Gaussianquadrature (which is just another way of utilizing Legendreinformation). While the diagonal property breaks down for deformed elements,the error made by taking a diagonal mass matrix and ignoring the rest (avariant of mass lumping, though not the one with an additional integrationerror as utilized in  [2.x.156] ) has been shown to not alter discretizationaccuracy. The Lagrange basis in the points of Gaussian quadrature is sometimesalso referred to as a collocation setup, as the nodal points of thepolynomials coincide (= are "co-located") with the points of quadrature, obviating someinterpolation operations. Given the fact that we want to use more quadraturepoints for nonlinear terms in  [2.x.157] , however, the collocationproperty is lost. (More precisely, it is still used in FEEvaluation andFEFaceEvaluation after a change of basis, see the matrix-free paper [2.x.158] .)
* In this tutorial program, we use the collocation idea for the application ofthe inverse mass matrix, but with a slight twist. Rather than using thecollocation via Lagrange polynomials in the points of Gaussian quadrature, weprefer a conventional Lagrange basis in Gauss-Lobatto points as those make theevaluation of face integrals cheap. This is because for Gauss-Lobattopoints, some of the node points are located on the faces of the celland it is not difficult to show that on any given face, the only shapefunctions with non-zero values are exactly the ones whose node pointsare in fact located on that face. One could of course also use theGauss-Lobatto quadrature (with some additional integration error) as was donein  [2.x.159] , but we do not want to sacrifice accuracy as thesequadrature formulas are generally of lower order than the generalGauss quadrature formulas. Instead, we use an idea described in the reference [2.x.160]  where it was proposed to change the basis for thesake of applying the inverse mass matrix. Let us denote by  [2.x.161]  the matrix ofshape functions evaluated at quadrature points, with shape functions in the rowof the matrix and quadrature points in columns. Then, the mass matrix on a cell [2.x.162]  is given by[1.x.81]Here,  [2.x.163]  is the diagonal matrix with the determinant of the Jacobian timesthe quadrature weight (JxW) as entries. The matrix  [2.x.164]  is constructed as theKronecker product (tensor product) of one-dimensional matrices, e.g. in 3D as[1.x.82]which is the result of the basis functions being a tensor product ofone-dimensional shape functions and the quadrature formula being the tensorproduct of 1D quadrature formulas. For the case that the number of polynomialsequals the number of quadrature points, all matrices in  [2.x.165] are square, and also the ingredients to  [2.x.166]  in the Kronecker product aresquare. Thus, one can invert each matrix to form the overall inverse,[1.x.83]This formula is of exactly the same structure as the steps in the forwardevaluation of integrals with sum factorization techniques (i.e., theFEEvaluation and MatrixFree framework of deal.II). Hence, we can utilize thesame code paths with a different interpolation matrix, [2.x.167]  rather than  [2.x.168] .
* The class  [2.x.169]  implements thisoperation: It changes from the basis contained in the finite element (in thiscase, FE_DGQ) to the Lagrange basis in Gaussian quadrature points. Here, theinverse of a diagonal mass matrix can be evaluated, which is simply the inverseof the `JxW` factors (i.e., the quadrature weight times the determinant of theJacobian from reference to real coordinates). Once this is done, we can changeback to the standard nodal Gauss-Lobatto basis.
* The advantage of this particular way of applying the inverse mass matrix isa cost similar to the forward application of a mass matrix, which is cheaperthan the evaluation of the spatial operator  [2.x.170] with over-integration and face integrals. (Wewill demonstrate this with detailed timing information in the[1.x.84].) In fact, itis so cheap that it is limited by the bandwidth of reading the source vector,reading the diagonal, and writing into the destination vector on most modernarchitectures. The hardware used for the result section allows to do thecomputations at least twice as fast as the streaming of the vectors frommemory.
* 

* [1.x.85][1.x.86]
* 

* In this tutorial program, we implement two test cases. The first case is aconvergence test limited to two space dimensions. It runs a so-calledisentropic vortex which is transported via a background flow field. The secondcase uses a more exciting setup: We start with a cylinder immersed in achannel, using the  [2.x.171]  function. Here, weimpose a subsonic initial field at Mach number of  [2.x.172]  with aconstant velocity in  [2.x.173]  direction. At the top and bottom walls as well as atthe cylinder, we impose a no-penetration (i.e., tangential flow)condition. This setup forces the flow to re-orient as compared to the initialcondition, which results in a big sound wave propagating away from thecylinder. In upstream direction, the wave travels more slowly (as ithas to move against the oncoming gas), including adiscontinuity in density and pressure. In downstream direction, the transportis faster as sound propagation and fluid flow go in the same direction, which smearsout the discontinuity somewhat. Once the sound wave hits the upper and lowerwalls, the sound is reflected back, creating some nice shapes as illustratedin the [1.x.87] below.
* 

*  [1.x.88] [1.x.89]
*  The include files are similar to the previous matrix-free tutorial programs  [2.x.174] ,  [2.x.175] , and  [2.x.176] 
* 

* 
* [1.x.90]
* 
*  The following file includes the CellwiseInverseMassMatrix data structure that we will use for the mass matrix inversion, the only new include file for this tutorial program:
* 

* 
* [1.x.91]
* 
*  Similarly to the other matrix-free tutorial programs, we collect all parameters that control the execution of the program at the top of the file. Besides the dimension and polynomial degree we want to run with, we also specify a number of points in the Gaussian quadrature formula we want to use for the nonlinear terms in the Euler equations. Furthermore, we specify the time interval for the time-dependent problem, and implement two different test cases. The first one is an analytical solution in 2D, whereas the second is a channel flow around a cylinder as described in the introduction. Depending on the test case, we also change the final time up to which we run the simulation, and a variable `output_tick` that specifies in which intervals we want to write output (assuming that the tick is larger than the time step size).
* 

* 
* [1.x.92]
* 
*  Next off are some details of the time integrator, namely a Courant number that scales the time step size in terms of the formula  [2.x.177] , as well as a selection of a few low-storage Runge--Kutta methods. We specify the Courant number per stage of the Runge--Kutta scheme, as this gives a more realistic expression of the numerical cost for schemes of various numbers of stages.
* 

* 
* [1.x.93]
* 
*  Eventually, we select a detail of the spatial discretization, namely the numerical flux (Riemann solver) at the faces between cells. For this program, we have implemented a modified variant of the Lax--Friedrichs flux and the Harten--Lax--van Leer (HLL) flux.
* 

* 
* [1.x.94]
* 
*   [1.x.95]  [1.x.96]
* 

* 
*  We now define a class with the exact solution for the test case 0 and one with a background flow field for test case 1 of the channel. Given that the Euler equations are a problem with  [2.x.178]  equations in  [2.x.179]  dimensions, we need to tell the Function base class about the correct number of components.
* 

* 
* [1.x.97]
* 
*  As far as the actual function implemented is concerned, the analytical test case is an isentropic vortex case (see e.g. the book by Hesthaven and Warburton, Example 6.1 in Section 6.6 on page 209) which fulfills the Euler equations with zero force term on the right hand side. Given that definition, we return either the density, the momentum, or the energy depending on which component is requested. Note that the original definition of the density involves the  [2.x.180] -th power of some expression. Since  [2.x.181]  has pretty slow implementations on some systems, we replace it by logarithm followed by exponentiation (of base 2), which is mathematically equivalent but usually much better optimized. This formula might lose accuracy in the last digits for very small numbers compared to  [2.x.182]  but we are happy with it anyway, since small numbers map to data close to 1.   
*   For the channel test case, we simply select a density of 1, a velocity of 0.4 in  [2.x.183]  direction and zero in the other directions, and an energy that corresponds to a speed of sound of 1.3 measured against the background velocity field, computed from the relation  [2.x.184] .
* 

* 
* [1.x.98]
* 
*   [1.x.99]  [1.x.100]
* 

* 
*  The next few lines implement a few low-storage variants of Runge--Kutta methods. These methods have specific Butcher tableaux with coefficients  [2.x.185]  and  [2.x.186]  as shown in the introduction. As usual in Runge--Kutta method, we can deduce time steps,  [2.x.187]  from those coefficients. The main advantage of this kind of scheme is the fact that only two vectors are needed per stage, namely the accumulated part of the solution  [2.x.188]  (that will hold the solution  [2.x.189]  at the new time  [2.x.190]  after the last stage), the update vector  [2.x.191]  that gets evaluated during the stages, plus one vector  [2.x.192]  to hold the evaluation of the operator. Such a Runge--Kutta setup reduces the memory storage and memory access. As the memory bandwidth is often the performance-limiting factor on modern hardware when the evaluation of the differential operator is well-optimized, performance can be improved over standard time integrators. This is true also when taking into account that a conventional Runge--Kutta scheme might allow for slightly larger time steps as more free parameters allow for better stability properties.   
*   In this tutorial programs, we concentrate on a few variants of low-storage schemes defined in the article by Kennedy, Carpenter, and Lewis (2000), as well as one variant described by Tselios and Simos (2007). There is a large series of other schemes available, which could be addressed by additional sets of coefficients or slightly different update formulas.   
*   We define a single class for the four integrators, distinguished by the enum described above. To each scheme, we then fill the vectors for the  [2.x.193]  and  [2.x.194]  to the given variables in the class.
* 

* 
* [1.x.101]
* 
*  First comes the three-stage scheme of order three by Kennedy et al. (2000). While its stability region is significantly smaller than for the other schemes, it only involves three stages, so it is very competitive in terms of the work per stage.
* 

* 
* [1.x.102]
* 
*  The next scheme is a five-stage scheme of order four, again defined in the paper by Kennedy et al. (2000).
* 

* 
* [1.x.103]
* 
*  The following scheme of seven stages and order four has been explicitly derived for acoustics problems. It is a balance of accuracy for imaginary eigenvalues among fourth order schemes, combined with a large stability region. Since DG schemes are dissipative among the highest frequencies, this does not necessarily translate to the highest possible time step per stage. In the context of the present tutorial program, the numerical flux plays a crucial role in the dissipation and thus also the maximal stable time step size. For the modified Lax--Friedrichs flux, this scheme is similar to the `stage_5_order_4` scheme in terms of step size per stage if only stability is considered, but somewhat less efficient for the HLL flux.
* 

* 
* [1.x.104]
* 
*  The last scheme included here is the nine-stage scheme of order five from Kennedy et al. (2000). It is the most accurate among the schemes used here, but the higher order of accuracy sacrifices some stability, so the step length normalized per stage is less than for the fourth order schemes.
* 

* 
* [1.x.105]
* 
*  The main function of the time integrator is to go through the stages, evaluate the operator, prepare the  [2.x.195]  vector for the next evaluation, and update the solution vector  [2.x.196] . We hand off the work to the `pde_operator` involved in order to be able to merge the vector operations of the Runge--Kutta setup with the evaluation of the differential operator for better performance, so all we do here is to delegate the vectors and coefficients.     
*   We separately call the operator for the first stage because we need slightly modified arguments there: We evaluate the solution from the old solution  [2.x.197]  rather than a  [2.x.198]  vector, so the first argument is `solution`. We here let the stage vector  [2.x.199]  also hold the temporary result of the evaluation, as it is not used otherwise. For all subsequent stages, we use the vector `vec_ki` as the second vector argument to store the result of the operator evaluation. Finally, when we are at the last stage, we must skip the computation of the vector  [2.x.200]  as there is no coefficient  [2.x.201]  available (nor will it be used).
* 

* 
* [1.x.106]
* 
*   [1.x.107]  [1.x.108]
* 

* 
*  In the following functions, we implement the various problem-specific operators pertaining to the Euler equations. Each function acts on the vector of conserved variables  [2.x.202]  that we hold in the solution vectors, and computes various derived quantities.   
*   First out is the computation of the velocity, that we derive from the momentum variable  [2.x.203]  by division by  [2.x.204] . One thing to note here is that we decorate all those functions with the keyword `DEAL_II_ALWAYS_INLINE`. This is a special macro that maps to a compiler-specific keyword that tells the compiler to never create a function call for any of those functions, and instead move the implementation [1.x.109] to where they are called. This is critical for performance because we call into some of those functions millions or billions of times: For example, we both use the velocity for the computation of the flux further down, but also for the computation of the pressure, and both of these places are evaluated at every quadrature point of every cell. Making sure these functions are inlined ensures not only that the processor does not have to execute a jump instruction into the function (and the corresponding return jump), but also that the compiler can re-use intermediate information from one function's context in code that comes after the place where the function was called. (We note that compilers are generally quite good at figuring out which functions to inline by themselves. Here is a place where compilers may or may not have figured it out by themselves but where we know for sure that inlining is a win.)   
*   Another trick we apply is a separate variable for the inverse density  [2.x.205] . This enables the compiler to only perform a single division for the flux, despite the division being used at several places. As divisions are around ten to twenty times as expensive as multiplications or additions, avoiding redundant divisions is crucial for performance. We note that taking the inverse first and later multiplying with it is not equivalent to a division in floating point arithmetic due to roundoff effects, so the compiler is not allowed to exchange one way by the other with standard optimization flags. However, it is also not particularly difficult to write the code in the right way.   
*   To summarize, the chosen strategy of always inlining and careful definition of expensive arithmetic operations allows us to write compact code without passing all intermediate results around, despite making sure that the code maps to excellent machine code.
* 

* 
* [1.x.110]
* 
*  The next function computes the pressure from the vector of conserved variables, using the formula  [2.x.206] . As explained above, we use the velocity from the `euler_velocity()` function. Note that we need to specify the first template argument `dim` here because the compiler is not able to deduce it from the arguments of the tensor, whereas the second argument (number type) can be automatically deduced.
* 

* 
* [1.x.111]
* 
*  Here is the definition of the Euler flux function, i.e., the definition of the actual equation. Given the velocity and pressure (that the compiler optimization will make sure are done only once), this is straight-forward given the equation stated in the introduction.
* 

* 
* [1.x.112]
* 
*  This next function is a helper to simplify the implementation of the numerical flux, implementing the action of a tensor of tensors (with non-standard outer dimension of size `dim + 2`, so the standard overloads provided by deal.II's tensor classes do not apply here) with another tensor of the same inner dimension, i.e., a matrix-vector product.
* 

* 
* [1.x.113]
* 
*  This function implements the numerical flux (Riemann solver). It gets the state from the two sides of an interface and the normal vector, oriented from the side of the solution  [2.x.207]  towards the solution  [2.x.208] . In finite volume methods which rely on piece-wise constant data, the numerical flux is the central ingredient as it is the only place where the physical information is entered. In DG methods, the numerical flux is less central due to the polynomials within the elements and the physical flux used there. As a result of higher-degree interpolation with consistent values from both sides in the limit of a continuous solution, the numerical flux can be seen as a control of the jump of the solution from both sides to weakly impose continuity. It is important to realize that a numerical flux alone cannot stabilize a high-order DG method in the presence of shocks, and thus any DG method must be combined with further shock-capturing techniques to handle those cases. In this tutorial, we focus on wave-like solutions of the Euler equations in the subsonic regime without strong discontinuities where our basic scheme is sufficient.   
*   Nonetheless, the numerical flux is decisive in terms of the numerical dissipation of the overall scheme and influences the admissible time step size with explicit Runge--Kutta methods. We consider two choices, a modified Lax--Friedrichs scheme and the widely used Harten--Lax--van Leer (HLL) flux. For both variants, we first need to get the velocities and pressures from both sides of the interface and evaluate the physical Euler flux.   
*   For the local Lax--Friedrichs flux, the definition is  [2.x.209] , where the factor  [2.x.210]  gives the maximal wave speed and  [2.x.211]  is the speed of sound. Here, we choose two modifications of that expression for reasons of computational efficiency, given the small impact of the flux on the solution. For the above definition of the factor  [2.x.212] , we would need to take four square roots, two for the two velocity norms and two for the speed of sound on either side. The first modification is hence to rather use  [2.x.213]  as an estimate of the maximal speed (which is at most a factor of 2 away from the actual maximum, as shown in the introduction). This allows us to pull the square root out of the maximum and get away with a single square root computation. The second modification is to further relax on the parameter  [2.x.214] ---the smaller it is, the smaller the dissipation factor (which is multiplied by the jump in  [2.x.215] , which might result in a smaller or bigger dissipation in the end). This allows us to fit the spectrum into the stability region of the explicit Runge--Kutta integrator with bigger time steps. However, we cannot make dissipation too small because otherwise imaginary eigenvalues grow larger. Finally, the current conservative formulation is not energy-stable in the limit of  [2.x.216]  as it is not skew-symmetric, and would need additional measures such as split-form DG schemes in that case.   
*   For the HLL flux, we follow the formula from literature, introducing an additional weighting of the two states from Lax--Friedrichs by a parameter  [2.x.217] . It is derived from the physical transport directions of the Euler equations in terms of the current direction of velocity and sound speed. For the velocity, we here choose a simple arithmetic average which is sufficient for DG scenarios and moderate jumps in material parameters.   
*   Since the numerical flux is multiplied by the normal vector in the weak form, we multiply by the result by the normal vector for all terms in the equation. In these multiplications, the `operator*` defined above enables a compact notation similar to the mathematical definition.   
*   In this and the following functions, we use variable suffixes `_m` and `_p` to indicate quantities derived from  [2.x.218]  and  [2.x.219] , i.e., values "here" and "there" relative to the current cell when looking at a neighbor cell.
* 

* 
* [1.x.114]
* 
*  This and the next function are helper functions to provide compact evaluation calls as multiple points get batched together via a VectorizedArray argument (see the  [2.x.220]  tutorial for details). This function is used for the subsonic outflow boundary conditions where we need to set the energy component to a prescribed value. The next one requests the solution on all components and is used for inflow boundaries where all components of the solution are set.
* 

* 
* [1.x.115]
* 
*   [1.x.116]  [1.x.117]
* 

* 
*  This class implements the evaluators for the Euler problem, in analogy to the `LaplaceOperator` class of  [2.x.221]  or  [2.x.222] . Since the present operator is non-linear and does not require a matrix interface (to be handed over to preconditioners), we skip the various `vmult` functions otherwise present in matrix-free operators and only implement an `apply` function as well as the combination of `apply` with the required vector updates for the low-storage Runge--Kutta time integrator mentioned above (called `perform_stage`). Furthermore, we have added three additional functions involving matrix-free routines, namely one to compute an estimate of the time step scaling (that is combined with the Courant number for the actual time step size) based on the velocity and speed of sound in the elements, one for the projection of solutions (specializing  [2.x.223]  for the DG case), and one to compute the errors against a possible analytical solution or norms against some background state.   
*   The rest of the class is similar to other matrix-free tutorials. As discussed in the introduction, we provide a few functions to allow a user to pass in various forms of boundary conditions on different parts of the domain boundary marked by  [2.x.224]  variables, as well as possible body forces.
* 

* 
* [1.x.118]
* 
*  For the initialization of the Euler operator, we set up the MatrixFree variable contained in the class. This can be done given a mapping to describe possible curved boundaries as well as a DoFHandler object describing the degrees of freedom. Since we use a discontinuous Galerkin discretization in this tutorial program where no constraints are imposed strongly on the solution field, we do not need to pass in an AffineConstraints object and rather use a dummy for the construction. With respect to quadrature, we want to select two different ways of computing the underlying integrals: The first is a flexible one, based on a template parameter `n_points_1d` (that will be assigned the `n_q_points_1d` value specified at the top of this file). More accurate integration is necessary to avoid the aliasing problem due to the variable coefficients in the Euler operator. The second less accurate quadrature formula is a tight one based on `fe_degree+1` and needed for the inverse mass matrix. While that formula provides an exact inverse only on affine element shapes and not on deformed elements, it enables the fast inversion of the mass matrix by tensor product techniques, necessary to ensure optimal computational efficiency overall.
* 

* 
* [1.x.119]
* 
*  The subsequent four member functions are the ones that must be called from outside to specify the various types of boundaries. For an inflow boundary, we must specify all components in terms of density  [2.x.225] , momentum  [2.x.226]  and energy  [2.x.227] . Given this information, we then store the function alongside the respective boundary id in a map member variable of this class. Likewise, we proceed for the subsonic outflow boundaries (where we request a function as well, which we use to retrieve the energy) and for wall (no-penetration) boundaries where we impose zero normal velocity (no function necessary, so we only request the boundary id). For the present DG code where boundary conditions are solely applied as part of the weak form (during time integration), the call to set the boundary conditions can appear both before or after the `reinit()` call to this class. This is different from continuous finite element codes where the boundary conditions determine the content of the AffineConstraints object that is sent into MatrixFree for initialization, thus requiring to be set before the initialization of the matrix-free data structures.   
*   The checks added in each of the four function are used to ensure that boundary conditions are mutually exclusive on the various parts of the boundary, i.e., that a user does not accidentally designate a boundary as both an inflow and say a subsonic outflow boundary.
* 

* 
* [1.x.120]
* 
*   [1.x.121]  [1.x.122]
* 

* 
*  Now we proceed to the local evaluators for the Euler problem. The evaluators are relatively simple and follow what has been presented in  [2.x.228] ,  [2.x.229] , or  [2.x.230] . The first notable difference is the fact that we use an FEEvaluation with a non-standard number of quadrature points. Whereas we previously always set the number of quadrature points to equal the polynomial degree plus one (ensuring exact integration on affine element shapes), we now set the number quadrature points as a separate variable (e.g. the polynomial degree plus two or three halves of the polynomial degree) to more accurately handle nonlinear terms. Since the evaluator is fed with the appropriate loop lengths via the template argument and keeps the number of quadrature points in the whole cell in the variable  [2.x.231]  we now automatically operate on the more accurate formula without further changes.   
*   The second difference is due to the fact that we are now evaluating a multi-component system, as opposed to the scalar systems considered previously. The matrix-free framework provides several ways to handle the multi-component case. The variant shown here utilizes an FEEvaluation object with multiple components embedded into it, specified by the fourth template argument `dim + 2` for the components in the Euler system. As a consequence, the return type of  [2.x.232]  is not a scalar any more (that would return a VectorizedArray type, collecting data from several elements), but a Tensor of `dim+2` components. The functionality is otherwise similar to the scalar case; it is handled by a template specialization of a base class, called FEEvaluationAccess. An alternative variant would have been to use several FEEvaluation objects, a scalar one for the density, a vector-valued one with `dim` components for the momentum, and another scalar evaluator for the energy. To ensure that those components point to the correct part of the solution, the constructor of FEEvaluation takes three optional integer arguments after the required MatrixFree field, namely the number of the DoFHandler for multi-DoFHandler systems (taking the first by default), the number of the quadrature point in case there are multiple Quadrature objects (see more below), and as a third argument the component within a vector system. As we have a single vector for all components, we would go with the third argument, and set it to `0` for the density, `1` for the vector-valued momentum, and `dim+1` for the energy slot. FEEvaluation then picks the appropriate subrange of the solution vector during  [2.x.233]  and  [2.x.234]  or the more compact  [2.x.235]  and  [2.x.236]  calls.   
*   When it comes to the evaluation of the body force vector, we distinguish between two cases for efficiency reasons: In case we have a constant function (derived from  [2.x.237]  we can precompute the value outside the loop over quadrature points and simply use the value everywhere. For a more general function, we instead need to call the `evaluate_function()` method we provided above; this path is more expensive because we need to access the memory associated with the quadrature point data.   
*   The rest follows the other tutorial programs. Since we have implemented all physics for the Euler equations in the separate `euler_flux()` function, all we have to do here is to call this function given the current solution evaluated at quadrature points, returned by `phi.get_value(q)`, and tell the FEEvaluation object to queue the flux for testing it by the gradients of the shape functions (which is a Tensor of outer `dim+2` components, each holding a tensor of `dim` components for the  [2.x.238]  component of the Euler flux). One final thing worth mentioning is the order in which we queue the data for testing by the value of the test function, `phi.submit_value()`, in case we are given an external function: We must do this after calling `phi.get_value(q)`, because `get_value()` (reading the solution) and `submit_value()` (queuing the value for multiplication by the test function and summation over quadrature points) access the same underlying data field. Here it would be easy to achieve also without temporary variable `w_q` since there is no mixing between values and gradients. For more complicated setups, one has to first copy out e.g. both the value and gradient at a quadrature point and then queue results again by  [2.x.239]  and  [2.x.240]    
*   As a final note, we mention that we do not use the first MatrixFree argument of this function, which is a call-back from  [2.x.241]  The interfaces imposes the present list of arguments, but since we are in a member function where the MatrixFree object is already available as the `data` variable, we stick with that to avoid confusion.
* 

* 
* [1.x.123]
* 
*  The next function concerns the computation of integrals on interior faces, where we need evaluators from both cells adjacent to the face. We associate the variable `phi_m` with the solution component  [2.x.242]  and the variable `phi_p` with the solution component  [2.x.243] . We distinguish the two sides in the constructor of FEFaceEvaluation by the second argument, with `true` for the interior side and `false` for the exterior side, with interior and exterior denoting the orientation with respect to the normal vector.   
*   Note that the calls  [2.x.244]  and  [2.x.245]  combine the access to the vectors and the sum factorization parts. This combined operation not only saves a line of code, but also contains an important optimization: Given that we use a nodal basis in terms of the Lagrange polynomials in the points of the Gauss-Lobatto quadrature formula, only  [2.x.246]  out of the  [2.x.247]  basis functions evaluate to non-zero on each face. Thus, the evaluator only accesses the necessary data in the vector and skips the parts which are multiplied by zero. If we had first read the vector, we would have needed to load all data from the vector, as the call in isolation would not know what data is required in subsequent operations. If the subsequent  [2.x.248]  call requests values and derivatives, indeed all  [2.x.249]  vector entries for each component are needed, as the normal derivative is nonzero for all basis functions.   
*   The arguments to the evaluators as well as the procedure is similar to the cell evaluation. We again use the more accurate (over-)integration scheme due to the nonlinear terms, specified as the third template argument in the list. At the quadrature points, we then go to our free-standing function for the numerical flux. It receives the solution evaluated at quadrature points from both sides (i.e.,  [2.x.250]  and  [2.x.251] ), as well as the normal vector onto the minus side. As explained above, the numerical flux is already multiplied by the normal vector from the minus side. We need to switch the sign because the boundary term comes with a minus sign in the weak form derived in the introduction. The flux is then queued for testing both on the minus sign and on the plus sign, with switched sign as the normal vector from the plus side is exactly opposed to the one from the minus side.
* 

* 
* [1.x.124]
* 
*  For faces located at the boundary, we need to impose the appropriate boundary conditions. In this tutorial program, we implement four cases as mentioned above. (A fifth case, for supersonic outflow conditions is discussed in the "Results" section below.) The discontinuous Galerkin method imposes boundary conditions not as constraints, but only weakly. Thus, the various conditions are imposed by finding an appropriate [1.x.125] quantity  [2.x.252]  that is then handed to the numerical flux function also used for the interior faces. In essence, we "pretend" a state on the outside of the domain in such a way that if that were reality, the solution of the PDE would satisfy the boundary conditions we want.   
*   For wall boundaries, we need to impose a no-normal-flux condition on the momentum variable, whereas we use a Neumann condition for the density and energy with  [2.x.253]  and  [2.x.254] . To achieve the no-normal flux condition, we set the exterior values to the interior values and subtract two times the velocity in wall-normal direction, i.e., in the direction of the normal vector.   
*   For inflow boundaries, we simply set the given Dirichlet data  [2.x.255]  as a boundary value. An alternative would have been to use  [2.x.256] , the so-called mirror principle.   
*   The imposition of outflow is essentially a Neumann condition, i.e., setting  [2.x.257] . For the case of subsonic outflow, we still need to impose a value for the energy, which we derive from the respective function. A special step is needed for the case of [1.x.126], i.e., the case where there is a momentum flux into the domain on the Neumann portion. According to the literature (a fact that can be derived by appropriate energy arguments), we must switch to another variant of the flux on inflow parts, see Gravemeier, Comerford, Yoshihara, Ismail, Wall, "A novel formulation for Neumann inflow conditions in biomechanics", Int. J. Numer. Meth. Biomed. Eng., vol. 28 (2012). Here, the momentum term needs to be added once again, which corresponds to removing the flux contribution on the momentum variables. We do this in a post-processing step, and only for the case when we both are at an outflow boundary and the dot product between the normal vector and the momentum (or, equivalently, velocity) is negative. As we work on data of several quadrature points at once for SIMD vectorizations, we here need to explicitly loop over the array entries of the SIMD array.   
*   In the implementation below, we check for the various types of boundaries at the level of quadrature points. Of course, we could also have moved the decision out of the quadrature point loop and treat entire faces as of the same kind, which avoids some map/set lookups in the inner loop over quadrature points. However, the loss of efficiency is hardly noticeable, so we opt for the simpler code here. Also note that the final `else` clause will catch the case when some part of the boundary was not assigned any boundary condition via  [2.x.258] 
* 

* 
* [1.x.127]
* 
*  The next function implements the inverse mass matrix operation. The algorithms and rationale have been discussed extensively in the introduction, so we here limit ourselves to the technicalities of the  [2.x.259]  class. It does similar operations as the forward evaluation of the mass matrix, except with a different interpolation matrix, representing the inverse  [2.x.260]  factors. These represent a change of basis from the specified basis (in this case, the Lagrange basis in the points of the Gauss--Lobatto quadrature formula) to the Lagrange basis in the points of the Gauss quadrature formula. In the latter basis, we can apply the inverse of the point-wise `JxW` factor, i.e., the quadrature weight times the determinant of the Jacobian of the mapping from reference to real coordinates. Once this is done, the basis is changed back to the nodal Gauss-Lobatto basis again. All of these operations are done by the `apply()` function below. What we need to provide is the local fields to operate on (which we extract from the global vector by an FEEvaluation object) and write the results back to the destination vector of the mass matrix operation.   
*   One thing to note is that we added two integer arguments (that are optional) to the constructor of FEEvaluation, the first being 0 (selecting among the DoFHandler in multi-DoFHandler systems; here, we only have one) and the second being 1 to make the quadrature formula selection. As we use the quadrature formula 0 for the over-integration of nonlinear terms, we use the formula 1 with the default  [2.x.261]  (or `fe_degree+1` in terms of the variable name) points for the mass matrix. This leads to square contributions to the mass matrix and ensures exact integration, as explained in the introduction.
* 

* 
* [1.x.128]
* 
*   [1.x.129]  [1.x.130]
* 

* 
*  We now come to the function which implements the evaluation of the Euler operator as a whole, i.e.,  [2.x.262] , calling into the local evaluators presented above. The steps should be clear from the previous code. One thing to note is that we need to adjust the time in the functions we have associated with the various parts of the boundary, in order to be consistent with the equation in case the boundary data is time-dependent. Then, we call  [2.x.263]  to perform the cell and face integrals, including the necessary ghost data exchange in the `src` vector. The seventh argument to the function, `true`, specifies that we want to zero the `dst` vector as part of the loop, before we start accumulating integrals into it. This variant is preferred over explicitly calling `dst = 0.;` before the loop as the zeroing operation is done on a subrange of the vector in parts that are written by the integrals nearby. This enhances data locality and allows for caching, saving one roundtrip of vector data to main memory and enhancing performance. The last two arguments to the loop determine which data is exchanged: Since we only access the values of the shape functions one faces, typical of first-order hyperbolic problems, and since we have a nodal basis with nodes at the reference element surface, we only need to exchange those parts. This again saves precious memory bandwidth.   
*   Once the spatial operator  [2.x.264]  is applied, we need to make a second round and apply the inverse mass matrix. Here, we call  [2.x.265]  since only cell integrals appear. The cell loop is cheaper than the full loop as access only goes to the degrees of freedom associated with the locally owned cells, which is simply the locally owned degrees of freedom for DG discretizations. Thus, no ghost exchange is needed here.   
*   Around all these functions, we put timer scopes to record the computational time for statistics about the contributions of the various parts.
* 

* 
* [1.x.131]
* 
*  Let us move to the function that does an entire stage of a Runge--Kutta update. It calls  [2.x.266]  followed by some updates to the vectors, namely `next_ri = solution + factor_ai k_i` and `solution += factor_solution k_i`. Rather than performing these steps through the vector interfaces, we here present an alternative strategy that is faster on cache-based architectures. As the memory consumed by the vectors is often much larger than what fits into caches, the data has to effectively come from the slow RAM memory. The situation can be improved by loop fusion, i.e., performing both the updates to `next_ki` and `solution` within a single sweep. In that case, we would read the two vectors `rhs` and `solution` and write into `next_ki` and `solution`, compared to at least 4 reads and two writes in the baseline case. Here, we go one step further and perform the loop immediately when the mass matrix inversion has finished on a part of the vector.  [2.x.267]  provides a mechanism to attach an  [2.x.268]  both before the loop over cells first touches a vector entry (which we do not use here, but is e.g. used for zeroing the vector) and a second  [2.x.269]  to be called after the loop last touches an entry. The callback is in form of a range over the given vector (in terms of the local index numbering in the MPI universe) that can be addressed by `local_element()` functions.   
*   For this second callback, we create a lambda that works on a range and write the respective update on this range. Ideally, we would add the `DEAL_II_OPENMP_SIMD_PRAGMA` before the local loop to suggest to the compiler to SIMD parallelize this loop (which means in practice that we ensure that there is no overlap, also called aliasing, between the index ranges of the pointers we use inside the loops). It turns out that at the time of this writing, GCC 7.2 fails to compile an OpenMP pragma inside a lambda function, so we comment this pragma out below. If your compiler is newer, you should be able to uncomment these lines again.   
*   Note that we select a different code path for the last Runge--Kutta stage when we do not need to update the `next_ri` vector. This strategy gives a considerable speedup. Whereas the inverse mass matrix and vector updates take more than 60% of the computational time with default vector updates on a 40-core machine, the percentage is around 35% with the more optimized variant. In other words, this is a speedup of around a third.
* 

* 
* [1.x.132]
* 
*  Having discussed the implementation of the functions that deal with advancing the solution by one time step, let us now move to functions that implement other, ancillary operations. Specifically, these are functions that compute projections, evaluate errors, and compute the speed of information transport on a cell.   
*   The first of these functions is essentially equivalent to  [2.x.270]  just much faster because it is specialized for DG elements where there is no need to set up and solve a linear system, as each element has independent basis functions. The reason why we show the code here, besides a small speedup of this non-critical operation, is that it shows additional functionality provided by  [2.x.271]    
*   The projection operation works as follows: If we denote the matrix of shape functions evaluated at quadrature points by  [2.x.272] , the projection on cell  [2.x.273]  is an operation of the form  [2.x.274] , where  [2.x.275]  is the diagonal matrix containing the determinant of the Jacobian times the quadrature weight (JxW),  [2.x.276]  is the cell-wise mass matrix, and  [2.x.277]  is the evaluation of the field to be projected onto quadrature points. (In reality the matrix  [2.x.278]  has additional structure through the tensor product, as explained in the introduction.) This system can now equivalently be written as  [2.x.279] . Now, the term  [2.x.280]  and then  [2.x.281]  cancel, resulting in the final expression  [2.x.282] . This operation is implemented by  [2.x.283]  The name is derived from the fact that this projection is simply the multiplication by  [2.x.284] , a basis change from the nodal basis in the points of the Gaussian quadrature to the given finite element basis. Note that we call  [2.x.285]  to write the result into the vector, overwriting previous content, rather than accumulating the results as typical in integration tasks
* 
*  -  we can do this because every vector entry has contributions from only a single cell for discontinuous Galerkin discretizations.
* 

* 
* [1.x.133]
* 
*  The next function again repeats functionality also provided by the deal.II library, namely  [2.x.286]  We here show the explicit code to highlight how the vectorization across several cells works and how to accumulate results via that interface: Recall that each [1.x.134] of the vectorized array holds data from a different cell. By the loop over all cell batches that are owned by the current MPI process, we could then fill a VectorizedArray of results; to obtain a global sum, we would need to further go on and sum across the entries in the SIMD array. However, such a procedure is not stable as the SIMD array could in fact not hold valid data for all its lanes. This happens when the number of locally owned cells is not a multiple of the SIMD width. To avoid invalid data, we must explicitly skip those invalid lanes when accessing the data. While one could imagine that we could make it work by simply setting the empty lanes to zero (and thus, not contribute to a sum), the situation is more complicated than that: What if we were to compute a velocity out of the momentum? Then, we would need to divide by the density, which is zero
* 
*  -  the result would consequently be NaN and contaminate the result. This trap is avoided by accumulating the results from the valid SIMD range as we loop through the cell batches, using the function  [2.x.287]  to give us the number of lanes with valid data. It equals  [2.x.288]  on most cells, but can be less on the last cell batch if the number of cells has a remainder compared to the SIMD width.
* 

* 
* [1.x.135]
* 
*  This final function of the EulerOperator class is used to estimate the transport speed, scaled by the mesh size, that is relevant for setting the time step size in the explicit time integrator. In the Euler equations, there are two speeds of transport, namely the convective velocity  [2.x.289]  and the propagation of sound waves with sound speed  [2.x.290]  relative to the medium moving at velocity  [2.x.291] .   
*   In the formula for the time step size, we are interested not by these absolute speeds, but by the amount of time it takes for information to cross a single cell. For information transported along with the medium,  [2.x.292]  is scaled by the mesh size, so an estimate of the maximal velocity can be obtained by computing  [2.x.293] , where  [2.x.294]  is the Jacobian of the transformation from real to the reference domain. Note that  [2.x.295]  returns the inverse and transpose Jacobian, representing the metric term from real to reference coordinates, so we do not need to transpose it again. We store this limit in the variable `convective_limit` in the code below.   
*   The sound propagation is isotropic, so we need to take mesh sizes in any direction into account. The appropriate mesh size scaling is then given by the minimal singular value of  [2.x.296]  or, equivalently, the maximal singular value of  [2.x.297] . Note that one could approximate this quantity by the minimal distance between vertices of a cell when ignoring curved cells. To get the maximal singular value of the Jacobian, the general strategy would be some LAPACK function. Since all we need here is an estimate, we can avoid the hassle of decomposing a tensor of VectorizedArray numbers into several matrices and go into an (expensive) eigenvalue function without vectorization, and instead use a few iterations (five in the code below) of the power method applied to  [2.x.298] . The speed of convergence of this method depends on the ratio of the largest to the next largest eigenvalue and the initial guess, which is the vector of all ones. This might suggest that we get slow convergence on cells close to a cube shape where all lengths are almost the same. However, this slow convergence means that the result will sit between the two largest singular values, which both are close to the maximal value anyway. In all other cases, convergence will be quick. Thus, we can merely hardcode 5 iterations here and be confident that the result is good.
* 

* 
* [1.x.136]
* 
*  Similarly to the previous function, we must make sure to accumulate speed only on the valid cells of a cell batch.
* 

* 
* [1.x.137]
* 
*   [1.x.138]  [1.x.139]
* 

* 
*  This class combines the EulerOperator class with the time integrator and the usual global data structures such as FiniteElement and DoFHandler, to actually run the simulations of the Euler problem.   
*   The member variables are a triangulation, a finite element, a mapping (to create high-order curved surfaces, see e.g.  [2.x.299] ), and a DoFHandler to describe the degrees of freedom. In addition, we keep an instance of the EulerOperator described above around, which will do all heavy lifting in terms of integrals, and some parameters for time integration like the current time or the time step size.   
*   Furthermore, we use a PostProcessor instance to write some additional information to the output file, in similarity to what was done in  [2.x.300] . The interface of the DataPostprocessor class is intuitive, requiring us to provide information about what needs to be evaluated (typically only the values of the solution, except for the Schlieren plot that we only enable in 2D where it makes sense), and the names of what gets evaluated. Note that it would also be possible to extract most information by calculator tools within visualization programs such as ParaView, but it is so much more convenient to do it already when writing the output.
* 

* 
* [1.x.140]
* 
*  For the main evaluation of the field variables, we first check that the lengths of the arrays equal the expected values (the lengths `2*dim+4` or `2*dim+5` are derived from the sizes of the names we specify in the get_names() function below). Then we loop over all evaluation points and fill the respective information: First we fill the primal solution variables of density  [2.x.301] , momentum  [2.x.302]  and energy  [2.x.303] , then we compute the derived velocity  [2.x.304] , the pressure  [2.x.305] , the speed of sound  [2.x.306] , as well as the Schlieren plot showing  [2.x.307]  in case it is enabled. (See  [2.x.308]  for another example where we create a Schlieren plot.)
* 

* 
* [1.x.141]
* 
*  For the interpretation of quantities, we have scalar density, energy, pressure, speed of sound, and the Schlieren plot, and vectors for the momentum and the velocity.
* 

* 
* [1.x.142]
* 
*  With respect to the necessary update flags, we only need the values for all quantities but the Schlieren plot, which is based on the density gradient.
* 

* 
* [1.x.143]
* 
*  The constructor for this class is unsurprising: We set up a parallel triangulation based on the `MPI_COMM_WORLD` communicator, a vector finite element with `dim+2` components for density, momentum, and energy, a high-order mapping of the same degree as the underlying finite element, and initialize the time and time step to zero.
* 

* 
* [1.x.144]
* 
*  As a mesh, this tutorial program implements two options, depending on the global variable `testcase`: For the analytical variant (`testcase==0`), the domain is  [2.x.309] , with Dirichlet boundary conditions (inflow) all around the domain. For `testcase==1`, we set the domain to a cylinder in a rectangular box, derived from the flow past cylinder testcase for incompressible viscous flow by Sch&auml;fer and Turek (1996). Here, we have a larger variety of boundaries. The inflow part at the left of the channel is given the inflow type, for which we choose a constant inflow profile, whereas we set a subsonic outflow at the right. For the boundary around the cylinder (boundary id equal to 2) as well as the channel walls (boundary id equal to 3) we use the wall boundary type, which is no-normal flow. Furthermore, for the 3D cylinder we also add a gravity force in vertical direction. Having the base mesh in place (including the manifolds set by  [2.x.310]  we can then perform the specified number of global refinements, create the unknown numbering from the DoFHandler, and hand the DoFHandler and Mapping objects to the initialization of the EulerOperator.
* 

* 
* [1.x.145]
* 
*  In the following, we output some statistics about the problem. Because we often end up with quite large numbers of cells or degrees of freedom, we would like to print them with a comma to separate each set of three digits. This can be done via "locales", although the way this works is not particularly intuitive.  [2.x.311]  explains this in slightly more detail.
* 

* 
* [1.x.146]
* 
*  For output, we first let the Euler operator compute the errors of the numerical results. More precisely, we compute the error against the analytical result for the analytical solution case, whereas we compute the deviation against the background field with constant density and energy and constant velocity in  [2.x.312]  direction for the second test case.   
*   The next step is to create output. This is similar to what is done in  [2.x.313] : We let the postprocessor defined above control most of the output, except for the primal field that we write directly. For the analytical solution test case, we also perform another projection of the analytical solution and print the difference between that field and the numerical solution. Once we have defined all quantities to be written, we build the patches for output. Similarly to  [2.x.314] , we create a high-order VTK output by setting the appropriate flag, which enables us to visualize fields of high polynomial degrees. Finally, we call the  [2.x.315]  function to write the result to the given file name. This function uses special MPI parallel write facilities, which are typically more optimized for parallel file systems than the standard library's  [2.x.316]  variants used in most other tutorial programs. A particularly nice feature of the `write_vtu_in_parallel()` function is the fact that it can combine output from all MPI ranks into a single file, making it unnecessary to have a central record of all such files (namely, the "pvtu" file).   
*   For parallel programs, it is often instructive to look at the partitioning of cells among processors. To this end, one can pass a vector of numbers to  [2.x.317]  that contains as many entries as the current processor has active cells; these numbers should then be the rank of the processor that owns each of these cells. Such a vector could, for example, be obtained from  [2.x.318]  On the other hand, on each MPI process, DataOut will only read those entries that correspond to locally owned cells, and these of course all have the same value: namely, the rank of the current process. What is in the remaining entries of the vector doesn't actually matter, and so we can just get away with a cheap trick: We just fillall* values of the vector we give to  [2.x.319]  with the rank of the current MPI process. The key is that on each process, only the entries corresponding to the locally owned cells will be read, ignoring the (wrong) values in other entries. The fact that every process submits a vector in which the correct subset of entries is correct is all that is necessary.
* 

* 
* [1.x.147]
* 
*  The  [2.x.320]  function puts all pieces together. It starts off by calling the function that creates the mesh and sets up data structures, and then initializing the time integrator and the two temporary vectors of the low-storage integrator. We call these vectors `rk_register_1` and `rk_register_2`, and use the first vector to represent the quantity  [2.x.321]  and the second one for  [2.x.322]  in the formulas for the Runge--Kutta scheme outlined in the introduction. Before we start the time loop, we compute the time step size by the  [2.x.323]  function. For reasons of comparison, we compare the result obtained there with the minimal mesh size and print them to screen. For velocities and speeds of sound close to unity as in this tutorial program, the predicted effective mesh size will be close, but they could vary if scaling were different.
* 

* 
* [1.x.148]
* 
*  Now we are ready to start the time loop, which we run until the time has reached the desired end time. Every 5 time steps, we compute a new estimate for the time step
* 
*  -  since the solution is nonlinear, it is most effective to adapt the value during the course of the simulation. In case the Courant number was chosen too aggressively, the simulation will typically blow up with time step NaN, so that is easy to detect here. One thing to note is that roundoff errors might propagate to the leading digits due to an interaction of slightly different time step selections that in turn lead to slightly different solutions. To decrease this sensitivity, it is common practice to round or truncate the time step size to a few digits, e.g. 3 in this case. In case the current time is near the prescribed 'tick' value for output (e.g. 0.02), we also write the output. After the end of the time loop, we summarize the computation by printing some statistics, which is mostly done by the  [2.x.324]  function.
* 

* 
* [1.x.149]
* 
*  The main() function is not surprising and follows what was done in all previous MPI programs: As we run an MPI program, we need to call `MPI_Init()` and `MPI_Finalize()`, which we do through the  [2.x.325]  data structure. Note that we run the program only with MPI, and set the thread count to 1.
* 

* 
* [1.x.150]
* [1.x.151][1.x.152]
* 

* [1.x.153][1.x.154]
* 

* Running the program with the default settings on a machine with 40 processesproduces the following output:
* [1.x.155]
* 
* The program output shows that all errors are small. This is due to the factthat we use a relatively fine mesh of  [2.x.326]  cells with polynomials of degree5 for a solution that is smooth. An interesting pattern shows for the timestep size: whereas it is 0.0069 up to time 5, it increases to 0.0096 for latertimes. The step size increases once the vortex with some motion on top of thespeed of sound (and thus faster propagation) leaves the computational domainbetween times 5 and 6.5. After that point, the flow is simply uniformin the same direction, and the maximum velocity of the gas is reducedcompared to the previous state where the uniform velocity was overlaidby the vortex. Our time step formula recognizes this effect.
* The final block of output shows detailed information about the timingof individual parts of the programs; it breaks this down by showingthe time taken by the fastest and the slowest processor, and theaverage time
* 
*  -  this is often useful in very large computations tofind whether there are processors that are consistently overheated(and consequently are throttling their clock speed) or consistentlyslow for other reasons.The summary shows that 1283 time steps have been performedin 1.02 seconds (looking at the average time among all MPI processes), whilethe output of 11 files has taken additional 0.96 seconds. Broken down per timestep and into the five Runge--Kutta stages, the compute time per evaluation is0.16 milliseconds. This high performance is typical of matrix-free evaluatorsand a reason why explicit time integration is very competitive againstimplicit solvers, especially for large-scale simulations. The breakdown ofcomputational times at the end of the program run shows that the evaluation ofintegrals in  [2.x.327]  contributes with around 0.92 seconds and theapplication of the inverse mass matrix with 0.06 seconds. Furthermore, theestimation of the transport speed for the time step size computationcontributes with another 0.05 seconds of compute time.
* If we use three more levels of global refinement and 9.4 million DoFs in total,the final statistics are as follows (for the modified Lax--Friedrichs flux, [2.x.328] , and the same system of 40 cores of dual-socket Intel Xeon Gold 6230):
* [1.x.156]
* 
* Per time step, the solver now takes 0.02 seconds, about 25 times as long asfor the small problem with 147k unknowns. Given that the problem involves 64times as many unknowns, the increase in computing time is notsurprising. Since we also do 8 times as many time steps, the compute timeshould in theory increase by a factor of 512. The actual increase is 205 s /1.02 s = 202. This is because the small problem size cannot fully utilize the40 cores due to communication overhead. This becomes clear if we look into thedetails of the operations done per time step. The evaluation of thedifferential operator  [2.x.329]  with nearest neighbor communication goesfrom 0.92 seconds to 127 seconds, i.e., it increases with a factor of 138. Onthe other hand, the cost for application of the inverse mass matrix and thevector updates, which do not need to communicate between the MPI processes atall, has increased by a factor of 1195. The increase is more than thetheoretical factor of 512 because the operation is limited by the bandwidthfrom RAM memory for the larger size while for the smaller size, all vectorsfit into the caches of the CPU. The numbers show that the mass matrixevaluation and vector update part consume almost 40% of the time spent by theRunge--Kutta stages
* 
*  -  despite using a low-storage Runge--Kutta integrator andmerging of vector operations! And despite using over-integration for the [2.x.330]  operator. For simpler differential operators and more expensivetime integrators, the proportion spent in the mass matrix and vector updatepart can also reach 70%. If we compute a throughput number in terms of DoFsprocessed per second and Runge--Kutta stage, we obtain [1.x.157] This throughput number isvery high, given that simply copying one vector to another one runs atonly around 10,000 MDoFs/s.
* If we go to the next-larger size with 37.7 million DoFs, the overallsimulation time is 2196 seconds, with 1978 seconds spent in the timestepping. The increase in run time is a factor of 9.3 for the L_h operator(1179 versus 127 seconds) and a factor of 10.3 for the inverse mass matrix andvector updates (797 vs 77.5 seconds). The reason for this non-optimal increasein run time can be traced back to cache effects on the given hardware (with 40MB of L2 cache and 55 MB of L3 cache): While not all of the relevant data fitsinto caches for 9.4 million DoFs (one vector takes 75 MB and we have threevectors plus some additional data in MatrixFree), there is capacity for one anda half vector nonetheless. Given that modern caches are more sophisticated thanthe naive least-recently-used strategy (where we would have little re-use asthe data is used in a streaming-like fashion), we can assume that a sizeablefraction of data can indeed be delivered from caches for the 9.4 million DoFscase. For the larger case, even with optimal caching less than 10 percent ofdata would fit into caches, with an associated loss in performance.
* 

* [1.x.158][1.x.159]
* 

* For the modified Lax--Friedrichs flux and measuring the error in the momentumvariable, we obtain the following convergence table (the rates are verysimilar for the density and energy variables):
*  [2.x.331] 
* If we switch to the Harten-Lax-van Leer flux, the results are as follows: [2.x.332] 
* The tables show that we get optimal  [2.x.333] convergence rates for both numerical fluxes. The errors are slightly smallerfor the Lax--Friedrichs flux for  [2.x.334] , but the picture is reversed for [2.x.335] ; in any case, the differences on this testcase are relativelysmall.
* For  [2.x.336] , we reach the roundoff accuracy of  [2.x.337]  with bothfluxes on the finest grids. Also note that the errors are absolute with adomain length of  [2.x.338] , so relative errors are below  [2.x.339] . The HLL fluxis somewhat better for the highest degree, which is due to a slight inaccuracyof the Lax--Friedrichs flux: The Lax--Friedrichs flux sets a Dirichletcondition on the solution that leaves the domain, which results in a smallartificial reflection, which is accentuated for the Lax--Friedrichsflux. Apart from that, we see that the influence of the numerical flux isminor, as the polynomial part inside elements is the main driver of theaccucary. The limited influence of the flux also has consequences when tryingto approach more challenging setups with the higher-order DG setup: Taking forexample the parameters and grid of  [2.x.340] , we get oscillations (which in turnmake density negative and make the solution explode) with both fluxes once thehigh-mass part comes near the boundary, as opposed to the low-order finitevolume case ( [2.x.341] ). Thus, any case that leads to shocks in the solutionnecessitates some form of limiting or artificial dissipation. For anotheralternative, see the  [2.x.342]  tutorial program.
* 

* [1.x.166][1.x.167]
* 

* For the test case of the flow around a cylinder in a channel, we need tochange the first code line to
* [1.x.168]
* This test case starts with a background field of a constant velocityof Mach number 0.31 and a constant initial density; the flow will haveto go around an obstacle in the form of a cylinder. Since we impose ano-penetration condition on the cylinder walls, the flow thatinitially impinges head-on onto to cylinder has to rearrange,which creates a big sound wave. The following pictures show the pressure attimes 0.1, 0.25, 0.5, and 1.0 (top left to bottom right) for the 2D case with5 levels of global refinement, using 102,400 cells with polynomial degree of5 and 14.7 million degrees of freedom over all 4 solution variables.We clearly see the discontinuity thatpropagates slowly in the upstream direction and more quickly in downstreamdirection in the first snapshot at time 0.1. At time 0.25, the sound wave hasreached the top and bottom walls and reflected back to the interior. From thedifferent distances of the reflected waves from lower and upper walls we cansee the slight asymmetry of the Sch&auml;fer-Turek test case represented by [2.x.343]  with somewhat more space above thecylinder compared to below. At later times, the picture is more chaotic withmany sound waves all over the place.
*  [2.x.344] 
* The next picture shows an elevation plot of the pressure at time 1.0 lookingfrom the channel inlet towards the outlet at the same resolution
* 
*  -  here,we can see the large numberof reflections. In the figure, two types of waves are visible. Thelarger-amplitude waves correspond to various reflections that happened as theinitial discontinuity hit the walls, whereas the small-amplitude waves ofsize similar to the elements correspond to numerical artifacts. They have theirorigin in the finite resolution of the scheme and appear as the discontinuitytravels through elements with high-order polynomials. This effect can be curedby increasing resolution. Apart from this effect, the rich wave structure isthe result of the transport accuracy of the high-order DG method.
*  [2.x.345] 
* With 2 levels of global refinement with 1,600 cells, the mesh and itspartitioning on 40 MPI processes looks as follows:
*  [2.x.346] 
* When we run the code with 4 levels of global refinements on 40 cores, we getthe following output:
* [1.x.169]
* 
* The norms shown here for the various quantities are the deviations [2.x.347] ,  [2.x.348] , and  [2.x.349]  against the background field (namely, theinitial condition). The distribution of run time is overall similar as in theprevious test case. The only slight difference is the larger proportion oftime spent in  [2.x.350]  as compared to the inverse mass matrix and vectorupdates. This is because the geometry is deformed and the matrix-freeframework needs to load additional arrays for the geometry from memory thatare compressed in the affine mesh case.
* Increasing the number of global refinements to 5, the output becomes:
* [1.x.170]
* 
* The effect on performance is similar to the analytical test case
* 
*  -  intheory, computation times should increase by a factor of 8, but we actuallysee an increase by a factor of 11 for the time steps (219.5 seconds versus2450 seconds). This can be traced back to caches, with the small case mostlyfitting in caches. An interesting effect, typical of programs with a mix oflocal communication (integrals  [2.x.351] ) and global communication (computation oftransport speed) with some load imbalance, can be observed by looking at theMPI ranks that encounter the minimal and maximal time of different phases,respectively. Rank 0 reports the fastest throughput for the "rk time steppingtotal" part. At the same time, it appears to be slowest for the "computetransport speed" part, almost a factor of 2 slower than theaverage and almost a factor of 4 compared to the faster rank.Since the latter involves global communication, we can attribute theslowness in this part to the fact that the local Runge--Kutta stages haveadvanced more quickly on this rank and need to wait until the other processorscatch up. At this point, one can wonder about the reason for this imbalance:The number of cells is almost the same on all MPI processes.However, the matrix-free framework is faster on affine and Cartesiancells located towards the outlet of the channel, to which the lower MPI ranksare assigned. On the other hand, rank 32, which reports the highest run timefor the Runga--Kutta stages, owns the curved cells near the cylinder, forwhich no data compression is possible. To improve throughput, we could assigndifferent weights to different cell types when partitioning the [2.x.352]  object, or even measure the run time for afew time steps and try to rebalance then.
* The throughput per Runge--Kutta stage can be computed to 2085 MDoFs/s for the14.7 million DoFs test case over the 346,000 Runge--Kutta stages, slightly slowerthan the Cartesian mesh throughput of 2360 MDoFs/s reported above.
* Finally, if we add one additional refinement, we record the following output:
* [1.x.171]
* 
* The "rk time stepping total" part corresponds to a throughput of 2010 MDoFs/s. Theoverall run time to perform 139k time steps is 20k seconds (5.7 hours) or 7time steps per second
* 
*  -  not so bad for having nearly 60 millionunknowns. More throughput can be achieved by adding more cores tothe computation.
* 

* [1.x.172][1.x.173]
* 

* Switching the channel test case to 3D with 3 global refinements, the output is
* [1.x.174]
* 
* The physics are similar to the 2D case, with a slight motion in the zdirection due to the gravitational force. The throughput per Runge--Kuttastage in this case is[1.x.175]
* The throughput is lower than in 2D because the computation of the  [2.x.353]  termis more expensive. This is due to over-integration with `degree+2` points andthe larger fraction of face integrals (worse volume-to-surface ratio) withmore expensive flux computations. If we only consider the inverse mass matrixand vector update part, we record a throughput of 4857 MDoFs/s for the 2D caseof the isentropic vortex with 37.7 million unknowns, whereas the 3D caseruns with 4535 MDoFs/s. The performance is similar because both cases are infact limited by the memory bandwidth.
* If we go to four levels of global refinement, we need to increase the numberof processes to fit everything in memory
* 
*  -  the computation needs around 350GB of RAM memory in this case. Also, the time it takes to complete 35k timesteps becomes more tolerable by adding additional resources. We therefore use6 nodes with 40 cores each, resulting in a computation with 240 MPI processes:
* [1.x.176]
* This simulation had nearly 2 billion unknowns
* 
*  -  quite a largecomputation indeed, and still only needed around 1.5 seconds per timestep.
* 

* [1.x.177][1.x.178]
* 

* The code presented here straight-forwardly extends to adaptive meshes, givenappropriate indicators for setting the refinement flags. Large-scaleadaptivity of a similar solver in the context of the acoustic wave equationhas been achieved by the [1.x.179]. However, in the present context, the benefits of adaptivity are oftenlimited to early times and effects close to the origin of sound waves, as thewaves eventually reflect and diffract. This leads to steep gradients all overthe place, similar to turbulent flow, and a more or less globallyrefined mesh.
* Another topic that we did not discuss in the results section is a comparisonof different time integration schemes. The program provides four variants oflow-storage Runga--Kutta integrators that each have slightly differentaccuracy and stability behavior. Among the schemes implemented here, thehigher-order ones provide additional accuracy but come with slightly lowerefficiency in terms of step size per stage before they violate the CFLcondition. An interesting extension would be to compare the low-storagevariants proposed here with standard Runge--Kutta integrators or to use vectoroperations that are run separate from the mass matrix operation and compareperformance.
* 

* [1.x.180][1.x.181]
* 

* As mentioned in the introduction, the modified Lax--Friedrichs flux and theHLL flux employed in this program are only two variants of a large body ofnumerical fluxes available in the literature on the Euler equations. Oneexample is the HLLC flux (Harten-Lax-van Leer-Contact) flux which adds theeffect of rarefaction waves missing in the HLL flux, or the Roe flux. Asmentioned in the introduction, the effect of numerical fluxes on high-order DGschemes is debatable (unlike for the case of low-order discretizations).
* A related improvement to increase the stability of the solver is to alsoconsider the spatial integral terms. A shortcoming in the rather naiveimplementation used above is the fact that the energy conservation of theoriginal Euler equations (in the absence of shocks) only holds up to adiscretization error. If the solution is under-resolved, the discretizationerror can give rise to an increase in the numerical energy and eventuallyrender the discretization unstable. This is because of the inexact numericalintegration of the terms in the Euler equations, which both contain rationalnonlinearities and higher-degree content from curved cells. A way out of thisdilemma are so-called skew-symmetric formulations, see  [2.x.354]  for asimple variant. Skew symmetry means that switching the role of the solution [2.x.355]  and test functions  [2.x.356]  in the weak form produces theexact negative of the original quantity, apart from some boundary terms. Inthe discrete setting, the challenge is to keep this skew symmetry also whenthe integrals are only computed approximately (in the continuous case,skew-symmetry is a consequence of integration by parts). Skew-symmetricnumerical schemes balance spatial derivatives in the conservative form [2.x.357]  with contributions in theconvective form  [2.x.358]  for some  [2.x.359] . The precise terms depend onthe equation and the integration formula, and can in some cases by understoodby special skew-symmetric finite difference schemes.
* To get started, interested readers could take a look athttps://github.com/kronbichler/advection_miniapp, where askew-symmetric DG formulation is implemented with deal.II for a simple advectionequation.
* [1.x.182][1.x.183]
* 

* As mentioned in the introduction, the solution to the Euler equations developsshocks as the Mach number increases, which require additional mechanisms tostabilize the scheme, e.g. in the form of limiters. The main challenge besidesactually implementing the limiter or artificial viscosity approach would be toload-balance the computations, as the additional computations involved forlimiting the oscillations in troubled cells would make them more expensive than theplain DG cells without limiting. Furthermore, additional numerical fluxes thatbetter cope with the discontinuities would also be an option.
* One ingredient also necessary for supersonic flows are appropriate boundaryconditions. As opposed to the subsonic outflow boundaries discussed in theintroduction and implemented in the program, all characteristics are outgoingfor supersonic outflow boundaries, so we do not want to prescribe any externaldata,[1.x.184]
* In the code, we would simply add the additional statement
* [1.x.185]
* in the `local_apply_boundary_face()` function.
* [1.x.186][1.x.187]
* 

* When the interest with an Euler solution is mostly in the propagation of soundwaves, it often makes sense to linearize the Euler equations around abackground state, i.e., a given density, velocity and energy (or pressure)field, and only compute the change against these fields. This is the settingof the wide field of aeroacoustics. Even though the resolution requirementsare sometimes considerably reduced, implementation gets somewhat morecomplicated as the linearization gives rise to additional terms. From a codeperspective, in the operator evaluation we also need to equip the code withthe state to linearize against. This information can be provided either byanalytical functions (that are evaluated in terms of the position of thequadrature points) or by a vector similar to the solution. Based on thatvector, we would create an additional FEEvaluation object to read from it andprovide the values of the field at quadrature points. If the backgroundvelocity is zero and the density is constant, the linearized Euler equationsfurther simplify and can equivalently be written in the form of theacoustic wave equation.
* A challenge in the context of sound propagation is often the definition ofboundary conditions, as the computational domain needs to be of finite size,whereas the actual simulation often spans an infinite (or at least muchlarger) physical domain. Conventional Dirichlet or Neumann boundary conditionsgive rise to reflections of the sound waves that eventually propagate back tothe region of interest and spoil the solution. Therefore, various variants ofnon-reflecting boundary conditions or sponge layers, often in the form of[1.x.188]
* 
*  -  where the solution is damped without reflection
* 
*  -  are common.
* 

* [1.x.189][1.x.190]
* 

* The solver presented in this tutorial program can also be extended to thecompressible Navier--Stokes equations by adding viscous terms, as described in [2.x.360] . To keep as much of the performance obtainedhere despite the additional cost of elliptic terms, e.g. via an interiorpenalty method, one can switch the basis from FE_DGQ to FE_DGQHermite like inthe  [2.x.361]  tutorial program.
* 

* [1.x.191][1.x.192]
* 

* In this tutorial, we used face-centric loops. Here, cell and face integralsare treated in separate loops, resulting in multiple writing accesses into theresult vector, which is relatively expensive on modern hardware since writingoperations generally result also in an implicit read operation. Element-centricloops, on the other hand, are processing a cell and in direct successionprocessing all its 2d faces. Although this kind of loop implies that fluxes haveto be computed twice (for each side of an interior face), the fact that theresult vector has to accessed only once might
* 
*  - and the fact that the resultingalgorithm is free of race-conditions and as such perfectly suitable forshared memory
* 
*  - already give a performance boost. If you are interested in theseadvanced topics, you can take a look at  [2.x.362]  where we take the presenttutorial and modify it so that we can use these features.
* 

* [1.x.193][1.x.194] [2.x.363] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-68_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28]
*  [2.x.2] 
* [1.x.29]
* [1.x.30][1.x.31]
* 

* [1.x.32][1.x.33]
* 

* Particles play an important part in numerical models for a large number of applications. Particles are routinely used as massless tracers to visualize the dynamic of a transient flow. They can also play an intrinsic role as part of a more complex finite element model, as is the case for the Particle-In-Cell (PIC) method  [2.x.3]  or they can even be used to simulate the motion of granular matter, as in the Discrete Element Method (DEM)  [2.x.4] . In the case of DEM, the resulting model is not related to the finite element method anymore, but just leads to a system of ordinary differential equation which describes the motion of the particles and the dynamic of their collisions. All of these models can be built using deal.II's particle handling capabilities.
* In the present step, we use particles as massless tracers to illustratethe dynamic of a vortical flow. Since the particles are massless tracers,the position of each particle  [2.x.5]  is described by thefollowing ordinary differential equation (ODE):[1.x.34]
* where  [2.x.6]  is the position of particle  [2.x.7]  and  [2.x.8]  the flow velocity at its position.In the present step, this ODE is solved using the explicit Euler method. The resulting scheme is:[1.x.35]
* where  [2.x.9]  and  [2.x.10]  are the positionof particle  [2.x.11]  at time  [2.x.12]  and  [2.x.13] , respectively and where  [2.x.14] is the time step. In the present step, the velocity at the location of particlesis obtained in two different fashions:
* 
*  - By evaluating the velocity function at the location of the particles;
* 
*  - By evaluating the velocity function on a background triangulation and, usinga  finite element support, interpolating at the position of the particle.
* The first approach is not practical, since the velocity profileis generally not known analytically. The second approach, based on interpolating a solutionat the position of the particles, mimics exactly what would be done in arealistic computational fluid dynamic simulation, and this follows the way we have also evaluatedthe finite element solution at particle locations in  [2.x.15] . In this step, we illustrate both strategies.
* We note that much greater accuracy could be obtained by using a fourthorder Runge-Kutta method or another appropriate scheme for the time integrationof the motion of the particles.  Implementing a more advanced time-integration schemewould be a straightforward extension of this step.
* [1.x.36][1.x.37]
* 

* In deal.II,  [2.x.16]  are very simple and flexible entities that can be usedto build PIC, DEM or any type of particle-based models. Particles have a locationin real space, a location in the reference space of the element in which theyare located and a unique ID. In the majority of cases, simulations that includeparticles require a significant number of them. Thus, it becomes interestingto handle all particles through an entity which agglomerates all particles.In deal.II, this is achieved through the use of the  [2.x.17]  class.
* By default, particles do not have a diameter,a mass or any other physical properties which we would generally expect of physical particles. However, througha ParticleHandler, particles have access to a  [2.x.18]  This PropertyPool isan array which can be used to store an arbitrary number of propertiesassociated with the particles. Consequently, users can build their ownparticle solver and attribute the desired properties to the particles (e.g., mass, charge,diameter, temperature, etc.). In the present tutorial, this is used tostore the value of the fluid velocity and the process id to which the particlesbelong.
* [1.x.38][1.x.39]
* 

* Although the present step is not computationally intensive, simulations thatinclude many particles can be computationally demanding and require parallelization.The present step showcases the distributed parallel capabilities of deal.II for particles.In general, there are three main challengesthat specifically arise in parallel distributed simulations that include particles:
* 
*  - Generating the particles on the distributed triangulation;
* 
*  - Exchanging the particles that leave local domains between the processors;
* 
*  - Load balancing the simulation so that every processor has a similar computational load.These challenges and their solution in deal.II have been discussed in more detail in [2.x.19] , but we will summarize them below.
* There are of course also questions on simply setting up a code that uses particles. These have largely already beenaddressed in  [2.x.20] . Some more advanced techniques will also be discussed in  [2.x.21] .
* [1.x.40][1.x.41]
* 

* Generating distributed particles in a scalable way is not straightforward sincethe processor to which they belong must first be identified before the cell inwhich they are located is found.  deal.II provides numerous capabilities togenerate particles through the  [2.x.22]  namespace.  Some of theseparticle generators create particles only on the locally owned subdomain. Forexample,  [2.x.23]  creates particlesat the same reference locations within each cell of the local subdomain and [2.x.24]  uses a globally defined probabilitydensity function to determine how many and where to generate particles locally.
* In other situations, such as the present step, particles must be generated atspecific locations on cells that may be owned only by a subset of the processors.In  most of these situations, the insertion of the particles is done for a verylimited number of time-steps and, consequently, does not constitute a largeportion of the computational cost. For these occasions, deal.II providesconvenient  [2.x.25]  that can globally insert the particles even ifthe particle is not located in a cell owned by the parallel process on which the call to create the particle is initiated. Thegenerators first locate on which subdomain the particles are situated, identifyin which cell they are located and exchange the necessary information amongthe processors to ensure that the particle is generated with the rightproperties. Consequently, this type of particle generation can be communicationintensive. The  [2.x.26]  and the [2.x.27]  generate particles using atriangulation and the points of an associated DoFHandler or quadraturerespectively. The triangulation that is used to generate the particles can bethe same triangulation that is used for the background mesh, in which case thesefunctions are very similar to the [2.x.28]  function described in theprevious paragraph. However, the triangulation used to generate particles canalso be different (non-matching) from the triangulation of the background grid,which is useful to generate particles in particular shapes (as in thisexample), or to transfer information between two different computational grids(as in  [2.x.29] ).  Furthermore, the  [2.x.30]  class provides the [2.x.31]  function which enables theglobal insertion of particles from a vector of arbitrary points and a globalvector of bounding boxes. In the present step, we use the [2.x.32]  function on a non-matching triangulation toinsert particles located at positions in the shape of a disk.
* [1.x.42][1.x.43]
* 

* As particles move around in parallel distributed computations they may leavethe locally owned subdomain and need to be transferred to their new ownerprocesses. This situation can arise in two very different ways: First, if theprevious owning process knows the new owner of the particles that were lost(for example because the particles moved from the locally owned cell of one processorinto an adjacent ghost cells of a distributedtriangulation) then the transfer can be handled efficiently as a point-to-pointcommunication between each process and the new owners. This transfer happensautomatically whenever particles are sorted into their new cells. Secondly,the previous owner may not know to which process the particle has moved. Inthis case the particle is discarded by default, as a global search for theowner can be expensive.  [2.x.33]  shows how such a discarded particle can stillbe collected, interpreted, and potentially reinserted by the user. In thepresent example we prevent the second case by imposing a CFL criterion on thetimestep to ensure particles will at most move into the ghost layer of thelocal process and can therefore be send to neighboring processes automatically.
* [1.x.44][1.x.45]
* 

* The last challenge that arises in parallel distributed computations usingparticles is to balance the computational load between work that is done on thegrid, for example solving the finite-element problem, and the work that is doneon the particles, for example advecting the particles or computing the forcesbetween particles or between particles and grid. By default, for example in [2.x.34] , deal.II distributes the background mesh as evenly as possible betweenthe available processes, that is it balances the number of cells on eachprocess. However, if some cells own many more particles than other cells, or ifthe particles of one cell are much more computationally expensive than theparticles in other cells, then this problem no longer scales efficiently (for adiscussion of what we consider "scalable" programs, see [2.x.35]  "this glossary entry"). Thus, we have to apply a form of"load balancing", which means we estimate the computational load that isassociated with each cell and its particles. Repartitioning the mesh thenaccounts for this combined computational load instead of the simplifiedassumption of the number of cells  [2.x.36] .
* In this section we only discussed the particle-specific challenges in distributedcomputation. Parallel challenges that particles share withfinite-element solutions (parallel output, data transfer during meshrefinement) can be addressed with the solutions found forfinite-element problems already discussed in other examples.
* [1.x.46][1.x.47]
* 

* In the present step, we use particles as massless tracers to illustratethe dynamics of a particular vortical flow: the Rayleigh--Kothe vortex. This flow patternis generally used as a complex test case for interface tracking methods(e.g., volume-of-fluid and level set approaches) sinceit leads to strong rotation and elongation of the fluid  [2.x.37] .
* The stream function  [2.x.38]  of this Rayleigh-Kothe vortex is defined as:
* [1.x.48]where  [2.x.39]  is half the period of the flow. The velocity profile in 2D ( [2.x.40] ) is :[1.x.49]
* 
* The velocity profile is illustrated in the following animation:
* [1.x.50]
* 
* It can be seen that this velocity reverses periodically due to the term [2.x.41]  and that material will end up at itsstarting position after every period of length  [2.x.42] . We will run this tutorialprogram for exactly one period and compare the final particle location to theinitial location to illustrate this flow property. This example uses the testcaseto produce two models that handle the particlesslightly differently. The first model prescribes the exact analytical velocitysolution as the velocity for each particle. Therefore in this model there is noerror in the assigned velocity to the particles, and any deviation of particlepositions from the analytical position at a given time results from the errorin solving the equation of motion for the particle inexactly, using a time stepping method. In the second model theanalytical velocity field is first interpolated to a finite-element vectorspace (to simulate the case that the velocity was obtained from solving afinite-element problem, in the same way as the ODE for each particle in  [2.x.43]  depends on a finite elementsolution). This finite-element "solution" is then evaluated atthe locations of the particles to solve their equation of motion. Thedifference between the two cases allows to assess whether the chosenfinite-element space is sufficiently accurate to advect the particles with theoptimal convergence rate of the chosen particle advection scheme, a questionthat is important in practice to determine the accuracy of the combinedalgorithm (see e.g.  [2.x.44] ).
* 

*  [1.x.51] [1.x.52]
*   [1.x.53]  [1.x.54]
* 

* 
*  

* 
* [1.x.55]
* 
*  From the following include file we import the ParticleHandler class that allows you to manage a collection of particles (objects of type  [2.x.45]  representing a collection of points with some attached properties (e.g., an id) floating on a  [2.x.46]  The methods and classes in the namespace Particles allows one to easily implement Particle-In-Cell methods and particle tracing on distributed triangulations:
* 

* 
* [1.x.56]
* 
*  We import the particles generator which allow us to insert the particles. In the present step, the particle are globally inserted using a non-matching hyper-shell triangulation:
* 

* 
* [1.x.57]
* 
*  Since the particles do not form a triangulation, they have their own specific DataOut class which will enable us to write them to commonly used parallel vtu format (or any number of other file formats):
* 

* 
* [1.x.58]
* 
*   [1.x.59]  [1.x.60]
* 

* 
*  Similarly to what is done in  [2.x.47] , we set up a class that holds all the parameters of our problem and derive it from the ParameterAcceptor class to simplify the management and creation of parameter files.   
*   The ParameterAcceptor paradigm requires all parameters to be writable by the ParameterAcceptor methods. In order to avoid bugs that would be very difficult to track down (such as writing things like `if (time = 0)` instead of `if(time == 0)`), we declare all the parameters in an external class, which is initialized before the actual `ParticleTracking` class, and pass it to the main class as a `const` reference.   
*   The constructor of the class is responsible for the connection between the members of this class and the corresponding entries in the ParameterHandler. Thanks to the use of the  [2.x.48]  method, this connection is trivial, but requires all members of this class to be writable.
* 

* 
* [1.x.61]
* 
*  This class consists largely of member variables that describe the details of the particle tracking simulation and its discretization. The following parameters are about where output should written to, the spatial discretization of the velocity (the default is  [2.x.49] ), the time step and the output frequency (how many time steps should elapse before we generate graphical output again):
* 

* 
* [1.x.62]
* 
*  We allow every grid to be refined independently. In this tutorial, no physics is resolved on the fluid grid, and its velocity is calculated analytically.
* 

* 
* [1.x.63]
* 
*  There remains the task of declaring what run-time parameters we can accept in input files. Since we have a very limited number of parameters, all parameters are declared in the same section.
* 

* 
* [1.x.64]
* 
*   [1.x.65]  [1.x.66]
* 

* 
*  The velocity profile is provided as a Function object. This function is hard-coded within the example.
* 

* 
* [1.x.67]
* 
*  The velocity profile for the Rayleigh-Kothe vertex is time-dependent. Consequently, the current time in the simulation (t) must be gathered from the Function object.
* 

* 
* [1.x.68]
* 
*   [1.x.69]  [1.x.70]
* 

* 
*  We are now ready to introduce the main class of our tutorial program.
* 

* 
* [1.x.71]
* 
*  This function is responsible for the initial generation of the particles on top of the background grid.
* 

* 
* [1.x.72]
* 
*  When the velocity profile is interpolated to the position of the particles, it must first be stored using degrees of freedom. Consequently, as is the case for other parallel case (e.g.  [2.x.50] ) we initialize the degrees of freedom on the background grid.
* 

* 
* [1.x.73]
* 
*  In one of the test cases, the function is mapped to the background grid and a finite element interpolation is used to calculate the velocity at the particle location. This function calculates the value of the function at the support point of the triangulation.
* 

* 
* [1.x.74]
* 
*  The next two functions are responsible for carrying out step of explicit Euler time integration for the cases where the velocity field is interpolated at the positions of the particles or calculated analytically, respectively.
* 

* 
* [1.x.75]
* 
*  The `cell_weight()` function indicates to the triangulation how much computational work is expected to happen on this cell, and consequently how the domain needs to be partitioned so that every MPI rank receives a roughly equal amount of work (potentially not an equal number of cells). While the function is called from the outside, it is connected to the corresponding signal from inside this class, therefore it can be `private`.
* 

* 
* [1.x.76]
* 
*  The following two functions are responsible for outputting the simulation results for the particles and for the velocity profile on the background mesh, respectively.
* 

* 
* [1.x.77]
* 
*  The private members of this class are similar to other parallel deal.II examples. The parameters are stored as a `const` member. It is important to note that we keep the `Vortex` class as a member since its time must be modified as the simulation proceeds.
* 

* 
*  

* 
* [1.x.78]
* 
*   [1.x.79]  [1.x.80]
* 

* 
*   [1.x.81]  [1.x.82]
* 

* 
*  The constructors and destructors are rather trivial. They are very similar to what is done in  [2.x.51] . We set the processors we want to work on to all machines available (`MPI_COMM_WORLD`) and initialize the  [2.x.52]  variable to only allow processor zero to output anything to the standard output.
* 

* 
*  

* 
* [1.x.83]
* 
*   [1.x.84]  [1.x.85]
* 

* 
*  This function is the key component that allow us to dynamically balance the computational load for this example. The function attributes a weight to every cell that represents the computational work on this cell. Here the majority of work is expected to happen on the particles, therefore the return value of this function (representing "work for this cell") is calculated based on the number of particles in the current cell. The function is connected to the cell_weight() signal inside the triangulation, and will be called once per cell, whenever the triangulation repartitions the domain between ranks (the connection is created inside the generate_particles() function of this class).
* 

* 
* [1.x.86]
* 
*  We do not assign any weight to cells we do not own (i.e., artificial or ghost cells)
* 

* 
* [1.x.87]
* 
*  This determines how important particle work is compared to cell work (by default every cell has a weight of 1000). We set the weight per particle much higher to indicate that the particle load is the only one that is important to distribute the cells in this example. The optimal value of this number depends on the application and can range from 0 (cheap particle operations, expensive cell operations) to much larger than 1000 (expensive particle operations, cheap cell operations, like presumed in this example).
* 

* 
* [1.x.88]
* 
*  This example does not use adaptive refinement, therefore every cell should have the status `CELL_PERSIST`. However this function can also be used to distribute load during refinement, therefore we consider refined or coarsened cells as well.
* 

* 
* [1.x.89]
* 
*   [1.x.90]  [1.x.91]
* 

* 
*  This function generates the tracer particles and the background triangulation on which these particles evolve.
* 

* 
* [1.x.92]
* 
*  We create a hyper cube triangulation which we globally refine. This triangulation covers the full trajectory of the particles.
* 

* 
* [1.x.93]
* 
*  In order to consider the particles when repartitioning the triangulation the algorithm needs to know three things:     
*   1. How much weight to assign to each cell (how many particles are in there); 2. How to pack the particles before shipping data around; 3. How to unpack the particles after repartitioning.     
*   We attach the correct functions to the signals inside  [2.x.53]  These signal will be called every time the repartition() function is called. These connections only need to be created once, so we might as well have set them up in the constructor of this class, but for the purpose of this example we want to group the particle related instructions.
* 

* 
* [1.x.94]
* 
*  This initializes the background triangulation where the particles are living and the number of properties of the particles.
* 

* 
* [1.x.95]
* 
*  We create a particle triangulation which is solely used to generate the points which will be used to insert the particles. This triangulation is a hyper shell which is offset from the center of the simulation domain. This will be used to generate a disk filled with particles which will allow an easy monitoring of the motion due to the vortex.
* 

* 
* [1.x.96]
* 
*  We generate the necessary bounding boxes for the particles generator. These bounding boxes are required to quickly identify in which process's subdomain the inserted particle lies, and which cell owns it.
* 

* 
* [1.x.97]
* 
*  We generate an empty vector of properties. We will attribute the properties to the particles once they are generated.
* 

* 
* [1.x.98]
* 
*  We generate the particles at the position of a single point quadrature. Consequently, one particle will be generated at the centroid of each cell.
* 

* 
* [1.x.99]
* 
*   [1.x.100]  [1.x.101]
* 

* 
*  This function sets up the background degrees of freedom used for the velocity interpolation and allocates the field vector where the entire solution of the velocity field is stored.
* 

* 
* [1.x.102]
* 
*  This function takes care of interpolating the vortex velocity field to the field vector. This is achieved rather easily by using the  [2.x.54]  function.
* 

* 
* [1.x.103]
* 
*   [1.x.104]  [1.x.105]
* 

* 
*  We integrate the particle trajectories using an analytically defined velocity field. This demonstrates a relatively trivial usage of the particles.
* 

* 
* [1.x.106]
* 
*  Looping over all particles in the domain using a particle iterator
* 

* 
* [1.x.107]
* 
*  We calculate the velocity of the particles using their current location.
* 

* 
* [1.x.108]
* 
*  This updates the position of the particles and sets the old position equal to the new position of the particle.
* 

* 
* [1.x.109]
* 
*  We store the processor id (a scalar) and the particle velocity (a vector) in the particle properties. In this example, this is done purely for visualization purposes.
* 

* 
* [1.x.110]
* 
*  In contrast to the previous function in this function we integrate the particle trajectories by interpolating the value of the velocity field at the degrees of freedom to the position of the particles.
* 

* 
* [1.x.111]
* 
*  We loop over all the local particles. Although this could be achieved directly by looping over all the cells, this would force us to loop over numerous cells which do not contain particles. Rather, we loop over all the particles, but, we get the reference of the cell in which the particle lies and then loop over all particles within that cell. This enables us to gather the values of the velocity out of the `velocity_field` vector once and use them for all particles that lie within the cell.
* 

* 
* [1.x.112]
* 
*  Next, compute the velocity at the particle locations by evaluating the finite element solution at the position of the particles. This is essentially an optimized version of the particle advection functionality in step 19, but instead of creating quadrature objects and FEValues objects for each cell, we do the evaluation by hand, which is somewhat more efficient and only matters for this tutorial, because the particle work is the dominant cost of the whole program.
* 

* 
* [1.x.113]
* 
*  Again, we store the particle velocity and the processor id in the particle properties for visualization purposes.
* 

* 
* [1.x.114]
* 
*   [1.x.115]  [1.x.116]
* 

* 
*  The next two functions take care of writing both the particles and the background mesh to vtu with a pvtu record. This ensures that the simulation results can be visualized when the simulation is launched in parallel.
* 

* 
* [1.x.117]
* 
*  Attach the solution data to data_out object
* 

* 
* [1.x.118]
* 
*   [1.x.119]  [1.x.120] This function orchestrates the entire simulation. It is very similar to the other time dependent tutorial programs
* 
*  -  take  [2.x.55]  or  [2.x.56]  as an example. Note that we use the DiscreteTime class to monitor the time, the time-step and the  [2.x.57] number. This function is relatively straightforward.
* 

* 
*  

* 
* [1.x.121]
* 
*  We set the initial property of the particles by doing an explicit Euler iteration with a time-step of 0 both in the case of the analytical and the interpolated approach.
* 

* 
* [1.x.122]
* 
*  The particles are advected by looping over time.
* 

* 
* [1.x.123]
* 
*  After the particles have been moved, it is necessary to identify in which cell they now reside. This is achieved by calling  [2.x.58] 
* 

* 
* [1.x.124]
* 
*   [1.x.125]  [1.x.126]
* 

* 
*  The remainder of the code, the `main()` function, is standard. We note that we run the particle tracking with the analytical velocity and the interpolated velocity and produce both results
* 

* 
* [1.x.127]
* [1.x.128][1.x.129]
* 

* The directory in which this program is run contains an example parameter file by default.If you do not specify a parameter file as an argument on the commandline, the program will try to read the file "parameters.prm" by default, andwill execute the code.
* On any number of cores, the simulation output will look like:
* [1.x.130]
* 
* We note that, by default, the simulation runs the particle tracking withan analytical velocity for 2000 iterations, then restarts from the beginning and runs the particle tracking withvelocity interpolation for the same duration. The results are written every10th iteration.
* [1.x.131][1.x.132]
* 

* The following animation displays the trajectory of the particles as theyare advected by the flow field. We see that after the complete duration of theflow, the particle go back to their initial configuration as is expected.
* [1.x.133]
* 
* [1.x.134][1.x.135]
* 

* The following animation shows the impact of dynamic load balancing. We clearlysee that the subdomains adapt themselves to balance the number of particles persubdomain. However, a perfect load balancing is not reached, in part due tothe coarseness of the background mesh.
* [1.x.136]
* 
* 

* [1.x.137][1.x.138]
* 

* This program highlights some of the main capabilities for handling particles in deal.II, notably theircapacity to be used in distributed parallel simulations. However, this step couldbe extended in numerous manners:
* 
*  - High-order time integration (for example using a Runge-Kutta 4 method) could beused to increase the accuracy or allow for larger time-step sizes with the same accuracy.
* 
*  - The full equation of motion (with inertia) could be solved for the particles. Inthis case the particles would need to have additional properties such as their mass,as in  [2.x.59] , and if one wanted to also consider interactions with the fluid, their diameter.
* 
*  - Coupling to a flow solver. This step could be straightforwardly coupled to any parallelprogram in which the Stokes ( [2.x.60] ,  [2.x.61] ) or the Navier-Stokes equations are solved (e.g.,  [2.x.62] ).
* 
*  - Computing the difference in final particle positions between the two modelswould allow to quantify the influence of the interpolation error on particle motion.
* 

* [1.x.139][1.x.140] [2.x.63] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-69_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] ,  [2.x.2] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31]
* [1.x.32]
*  [2.x.3]  [2.x.4] Sandia National Laboratories is a multimission laboratorymanaged and operated by National Technology & Engineering Solutions of Sandia,LLC, a wholly owned subsidiary of Honeywell International Inc., for the U.S.Department of Energy's National Nuclear Security Administration under contractDE-NA0003525. This document describes objective technical results and analysis.Any subjective views or opinions that might be expressed in the paper do notnecessarily represent the views of the U.S. Department of Energy or the UnitedStates Government. [2.x.5] 
*  [2.x.6]  This tutorial step implements a first-order accurate [1.x.33] based on a first-order [1.x.34]for solving Euler's equations of gas dynamics  [2.x.7] . Assuch it is presented primarily for educational purposes. For actualresearch computations you might want to consider exploring a corresponding[1.x.35] that uses [1.x.36] techniques, and strong stability-preserving (SSP) timeintegration, see  [2.x.8] ([1.x.37]).
*  [2.x.9] 
* [1.x.38][1.x.39][1.x.40]
* 

* This tutorial presents a first-order scheme for solving compressibleEuler's equations that is based on three ingredients: a[1.x.41]-type discretization of Euler's equations in the contextof finite elements; a graph-viscosity stabilization based on a[1.x.42] upper bound of the local wave speed; and explicittime-stepping. As such, the ideas and techniques presented in this tutorialstep are drastically different from those used in  [2.x.10] , which focuses onthe use of automatic differentiation. From a programming perspective thistutorial will focus on a number of techniques found in large-scalecomputations: hybrid thread-MPI parallelization; efficient local numberingof degrees of freedom; concurrent post-processing and write-out of resultsusing worker threads; as well as checkpointing and restart.
* It should be noted that first-order schemes in the context of hyperbolicconservation laws require prohibitively many degrees of freedom to resolvecertain key features of the simulated fluid, and thus, typically only serveas elementary building blocks in higher-order schemes [2.x.11] . However, we hope that the reader still finds thetutorial step to be a good starting point (in particular with respect tothe programming techniques) before jumping into full research codes such asthe second-order scheme discussed in  [2.x.12] .
* 

* [1.x.43][1.x.44][1.x.45]
* 

* The compressible Euler's equations of gas dynamics are written inconservative form as follows:
* [1.x.46]
* where  [2.x.13] , and  [2.x.14] , and  [2.x.15]  is the spacedimension. We say that  [2.x.16]  is the state and [2.x.17]  is the flux ofthe system. In the case of Euler's equations the state is given by [2.x.18] : where  [2.x.19] denotes the density,  [2.x.20]  is the momentum, and  [2.x.21]  is the total energy of the system. The flux of the system [2.x.22]  is defined as
* [1.x.47]
* where  [2.x.23]  is the identity matrix and [2.x.24]  denotes the tensor product. Here, we have introduced the pressure [2.x.25]  that, in general, is defined by a closed-form equation of state.In this tutorial we limit the discussion to the class of polytropicideal gases for which the pressure is given by
* [1.x.48]
* where the factor  [2.x.26]  denotes the [1.x.49].
* 

* [1.x.50][1.x.51]
* 

* Hyperbolic conservation laws, such as
* [1.x.52]
* pose a significant challenge with respect to solution theory. An evidentobservation is that rewriting the equation in variational form and testing withthe solution itself does not lead to an energy estimate because the pairing [2.x.27]  (understood asthe  [2.x.28]  inner product or duality pairing) is not guaranteed to benon-negative. Notions such as energy-stability or  [2.x.29] -stability are(in general) meaningless in this context.
* Historically, the most fruitful step taken in order to deepen theunderstanding of hyperbolic conservation laws was to assume that thesolution is formally defined as  [2.x.30]  where  [2.x.31]  is the solutionof the parabolic regularization
* [1.x.53]
* Such solutions, which are understood as the solution recovered in thezero-viscosity limit, are often referred to as [1.x.54].(This is, because physically  [2.x.32]  can be understood as related to the viscosity of thefluid, i.e., a quantity that indicates the amount of friction neighboring gas particles moving atdifferent speeds exert on each other. The Euler equations themselves are derived underthe assumption of no friction, but can physically be expected to describe the limitingcase of vanishing friction or viscosity.)Global existence and uniqueness of such solutions is an open issue.However, we know at least that if such viscosity solutions exists they haveto satisfy the constraint  [2.x.33]  forall  [2.x.34]  and  [2.x.35]  where
* [1.x.55]
* Here,  [2.x.36]  denotes the specific entropy
* [1.x.56]
* We will refer to  [2.x.37]  as the invariant set of Euler's equations.In other words, a state  [2.x.38]  obeyspositivity of the density, positivity of the internal energy, and a localminimum principle on the specific entropy. This condition is a simplifiedversion of a class of pointwise stability constraints satisfied by theexact (viscosity) solution. By pointwise we mean that the constraint has tobe satisfied at every point of the domain, not just in an averaged(integral, or high order moments) sense.
* In context of a numerical approximation, a violation of such a constrainthas dire consequences: it almost surely leads to catastrophic failure ofthe numerical scheme, loss of hyperbolicity, and overall, loss ofwell-posedness of the (discrete) problem. It would also mean that we have computedsomething that can not be interpreted physically. (For example, what are we to makeof a computed solution with a negative density?) In the following we willformulate a scheme that ensures that the discrete approximation of [2.x.39]  remains in  [2.x.40] .
* 

* [1.x.57][1.x.58]
* 

* Following  [2.x.41] ,  [2.x.42] ,  [2.x.43] , and  [2.x.44] , at this point it might looktempting to base a discretization of Euler's equations on a (semi-discrete)variational formulation:
* [1.x.59]
* Here,  [2.x.45]  is an appropriate finite element space, and [2.x.46]  is some linear stabilization method(possibly complemented with some ad-hoc shock-capturing technique, see forinstance Chapter 5 of  [2.x.47]  and references therein). Mosttime-dependent discretization approaches described in the deal.II tutorialsare based on such a (semi-discrete) variational approach. Fundamentally,from an analysis perspective, variational discretizations are conceivedto provide some notion of global (integral) stability, meaning anestimate of the form
* [1.x.60]
* holds true, where  [2.x.48]  could represent the [2.x.49] -norm or, more generally, some discrete (possibly meshdependent) energy-norm. Variational discretizations of hyperbolicconservation laws have been very popular since the mid eighties, inparticular combined with SUPG-type stabilization and/or upwindingtechniques (see the early work of  [2.x.50]  and  [2.x.51] ). Theyhave proven to be some of the best approaches for simulations in the subsonicshockless regime and similarly benign situations.
* <!-- In particular, tutorial  [2.x.52]  focuses on Euler's equation of gasdynamics in the subsonic regime using dG techniques.
* 
*  - >
* However, in the transonic and supersonic regimes, and shock-hydrodynamicsapplications the use of variational schemes might be questionable. In fact,at the time of this writing, most shock-hydrodynamics codes are stillfirmly grounded on finite volume methods. The main reason for failure ofvariational schemes in such extreme regimes is the lack of pointwisestability. This stems from the fact that [1.x.61] bounds onintegrated quantities (e.g. integrals of moments) have in general noimplications on pointwise properties of the solution. While some of theseproblems might be alleviated by the (perpetual) chase of the right shockcapturing scheme, finite difference-like and finite volume schemes stillhave an edge in many regards.
* In this tutorial step we therefore depart from variational schemes. We willpresent a completely algebraic formulation (with the flavor of acollocation-type scheme) that preserves constraints pointwise, i.e.,
* [1.x.62]
* Contrary to finite difference/volume schemes, the scheme implemented inthis step maximizes the use of finite element software infrastructure,works on any mesh, in any space dimension, and is theoretically guaranteedto always work, all the time, no exception. This illustrates that deal.IIcan be used far beyond the context of variational schemes in Hilbert spacesand that a large number of classes, modules and namespaces from deal.II canbe adapted for such a purpose.
* 

* [1.x.63][1.x.64]
* 

* Let  [2.x.53]  be scalar-valued finite dimensional space spanned by abasis  [2.x.54]  where:  [2.x.55]  and  [2.x.56]  is the set of all indices (nonnegativeintegers) identifying each scalar Degree of Freedom (DOF) in the mesh.Therefore a scalar finite element functional  [2.x.57]  canbe written as  [2.x.58]  with  [2.x.59] . We introduce the notation for vector-valued approximationspaces  [2.x.60] . Let  [2.x.61] , then it can be written as  [2.x.62]  where  [2.x.63]  and  [2.x.64]  is a scalar-valued shape function.
*  [2.x.65]  We purposely refrain from using vector-valued finite elementspaces in our notation. Vector-valued finite element spacesare natural for variational formulations of PDE systems (e.g. Navier-Stokes).In such context, the interactions that have to be computed describe[1.x.65]: with proper renumbering of thevector-valued DoFHandler (i.e. initialized with an FESystem) it is possibleto compute the block-matrices (required in order to advance the solution)with relative ease. However, the interactions that have to be computed inthe context of time-explicit collocation-type schemes (such as finitedifferences and/or the scheme presented in this tutorial) can bebetter described as [1.x.66] (not between DOFs).In addition, in our case we do not solve a linear equation in order toadvance the solution. This leaves very little reason to use vector-valuedfinite element spaces both in theory and/or practice.
* We will use the usual Lagrange finite elements: let  [2.x.66]  denote the set of all support points (see  [2.x.67]  "thisglossary entry"), where  [2.x.68] . Then each index  [2.x.69]  uniquely identifies a support point  [2.x.70] , as well as ascalar-valued shape function  [2.x.71] . With this notation at hand we can definethe (explicit time stepping) scheme as:
* [1.x.67]
* where
* 

* 
* 

* 
* 
*  -  [2.x.72]     is the lumped mass matrix
* 

* 
* 

* 
* 
*  -  [2.x.73]  is the time step size
* 

* 
* 

* 
* 
*  -  [2.x.74]  (note that  [2.x.75] )    is a vector-valued matrix that was used to approximate the divergence    of the flux in a weak sense.
* 

* 
* 

* 
* 
*  -  [2.x.76]  is the adjacency list    containing all degrees of freedom coupling to the index  [2.x.77] . In other    words  [2.x.78]  contains all nonzero column indices for row    index i.  [2.x.79]  will also be called a "stencil".
* 

* 
* 

* 
* 
*  -  [2.x.80]  is the flux  [2.x.81]  of the    hyperbolic system evaluated for the state  [2.x.82]  associated    with support point  [2.x.83] .
* 

* 
* 

* 
* 
*  -  [2.x.84]  if  [2.x.85]  is the so    called [1.x.68]. The graph viscosity serves as a    stabilization term, it is somewhat the discrete counterpart of     [2.x.86]  that appears in the notion of viscosity    solution described above. We will base our construction of  [2.x.87]  on    an estimate of the maximal local wavespeed  [2.x.88]  that    will be explained in detail in a moment.
* 

* 
* 

* 
* 
*  - the diagonal entries of the viscosity matrix are defined as     [2.x.89] .
* 

* 
* 

* 
* 
*  -  [2.x.90]  is a    normalization of the  [2.x.91]  matrix that enters the    approximate Riemann solver with which we compute an the approximations     [2.x.92]  on the local wavespeed. (This will be explained    further down below).
* The definition of  [2.x.93]  is far from trivial and we will postpone the precisedefinition in order to focus first on some algorithmic and implementationquestions. We note that
* 

* 
* 

* 
* 
*  -  [2.x.94]  and  [2.x.95]  do not evolve in time (provided we keep the    discretization fixed). It thus makes sense to assemble these    matrices/vectors once in a so called [1.x.69] and reuse    them in every time step. They are part of what we are going to call    off-line data.
* 

* 
* 

* 
* 
*  - At every time step we have to evaluate  [2.x.96]  and     [2.x.97] , which will    constitute the bulk of the computational cost.
* Consider the following pseudo-code, illustrating a possible straightforward strategy for computing the solution  [2.x.98]  at a newtime  [2.x.99]  given a known state  [2.x.100]  at time [2.x.101] :
* [1.x.70]
* 
* We note here that:
* 
*  - This "assembly" does not require any form of quadrature or cell-loops.
* 
*  - Here  [2.x.102]  and  [2.x.103]  are a global matrix and a global vectorcontaining all the vectors  [2.x.104]  and all the states [2.x.105]  respectively.
* 
*  -  [2.x.106] ,  [2.x.107] , and [2.x.108]  are hypothetical implementations thateither collect (from) or write (into) global matrices and vectors.
* 
*  - If we assume a Cartesian mesh in two spacedimensions, first-order polynomial space  [2.x.109] , and that [2.x.110]  is an interior node (i.e.  [2.x.111]  is not on the boundaryof the domain) then:  [2.x.112]  should containnine state vector elements (i.e. all the states in the patch/macro elementassociated to the shape function  [2.x.113] ). This is one of the majordifferences with the usual cell-based loop where the gather functionality(encoded in FEValuesBase<dim, spacedim>.get_function_values() in the caseof deal.II) only collects values for the local cell (just a subset of thepatch).
* The actual implementation will deviate from above code in one key aspect:the time-step size  [2.x.114]  has to be chosen subject to a CFL condition
* [1.x.71]
* where  [2.x.115]  is a chosen constant. This will require tocompute all  [2.x.116]  in a separate step prior to actually performing aboveupdate. The core principle remains unchanged, though: we do not loop overcells but rather over all edges of the sparsity graph.
*  [2.x.117]  It is not uncommon to encounter such fully-algebraic schemes (i.e.no bilinear forms, no cell loops, and no quadrature) outside of the finiteelement community in the wider CFD community. There is a rich history ofapplication of this kind of schemes, also called [1.x.72] or[1.x.73] finite element schemes (see for instance [2.x.118]  for a historical overview). However, it is important tohighlight that the algebraic structure of the scheme (presented in thistutorial) and the node-loops are not just a performance gimmick. Actually, thestructure of this scheme was born out of theoretical necessity: the proof ofpointwise stability of the scheme hinges on the specific algebraic structure ofthe scheme. In addition, it is not possible to compute the algebraicviscosities  [2.x.119]  using cell-loops since they depend nonlinearly oninformation that spans more than one cell (superposition does not hold: addingcontributions from separate cells does not lead to the right result).
* [1.x.74][1.x.75]
* 

* In the example considered in this tutorial step we use three different types ofboundary conditions: essential-like boundary conditions (we prescribe astate at the left boundary of our domain), outflow boundary conditions(also called "do-nothing" boundary conditions) at the right boundary of thedomain, and "reflecting" boundary conditions  [2.x.120]  (also called "slip" boundary conditions) at the top,bottom, and surface of the obstacle. We will not discuss much aboutessential and "do-nothing" boundary conditions since their implementationis relatively easy and the reader will be able to pick-up theimplementation directly from the (documented) source code. In this portionof the introduction we will focus only on the "reflecting" boundaryconditions which are somewhat more tricky.
*  [2.x.121]  At the time of this writing (early 2020) it is not unreasonable to saythat both analysis and implementation of stable boundary conditions forhyperbolic systems of conservation laws is an open issue. For the case ofvariational formulations, stable boundary conditions are those leading to awell-posed (coercive) bilinear form. But for general hyperbolicsystems of conservation laws (and for the algebraic formulation used in thistutorial) coercivity has no applicability and/or meaning as a notion ofstability. In this tutorial step we will use preservation of the invariant setas our main notion of stability which (at the very least) guaranteeswell-posedness of the discrete problem.
* For the case of the reflecting boundary conditions we will proceed as follows:
* 
*  - For every time step advance in time satisfying no boundary condition at all.
* 
*  - Let  [2.x.122]  be the portion of the boundary where we want to  enforce reflecting boundary conditions. At the end of the time step we enforce  reflecting boundary conditions strongly in a post-processing step where we  execute the projection   
* [1.x.76]
*   that removes the normal component of  [2.x.123] . This is a somewhat  naive idea that preserves a few fundamental properties of the PDE as we  explain below.
* This is approach is usually called "explicit treatment of boundary conditions".The well seasoned finite element person might find this approach questionable.No doubt, when solving parabolic, or elliptic equations, we typically enforceessential (Dirichlet-like) boundary conditions by making them part of theapproximation space  [2.x.124] , and treat natural (e.g. Neumann) boundaryconditions as part of the variational formulation. We also know that explicittreatment of boundary conditions (in the context of parabolic PDEs) almostsurely leads to catastrophic consequences. However, in the context of nonlinearhyperbolic equations we have that:
* 
*  - It is relatively easy to prove that (for the case of reflecting boundaryconditions) explicit treatment of boundary conditions is not only conservativebut also guarantees preservation of the property  [2.x.125] for all  [2.x.126]  (well-posedness). This is perhaps the mostimportant reason to use explicit enforcement of boundary conditions.
* 
*  - To the best of our knowledge: we are not aware of any mathematical resultproving that it is possible to guarantee the property  [2.x.127]  for all  [2.x.128]  when using either direct enforcement ofboundary conditions into the approximation space, or weak enforcement using theNitsche penalty method (which is for example widely used in discontinuousGalerkin schemes). In addition, some of these traditional ideas lead to quiterestrictive time step constraints.
* 
*  - There is enough numerical evidence suggesting that explicit treatment ofDirichlet-like boundary conditions is stable under CFL conditions and does notintroduce any loss in accuracy.
* If  [2.x.129] represents Euler's equation with reflecting boundary conditions on the entiretyof the boundary (i.e.  [2.x.130] ) and weintegrate in space and time  [2.x.131]  we would obtain
* [1.x.77]
* Note that momentum is NOT a conserved quantity (interaction with walls leads tomomentum gain/loss): however  [2.x.132]  has to satisfy a momentum balance.Even though we will not use reflecting boundary conditions in the entirety ofthe domain, we would like to know that our implementation of reflectingboundary conditions is consistent with the conservation properties mentionedabove. In particular, if we use the projection  [2.x.133]  in theentirety of the domain the following discrete mass-balance can be guaranteed:
* [1.x.78]
* where  [2.x.134]  is the pressure at the nodes that lie at the boundary. Clearly [2.x.135]  is the discrete counterpart of  [2.x.136] . Theproof of identity  [2.x.137]  is omitted, but we briefly mention thatit hinges on the definition of the [1.x.79] [2.x.138]  provided in  [2.x.139] . We also note thatthis enforcement of reflecting boundary conditions is different from the oneoriginally advanced in  [2.x.140] .
* 

*  [1.x.80] [1.x.81]
*   [1.x.82]  [1.x.83]
* 

* 
*  The set of include files is quite standard. The most intriguing part is the fact that we will rely solely on deal.II data structures for MPI parallelization, in particular  [2.x.141]  and  [2.x.142]  included through  [2.x.143]  and  [2.x.144] . Instead of a Trilinos, or PETSc specific matrix class, we will use a non-distributed  [2.x.145]  ( [2.x.146] ) to store the local part of the  [2.x.147] ,  [2.x.148]  and  [2.x.149]  matrices.
* 

* 
* [1.x.84]
* 
*  In addition to above deal.II specific includes, we also include four boost headers. The first two are for binary archives that we will use for implementing a check-pointing and restart mechanism.
* 

* 
* [1.x.85]
* 
*  The last two header files are for creating custom iterator ranges over integer intervals.
* 

* 
* [1.x.86]
* 
*  For  [2.x.150]   [2.x.151]   [2.x.152]   [2.x.153]  and  [2.x.154] 
* 

* 
* [1.x.87]
* 
*   [1.x.88]  [1.x.89]
* 

* 
*  We begin our actual implementation by declaring all classes with their data structures and methods upfront. In contrast to previous example steps we use a more fine-grained encapsulation of concepts, data structures, and parameters into individual classes. A single class thus usually centers around either a single data structure (such as the Triangulation) in the  [2.x.155]  class, or a single method (such as the  [2.x.156]  function of the  [2.x.157]  class). We typically declare parameter variables and scratch data object `private` and make methods and data structures used by other classes `public`.
* 

* 
* 

* 
*  [2.x.158]  A cleaner approach would be to guard access to all data structures by [1.x.90]. For the sake of brevity, we refrain from that approach, though.
* 

* 
*  We also note that the vast majority of classes is derived from ParameterAcceptor. This facilitates the population of all the global parameters into a single (global) ParameterHandler. More explanations about the use of inheritance from ParameterAcceptor as a global subscription mechanism can be found in  [2.x.159] .
* 

* 
* [1.x.91]
* 
*  We start with defining a number of  [2.x.160]  constants used throughout the tutorial step. This allows us to refer to boundary types by a mnemonic (such as  [2.x.161] ) rather than a numerical value.
* 

* 
*  

* 
* [1.x.92]
* 
*   [1.x.93]  [1.x.94]   
*   The class  [2.x.162]  contains all data structures concerning the mesh (triangulation) and discretization (mapping, finite element, quadrature) of the problem. As mentioned, we use the ParameterAcceptor class to automatically populate problem-specific parameters, such as the geometry information ( [2.x.163] , etc.) or the refinement level ( [2.x.164] ) from a parameter file. This requires us to split the initialization of data structures into two functions: We initialize everything that does not depend on parameters in the constructor, and defer the creation of the mesh to the  [2.x.165]  method that can be called once all parameters are read in via  [2.x.166] 
* 

* 
* [1.x.95]
* 
*   [1.x.96]  [1.x.97]   
*   The class  [2.x.167]  contains pretty much all components of the discretization that do not evolve in time, in particular, the DoFHandler, SparsityPattern, boundary maps, the lumped mass matrix,  [2.x.168]  and  [2.x.169]  matrices. Here, the term [1.x.98] refers to the fact that all the class members of  [2.x.170]  have well-defined values independent of the current time step. This means that they can be initialized ahead of time (at [1.x.99]) and are not meant to be modified at any later time step. For instance, the sparsity pattern should not change as we advance in time (we are not doing any form of adaptivity in space). Similarly, the entries of the lumped mass matrix should not be modified as we advance in time either.   
*   We also compute and store a  [2.x.171]  that contains a map from a global index of type  [2.x.172]  of a boundary degree of freedom to a tuple consisting of a normal vector, the boundary id, and the position associated with the degree of freedom. We have to compute and store this geometric information in this class because we won't have access to geometric (or cell-based) information later on in the algebraic loops over the sparsity pattern.   
*  

* 
*  [2.x.173]  Even though this class currently does not have any parameters that could be read in from a parameter file we nevertheless derive from ParameterAcceptor and follow the same idiom of providing a  [2.x.174] ) method as for the class Discretization.
* 

* 
* [1.x.100]
* 
*   [1.x.101]  [1.x.102]   
*   The member functions of this class are utility functions and data structures specific to Euler's equations:
* 

* 
* 
*  - The type alias  [2.x.175]  is used for the states  [2.x.176] 
* 

* 
* 
*  - The type alias  [2.x.177]  is used for the fluxes  [2.x.178] .
* 

* 
* 
*  - The  [2.x.179]  function extracts  [2.x.180]  out of the state vector  [2.x.181]  and stores it in a  [2.x.182] .
* 

* 
* 
*  - The  [2.x.183]  function computes  [2.x.184]  from a given state vector  [2.x.185] .   
*   The purpose of the class members  [2.x.186] ,  [2.x.187]  is evident from their names. We also provide a function  [2.x.188] , that computes the wave speed estimate mentioned above,  [2.x.189] , which is used in the computation of the  [2.x.190]  matrix.   
*  

* 
*  [2.x.191]  The  [2.x.192]  macro expands to a (compiler specific) pragma that ensures that the corresponding function defined in this class is always inlined, i.e., the function body is put in place for every invocation of the function, and no call (and code indirection) is generated. This is stronger than the  [2.x.193]  keyword, which is more or less a (mild) suggestion to the compiler that the programmer thinks it would be beneficial to inline the function.  [2.x.194]  should only be used rarely and with caution in situations such as this one, where we actually know (due to benchmarking) that inlining the function in question improves performance.   
*   Finally, we observe that this is the only class in this tutorial step that is tied to a particular "physics" or "hyperbolic conservation law" (in this case Euler's equations). All the other classes are primarily "discretization" classes, very much agnostic of the particular physics being solved.
* 

* 
* [1.x.103]
* 
*   [1.x.104]  [1.x.105]   
*   The class  [2.x.195] 's only public data attribute is a  [2.x.196]   [2.x.197]  that computes the initial state of a given point and time. This function is used for populating the initial flow field as well as setting Dirichlet boundary conditions (at inflow boundaries) explicitly in every time step.   
*   For the purpose of this example step we simply implement a homogeneous uniform flow field for which the direction and a 1D primitive state (density, velocity, pressure) are read from the parameter file.   
*   It would be desirable to initialize the class in a single shot: initialize/set the parameters and define the class members that depend on these default parameters. However, since we do not know the actual values for the parameters, this would be sort of meaningless and unsafe in general (we would like to have mechanisms to check the consistency of the input parameters). Instead of defining another  [2.x.198]  method to be called (by-hand) after the call to  [2.x.199]  we provide an "implementation" for the class member  [2.x.200]  which is automatically called when invoking  [2.x.201]  for every class that inherits from ParameterAceptor.
* 

* 
* [1.x.106]
* 
*  We declare a private callback function that will be wired up to the  [2.x.202]  signal.
* 

* 
* [1.x.107]
* 
*   [1.x.108]  [1.x.109]   
*   With the  [2.x.203]  classes at hand we can now implement the explicit time-stepping scheme that was introduced in the discussion above. The main method of the  [2.x.204]  class is <code>make_one_step(vector_type &U, double t)</code> that takes a reference to a state vector  [2.x.205]  (as input arguments) computes the updated solution, stores it in the vector  [2.x.206] , and returns the chosen  [2.x.207] size  [2.x.208] .   
*   The other important method is  [2.x.209]  which primarily sets the proper partition and sparsity pattern for the temporary vector  [2.x.210]  respectively.
* 

* 
* [1.x.110]
* 
*   [1.x.111]  [1.x.112]   
*   At its core, the Schlieren class implements the class member  [2.x.211] . The main purpose of this class member is to compute an auxiliary finite element field  [2.x.212] , that is defined at each node by [1.x.113] where  [2.x.213]  can in principle be any scalar quantity. In practice though, the density is a natural candidate, viz.  [2.x.214] . [1.x.114] postprocessing is a standard method for enhancing the contrast of a visualization inspired by actual experimental X-ray and shadowgraphy techniques of visualization. (See  [2.x.215]  for another example where we create a Schlieren plot.)
* 

* 
* [1.x.115]
* 
*   [1.x.116]  [1.x.117]   
*   Now, all that is left to do is to chain the methods implemented in the  [2.x.216] , and  [2.x.217]  classes together. We do this in a separate class  [2.x.218]  that contains an object of every class and again reads in a number of parameters with the help of the ParameterAcceptor class.
* 

* 
* [1.x.118]
* 
*   [1.x.119]  [1.x.120]
* 

* 
*   [1.x.121]  [1.x.122]
* 

* 
*  The first major task at hand is the typical triplet of grid generation, setup of data structures, and assembly. A notable novelty in this example step is the use of the ParameterAcceptor class that we use to populate parameter values: we first initialize the ParameterAcceptor class by calling its constructor with a string  [2.x.219]  denoting the correct subsection in the parameter file. Then, in the constructor body every parameter value is initialized to a sensible default value and registered with the ParameterAcceptor class with a call to  [2.x.220] 
* 

* 
* [1.x.123]
* 
*  Note that in the previous constructor we only passed the MPI communicator to the  [2.x.221]  but we still have not initialized the underlying geometry/mesh. As mentioned earlier, we have to postpone this task to the  [2.x.222]  function that gets called after the  [2.x.223]  function has populated all parameter variables with the final values read from the parameter file.   
*   The  [2.x.224]  function is the last class member that has to be implemented. It creates the actual triangulation that is a benchmark configuration consisting of a channel with a disk obstacle, see  [2.x.225] . We construct the geometry by modifying the mesh generated by  [2.x.226]  We refer to  [2.x.227] ,  [2.x.228] , and  [2.x.229]  for an overview how to create advanced meshes. We first create 4 temporary (non distributed) coarse triangulations that we stitch together with the  [2.x.230]  function. We center the disk at  [2.x.231]  with a diameter of  [2.x.232] . The lower left corner of the channel has coordinates ( [2.x.233] ) and the upper right corner has ( [2.x.234] ,  [2.x.235] ).
* 

* 
* [1.x.124]
* 
*  We have to fix up the left edge that is currently located at  [2.x.236]  [2.x.237]  and has to be shifted to  [2.x.238]  [2.x.239] . As a last step the boundary has to be colorized with  [2.x.240]  on the right,  [2.x.241]  on the upper and lower outer boundaries and the obstacle.
* 

* 
*  

* 
* [1.x.125]
* 
*   [1.x.126]  [1.x.127]
* 

* 
*  Not much is done in the constructor of  [2.x.242]  other than initializing the corresponding class members in the initialization list.
* 

* 
* [1.x.128]
* 
*  Now we can initialize the DoFHandler, extract the IndexSet objects for locally owned and locally relevant DOFs, and initialize a  [2.x.243]  object that is needed for distributed vectors.
* 

* 
* [1.x.129]
* 
*   [1.x.130]  [1.x.131]
* 

* 
*  We are now in a position to create the sparsity pattern for our matrices. There are quite a few peculiarities that need a detailed explanation. We avoid using a distributed matrix class (as for example provided by Trilinos or PETSc) and instead rely on deal.II's own SparseMatrix object to store the local part of all matrices. This design decision is motivated by the fact that (a) we actually never perform a matrix-vector multiplication, and (b) we can always assemble the local part of a matrix exclusively on a given MPI rank. Instead, we will compute nonlinear updates while iterating over (the local part) of a connectivity stencil; a task for which deal.II's own SparsityPattern is specifically optimized for.     
*   This design consideration has a caveat, though. What makes the deal.II SparseMatrix class fast is the [1.x.132] used in the SparsityPattern (see  [2.x.244] ). This, unfortunately, does not play nicely with a global distributed index range because a sparsity pattern with CSR cannot contain "holes" in the index range. The distributed matrices offered by deal.II avoid this by translating from a global index range into a contiguous local index range. But this is precisely the type of index manipulation we want to avoid in our iteration over the stencil because it creates a measurable overhead.     
*   The  [2.x.245]  class already implements the translation from a global index range to a contiguous local (per MPI rank) index range: we don't have to reinvent the wheel. We just need to use that translation capability (once and only once) in order to create a "local" sparsity pattern for the contiguous index range  [2.x.246]  [2.x.247]  [2.x.248] . That capability can be invoked by the  [2.x.249]  function. Once the sparsity pattern is created using local indices, all that is left to do is to ensure that (when implementing our scatter and gather auxiliary functions) we always access elements of a distributed vector by a call to  [2.x.250]  This way we avoid index translations altogether and operate exclusively with local indices.
* 

* 
*  

* 
* [1.x.133]
* 
*  We have to create the "local" sparsity pattern by hand. We therefore loop over all locally owned and ghosted cells (see  [2.x.251]  GlossArtificialCell) and extract the (global)  [2.x.252]  associated with the cell DOFs and renumber them using  [2.x.253] .       
*  

* 
*  [2.x.254]  In the case of a locally owned dof, such renumbering consist of applying a shift (i.e. we subtract an offset) such that now they will become a number in the integer interval  [2.x.255]  [2.x.256]  [2.x.257] . However, in the case of a ghosted dof (i.e. not locally owned) the situation is quite different, since the global indices associated with ghosted DOFs will not be (in general) a contiguous set of integers.
* 

* 
*  

* 
* [1.x.134]
* 
*  This concludes the setup of the DoFHandler and SparseMatrix objects. Next, we have to assemble various matrices. We define a number of helper functions and data structures in an anonymous namespace.
* 

* 
*  

* 
* [1.x.135]
* 
*   [2.x.258]  class that will be used to assemble the offline data matrices using WorkStream. It acts as a container: it is just a struct where WorkStream stores the local cell contributions. Note that it also contains a class member  [2.x.259]  used to store the local contributions required to compute the normals at the boundary.
* 

* 
*  

* 
* [1.x.136]
* 
*  Next we introduce a number of helper functions that are all concerned about reading and writing matrix and vector entries. They are mainly motivated by providing slightly more efficient code and [1.x.137] for otherwise somewhat tedious code.
* 

* 
*  The first function we introduce,  [2.x.260] , will be used to read the value stored at the entry pointed by a SparsityPattern iterator  [2.x.261] . The function works around a small deficiency in the SparseMatrix interface: The SparsityPattern is concerned with all index operations of the sparse matrix stored in CRS format. As such the iterator already knows the global index of the corresponding matrix entry in the low-level vector stored in the SparseMatrix object. Due to the lack of an interface in the SparseMatrix for accessing the element directly with a SparsityPattern iterator, we unfortunately have to create a temporary SparseMatrix iterator. We simply hide this in the  [2.x.262]  function.
* 

* 
*  

* 
* [1.x.138]
* 
*  The  [2.x.263]  helper is the inverse operation of  [2.x.264] : Given an iterator and a value, it sets the entry pointed to by the iterator in the matrix.
* 

* 
*  

* 
* [1.x.139]
* 
*   [2.x.265] : we note that  [2.x.266] . If  [2.x.267]  then  [2.x.268] . Which basically implies that we need one matrix per space dimension to store the  [2.x.269]  vectors. Similar observation follows for the matrix  [2.x.270] . The purpose of  [2.x.271]  is to retrieve those entries and store them into a  [2.x.272]  for our convenience.
* 

* 
*  

* 
* [1.x.140]
* 
*   [2.x.273]  (first interface): this first function signature, having three input arguments, will be used to retrieve the individual components  [2.x.274]  of a matrix. The functionality of  [2.x.275]  and  [2.x.276]  is very much the same, but their context is different: the function  [2.x.277]  does not rely on an iterator (that actually knows the value pointed to) but rather on the indices  [2.x.278]  of the entry in order to retrieve its actual value. We should expect  [2.x.279]  to be slightly more expensive than  [2.x.280] . The use of  [2.x.281]  will be limited to the task of computing the algebraic viscosity  [2.x.282]  in the particular case that when both  [2.x.283]  and  [2.x.284]  lie at the boundary.     
*  

* 
*  [2.x.285]  The reader should be aware that accessing an arbitrary  [2.x.286]  entry of a matrix (say for instance Trilinos or PETSc matrices) is in general unacceptably expensive. Here is where we might want to keep an eye on complexity: we want this operation to have constant complexity, which is the case of the current implementation using deal.II matrices.
* 

* 
*  

* 
* [1.x.141]
* 
*   [2.x.287]  (second interface): this second function signature having two input arguments will be used to gather the state at a node  [2.x.288]  and return it as a  [2.x.289]  for our convenience.
* 

* 
*  

* 
* [1.x.142]
* 
*   [2.x.290] : this function has three input arguments, the first one is meant to be a "global object" (say a locally owned or locally relevant vector), the second argument which could be a  [2.x.291] , and the last argument which represents a index of the global object. This function will be primarily used to write the updated nodal values, stored as  [2.x.292] , into the global objects.
* 

* 
*  

* 
* [1.x.143]
* 
*  We are now in a position to assemble all matrices stored in  [2.x.293] : the lumped mass entries  [2.x.294] , the vector-valued matrices  [2.x.295]  and  [2.x.296] , and the boundary normals  [2.x.297] .   
*   In order to exploit thread parallelization we use the WorkStream approach detailed in the  [2.x.298]  "Parallel computing with multiple processors" accessing shared memory. As customary this requires definition of
* 

* 
* 
*  - Scratch data (i.e. input info required to carry out computations): in this case it is  [2.x.299] .
* 

* 
* 
*  - The worker: in our case this is the  [2.x.300]  function that actually computes the local (i.e. current cell) contributions from the scratch data.
* 

* 
* 
*  - A copy data: a struct that contains all the local assembly contributions, in this case  [2.x.301] .
* 

* 
* 
*  - A copy data routine: in this case it is  [2.x.302]  in charge of actually coping these local contributions into the global objects (matrices and/or vectors)   
*   Most of the following lines are spent in the definition of the worker  [2.x.303]  and the copy data routine  [2.x.304] . There is not much to say about the WorkStream framework since the vast majority of ideas are reasonably well-documented in  [2.x.305] ,  [2.x.306]  and  [2.x.307]  among others.   
*   Finally, assuming that  [2.x.308]  is a support point at the boundary, the (nodal) normals are defined as:   
*  

* 
* [1.x.144]
*    
*   We will compute the numerator of this expression first and store it in  [2.x.309] . We will normalize these vectors in a posterior loop.
* 

* 
*  

* 
* [1.x.145]
* 
*  What follows is the initialization of the scratch data required by WorkStream
* 

* 
*  

* 
* [1.x.146]
* 
*  We compute the local contributions for the lumped mass matrix entries  [2.x.310]  and and vectors  [2.x.311]  in the usual fashion:
* 

* 
* [1.x.147]
* 
*  Now we have to compute the boundary normals. Note that the following loop does not do much unless the element has faces on the boundary of the domain.
* 

* 
* [1.x.148]
* 
*  Note that "normal" will only represent the contributions from one of the faces in the support of the shape function phi_j. So we cannot normalize this local contribution right here, we have to take it "as is", store it and pass it to the copy data routine. The proper normalization requires an additional loop on nodes. This is done in the copy function below.
* 

* 
* [1.x.149]
* 
*  Last, we provide a copy_local_to_global function as required for the WorkStream
* 

* 
* [1.x.150]
* 
*  At this point in time we are done with the computation of  [2.x.312]  and  [2.x.313] , but so far the matrix  [2.x.314]  contains just a copy of the matrix  [2.x.315] . That's not what we really want: we have to normalize its entries. In addition, we have not filled the entries of the matrix  [2.x.316]   and the vectors stored in the map  [2.x.317]  are not normalized.     
*   In principle, this is just offline data, it doesn't make much sense to over-optimize their computation, since their cost will get amortized over the many time steps that we are going to use. However, computing/storing the entries of the matrix  [2.x.318]  are perfect to illustrate thread-parallel node-loops:
* 

* 
* 
*  - we want to visit every node  [2.x.319]  in the mesh/sparsity graph,
* 

* 
* 
*  - and for every such node we want to visit to every  [2.x.320]  such that  [2.x.321] .     
*   From an algebraic point of view, this is equivalent to: visiting every row in the matrix and for each one of these rows execute a loop on the columns. Node-loops is a core theme of this tutorial step (see the pseudo-code in the introduction) that will repeat over and over again. That's why this is the right time to introduce them.     
*   We have the thread parallelization capability  [2.x.322]  that is somehow more general than the WorkStream framework. In particular,  [2.x.323]  can be used for our node-loops. This functionality requires four input arguments which we explain in detail (for the specific case of our thread-parallel node loops):
* 

* 
* 
*  - The iterator  [2.x.324]  points to a row index.
* 

* 
* 
*  - The iterator  [2.x.325]  points to a numerically higher row index.
* 

* 
* 
*  - The function  [2.x.326]  and  [2.x.327]  define a sub-range within the range spanned by the end and begin iterators defined in the two previous bullets) applies an operation to every iterator in such subrange. We may as well call  [2.x.328]  the "worker".
* 

* 
* 
*  - Grainsize: minimum number of iterators (in this case representing rows) processed by each thread. We decided for a minimum of 4096 rows.     
*   A minor caveat here is that the iterators  [2.x.329]  and  [2.x.330]  supplied to  [2.x.331]  have to be random access iterators: internally,  [2.x.332]  will break the range defined by the  [2.x.333]  and  [2.x.334]  iterators into subranges (we want to be able to read any entry in those subranges with constant complexity). In order to provide such iterators we resort to  [2.x.335]      
*   The bulk of the following piece of code is spent defining the "worker"  [2.x.336] : i.e. the  operation applied at each row of the sub-range. Given a fixed  [2.x.337]  we want to visit every column/entry in such row. In order to execute such columns-loops we use [1.x.151] from the standard library, where:
* 

* 
* 
*  -  [2.x.338]  gives us an iterator starting at the first column of the row,
* 

* 
* 
*  -  [2.x.339]  is an iterator pointing at the last column of the row,
* 

* 
* 
*  - the last argument required by  [2.x.340]  is the operation applied at each nonzero entry (a lambda expression in this case) of such row.     
*   We note that,  [2.x.341]  will operate on disjoint sets of rows (the subranges) and our goal is to write into these rows. Because of the simple nature of the operations we want to carry out (computation and storage of normals, and normalization of the  [2.x.342]  of entries) threads cannot conflict attempting to write the same entry (we do not need a scheduler).
* 

* 
* [1.x.152]
* 
*  First column-loop: we compute and store the entries of the matrix norm_matrix and write normalized entries into the matrix nij_matrix:
* 

* 
* [1.x.153]
* 
*  Finally, we normalize the vectors stored in  [2.x.343] . This operation has not been thread parallelized as it would neither illustrate any important concept nor lead to any noticeable speed gain.
* 

* 
* [1.x.154]
* 
*  At this point we are very much done with anything related to offline data.
* 

* 
*   [1.x.155]  [1.x.156]
* 

* 
*  In this section we describe the implementation of the class members of the  [2.x.344]  class. Most of the code here is specific to the compressible Euler's equations with an ideal gas law. If we wanted to re-purpose  [2.x.345]  for a different conservation law (say for: instance the shallow water equation) most of the implementation of this class would have to change. But most of the other classes (in particular those defining loop structures) would remain unchanged.   
*   We start by implementing a number of small member functions for computing  [2.x.346] ,  [2.x.347] , and the flux  [2.x.348]  of the system. The functionality of each one of these functions is self-explanatory from their names.
* 

* 
*  

* 
* [1.x.157]
* 
*  Now we discuss the computation of  [2.x.349] . The analysis and derivation of sharp upper-bounds of maximum wavespeeds of Riemann problems is a very technical endeavor and we cannot include an advanced discussion about it in this tutorial. In this portion of the documentation we will limit ourselves to sketch the main functionality of our implementation functions and point to specific academic references in order to help the (interested) reader trace the source (and proper mathematical justification) of these ideas.   
*   In general, obtaining a sharp guaranteed upper-bound on the maximum wavespeed requires solving a quite expensive scalar nonlinear problem. This is typically done with an iterative solver. In order to simplify the presentation in this example step we decided not to include such an iterative scheme. Instead, we will just use an initial guess as a guess for an upper bound on the maximum wavespeed. More precisely, equations (2.11) (3.7), (3.8) and (4.3) of  [2.x.350]  are enough to define a guaranteed upper bound on the maximum wavespeed. This estimate is returned by a call to the function  [2.x.351] . At its core the construction of such an upper bound uses the so-called two-rarefaction approximation for the intermediate pressure  [2.x.352] , see for instance Equation (4.46), page 128 in  [2.x.353] .   
*   The estimate returned by  [2.x.354]  is guaranteed to be an upper bound, it is in general quite sharp, and overall sufficient for our purposes. However, for some specific situations (in particular when one of states is close to vacuum conditions) such an estimate will be overly pessimistic. That's why we used a second estimate to avoid this degeneracy that will be invoked by a call to the function  [2.x.355] . The most important function here is  [2.x.356]  which takes the minimum between the estimates returned by  [2.x.357]  and  [2.x.358] .   
*   We start again by defining a couple of helper functions:   
*   The first function takes a state  [2.x.359]  and a unit vector  [2.x.360]  and computes the [1.x.158] 1D state in direction of the unit vector.
* 

* 
* [1.x.159]
* 
*  For this, we have to change the momentum to  [2.x.361]  and have to subtract the kinetic energy of the perpendicular part from the total energy:
* 

* 
* [1.x.160]
* 
*  We return the 1D state in [1.x.161] variables instead of conserved quantities. The return array consists of density  [2.x.362] , velocity  [2.x.363] , pressure  [2.x.364]  and local speed of sound  [2.x.365] :
* 

* 
*  

* 
* [1.x.162]
* 
*  At this point we also define two small functions that return the positive and negative part of a double.
* 

* 
*  

* 
* [1.x.163]
* 
*  Next, we need two local wavenumbers that are defined in terms of a primitive state  [2.x.366]  and a given pressure  [2.x.367]   [2.x.368]   Eqn. (3.7):

* 
* [1.x.164]
*  Here, the  [2.x.369]  denotes the positive part of the given argument.
* 

* 
*  

* 
* [1.x.165]
* 
*  Analougously  [2.x.370]  Eqn. (3.8):

* 
* [1.x.166]
* 
* 

* 
*  

* 
* [1.x.167]
* 
*  All that is left to do is to compute the maximum of  [2.x.371]  and  [2.x.372]  computed from the left and right primitive state ( [2.x.373]  Eqn. (2.11)), where  [2.x.374]  is given by  [2.x.375]  Eqn (4.3):
* 

* 
*  

* 
* [1.x.168]
* 
*  We compute the second upper bound of the maximal wavespeed that is, in general, not as sharp as the two-rarefaction estimate. But it will save the day in the context of near vacuum conditions when the two-rarefaction approximation might attain extreme values:

* 
* [1.x.169]
* 

* 
*  [2.x.376]  The constant 5.0 multiplying the maximum of the sound speeds is [1.x.170] an ad-hoc constant, [1.x.171] a tuning parameter. It defines an upper bound for any  [2.x.377] . Do not play with it!
* 

* 
*  

* 
* [1.x.172]
* 
*  The following is the main function that we are going to call in order to compute  [2.x.378] . We simply compute both maximal wavespeed estimates and return the minimum.
* 

* 
*  

* 
* [1.x.173]
* 
*  We conclude this section by defining static arrays  [2.x.379]  that contain strings describing the component names of our state vector. We have template specializations for dimensions one, two and three, that are used later in DataOut for naming the corresponding components:
* 

* 
*  

* 
* [1.x.174]
* 
*   [1.x.175]  [1.x.176]
* 

* 
*  The last preparatory step, before we discuss the implementation of the forward Euler scheme, is to briefly implement the `InitialValues` class.   
*   In the constructor we initialize all parameters with default values, declare all parameters for the `ParameterAcceptor` class and connect the  [2.x.380]  slot to the respective signal.   
*   The  [2.x.381]  slot will be invoked from ParameterAceptor after the call to  [2.x.382]  In that regard, its use is appropriate for situations where the parameters have to be postprocessed (in some sense) or some consistency condition between the parameters has to be checked.
* 

* 
*  

* 
* [1.x.177]
* 
*  So far the constructor of  [2.x.383]  has defined default values for the two private members  [2.x.384]  and added them to the parameter list. But we have not defined an implementation of the only public member that we really care about, which is  [2.x.385]  (the function that we are going to call to actually evaluate the initial solution at the mesh nodes). At the top of the function, we have to ensure that the provided initial direction is not the zero vector.   
*  

* 
*  [2.x.386]  As commented, we could have avoided using the method  [2.x.387]  and defined a class member  [2.x.388]  in order to define the implementation of  [2.x.389] . But for illustrative purposes we want to document a different way here and use the call back signal from ParameterAcceptor.
* 

* 
*  

* 
* [1.x.178]
* 
*  Next, we implement the  [2.x.390]  function object with a lambda function computing a uniform flow field. For this we have to translate a given primitive 1d state (density  [2.x.391] , velocity  [2.x.392] , and pressure  [2.x.393] ) into a conserved n-dimensional state (density  [2.x.394] , momentum  [2.x.395] , and total energy  [2.x.396] ).
* 

* 
*  

* 
* [1.x.179]
* 
*   [1.x.180]  [1.x.181]
* 

* 
*  The constructor of the  [2.x.397]  class does not contain any surprising code:
* 

* 
*  

* 
* [1.x.182]
* 
*  In the class member  [2.x.398]  we initialize the temporary vector  [2.x.399] . The vector  [2.x.400]  will be used to store the solution update temporarily before its contents is swapped with the old vector.
* 

* 
*  

* 
* [1.x.183]
* 
*  It is now time to implement the forward Euler step. Given a (writable reference) to the old state  [2.x.401]  at time  [2.x.402]  we update the state  [2.x.403]  in place and return the chosen time-step size. We first declare a number of read-only references to various different variables and data structures. We do this is mainly to have shorter variable names (e.g.,  [2.x.404]  instead of  [2.x.405] ).
* 

* 
*  

* 
* [1.x.184]
* 
*  [1.x.185]: Computing the  [2.x.406]  graph viscosity matrix.     
*   It is important to highlight that the viscosity matrix has to be symmetric, i.e.,  [2.x.407] . In this regard we note here that  [2.x.408]  (or equivalently  [2.x.409] ) provided either  [2.x.410]  or  [2.x.411]  is a support point located away from the boundary. In this case we can check that  [2.x.412]  by construction, which guarantees the property  [2.x.413] .     
*   However, if both support points  [2.x.414]  or  [2.x.415]  happen to lie on the boundary, then, the equalities  [2.x.416]  and  [2.x.417]  do not necessarily hold true. The only mathematically safe solution for this dilemma is to compute both of them  [2.x.418]  and  [2.x.419]  and take the maximum.     
*   Overall, the computation of  [2.x.420]  is quite expensive. In order to save some computing time we exploit the fact that the viscosity matrix has to be symmetric (as mentioned above): we only compute the upper-triangular entries of  [2.x.421]  and copy the corresponding entries to the lower-triangular counterpart.     
*   We use again  [2.x.422]  for thread-parallel for loops. Pretty much all the ideas for parallel traversal that we introduced when discussing the assembly of the matrix  [2.x.423]  and the normalization of  [2.x.424]  above are used here again.     
*   We define again a "worker" function  [2.x.425]  that computes the viscosity  [2.x.426]  for a subrange [i1, i2) of column indices:
* 

* 
* [1.x.186]
* 
*  For a given column index i we iterate over the columns of the sparsity pattern from  [2.x.427]  to  [2.x.428] :
* 

* 
* [1.x.187]
* 
*  We only compute  [2.x.429]  if  [2.x.430]  (upper triangular entries) and later copy the values over to  [2.x.431] .
* 

* 
* [1.x.188]
* 
*  If both support points happen to be at the boundary we have to compute  [2.x.432]  as well and then take  [2.x.433] . After this we can finally set the upper triangular and lower triangular entries.
* 

* 
* [1.x.189]
* 
*  [1.x.190]: Compute diagonal entries  [2.x.434]  and  [2.x.435] .
* 

* 
*  So far we have computed all off-diagonal entries of the matrix  [2.x.436] . We still have to fill its diagonal entries defined as  [2.x.437] . We use again  [2.x.438]  for this purpose. While computing the  [2.x.439] s we also determine the largest admissible time-step, which is defined as [1.x.191] Note that the operation  [2.x.440]  is intrinsically global, it operates on all nodes: first we have to take the minimum over all threads (of a given node) and then we have to take the minimum over all MPI processes. In the current implementation:
* 

* 
* 
*  - We store   [2.x.441]  (per node) as [1.x.192]. The internal implementation of  [2.x.442]  will take care of guarding any possible race condition when more than one thread attempts to read and/or write  [2.x.443]  at the same time.
* 

* 
* 
*  - In order to take the minimum over all MPI process we use the utility function  [2.x.444] .
* 

* 
*  

* 
* [1.x.193]
* 
*  on_subranges() will be executed on every thread individually. The variable  [2.x.445]  is thus stored thread locally.
* 

* 
*  

* 
* [1.x.194]
* 
*  We store the negative sum of the d_ij entries at the diagonal position
* 

* 
* [1.x.195]
* 
*  and compute the maximal local time-step size  [2.x.446] :
* 

* 
* [1.x.196]
* 
*   [2.x.447]  contains the largest possible time-step size computed for the (thread local) subrange. At this point we have to synchronize the value over all threads. This is were we use the [1.x.197] [1.x.198] update mechanism:
* 

* 
* [1.x.199]
* 
*  After all threads have finished we can simply synchronize the value over all MPI processes:
* 

* 
*  

* 
* [1.x.200]
* 
*  This is a good point to verify that the computed  [2.x.448]  is indeed a valid floating point number.
* 

* 
* [1.x.201]
* 
*  [1.x.202]: Perform update.
* 

* 
*  At this point, we have computed all viscosity coefficients  [2.x.449]  and we know the maximal admissible time-step size  [2.x.450] . This means we can now compute the update:     
*   [1.x.203]     
*   This update formula is slightly different from what was discussed in the introduction (in the pseudo-code). However, it can be shown that both equations are algebraically equivalent (they will produce the same numerical values). We favor this second formula since it has natural cancellation properties that might help avoid numerical artifacts.
* 

* 
*  

* 
* [1.x.204]
* 
*  [1.x.205]: Fix up boundary states.
* 

* 
*  As a last step in the Forward Euler method, we have to fix up all boundary states. As discussed in the intro we
* 

* 
* 
*  - advance in time satisfying no boundary condition at all,
* 

* 
* 
*  - at the end of the time step enforce boundary conditions strongly in a post-processing step.     
*   Here, we compute the correction [1.x.206] which removes the normal component of  [2.x.451] .
* 

* 
*  

* 
* [1.x.207]
* 
*  We only iterate over the locally owned subset:
* 

* 
* [1.x.208]
* 
*  On free slip boundaries we remove the normal component of the momentum:
* 

* 
* [1.x.209]
* 
*  On Dirichlet boundaries we enforce initial conditions strongly:
* 

* 
* [1.x.210]
* 
*  [1.x.211]: We now update the ghost layer over all MPI ranks, swap the temporary vector with the solution vector  [2.x.452]  (that will get returned by reference) and return the chosen time-step size  [2.x.453] :
* 

* 
*  

* 
* [1.x.212]
* 
*   [1.x.213]  [1.x.214]   
*   At various intervals we will output the current state  [2.x.454]  of the solution together with a so-called Schlieren plot. The constructor of the  [2.x.455]  class again contains no surprises. We simply supply default values to and register two parameters:
* 

* 
* 
*  - schlieren_beta: is an ad-hoc positive amplification factor in order to enhance the contrast in the visualization. Its actual value is a matter of taste.
* 

* 
* 
*  - schlieren_index: is an integer indicating which component of the state  [2.x.456]  are we going to use in order to generate the visualization.
* 

* 
*  

* 
* [1.x.215]
* 
*  Again, the  [2.x.457]  function initializes two temporary the vectors ( [2.x.458] ).
* 

* 
*  

* 
* [1.x.216]
* 
*  We now discuss the implementation of the class member  [2.x.459] , which basically takes a component of the state vector  [2.x.460]  and computes the Schlieren indicator for such component (the formula of the Schlieren indicator can be found just before the declaration of the class  [2.x.461] ). We start by noting that this formula requires the "nodal gradients"  [2.x.462] . However, nodal values of gradients are not defined for  [2.x.463]  finite element functions. More generally, pointwise values of gradients are not defined for  [2.x.464]  functions. The simplest technique we can use to recover gradients at nodes is weighted-averaging i.e.   
*   [1.x.217]   
*   where  [2.x.465]  is the support of the shape function  [2.x.466] , and  [2.x.467]  is the weight. The weight could be any positive function such as  [2.x.468]  (that would allow us to recover the usual notion of mean value). But as usual, the goal is to reuse the off-line data as much as possible. In this sense, the most natural choice of weight is  [2.x.469] . Inserting this choice of weight and the expansion  [2.x.470]  into  [2.x.471]  we get :   
*   [1.x.218]   
*   Using this last formula we can recover averaged nodal gradients without resorting to any form of quadrature. This idea aligns quite well with the whole spirit of edge-based schemes (or algebraic schemes) where we want to operate on matrices and vectors as directly as it could be possible avoiding by all means assembly of bilinear forms, cell-loops, quadrature, or any other intermediate construct/operation between the input arguments (the state from the previous time-step) and the actual matrices and vectors required to compute the update.   
*   The second thing to note is that we have to compute global minimum and maximum  [2.x.472]  and  [2.x.473] . Following the same ideas used to compute the time step size in the class member  [2.x.474]  we define  [2.x.475]  and  [2.x.476]  as atomic doubles in order to resolve any conflicts between threads. As usual, we use  [2.x.477]  and  [2.x.478]  to find the global maximum/minimum among all MPI processes.   
*   Finally, it is not possible to compute the Schlieren indicator in a single loop over all nodes. The entire operation requires two loops over nodes:
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - The first loop computes  [2.x.479]  for all  [2.x.480]  in the mesh, and the bounds  [2.x.481]  and  [2.x.482] .
* 

* 
* 
*  - The second loop finally computes the Schlieren indicator using the formula   
*   [1.x.219]   
*   This means that we will have to define two workers  [2.x.483]  for each one of these stages.
* 

* 
*  

* 
* [1.x.220]
* 
*  We define the r_i_max and r_i_min in the current MPI process as atomic doubles in order to avoid race conditions between threads:
* 

* 
* [1.x.221]
* 
*  First loop: compute the averaged gradient at each node and the global maxima and minima of the gradients.
* 

* 
* [1.x.222]
* 
*  We fix up the gradient r_i at free slip boundaries similarly to how we fixed up boundary states in the forward Euler step. This avoids sharp, artificial gradients in the Schlieren plot at free slip boundaries and is a purely cosmetic choice.
* 

* 
*  

* 
* [1.x.223]
* 
*  We remind the reader that we are not interested in the nodal gradients per se. We only want their norms in order to compute the Schlieren indicator (weighted with the lumped mass matrix  [2.x.484] ):
* 

* 
* [1.x.224]
* 
*  We compare the current_r_i_max and current_r_i_min (in the current subrange) with r_i_max and r_i_min (for the current MPI process) and update them if necessary:
* 

* 
*  

* 
* [1.x.225]
* 
*  And synchronize  [2.x.485]  over all MPI processes.
* 

* 
*  

* 
* [1.x.226]
* 
*  Second loop: we now have the vector  [2.x.486]  and the scalars  [2.x.487]  at our disposal. We are thus in a position to actually compute the Schlieren indicator.
* 

* 
*  

* 
* [1.x.227]
* 
*  And finally, exchange ghost elements.
* 

* 
* [1.x.228]
* 
*   [1.x.229]  [1.x.230]   
*   With all classes implemented it is time to create an instance of  [2.x.488] ,  [2.x.489] , and  [2.x.490] , and run the forward Euler step in a loop.   
*   In the constructor of  [2.x.491]  we now initialize an instance of all classes, and declare a number of parameters controlling output. Most notable, we declare a boolean parameter  [2.x.492]  that will control whether the program attempts to restart from an interrupted computation, or not.
* 

* 
*  

* 
* [1.x.231]
* 
*  We start by implementing a helper function  [2.x.493]  in an anonymous namespace that is used to output messages in the terminal with some nice formatting.
* 

* 
*  

* 
* [1.x.232]
* 
*  With  [2.x.494]  in place it is now time to implement the  [2.x.495]  that contains the main loop of our program.
* 

* 
*  

* 
* [1.x.233]
* 
*  We start by reading in parameters and initializing all objects. We note here that the call to  [2.x.496]  reads in all parameters from the parameter file (whose name is given as a string argument). ParameterAcceptor handles a global ParameterHandler that is initialized with subsections and parameter declarations for all class instances that are derived from ParameterAceptor. The call to initialize enters the subsection for each each derived class, and sets all variables that were added using  [2.x.497] 
* 

* 
*  

* 
* [1.x.234]
* 
*  Next we create the triangulation, assemble all matrices, set up scratch space, and initialize the DataOut<dim> object:
* 

* 
*  

* 
* [1.x.235]
* 
*  We will store the current time and state in the variable  [2.x.498] :
* 

* 
*  

* 
* [1.x.236]
* 
*   [1.x.237]  [1.x.238]     
*   By default the boolean  [2.x.499]  is set to false, i.e. the following code snippet is not run. However, if  [2.x.500]  we indicate that we have indeed an interrupted computation and the program shall restart by reading in an old state consisting of  [2.x.501] ,  [2.x.502]  from a checkpoint file. These checkpoint files will be created in the  [2.x.503]  routine discussed below.
* 

* 
*  

* 
* [1.x.239]
* 
*  We use a  [2.x.504]  to store and read in the contents the checkpointed state.
* 

* 
*  

* 
* [1.x.240]
* 
*   [2.x.505]  iterates over all components of the state vector  [2.x.506] . We read in every entry of the component in sequence and update the ghost layer afterwards:
* 

* 
* [1.x.241]
* 
*  With either the initial state set up, or an interrupted state restored it is time to enter the main loop:
* 

* 
*  

* 
* [1.x.242]
* 
*  We first print an informative status message
* 

* 
*  

* 
* [1.x.243]
* 
*  and then perform a single forward Euler step. Note that the state vector  [2.x.507]  is updated in place and that  [2.x.508]  returns the chosen step size.
* 

* 
*  

* 
* [1.x.244]
* 
*  Post processing, generating output and writing out the current state is a CPU and IO intensive task that we cannot afford to do every time step
* 
*  - in particular with explicit time stepping. We thus only schedule output by calling the  [2.x.509]  function if we are past a threshold set by  [2.x.510] .
* 

* 
*  

* 
* [1.x.245]
* 
*  We wait for any remaining background output thread to finish before printing a summary and exiting.
* 

* 
* [1.x.246]
* 
*  The  [2.x.511]  takes an initial time "t" as input argument and populates a state vector  [2.x.512]  with the help of the  [2.x.513]  object.
* 

* 
*  

* 
* [1.x.247]
* 
*  The function signature of  [2.x.514]  is not quite right for  [2.x.515]  We work around this issue by, first, creating a lambda function that for a given position  [2.x.516]  returns just the value of the  [2.x.517] th component. This lambda in turn is converted to a  [2.x.518]  with the help of the ScalarFunctionFromFunctionObject wrapper.
* 

* 
*  

* 
* [1.x.248]
* 
*   [1.x.249]  [1.x.250]   
*   Writing out the final vtk files is quite an IO intensive task that can stall the main loop for a while. In order to avoid this we use an [1.x.251] strategy by creating a background thread that will perform IO while the main loop is allowed to continue. In order for this to work we have to be mindful of two things:
* 

* 
* 
*  - Before running the  [2.x.519]  thread, we have to create a copy of the state vector  [2.x.520] . We store it in the vector  [2.x.521] .
* 

* 
* 
*  - We have to avoid any MPI communication in the background thread, otherwise the program might deadlock. This implies that we have to run the postprocessing outside of the worker thread.
* 

* 
*  

* 
* [1.x.252]
* 
*  If the asynchronous writeback option is set we launch a background thread performing all the slow IO to disc. In that case we have to make sure that the background thread actually finished running. If not, we have to wait to for it to finish. We launch said background thread with [1.x.253] that returns a [1.x.254] object. This  [2.x.522]  object contains the return value of the function, which is in our case simply  [2.x.523] .
* 

* 
*  

* 
* [1.x.255]
* 
*  At this point we make a copy of the state vector, run the schlieren postprocessor, and run  [2.x.524]  The actual output code is standard: We create a DataOut instance, attach all data vectors we want to output and call  [2.x.525]  There is one twist, however. In order to perform asynchronous IO on a background thread we create the DataOut<dim> object as a shared pointer that we pass on to the worker thread to ensure that once we exit this function and the worker thread finishes the DataOut<dim> object gets destroyed again.
* 

* 
*  

* 
* [1.x.256]
* 
*  Next we create a lambda function for the background thread. We [1.x.257] the  [2.x.526]  pointer as well as most of the arguments of the output function by value so that we have access to them inside the lambda function.
* 

* 
* [1.x.258]
* 
*  We checkpoint the current state by doing the precise inverse operation to what we discussed for the [1.x.259]:
* 

* 
*  

* 
* [1.x.260]
* 
*  If the asynchronous writeback option is set we launch a new background thread with the help of [1.x.261] function. The function returns a [1.x.262] object that we can use to query the status of the background thread. At this point we can return from the  [2.x.527]  function and resume with the time stepping in the main loop
* 
*  - the thread will run in the background.
* 

* 
* [1.x.263]
* 
*  And finally, the main function.
* 

* 
*  

* 
* [1.x.264]
* [1.x.265][1.x.266][1.x.267]
* 

* Running the program with default parameters in release mode takes about 1minute on a 4 core machine (with hyperthreading):
* [1.x.268]
* 
* One thing that becomes evident is the fact that the program spends twothirds of the execution time computing the graph viscosity d_ij and about athird of the execution time in performing the update, where computing theflux  [2.x.528]  is the expensive operation. The preset default resolution isabout 37k gridpoints, which amounts to about 148k spatial degrees offreedom in 2D. An animated schlieren plot of the solution looks as follows:
*  [2.x.529] 
* It is evident that 37k gridpoints for the first-order method is nowherenear the resolution needed to resolve any flow features. For comparison,here is a "reference" computation with a second-order method and about 9.5Mgridpoints ([1.x.269]):
*  [2.x.530] 
* So, we give the first-order method a second chance and run it with about2.4M gridpoints on a small compute server:
* [1.x.270]
* 
* And with the following result:
*  [2.x.531] 
* That's substantially better, although of course at the price of having runthe code for roughly 2 hours on 16 cores.
* 

* 
* [1.x.271][1.x.272][1.x.273]
* 

* The program showcased here is really only first-order accurate, asdiscussed above. The pictures above illustrate how much diffusion thatintroduces and how far the solution is from one that actually resolvesthe features we care about.
* This can be fixed, but it would exceed what atutorial* is about.Nevertheless, it is worth showing what one can achieve by adding asecond-order scheme. For example, here is a video computed with [1.x.274]that shows (with a different color scheme) a 2d simulation that correspondsto the cases shown above:
* [1.x.275]
* 
* This simulation was done with 38 million degrees of freedom(continuous  [2.x.532]  finite elements) per component of the solutionvector. The exquisite detail of the solution is remarkable for thesekinds of simulations, including in the sub-sonic region behind theobstacle.
* One can also with relative ease further extend this to the 3d case:
* [1.x.276]
* 
* Solving this becomes expensive, however: The simulation was done with1,817 million degrees of freedom (continuous  [2.x.533]  finite elements)per component (for a total of 9.09 billion spatial degrees of freedom)and ran on 30,720 MPI ranks. The code achieved an average througput of969M grid points per second (0.04M gridpoints per second per CPU). Thefront and back wall show a "Schlieren plot": the magnitude of thegradient of the density on an exponential scale from white (low) toblack (high). All other cutplanes and the surface of the obstacle showthe magnitude of the vorticity on a white (low)
* 
*  - yellow (medium)
* 
*  - ed (high) scale. (The scales of the individual cutplanes have beenadjusted for a nicer visualization.)
* 

* [1.x.277][1.x.278] [2.x.534] 
* [0.x.4]

include/deal.II-translator/A-tutorial/step-70_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] ,  [2.x.2] ,  [2.x.3] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28]
*  [2.x.4] 
* [1.x.29]
*  [2.x.5] 
* 

* [1.x.30][1.x.31]
* 

* [1.x.32][1.x.33]
* 

* In this tutorial we consider a mixing problem in the laminar flow regime.Such problems occur in a wide range of applications ranging from chemical engineering to powergeneration (e.g. turbomachinery). Mixing problems are particularly hard to solve numerically,because they often involve a container (with fixed boundaries, and possiblycomplex geometries such as baffles), represented by the domain  [2.x.6] ,and one (or more) immersed and rotating impellers (represented by the domain  [2.x.7] ).The domain in which we would like to solve the flow equations is the (timedependent) difference between the two domains, namely: [2.x.8] .
* For rotating impellers, the use of Arbitrary Lagrangian Eulerian formulations(in which the fluid domain
* 
*  -  along with the mesh!
* 
*  -  is smoothly deformed to follow the deformationsof the immersed solid) is not possible, unless only small times (i.e.,small fluid domain deformations) are considered. If one wants to track theevolution of the flow across multiple rotations of the impellers, the resultingdeformed grid would simply be too distorted to be useful.
* In this case, a viable alternative strategy would be to use non-matchingmethods (similarly to what we have done in  [2.x.9] ), where a background fixedgrid (that may or may not be locally refined in time to better capture the solidmotion) is coupled with a rotating, independent, grid.
* In order to maintain the same notations used in  [2.x.10] , we use  [2.x.11]  todenote the domain in  [2.x.12]  representing the container of boththe fluid and the impeller, and we use  [2.x.13]  in  [2.x.14]  to denoteeither the full impeller (when its `spacedim` measure is non-negligible, i.e.,when we can represent it as a grid of dimension `dim` equal to `spacedim`),a co-dimension one representation of a thin impeller, or just the boundary ofthe full impeller.
* The domain  [2.x.15]  is embedded in  [2.x.16]  ( [2.x.17] ) and itis non-matching: It does not, in general, align with any of thefeatures of the volume mesh. We solve a partial differential equation on  [2.x.18] ,enforcing some conditions on the solution of the problem on the embeddeddomain  [2.x.19]  by some penalization techniques. In the current case,the condition is that the velocity of the fluid at points on  [2.x.20] equal the velocity of the solid impeller at that point.
* The technique we describe here is presented in the literature using one of manynames: the [1.x.34] and the [1.x.35] among others.  The main principle is that the discretization of thetwo grids are kept completely independent. In the present tutorial, thisapproach is used to solve for the motion of a viscous fluid, described by theStokes equation, that is agitated by a rigid non-deformable impeller.
* Thus, the equations solved in  [2.x.21]  are the Stokes equations for a creepingflow (i.e. a flow where  [2.x.22] ) and a no-slip boundarycondition is applied on the movingembedded domain*  [2.x.23]  associated withthe impeller. However, this tutorial could be readily extendedto other equations (e.g. the Navier-Stokes equations, linear elasticityequation, etc.). It can be seen as a natural extension of  [2.x.24]  thatenables the solution of large problems using a distributed parallel computingarchitecture via MPI.
* However, contrary to  [2.x.25] , the Dirichlet boundary conditions on  [2.x.26] are imposed weakly instead of through the use of Lagrange multipliers, and weconcentrate on dealing with the coupling of two fully distributedtriangulations (a combination that was not possible in the implementation of [2.x.27] ).
* There are two interesting scenarios that occur when one wants to enforceconditions on the embedded domain  [2.x.28] :
* 
*  - The geometrical dimension `dim` of the embedded domain  [2.x.29]  is the same ofthe domain  [2.x.30]  (`spacedim`), that is, the spacedim-dimensional measure of [2.x.31]  is not zero. In this case, the imposition of the Dirichlet boundaryboundary condition on  [2.x.32]  is done through a volumetric penalization. If theapplied penalization only depends on the velocity, this is often referredto as  [2.x.33]  penalization whereas if the penalization dependson both the velocity and its gradient, it is an  [2.x.34]  penalization.The case of the  [2.x.35]  penalization is very similar to a Darcy-typeapproach. Both  [2.x.36]  and  [2.x.37]  penalizations have beenanalyzed extensively (see, for example,  [2.x.38] ).
* 
*  - The embedded domain  [2.x.39]  has an intrinsic dimension `dim` which is smallerthan that of  [2.x.40]  (`spacedim`), thus its spacedim-dimensional measure iszero; for example it is a curve embedded in a two dimensional domain, or asurface embedded in a three-dimensional domain. This is of coursephysically impossible, but one may consider very thin sheets of metalmoving in a fluid as essentially lower-dimensional if the thickness ofthe sheet is negligible. In this case, the boundarycondition is imposed weakly on  [2.x.41]  by applying the[1.x.36] method (see [2.x.42] ).
* Both approaches have very similar requirements and result in highlysimilar formulations. Thus, we treat them almost in the same way.
* In this tutorial program we are not interested in further details on  [2.x.43] :we assume that the dimension of the embedded domain (`dim`) is always smaller byone or equal with respect to the dimension of the embedding domain  [2.x.44] (`spacedim`).
* We are going to solve the following differential problem: given a sufficientlyregular function  [2.x.45]  on  [2.x.46] , find the solution  [2.x.47]  to
* [1.x.37]
* 
* This equation, which we have normalized by scaling the time units insuch a way that the viscosity has a numerical value of 1, describesslow, viscous flow such as honey or lava.The main goal of this tutorial is to show how to impose the velocity fieldcondition  [2.x.48]  on a non-matching  [2.x.49]  in a weak way,using a penalization method. A more extensive discussion of the Stokesproblem including body forces, different boundary conditions, and solutionstrategies can be found in  [2.x.50] .
* Let us start by considering the Stokes problem alone, in the entire domain [2.x.51] . We look for a velocity field  [2.x.52]  and a pressure field  [2.x.53] that satisfy the Stokes equations with homogeneous boundary conditionson  [2.x.54] .
* The weak form of the Stokes equations is obtained by first writing it in vectorform as[1.x.38]
* forming the dot product from the left with a vector-valued testfunction  [2.x.55] , and integratingover the domain  [2.x.56] , yielding the following set of equations:[1.x.39]
* which has to hold for all test functions  [2.x.57] .
* 

* Integrating by parts and exploiting the boundary conditions on  [2.x.58] ,we obtain the following variational problem:[1.x.40]
* 
* where  [2.x.59]  represents the  [2.x.60]  scalarproduct. This is the same variational form used in  [2.x.61] .
* This variational formulation does not take into account the embedded domain.Contrary to  [2.x.62] , we do not enforce strongly the constraints of [2.x.63]  on  [2.x.64] , but enforce them weakly via a penalization term.
* The analysis of this weak imposition of the boundary condition depends on thespacedim-dimensional measure of  [2.x.65]  as either positive (if `dim` is equalto `spacedim`) or zero (if `dim` is smaller than `spacedim`). We discuss bothscenarios.
* 

* [1.x.41][1.x.42]
* 

* In this case, we assume that  [2.x.66]  is the boundary of the actual impeller,that is, a closed curve embedded in a two-dimensional domain or a closedsurface in a three-dimensional domain. The idea of this method starts byconsidering a weak imposition of the Dirichlet boundary condition on  [2.x.67] ,following the Nitsche method. This is achieved by using the following modified formulationon the fluid domain, where no strong conditions on the test functions on  [2.x.68]  are imposed:
* [1.x.43]
* 
* The integrals over  [2.x.69]  are lower-dimensional integrals. It can be shown (see [2.x.70] ) that there exists a positive constant [2.x.71]  so that if  [2.x.72] , the weak imposition of the boundary willbe consistent and stable. The first two additional integrals on  [2.x.73]  (thesecond line in the equation above) appear naturally after integrating by parts,when one does not assume that  [2.x.74]  is zero on [2.x.75] .
* The third line in the equation above contains two terms that are added to ensureconsistency of the weak form, and a stabilization term, that is there to enforcethe boundary condition with an error which is consistent with the approximationerror. The consistency terms and the stabilization term are added to theright hand side with the actual boundary data  [2.x.76] .
* When  [2.x.77]  satisfies the condition  [2.x.78]  on  [2.x.79] ,all the consistency and stability integrals on  [2.x.80]  cancel out, and one isleft with the usual weak form of Stokes flow, that is, the above formulation isconsistent.
* We note that an alternative (non-symmetric) formulation can be used :
* [1.x.44]
* Note the different sign of the first terms on the third and fourth lines.In this case, the stability and consistency conditions become  [2.x.81] . Inthe symmetric case, the value of  [2.x.82]  is dependent on  [2.x.83] , and it is ingeneral chosen such that  [2.x.84]  with  [2.x.85] a measure of size of the face being integrated and  [2.x.86]  a constant such that [2.x.87] . This is as one usually does with the Nitschepenalty method to enforcing Dirichlet boundary conditions.
* The non-symmetric approach, on the other hand, is related to how oneenforced continuity for the non-symmetric interior penalty method fordiscontinuous Galerkin methods (the "NIPG" method  [2.x.88] ).Even if the non-symmetric case seems advantageous w.r.t.possible choices of stabilization parameters, we opt for the symmetricdiscretization, since in this case it can be shown that the dual problem isalso consistent, leading to a solution where not only the energy norm of thesolution converges with the correct order, but also its  [2.x.89] norm. Furthermore, the resulting matrix remains symmetric.
* The above formulation works under the assumption that the domain is discretizedexactly. However, if the deformation of the impeller is a rigid bodymotion, it is possible to artificially extend the solution of the Stokesproblem inside the propeller itself, since a rigid body motion is also asolution to the Stokes problem. The idea is then to solve the same problem,inside  [2.x.90] , imposing the same boundary conditions on [2.x.91] , using the same penalization technique, and testing with testfunctions  [2.x.92]  which are globally continuous over  [2.x.93] .
* This results in the following (intermediate) formulation:[1.x.45]
* where the jump terms, denoted with  [2.x.94] , are computed withrespect to a fixed orientation of the normal vector  [2.x.95] . Thefactor of 2 appears in front of  [2.x.96]  since we see every part of [2.x.97]  twice, once from within the fluid and once from within theobstacle moving around in it. (For all of the other integrals over [2.x.98] , we visit each part of  [2.x.99]  twice, but with oppositesigns, and consequently get the jump terms.)
* Here we notice that, unlike in discontinuous Galerkin methods, the testand trial functions are continuous across  [2.x.100] . Moreover, if  [2.x.101]  isnot aligned with cell boundaries, all the jump terms are also zero, since, ingeneral, finite element function spaces are smooth inside each cell, and if [2.x.102]  cuts through an element intersecting its boundary only at a finitenumber of points, all the contributions on  [2.x.103] , with the exception ofthe stabilization ones, can be neglected from the formulation, resulting inthe following final form of the variational formulation:
* [1.x.46]
* 
* In  [2.x.104] , the imposition of the constraintrequired the addition of new variables in the form of Lagrange multipliers.This is not the case for this tutorial program. The imposition of theboundary condition using Nitsche's method only modifies the system matrixand the right-hand side without adding additional unknowns.However, the velocity vector  [2.x.105]  on the embedded domain will not matchexactly the prescribed velocity  [2.x.106] , but only up to a numerical errorwhich is in the same order as the interpolation error of the finite elementmethod. Furthermore, as in  [2.x.107] , we still need to integrate over thenon-matching embedded grid in order to construct the boundary term necessaryto impose the boundary condition over  [2.x.108] .
* 

* [1.x.47][1.x.48]
* 

* In this case,  [2.x.109]  has the same dimension, but is embedded into [2.x.110] . We can think of this as a thick object moving around in the fluid.In the case of  [2.x.111]  penalization, the additional penalizationterm can be interpreted as a Darcy term within  [2.x.112] , resulting in:
* [1.x.49]
* 
* Here, integrals over  [2.x.113]  are simply integrals over a part of the volume.The  [2.x.114]  penalization thus consists in adding a volumetric term thatconstrains the velocity of the fluid to adhere to the velocity of the rigid bodywithin  [2.x.115] . Also in this case,  [2.x.116]  must be chosen sufficiently largein order to ensure that the Dirichlet boundary condition in  [2.x.117]  issufficiently respected, but not too high in order to maintain the properconditioning of the system matrix.
* A  [2.x.118]  penalization may be constructed in a similar manner, with theaddition of a viscous component to the penalization that dampens the velocitygradient within  [2.x.119] :
* [1.x.50]
* 
* Notice that the  [2.x.120]  penalization (`dim` equal to `spacedim`) and the Nitschepenalization (`dim` equal to `spacedim-1`) result in the exact same numericalimplementation, thanks to the dimension independent capabilities of deal.II.
* 

* [1.x.51][1.x.52]
* 

* In this tutorial, both the embedded grid  [2.x.121]  and the embeddinggrid are described using a  [2.x.122]  These twotriangulations can be built from functions in the GridGenerator namespace or by readinga mesh file produced with another application (e.g. GMSH, see thediscussion in  [2.x.123] ). This is slightlymore general than what was previously done in  [2.x.124] .
* The addition of the immersed boundary method, whetherit is in the `dim=spacedim` or `dim<spacedim` case, only introducesadditional terms in the system matrix and the right-hand side of thesystem which result from the integration over  [2.x.125] . This does notmodify the number of variables for which the problemmust be solved. The challenge is thus related to the integralsthat must be carried over  [2.x.126] .
* As usual in finite elements we split this integral into contributions from allcells of the triangulation used todiscretize  [2.x.127] , we transform the integral on  [2.x.128]  to an integral on thereference element  [2.x.129] , where  [2.x.130]  is the mapping from  [2.x.131]  to  [2.x.132] ,and compute the integral on  [2.x.133]  using a quadrature formula. For example:
* [1.x.53]
* Computing this sum is non-trivial because we have to evaluate  [2.x.134] . In general, if  [2.x.135]  and  [2.x.136]  are not aligned, the point [2.x.137]  is completely arbitrary with respect to  [2.x.138] , and unlesswe figure out a way to interpolate all basis functions of  [2.x.139]  on anarbitrary point on  [2.x.140] , we cannot compute the integral needed.
* 

* To evaluate  [2.x.141]  the following steps needs to betaken (as shown in the picture below):
* 
*  - For a given cell  [2.x.142]  in  [2.x.143]  compute the real point  [2.x.144] , where  [2.x.145]  is one of the quadrature points used for the integral on  [2.x.146] . This is the easy part: [2.x.147]  gives us the real-space locations of allquadrature points.
* 
*  - Find the cell of  [2.x.148]  in which  [2.x.149]  lies. We shall call this element  [2.x.150] .
* 
*  - Find the reference coordinates within  [2.x.151]  of  [2.x.152] . For this, weneed the inverse of the mapping  [2.x.153]  thattransforms the reference element  [2.x.154]  into the element  [2.x.155] :  [2.x.156] .
* 
*  - Evaluate the basis function  [2.x.157]  of the  [2.x.158]  mesh at this  point  [2.x.159] . This is, again, relatively simple using FEValues.
* 

*  [2.x.160] 
* In  [2.x.161] , the second through fourth steps above were computed by calling, in turn,
* 
*  -  [2.x.162]  followed by
* 
*  -  [2.x.163]  We then
* 
*  - construct a custom Quadrature formula, containing the point in the reference cell and then
* 
*  - construct an FEValues object, with the given quadrature formula, and initialized with the cell obtained in the first step.
* Although this approach could work for the present case, it does not lends itselfreadily to parallel simulations using distributed triangulations. Indeed,since the position of the quadrature points on the cells of theembedded domain  [2.x.164]  do not match that of the embedding triangulationand since  [2.x.165]  is constantly moving, this would require that the triangulation representing [2.x.166]  be stored in it's entirety for all of the processors. As the numberof processor and the number of cells in  [2.x.167]  increases, this leadsto a severe bottleneck in terms of memory. Consequently, an alternative strategy is soughtin this step.
* 

* [1.x.54][1.x.55]
* 

* Remember that for both the penalization approach ( [2.x.168]  or  [2.x.169] )and the Nitsche method, we want to compute integrals that are approximated bythe quadrature. That is, we need to compute[1.x.56]If you followed the discussion above, then you will recall that  [2.x.170] and  [2.x.171]  are shape functions defined on the fluid mesh.The only things defined on the solid mesh are: [2.x.172] , which is the location of a quadrature point on a solid cell thatis part of  [2.x.173] ,  [2.x.174]  is the determinant of its Jacobian, and  [2.x.175]  the correspondingquadrature weight.
* The important part to realize is now this:  [2.x.176]  is a property ofthe quadrature formula and does not change with time. Furthermore,the Jacobian matrix of  [2.x.177]  itself changes as the solid obstaclemoves around in the fluid, but because the solid is considerednon-deforming (it only translates and rotates, but doesn't dilate),the determinant of the Jacobian remains constant. As a consequence,the product  [2.x.178]  (which we typically denote by `JxW`)remains constant for each quadrature point. So the only thing we needkeep track of are the positions  [2.x.179] 
* 
*  -  but thesemove with the velocity of the solid domain.
* In other words, we don't actually need to keep the solid mesh at all.All we need is the positions  [2.x.180]  and corresponding `JxW` values.Since both of these properties are point-properties (or point-vectors) that areattached to the solid material, they can be idealized as a set of disconnectedinfinitesimally small "particles", which carry the required `JxW` information with themovement of the solid. deal.II has the ability to distribute andstore such a set of particles in large-scale parallel computations in the form ofthe ParticleHandler class (for details on the implementation see  [2.x.181] ),and we will make use of this functionality in this tutorial.
* Thus, the approach taken in this step is as follows:
* 
*  - Create a  [2.x.182]  for the domain  [2.x.183] ;
* 
*  - Create  [2.x.184]  at the positions of the quadrature points on  [2.x.185] ;
* 
*  - Call the  [2.x.186]  function,  to distribute the particles across processors,following the solid  triangulation*;
* 
*  - Attach the `JxW` values as a "property" to each  [2.x.187]  object.
* This structure is relatively expensive to generate, but must only be generatedonce per simulation. Once the  [2.x.188]  is generated and therequired information is attached to the particle, the integrals over  [2.x.189] can be carried out by exploiting the fact that particles are grouped cellwiseinside ParticleHandler, allowing us to:
* 
*  - Looping over all cells of  [2.x.190]  that contain at least one particle
* 
*  - Looping over all particles in the given cell
* 
*  - Compute the integrals and fill the global matrix.
* Since the  [2.x.191]  can manage the exchange of particles fromone processor to the other, the embeddedtriangulation can be moved or deformed by displacing the particles.The only constraint associated with this displacement is that particles shouldbe displaced by a distance that is no larger than the size of onecell. That's because that is the limit to which [2.x.192]  can track which cell a particle that leavesits current cell now resides in.
* Once the entire problem (the Stokes problem and the immersed boundaryimposition) is assembled,the final saddle point problem is solved by an iterative solver, applied to theSchur complement  [2.x.193]  (whose construction is described, for example, in  [2.x.194] ),and we construct  [2.x.195]  using LinearOperator classes.
* 

* [1.x.57][1.x.58]
* 

* The problem we solve here is a demonstration of the time-reversibility of Stokesflow. This is often illustrated in science education experiments with aTaylor-Couette flow and dye droplets that revert back to their original shapeafter the fluid has been displaced in a periodic manner.
* [1.x.59]
* 
* In the present problem, a very viscous fluid is agitated by the rotation ofan impeller, which, in 2D, is modeled by a rectangular grid. The impellerrotates for a given number of revolutions, after which the flow is reversed suchthat the same number of revolutions is carried out in the opposite direction. Werecall that since the Stokes equations are self-adjoint, creeping flows arereversible. Consequently, if the impeller motion is reversed in the oppositedirection, the fluid should return to its original position. In the presentcase, this is illustrated by inserting a circle of passive tracer particles thatare advected by the fluid and which return to their original position, thusdemonstrating the time-reversibility of the flow.
* 

* [1.x.60][1.x.61]
* 

* This tutorial program uses a number of techniques on imposing velocityconditions on non-matching interfaces in the interior of the fluid.For more background material, you may want to look up the following references: [2.x.196] , [2.x.197] , [2.x.198] , [2.x.199] , [2.x.200] .
* 

*  [1.x.62] [1.x.63]
*   [1.x.64]  [1.x.65] Most of these have been introduced elsewhere, we'll comment only on the new ones. The switches close to the top that allow selecting between PETSc and Trilinos linear algebra capabilities are similar to the ones in  [2.x.201]  and  [2.x.202] .
* 

* 
*  

* 
* [1.x.66]
* 
*  These are the only new include files with regard to  [2.x.203] . In this tutorial, the non-matching coupling between the solid and the fluid is computed using an intermediate data structure that keeps track of how the locations of quadrature points of the solid evolve within the fluid mesh. This data structure needs to keep track of the position of the quadrature points on each cell describing the solid domain, of the quadrature weights, and possibly of the normal vector to each point, if the solid domain is of co-dimension one.
* 

* 
*  Deal.II offers these facilities in the Particles namespace, through the ParticleHandler class. ParticleHandler is a class that allows you to manage a collection of particles (objects of type  [2.x.204]  representing a collection of points with some attached properties (e.g., an id) floating on a  [2.x.205]  The methods and classes in the namespace Particles allows one to easily implement Particle-In-Cell methods and particle tracing on distributed triangulations.
* 

* 
*  We "abuse" this data structure to store information about the location of solid quadrature points embedded in the surrounding fluid grid, including integration weights, and possibly surface normals. The reason why we use this additional data structure is related to the fact that the solid and the fluid grids might be non-overlapping, and if we were using two separate triangulation objects, would be distributed independently among parallel processes.
* 

* 
*  In order to couple the two problems, we rely on the ParticleHandler class, storing in each particle the position of a solid quadrature point (which is in general not aligned to any of the fluid quadrature points), its weight, and any other information that may be required to couple the two problems. These locations are then propagated along with the (prescribed) velocity of the solid impeller.
* 

* 
*  Ownership of the solid quadrature points is initially inherited from the MPI partitioning on the solid mesh itself. The Particles so generated are later distributed to the fluid mesh using the methods of the ParticleHandler class. This allows transparent exchange of information between MPI processes about the overlapping pattern between fluid cells and solid quadrature points.
* 

* 
* [1.x.67]
* 
*  When generating the grids, we allow reading it from a file, and if deal.II has been built with OpenCASCADE support, we also allow reading CAD files and use them as manifold descriptors for the grid (see  [2.x.206]  for a detailed description of the various Manifold descriptors that are available in the OpenCASCADE namespace)
* 

* 
* [1.x.68]
* 
*   [1.x.69]  [1.x.70]
* 

* 
*  Similarly to what we have done in  [2.x.207] , we set up a class that holds all the parameters of our problem and derive it from the ParameterAcceptor class to simplify the management and creation of parameter files.   
*   The ParameterAcceptor paradigm requires all parameters to be writable by the ParameterAcceptor methods. In order to avoid bugs that would be very difficult to track down (such as writing things like `time = 0` instead of `time == 0`), we declare all the parameters in an external class, which is initialized before the actual `StokesImmersedProblem` class, and pass it to the main class as a `const` reference.   
*   The constructor of the class is responsible for the connection between the members of this class and the corresponding entries in the ParameterHandler. Thanks to the use of the  [2.x.208]  method, this connection is trivial, but requires all members of this class to be writeable.
* 

* 
* [1.x.71]
* 
*  however, since this class will be passed as a `const` reference to the StokesImmersedProblem class, we have to make sure we can still set the time correctly in the objects derived by the Function class defined herein. In order to do so, we declare both the  [2.x.209]  and  [2.x.210]  members to be `mutable`, and define the following little helper method that sets their time to the correct value.
* 

* 
* [1.x.72]
* 
*  The remainder of the class consists largely of member variables that describe the details of the simulation and its discretization. The following parameters are about where output should land, the spatial and temporal discretization (the default is the  [2.x.211]  Taylor-Hood discretization which uses a polynomial degree of 2 for the velocity), and how many time steps should elapse before we generate graphical output again:
* 

* 
* [1.x.73]
* 
*  We allow every grid to be refined independently. In this tutorial, no physics is resolved on the solid grid, and its velocity is given as a datum. However it is relatively straightforward to incorporate some elasticity model in this tutorial, and transform it into a fully fledged FSI solver.
* 

* 
* [1.x.74]
* 
*  To provide a rough description of the fluid domain, we use the method extract_rtree_level() applied to the tree of bounding boxes of each locally owned cell of the fluid triangulation. The higher the level of the tree, the larger the number of extracted bounding boxes, and the more accurate is the description of the fluid domain. However, a large number of bounding boxes also implies a large communication cost, since the collection of bounding boxes is gathered by all processes.
* 

* 
* [1.x.75]
* 
*  The only two numerical parameters used in the equations are the viscosity of the fluid, and the penalty term  [2.x.212]  used in the Nitsche formulation:
* 

* 
* [1.x.76]
* 
*  By default, we create a hyper_cube without colorization, and we use homogeneous Dirichlet boundary conditions. In this set we store the boundary ids to use when setting the boundary conditions:
* 

* 
* [1.x.77]
* 
*  We illustrate here another way to create a Triangulation from a parameter file, using the method  [2.x.213]  that takes the name of a function in the GridGenerator namespace, and its arguments as a single string representing the arguments as a tuple.     
*   The mechanism with which the arguments are parsed from and to a string is explained in detail in the  [2.x.214]  class, which is used to translate from strings to most of the basic STL types (vectors, maps, tuples) and basic deal.II types (Point, Tensor, BoundingBox, etc.).     
*   In general objects that can be represented by rank 1 uniform elements (i.e.,  [2.x.215]  Point<dim>,  [2.x.216]  etc.) are comma separated. Additional ranks take a semicolon, allowing you to parse strings into objects of type  [2.x.217]  or, for example,  [2.x.218]  as `0.0, 0.1; 0.1, 0.2`. This string could be interpreted as a vector of two Point objects, or a vector of vector of doubles.     
*   When the entries are not uniform, as in the tuple case, we use a colon to separate the various entries. For example, a string like `5: 0.1, 0.2` could be used to parse an object of type  [2.x.219]  Point<2>>` or a  [2.x.220]   [2.x.221]      
*   In our case most of the arguments are Point objects (representing centers, corners, subdivision elements, etc.), integer values (number of subdivisions), double values (radius, lengths, etc.), or boolean options (such as the `colorize` option that many GridGenerator functions take).     
*   In the example below, we set reasonable default values, but these can be changed at run time by selecting any other supported function of the GridGenerator namespace. If the GridGenerator function fails, this program will interpret the name of the grid as a vtk grid filename, and the arguments as a map from manifold_id to the CAD files describing the geometry of the domain. Every CAD file will be analyzed and a Manifold of the OpenCASCADE namespace will be generated according to the content of the CAD file itself.     
*   To be as generic as possible, we do this for each of the generated grids: the fluid grid, the solid grid, but also the tracer particles which are also generated using a triangulation.
* 

* 
* [1.x.78]
* 
*  Similarly, we allow for different local refinement strategies. In particular, we limit the maximum number of refinement levels, in order to control the minimum size of the fluid grid, and guarantee that it is compatible with the solid grid. The minimum number of refinement levels is also controlled to ensured sufficient accuracy in the bulk of the flow. Additionally, we perform local refinement based on standard error estimators on the fluid velocity field.     
*   We permit the user to choose between the two most common refinement strategies, namely `fixed_number` or `fixed_fraction`, that refer to the methods  [2.x.222]  and  [2.x.223]      
*   Refinement may be done every few time steps, instead of continuously, and we control this value by the `refinement_frequency` parameter:
* 

* 
* [1.x.79]
* 
*  Finally, the following two function objects are used to control the source term of Stokes flow and the angular velocity at which we move the solid body. In a more realistic simulation, the solid velocity or its deformation would come from the solution of an auxiliary problem on the solid domain. In this example step we leave this part aside, and simply impose a fixed rotational velocity field along the z-axis on the immersed solid, governed by a function that can be specified in the parameter file:
* 

* 
* [1.x.80]
* 
*  There remains the task of declaring what run-time parameters we can accept in input files. We split the parameters in various categories, by putting them in different sections of the ParameterHandler class. We begin by declaring all the global parameters used by StokesImmersedProblem in the global scope:
* 

* 
* [1.x.81]
* 
*  Next section is dedicated to the parameters used to create the various grids. We will need three different triangulations: `Fluid grid` is used to define the fluid domain, `Solid grid` defines the solid domain, and `Particle grid` is used to distribute some tracer particles, that are advected with the velocity and only used as passive tracers.
* 

* 
* [1.x.82]
* 
*  The final task is to correct the default dimension for the right hand side function and define a meaningful default angular velocity instead of zero.
* 

* 
* [1.x.83]
* 
*  Once the angular velocity is provided as a Function object, we reconstruct the pointwise solid velocity through the following class which derives from the Function class. It provides the value of the velocity of the solid body at a given position by assuming that the body rotates around the origin (or the  [2.x.224]  axis in 3d) with a given angular velocity.
* 

* 
* [1.x.84]
* 
*  We assume that the angular velocity is directed along the z-axis, i.e., we model the actual angular velocity as if it was a two-dimensional rotation, irrespective of the actual value of `spacedim`.
* 

* 
* [1.x.85]
* 
*  Similarly, we assume that the solid position can be computed explicitly at each time step, exploiting the knowledge of the angular velocity. We compute the exact position of the solid particle assuming that the solid is rotated by an amount equal to the time step multiplied by the angular velocity computed at the point `p`:
* 

* 
* [1.x.86]
* 
*   [1.x.87]  [1.x.88]
* 

* 
*  We are now ready to introduce the main class of our tutorial program. As usual, other than the constructor, we leave a single public entry point: the `run()` method. Everything else is left `private`, and accessed through the run method itself.
* 

* 
* [1.x.89]
* 
*  The next section contains the `private` members of the class. The first method is similar to what is present in previous example. However it not only takes care of generating the grid for the fluid, but also the grid for the solid. The second computes the largest time step that guarantees that each particle moves of at most one cell. This is important to ensure that the  [2.x.225]  can find which cell a particle ends up in, as it can only look from one cell to its immediate neighbors (because, in a parallel setting, every MPI process only knows about the cells it owns as well as their immediate neighbors).
* 

* 
* [1.x.90]
* 
*  The next two functions initialize the  [2.x.226]  objects used in this class. We have two such objects: One represents passive tracers, used to plot the trajectories of fluid particles, while the the other represents material particles of the solid, which are placed at quadrature points of the solid grid.
* 

* 
* [1.x.91]
* 
*  The remainder of the set up is split in two parts: The first of the following two functions creates all objects that are needed once per simulation, whereas the other sets up all objects that need to be reinitialized at every refinement step.
* 

* 
* [1.x.92]
* 
*  The assembly routine is very similar to other Stokes assembly routines, with the exception of the Nitsche restriction part, which exploits one of the particle handlers to integrate on a non-matching part of the fluid domain, corresponding to the position of the solid. We split these two parts into two separate functions.
* 

* 
* [1.x.93]
* 
*  The remaining functions solve the linear system (which looks almost identical to the one in  [2.x.227] ) and then postprocess the solution: The refine_and_transfer() method is called only every `refinement_frequency` steps to adapt the mesh and also make sure that all the fields that were computed on the time step before refinement are transferred correctly to the new grid. This includes vector fields, as well as particle information. Similarly, we call the two output methods only every `output_frequency` steps.
* 

* 
* [1.x.94]
* 
*  Let us then move on to the member functions of the class. The first deals with run-time parameters that are read from a parameter file. As noted before, we make sure we cannot modify this object from within this class, by making it a `const` reference.
* 

* 
* [1.x.95]
* 
*  Then there is also the MPI communicator object that we will use to let processes send information across the network if the program runs in parallel, along with the `pcout` object and timer information that has also been employed by  [2.x.228] , for example:
* 

* 
* [1.x.96]
* 
*  Next is one of the main novelties with regard to  [2.x.229] . Here we assume that both the solid and the fluid are fully distributed triangulations. This allows the problem to scale to a very large number of degrees of freedom, at the cost of communicating all the overlapping regions between non matching triangulations. This is especially tricky, since we make no assumptions on the relative position or distribution of the various subdomains of the two triangulations. In particular, we assume that every process owns only a part of the `solid_tria`, and only a part of the `fluid_tria`, not necessarily in the same physical region, and not necessarily overlapping.     
*   We could in principle try to create the initial subdivisions in such a way that each process's subdomains overlap between the solid and the fluid regions. However, this overlap would be destroyed during the simulation, and we would have to redistribute the DoFs again and again. The approach we follow in this tutorial is more flexible, and not much more expensive. We make two all-to-all communications at the beginning of the simulation to exchange information about an (approximate) information of the geometrical occupancy of each processor (done through a collection of bounding boxes).     
*   This information is used by the  [2.x.230]  class to exchange (using a some-to-some communication pattern) all particles, so that every process knows about the particles that live on the region occupied by the fluid subdomain that it owns.     
*   In order to couple the overlapping regions, we exploit the facilities implemented in the ParticleHandler class.
* 

* 
* [1.x.97]
* 
*  Next come descriptions of the finite elements in use, along with appropriate quadrature formulas and the corresponding DoFHandler objects. For the current implementation, only `fluid_fe` is really necessary. For completeness, and to allow easy extension, we also keep the `solid_fe` around, which is however initialized to a FE_Nothing finite element space, i.e., one that has no degrees of freedom.     
*   We declare both finite element spaces as  [2.x.231]  objects rather than regular member variables, to allow their generation after `StokesImmersedProblemParameters` has been initialized. In particular, they will be initialized in the `initial_setup()` method.
* 

* 
* [1.x.98]
* 
*  Similarly to how things are done in  [2.x.232] , we use a block system to treat the Stokes part of the problem, and follow very closely what was done there.
* 

* 
* [1.x.99]
* 
*  Using this partitioning of degrees of freedom, we can then define all of the objects necessary to describe the linear systems in question:
* 

* 
* [1.x.100]
* 
*  Let us move to the particles side of this program. There are two  [2.x.233]  objects used to couple the solid with the fluid, and to describe the passive tracers. These, in many ways, play a role similar to the DoFHandler class used in the discretization, i.e., they provide for an enumeration of particles and allow querying information about each particle.
* 

* 
* [1.x.101]
* 
*  For every tracer particle, we need to compute the velocity field in its current position, and update its position using a discrete time stepping scheme. We do this using distributed linear algebra objects that store the coordinates of each particle's location or velocity. That is, these vectors have `tracer_particle_handler.n_global_particles() spacedim` entries that we will store in a way so that parts of the vector are partitioned across all processes. (Implicitly, we here make the assumption that the `spacedim` coordinates of each particle are stored in consecutive entries of the vector.) Thus, we need to determine who the owner of each vector entry is. We set this owner to be equal to the process that generated that particle at time  [2.x.234] . This information is stored for every process in the `locally_owned_tracer_particle_coordinates` IndexSet.     
*   Once the particles have been distributed around to match the process that owns the region where the particle lives, we will need read access from that process to the corresponding velocity field. We achieve this by filling a read only velocity vector field that contains the relevant information in ghost entries. This is achieved using the `locally_relevant_tracer_particle_coordinates` IndexSet, that keeps track of how things change during the simulation, i.e., it keeps track of where particles that the current process owns have ended up being, and who owns the particles that ended up in my subdomain.     
*   While this is not the most efficient strategy, we keep it this way to illustrate how things would work in a real fluid-structure interaction (FSI) problem. If a particle is linked to a specific solid degree of freedom, we are not free to choose who owns it, and we have to communicate this information around. We illustrate this here, and show that the communication pattern is point-to-point, and negligible in terms of total cost of the algorithm.     
*   The vectors defined based on these subdivisions are then used to store the particles velocities (read-only, with ghost entries) and their displacement (read/write, no ghost entries).
* 

* 
* [1.x.102]
* 
*  One of the key points of this tutorial program is the coupling between two independent  [2.x.235]  objects, one of which may be moving and deforming (with possibly large deformations) with respect to the other. When both the fluid and the solid triangulations are of type  [2.x.236]  every process has access only to its fraction of locally owned cells of each of the two triangulations. As mentioned above, in general, the locally owned domains are not overlapping.     
*   In order to allow for the efficient exchange of information between non-overlapping  [2.x.237]  objects, some algorithms of the library require the user to provide a rough description of the area occupied by the locally owned part of the triangulation, in the form of a collection of axis-aligned bounding boxes for each process, that provide a full covering of the locally owned part of the domain. This kind of information can then be used in situations where one needs to send information to the owner of the cell surrounding a known location, without knowing who that owner may in fact be. But, if one knows a collection of bounding boxes for the geometric area or volume each process owns, then we can determine a subset of all processes that might possibly own the cell in which that location lies: namely, all of those processes whose bounding boxes contain that point. Instead of sending the information associated to that location to all processes, one can then get away with only sending it to a small subset of the processes with point-to-point communication primitives. (You will notice that this also allows for the typical time-vs-memory trade-off: The more data we are willing to store about each process's owned area
* 
*  -  in the form of more refined bounding box information
* 
*  -  the less communication we have to perform.)     
*   We construct this information by gathering a vector (of length  [2.x.238]  of vectors of BoundingBox objects. We fill this vector using the extract_rtree_level() function, and allow the user to select what level of the tree to extract. The "level" corresponds to how coarse/fine the overlap of the area with bounding boxes should be.     
*   As an example, this is what would be extracted by the extract_rtree_level() function applied to a two dimensional hyper ball, distributed over three processes. Each image shows in green the bounding boxes associated to the locally owned cells of the triangulation on each process, and in violet the bounding boxes extracted from the rtree:     
*    [2.x.239]   [2.x.240]   [2.x.241]      
*   We store these boxes in a global member variable, which is updated at every refinement step:
* 

* 
* [1.x.103]
* 
*   [1.x.104]  [1.x.105]
* 

* 
*   [1.x.106]  [1.x.107]
* 

* 
*  In the constructor, we create the mpi_communicator as well as the triangulations and dof_handler for both the fluid and the solid. Using the mpi_communicator, both the ConditionalOStream and TimerOutput object are constructed.
* 

* 
* [1.x.108]
* 
*  In order to generate the grid, we first try to use the functions in the deal.II GridGenerator namespace, by leveraging the  [2.x.242]  If this function fails, then we use the following method, where the name is interpreted as a filename, and the arguments are interpreted as a map from manifold ids to CAD files, and are converted to Manifold descriptors using the OpenCASCADE namespace facilities. At the top, we read the file into a triangulation:
* 

* 
* [1.x.109]
* 
*  If we got to this point, then the Triangulation has been read, and we are ready to attach to it the correct manifold descriptions. We perform the next lines of code only if deal.II has been built with OpenCASCADE support. For each entry in the map, we try to open the corresponding CAD file, we analyze it, and according to its content, opt for either a  [2.x.243]  (if the CAD file contains a single `TopoDS_Edge` or a single `TopoDS_Wire`) or a  [2.x.244]  if the file contains a single face. Notice that if the CAD files do not contain single wires, edges, or faces, an assertion will be throw in the generation of the Manifold.     
*   We use the  [2.x.245]  class to do the conversion from the string to a map between manifold ids and file names for us:
* 

* 
* [1.x.110]
* 
*  Now we check how many faces are contained in the `Shape`. OpenCASCADE is intrinsically 3D, so if this number is zero, we interpret this as a line manifold, otherwise as a  [2.x.246]  in `spacedim` = 3, or  [2.x.247]  in `spacedim` = 2.
* 

* 
* [1.x.111]
* 
*  We use this trick, because  [2.x.248]  is only implemented for spacedim = 3. The check above makes sure that things actually work correctly.
* 

* 
* [1.x.112]
* 
*  We also allow surface descriptions in two dimensional spaces based on single NURBS patches. For this to work, the CAD file must contain a single `TopoDS_Face`.
* 

* 
* [1.x.113]
* 
*  Now let's put things together, and make all the necessary grids. As mentioned above, we first try to generate the grid internally, and if we fail (i.e., if we end up in the `catch` clause), then we proceed with the above function.   
*   We repeat this pattern for both the fluid and the solid mesh.
* 

* 
* [1.x.114]
* 
*   [1.x.115]  [1.x.116]
* 

* 
*  Once the solid and fluid grids have been created, we start filling the  [2.x.249]  objects. The first one we take care of is the one we use to keep track of passive tracers in the fluid. These are simply transported along, and in some sense their locations are unimportant: We just want to use them to see where flow is being transported. We could use any way we choose to determine where they are initially located. A convenient one is to create the initial locations as the vertices of a mesh in a shape of our choice
* 
*  -  a choice determined by one of the run-time parameters in the parameter file.   
*   In this implementation, we create tracers using the support points of a FE_Q finite element space defined on a temporary grid, which is then discarded. Of this grid, we only keep around the  [2.x.250]  objects (stored in a  [2.x.251]  class) associated to the support points.   
*   The  [2.x.252]  class offers the possibility to insert a set of particles that live physically in the part of the domain owned by the active process. However, in this case this function would not suffice. The particles generated as the locally owned support points of an FE_Q object on an arbitrary grid (non-matching with regard to the fluid grid) have no reasons to lie in the same physical region of the locally owned subdomain of the fluid grid. In fact this will almost never be the case, especially since we want to keep track of what is happening to the particles themselves.   
*   In particle-in-cell methods (PIC), it is often customary to assign ownership of the particles to the process where the particles lie. In this tutorial we illustrate a different approach, which is useful if one wants to keep track of information related to the particles (for example, if a particle is associated to a given degree of freedom, which is owned by a specific process and not necessarily the same process that owns the fluid cell where the particle happens to be at any given time). In the approach used here, ownership of the particles is assigned once at the beginning, and one-to-one communication happens whenever the original owner needs information from the process that owns the cell where the particle lives. We make sure that we set ownership of the particles using the initial particle distribution, and keep the same ownership throughout the execution of the program.   
*   With this overview out of the way, let us see what the function does. At the top, we create a temporary triangulation and DoFHandler object from which we will take the node locations for initial particle locations:
* 

* 
* [1.x.117]
* 
*  This is where things start to get complicated. Since we may run this program in a parallel environment, every parallel process will now have created these temporary triangulations and DoFHandlers. But, in fully distributed triangulations, the active process only knows about the locally owned cells, and has no idea of how other processes have distributed their own cells. This is true for both the temporary triangulation created above as well as the fluid triangulation into which we want to embed the particles below. On the other hand, these locally known portions of the two triangulations will, in general, not overlap. That is, the locations of the particles we will create from the node locations of the temporary mesh are arbitrary, and may fall within a region of the fluid triangulation that the current process doesn't have access to (i.e., a region of the fluid domain where cells are artificial). In order to understand who to send those particles to, we need to have a (rough) idea of how the fluid grid is distributed among processors.     
*   We construct this information by first building an index tree of boxes bounding the locally owned cells, and then extracting one of the first levels of the tree:
* 

* 
* [1.x.118]
* 
*  Each process now has a collection of bounding boxes that completely enclose all locally owned processes (but that may overlap the bounding boxes of other processes). We then exchange this information between all participating processes so that every process knows the bounding boxes of all other processes.     
*   Equipped with this knowledge, we can then initialize the `tracer_particle_handler` to the fluid mesh and generate the particles from the support points of the (temporary) tracer particles triangulation. This function call uses the `global_bounding_boxes` object we just constructed to figure out where to send the particles whose locations were derived from the locally owned part of the `particles_dof_handler`. At the end of this call, every particle will have been distributed to the correct process (i.e., the process that owns the fluid cell where the particle lives). We also output their number to the screen at this point.
* 

* 
* [1.x.119]
* 
*  Each particle so created has a unique ID. At some point in the algorithm below, we will need vectors containing position and velocity information for each particle. This vector will have size `n_particles spacedim`, and we will have to store the elements of this vector in a way so that each parallel process "owns" those elements that correspond to coordinates of the particles it owns. In other words, we have to partition the index space between zero and `n_particles spacedim` among all processes. We can do this by querying the `tracer_particle_handler` for the IDs of its locally relevant particles, and construct the indices that would be needed to store in a (parallel distributed) vector of the position and velocity of all particles where we implicitly assume that we store the coordinates of each location or velocity in `spacedim` successive vector elements (this is what the  [2.x.253]  function does).
* 

* 
* [1.x.120]
* 
*  At the beginning of the simulation, all particles are in their original position. When particles move, they may traverse to a part of the domain which is owned by another process. If this happens, the current process keeps formally "ownership" of the particles, but may need read access from the process where the particle has landed. We keep this information in another index set, which stores the indices of all particles that are currently on the current process's subdomain, independently if they have always been here or not.     
*   Keeping this index set around allows us to leverage linear algebra classes for all communications regarding positions and velocities of the particles. This mimics what would happen in the case where another problem was solved in the solid domain (as in fluid-structure interaction. In this latter case, additional DOFs on the solid domain would be coupled to what is occurring in the fluid domain.
* 

* 
* [1.x.121]
* 
*  Finally, we make sure that upon refinement, particles are correctly transferred. When performing local refinement or coarsening, particles will land in another cell. We could in principle redistribute all particles after refining, however this would be overly expensive.     
*   The  [2.x.254]  class has a way to transfer information from a cell to its children or to its parent upon refinement, without the need to reconstruct the entire data structure. This is done by registering two callback functions to the triangulation. These functions will receive a signal when refinement is about to happen, and when it has just happened, and will take care of transferring all information to the newly refined grid with minimal computational cost.
* 

* 
* [1.x.122]
* 
*  Similarly to what we have done for passive tracers, we next set up the particles that track the quadrature points of the solid mesh. The main difference here is that we also want to attach a weight value (the "JxW" value of the quadrature point) to each of particle, so that we can compute integrals even without direct access to the original solid grid.   
*   This is achieved by leveraging the "properties" concept of the  [2.x.255]  class. It is possible to store (in a memory efficient way) an arbitrary number of `double` numbers for each of the  [2.x.256]  objects inside a  [2.x.257]  object. We use this possibility to store the JxW values of the quadrature points of the solid grid.   
*   In our case, we only need to store one property per particle: the JxW value of the integration on the solid grid. This is passed at construction time to the solid_particle_handler object as the last argument
* 

* 
* [1.x.123]
* 
*  The number of particles that we generate locally is equal to the total number of locally owned cells times the number of quadrature points used in each cell. We store all these points in a vector, and their corresponding properties in a vector of vectors:
* 

* 
* [1.x.124]
* 
*  We proceed in the same way we did with the tracer particles, reusing the computed bounding boxes. However, we first check that the `global_fluid_bounding_boxes` object has been actually filled. This should certainly be the case here, since this method is called after the one that initializes the tracer particles. However, we want to make sure that if in the future someone decides (for whatever reason) to initialize first the solid particle handler, or to copy just this part of the tutorial, a meaningful exception is thrown when things don't work as expected     
*   Since we have already stored the position of the quadrature points, we can use these positions to insert the particles directly using the `solid_particle_handler` instead of having to go through a  [2.x.258]  function:
* 

* 
* [1.x.125]
* 
*  As in the previous function, we end by making sure that upon refinement, particles are correctly transferred:
* 

* 
* [1.x.126]
* 
*   [1.x.127]  [1.x.128]
* 

* 
*  We set up the finite element space and the quadrature formula to be used throughout the step. For the fluid, we use Taylor-Hood elements (e.g.  [2.x.259] ). Since we do not solve any equation on the solid domain, an empty finite element space is generated. A natural extension of this program would be to solve a fluid structure interaction problem, which would require that the `solid_fe` use more useful FiniteElement class.   
*   Like for many other functions, we store the time necessary to carry out the operations we perform here. The current function puts its timing information into a section with label "Initial setup". Numerous other calls to this timer are made in various functions. They allow to monitor the absolute and relative cost of each individual function to identify bottlenecks.
* 

* 
* [1.x.129]
* 
*  We next construct the distributed block matrices and vectors which are used to solve the linear equations that arise from the problem. This function is adapted from  [2.x.260]  and we refer to this step for a thorough explanation.
* 

* 
* [1.x.130]
* 
*   [1.x.131]  [1.x.132]
* 

* 
*  We assemble the system matrix, the preconditioner matrix, and the right hand side. The code is adapted from  [2.x.261] , which is essentially what  [2.x.262]  also has, and is pretty standard if you know what the Stokes equations look like.
* 

* 
* [1.x.133]
* 
*  The following method is then the one that deals with the penalty terms that result from imposing the velocity on the impeller. It is, in a sense, the heart of the tutorial, but it is relatively straightforward. Here we exploit the `solid_particle_handler` to compute the Nitsche restriction or the penalization in the embedded domain.
* 

* 
* [1.x.134]
* 
*  We loop over all the local particles. Although this could be achieved directly by looping over all the cells, this would force us to loop over numerous cells which do not contain particles. Consequently, we loop over all the particles, but, we get the reference of the cell in which the particle lies and then loop over all particles within that cell. This enables us to skip the cells which do not contain particles, yet to assemble the local matrix and rhs of each cell to apply the Nitsche restriction. Once we are done with all particles on one cell, we advance the `particle` iterator to the particle past the end of the ones on the current cell (this is the last line of the `while` loop's body).
* 

* 
* [1.x.135]
* 
*  We get an iterator to the cell within which the particle lies from the particle itself. We can then assemble the additional terms in the system matrix and the right hand side as we would normally.
* 

* 
* [1.x.136]
* 
*  So then let us get the collection of cells that are located on this cell and iterate over them. From each particle we gather the location and the reference location of the particle as well as the additional information that is attached to the particle. In the present case, this information is the "JxW" of the quadrature points which were used to generate the particles.         
*   Using this information, we can add the contribution of the quadrature point to the local_matrix and local_rhs. We can evaluate the value of the shape function at the position of each particle easily by using its reference location.
* 

* 
* [1.x.137]
* 
*   [1.x.138]  [1.x.139]
* 

* 
*  This function solves the linear system with FGMRES with a block diagonal preconditioner and an algebraic multigrid (AMG) method for the diagonal blocks. The preconditioner applies a V cycle to the  [2.x.263]  (i.e., the velocity-velocity) block and a CG with the mass matrix for the  [2.x.264]  block (which is our approximation to the Schur complement: the pressure mass matrix assembled above).
* 

* 
* [1.x.140]
* 
*   [1.x.141]  [1.x.142]
* 

* 
*  We deal with mesh refinement in a completely standard way:
* 

* 
* [1.x.143]
* 
*   [1.x.144]  [1.x.145]
* 

* 
*  We output the results (velocity and pressure) on the fluid domain using the standard parallel capabilities of deal.II. A single compressed vtu file is written that agglomerates the information of all processors. An additional `.pvd` record is written to associate the physical time to the vtu files.
* 

* 
* [1.x.146]
* 
*  Similarly, we write the particles (either from the solid or the tracers) as a single compressed vtu file through the  [2.x.265]  object. This simple object does not write the additional information attached as "properties" to the particles, but only writes their id
* 
*  -  but then, we don't care about the "JxW" values of these particle locations anyway, so no information that we may have wanted to visualize is lost.
* 

* 
* [1.x.147]
* 
*   [1.x.148]  [1.x.149]
* 

* 
*  This function now orchestrates the entire simulation. It is very similar to the other time dependent tutorial programs
* 
*  -  take  [2.x.266]  or  [2.x.267]  as an example. At the beginning, we output some status information and also save all current parameters to a file in the output directory, for reproducibility.
* 

* 
* [1.x.150]
* 
*  We then start the time loop. We initialize all the elements of the simulation in the first cycle
* 

* 
* [1.x.151]
* 
*  After the first time step, we displace the solid body at the beginning of each time step to take into account the fact that is has moved.
* 

* 
* [1.x.152]
* 
*  In order to update the state of the system, we first interpolate the fluid velocity at the position of the tracer particles and, with a naive explicit Euler scheme, advect the massless tracer particles.
* 

* 
* [1.x.153]
* 
*  Using these new locations, we can then assemble the Stokes system and solve it.
* 

* 
* [1.x.154]
* 
*  With the appropriate frequencies, we then write the information of the solid particles, the tracer particles, and the fluid domain into files for visualization, and end the time step by adapting the mesh.
* 

* 
* [1.x.155]
* 
*   [1.x.156]  [1.x.157]
* 

* 
*  The remainder of the code, the `main()` function, is standard, with the exception of the handling of input parameter files. We allow the user to specify an optional parameter file as an argument to the program. If nothing is specified, we use the default file "parameters.prm", which is created if non existent. The file name is scanned for the the string "23" first, and "3" afterwards. If the filename contains the string "23", the problem classes are instantiated with template arguments 2 and 3 respectively. If only the string "3" is found, then both template arguments are set to 3, otherwise both are set to 2.
* 

* 
*  If the program is called without any command line arguments (i.e., `argc==1`), then we just use "parameters.prm" by default.
* 

* 
* [1.x.158]
* [1.x.159][1.x.160]
* 

* The directory in which this program is run contains a number of sampleparameter files that you can use to reproduce the results presented in thissection. If you do not specify a parameter file as an argument on the commandline, the program will try to read the file "`parameters.prm`" by default, andwill execute the two dimensional version of the code. As explained inthe discussion of the source code, ifyour file name contains the string "23", then the program will run a threedimensional problem, with immersed solid of co-dimension one. If it containsthe string "3", it will run a three dimensional problem, with immersed solid ofco-dimension zero, otherwise it will run a two dimensional problem withimmersed solid of co-dimension zero.
* Regardless of the specific parameter file name, if the specified file does notexist, when you execute the program you will get an exception that no such filecan be found:
* [1.x.161]
* 
* However, as the error message already states, the code that triggers theexception will also generate the specified file ("`parameters.prm`" in this case)that simply contains the default values for all parameters this program caresabout (for the correct dimension and co-dimension, according to the whether astring "23" or "3" is contained in the file name). By inspection of the defaultparameter file, we see the following:
* [1.x.162]
* 
* If you now run the program, you will get a file called `parameters_22.prm` inthe directory specified by the parameter `Output directory` (which defaults tothe current directory) containing a shorter version of the above parameters(without comments and documentation), documenting all parameters that were usedto run your program:
* [1.x.163]
* 
* The rationale behind creating first `parameters.prm` file (the first time theprogram is run) and then a `output/parameters_22.prm` (every time yourun the program with an existing input file), is because you may wantto leave most parameters to theirdefault values, and only modify a handful of them, while still beeing able toreproduce the results and inspect what parameters were used for a specificsimulation. It is generally good scientific practice to store theparameter file you used for a simulation along with the simulationoutput so that you can repeat the exact same run at a later time if necessary.
* Another reason is because the input file may only contain thoseparameters that differ from their defaults.For example, you could use the following (perfectly valid) parameter file withthis tutorial program:
* [1.x.164]
* and you would run the program with Q3/Q2 Taylor-Hood finite elements, for 101steps, using a Nitsche penalty of `10`, and leaving all the other parameters totheir default value. The output directory then contains a record ofnot just these parameters, but indeed all parameters used in thesimulation. You can inspect all the other parameters in theproduced file `parameters_22.prm`.
* 

* [1.x.165][1.x.166]
* 

* The default problem generates a co-dimension zero impeller, consisting of arotating rectangular grid, where the rotation is for half a time unit in onedirection, and half a time unit in the opposite direction, with constant angularvelocity equal to  [2.x.268] . Consequently, the impeller does half arotation and returns to its original position. The following animationdisplays the velocity magnitude, the motion of the solid impeller and of thetracer particles.
* 

*  [2.x.269] 
* On one core, the output of the program will look like the following:
* [1.x.167]
* 
* You may notice that assembling the coupling system is more expensive thanassembling the Stokes part. This depends highly on the number of Gauss points(solid particles) that are used to apply the Nitsche restriction.In the present case, a relatively low number of tracer particles are used.Consequently, tracking their motion is relatively cheap.
* The following movie shows the evolution of the solution over time:
* [1.x.168]
* 
* The movie shows the rotating obstacle in gray (actually asuperposition of the solid particles plotted with large enough dotsthat they overlap), [1.x.169] in light colors (including the corner verticesthat form at specific times during the simulation), and the tracer particles inbluish tones.
* The simulation shows that at the end time,the tracer particles have somewhat returned to theiroriginal position, although they have been distorted by the flow field. Thefollowing image compares the initial and the final position of the particlesafter one time unit of flow.
*  [2.x.270] 
* In this case, we see that the tracer particles that were outside of the sweptvolume of the impeller have returned very close to their initial position,whereas those in the swept volume were slightly more deformed. This deformationis non-physical. It is caused by the numerical error induced by the explicitEuler scheme used to advect the particles, by the loss of accuracy due to thefictitious domain and, finally, by the discretization error on the Stokesequations. The first two errors are the leading cause of this deformation andthey could be alleviated by the use of a finer mesh and a lower time step.
* 

* [1.x.170][1.x.171]
* 

* To play around a little bit, we complicate the fictitious domain (taken fromhttps://grabcad.com/library/lungstors-blower-1), and run a co-dimension onesimulation in three space dimensions, using the following"`parameters_23.prm`" file:
* [1.x.172]
* 
* In this case, the timing outputs are a bit different:
* [1.x.173]
* 
* Now, the solver is taking most of the solution time in three dimensions,and the particle motion and Nitsche assembly remain relativelyunimportant as far as run time is concerned.
* 

* 
* [1.x.174]
* 
* 

* [1.x.175][1.x.176][1.x.177]
* 

* The current tutorial program shows a one-way coupling between the fluid and thesolid, where the solid motion is imposed (and not solved for), and read in thesolid domain by exploiting the location and the weights of the solid quadraturepoints.
* The structure of the code already allows one to implement a two-way coupling,by exploiting the possibility to read values of the fluid velocity on thequadrature points of the solid grid. For this to be more efficient in terms ofMPI communication patterns, one should maintain ownership of the quadraturepoints on the solid processor that owns the cells where they have been created.In the current code, it is sufficient to define the IndexSet of the vectorsused to exchange information of the quadrature points by using the solidpartition instead of the initial fluid partition.
* This allows the combination of the technique used in this tutorial program withthose presented in the tutorial  [2.x.271]  to solve a fluid structure interactionproblem with distributed Lagrange multipliers, on [2.x.272]  objects.
* The timings above show that the current preconditioning strategy does not workwell for Nitsche penalization, and we should come up with a betterpreconditioner if we want to aim at larger problems. Moreover, a checkpointrestart strategy should be implemented to allow for longer simulations to beinterrupted and restored, as it is done for example in the  [2.x.273]  tutorial.
* 

* [1.x.178][1.x.179] [2.x.274] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-7_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27]
* [1.x.28][1.x.29][1.x.30]
* 

* In this program, we will mainly consider two aspects: [2.x.2]    [2.x.3]  Verification of correctness of the program and generation of convergence  tables;   [2.x.4]  Non-homogeneous Neumann boundary conditions for the Helmholtz equation. [2.x.5] Besides these topics, again a variety of improvements and tricks will beshown.
* 

* [1.x.31][1.x.32]
* 

* There has probably never been anon-trivial finite element program that worked right from the start. It istherefore necessary to find ways to verify whether a computed solution iscorrect or not. Usually, this is done by choosing the set-up of a simulationin such a way that we know the exact continuous solution and evaluate the differencebetween continuous and computed discrete solution. If this differenceconverges to zero with the right order of convergence, this is already a goodindication of correctness, although there may be other sources of errorpersisting which have only a small contribution to the total error or are ofhigher order. In the context of finite element simulations, this techniqueof picking the solution by choosing appropriate right hand sides andboundary conditionsis often called the [1.x.33].
* In this example, we will not go into the theories of systematic softwareverification which is a very complicated problem. Rather we will demonstratethe tools which deal.II can offer in this respect. This is basically centeredaround the functionality of a single function,  [2.x.6] This function computes the difference between a given continuous function anda finite element field in various norms on each cell.Of course, like with any other integral, we can only evaluate these norms using quadrature formulas;the choice of the right quadrature formula is therefore crucial to theaccurate evaluation of the error. This holds in particular for the  [2.x.7] norm, where we evaluate the maximal deviation of numerical and exact solutiononly at the quadrature points; one should then not try to use a quadraturerule whose evaluation occurs only at points where[super-convergence](https://en.wikipedia.org/wiki/Superconvergence) might occur, such asthe Gauss points of the lowest-order Gauss quadrature formula for which theintegrals in the assembly of the matrix is correct (e.g., for linear elements,do not use the QGauss(2) quadrature formula). In fact, this is generally goodadvice also for the other norms: if your quadrature points are fortuitouslychosen at locations where the error happens to be particularly small due tosuperconvergence, the computed error will look like it is much smaller thanit really is and may even suggest a higher convergence order. Consequently,we will choose a different quadrature formula for the integration of theseerror norms than for the assembly of the linear system.
* The function  [2.x.8]  evaluates the desired norm on eachcell  [2.x.9]  of the triangulation and returns a vector which holds thesevalues for each cell. From the local values, we can then obtain the global error. Forexample, if the vector  [2.x.10]  with element  [2.x.11]  for all cells [2.x.12]  contains the local  [2.x.13]  norms  [2.x.14] , then[1.x.34]is the global  [2.x.15]  error  [2.x.16] .
* In the program, we will show how to evaluate and use these quantities, and wewill monitor their values under mesh refinement. Of course, we have to choosethe problem at hand such that we can explicitly state the solution and itsderivatives, but since we want to evaluate the correctness of the program,this is only reasonable. If we know that the program produces the correctsolution for one (or, if one wants to be really sure: many) specificallychosen right hand sides, we can be rather confident that it will also computethe correct solution for problems where we don't know the exact values.
* In addition to simply computing these quantities, we will show how to generatenicely formatted tables from the data generated by this program thatautomatically computes convergence rates etc. In addition, we will comparedifferent strategies for mesh refinement.
* 

* [1.x.35][1.x.36]
* 

* The second, totallyunrelated, subject of this example program is the use of non-homogeneousboundary conditions. These are included into the variational form usingboundary integrals which we have to evaluate numerically when assembling theright hand side vector.
* Before we go into programming, let's have a brief look at the mathematicalformulation. The equation that we want to solve here is the Helmholtz equation"with the nice sign":[1.x.37]on the square  [2.x.17]  with  [2.x.18] , augmented by Dirichlet boundary conditions[1.x.38]on some part  [2.x.19]  of the boundary  [2.x.20] , and Neumann conditions[1.x.39]on the rest  [2.x.21] .In our particular testcase, we will use  [2.x.22] .(We say that this equation has the "nice sign" because the operator [2.x.23]  with the identity  [2.x.24]  and  [2.x.25]  is a positive definiteoperator; the [1.x.40] is  [2.x.26]  and results from modelingtime-harmonic processes. The operator is not positivedefinite if  [2.x.27]  is large, and this leads to all sorts of issueswe need not discuss here. The operator may also not be invertible
* 
*  - i.e., the equation does not have a unique solution
* 
*  -  if  [2.x.28] happens to be one of the eigenvalues of  [2.x.29] .)
* Because we want to verify the convergence of our numerical solution  [2.x.30] ,we want a setup so that we know the exact solution  [2.x.31] . This is wherethe Method of Manufactured Solutions comes in. To this end, let uschoose a function[1.x.41]where the centers  [2.x.32]  of the exponentials are   [2.x.33] ,   [2.x.34] , and   [2.x.35] ,and the half width is set to  [2.x.36] . The method of manufacturedsolution then says: choose
* [1.x.42]
* With this particular choice, we infer that of course the solution of theoriginal problem happens to be  [2.x.37] . In other words, by choosingthe right hand sides of the equation and the boundary conditions in aparticular way, we have manufactured ourselves a problem to which weknow the solution. This allows us then to compute the error of ournumerical solution. In the code below, we represent  [2.x.38]  by the [2.x.39]  class, and other classes will be used todenote  [2.x.40]  and  [2.x.41] .
* Using the above definitions, we can state the weak formulation of theequation, which reads: find  [2.x.42]  suchthat[1.x.43]for all test functions  [2.x.43] . Theboundary term  [2.x.44]  has appeared by integration by parts andusing  [2.x.45]  on  [2.x.46]  and  [2.x.47]  on  [2.x.48] . The cellmatrices and vectors which we use to build the global matrices and right handside vectors in the discrete formulation therefore look like this:[1.x.44]
* Since the generation of the domain integrals has been shown in previousexamples several times, only the generation of the contour integral is ofinterest here. It basically works along the following lines: for domainintegrals we have the  [2.x.49]  class that provides values andgradients of the shape values, as well as Jacobian determinants and otherinformation and specified quadrature points in the cell; likewise, there is aclass  [2.x.50]  that performs these tasks for integrations onfaces of cells. One provides it with a quadrature formula for a manifold withdimension one less than the dimension of the domain is, and the cell and thenumber of its face on which we want to perform the integration. The class willthen compute the values, gradients, normal vectors, weights, etc. at thequadrature points on this face, which we can then use in the same way as forthe domain integrals. The details of how this is done are shown in thefollowing program.
* 

* [1.x.45][1.x.46]
* 

* Besides the mathematical topics outlined above, we also want to use thisprogram to illustrate one aspect of good programming practice, namely the useof namespaces. In programming the deal.II library, we have take great care notto use names for classes and global functions that are overly generic, say [2.x.51]  etc. Furthermore, we have put everything intonamespace  [2.x.52] . But when one writes application programs thataren't meant for others to use, one doesn't always pay this much attention. Ifyou follow the programming style of  [2.x.53]  through  [2.x.54] , these functionsthen end up in the global namespace where, unfortunately, a lot of other stuffalso lives (basically everything the C language provides, along witheverything you get from the operating system through header files). To makethings a bit worse, the designers of the C language were also not alwayscareful in avoiding generic names; for example, the symbols <code>j1,jn</code> are defined in C header files (they denote Bessel functions).
* To avoid the problems that result if names of different functions or variablescollide (often with confusing error messages), it is good practice to puteverything you do into a [1.x.47]. Followingthis style, we will open a namespace  [2.x.55]  at the top of theprogram, import the deal.II namespace into it, put everything that's specificto this program (with the exception of  [2.x.56] , which must be inthe global namespace) into it, and only close it at the bottom of the file. Inother words, the structure of the program is of the kind
* [1.x.48]
* We will follow this scheme throughout the remainder of the deal.II tutorial.
* 

*  [1.x.49] [1.x.50]
*   [1.x.51]  [1.x.52]
* 

* 
*  These first include files have all been treated in previous examples, so we won't explain what is in them again.
* 

* 
* [1.x.53]
* 
*  In this example, we will not use the numeration scheme which is used per default by the DoFHandler class, but will renumber them using the Cuthill-McKee algorithm. As has already been explained in  [2.x.57] , the necessary functions are declared in the following file:
* 

* 
* [1.x.54]
* 
*  Then we will show a little trick how we can make sure that objects are not deleted while they are still in use. For this purpose, deal.II has the SmartPointer helper class, which is declared in this file:
* 

* 
* [1.x.55]
* 
*  Next, we will want to use the function  [2.x.58]  mentioned in the introduction, and we are going to use a ConvergenceTable that collects all important data during a run and prints it at the end as a table. These comes from the following two files:
* 

* 
* [1.x.56]
* 
*  And finally, we need to use the FEFaceValues class, which is declared in the same file as the FEValues class:
* 

* 
* [1.x.57]
* 
*  The last step before we go on with the actual implementation is to open a namespace  [2.x.59]  into which we will put everything, as discussed at the end of the introduction, and to import the members of namespace  [2.x.60]  into it:
* 

* 
* [1.x.58]
* 
*   [1.x.59]  [1.x.60]
* 

* 
*  Before implementing the classes that actually solve something, we first declare and define some function classes that represent right hand side and solution classes. Since we want to compare the numerically obtained solution to the exact continuous one, we need a function object that represents the continuous solution. On the other hand, we need the right hand side function, and that one of course shares some characteristics with the solution. In order to reduce dependencies which arise if we have to change something in both classes at the same time, we move the common characteristics of both functions into a base class.   
*   The common characteristics for solution (as explained in the introduction, we choose a sum of three exponentials) and right hand side, are these: the number of exponentials, their centers, and their half width. We declare them in the following class. Since the number of exponentials is a compile-time constant we use a fixed-length  [2.x.61]  to store the center points:
* 

* 
* [1.x.61]
* 
*  The variables which denote the centers and the width of the exponentials have just been declared, now we still need to assign values to them. Here, we can show another small piece of template sorcery, namely how we can assign different values to these variables depending on the dimension. We will only use the 2d case in the program, but we show the 1d case for exposition of a useful technique.   
*   First we assign values to the centers for the 1d case, where we place the centers equidistantly at
* 
*  - /3, 0, and 1/3. The <code>template &lt;&gt;</code> header for this definition indicates an explicit specialization. This means, that the variable belongs to a template, but that instead of providing the compiler with a template from which it can specialize a concrete variable by substituting  [2.x.62]  with some concrete value, we provide a specialization ourselves, in this case for  [2.x.63] . If the compiler then sees a reference to this variable in a place where the template argument equals one, it knows that it doesn't have to generate the variable from a template by substituting  [2.x.64] , but can immediately use the following definition:
* 

* 
* [1.x.62]
* 
*  Likewise, we can provide an explicit specialization for  [2.x.65] . We place the centers for the 2d case as follows:
* 

* 
* [1.x.63]
* 
*  There remains to assign a value to the half-width of the exponentials. We would like to use the same value for all dimensions. In this case, we simply provide the compiler with a template from which it can generate a concrete instantiation by substituting  [2.x.66]  with a concrete value:
* 

* 
* [1.x.64]
* 
*  After declaring and defining the characteristics of solution and right hand side, we can declare the classes representing these two. They both represent continuous functions, so they are derived from the Function&lt;dim&gt; base class, and they also inherit the characteristics defined in the SolutionBase class.   
*   The actual classes are declared in the following. Note that in order to compute the error of the numerical solution against the continuous one in the L2 and H1 (semi-)norms, we have to provide value and gradient of the exact solution. This is more than we have done in previous examples, where all we provided was the value at one or a list of points. Fortunately, the Function class also has virtual functions for the gradient, so we can simply overload the respective virtual member functions in the Function base class. Note that the gradient of a function in  [2.x.67]  space dimensions is a vector of size  [2.x.68] , i.e. a tensor of rank 1 and dimension  [2.x.69] . As for so many other things, the library provides a suitable class for this. One new thing about this class is that it explicitly uses the Tensor objects, which previously appeared as intermediate terms in  [2.x.70]  and  [2.x.71] . A tensor is a generalization of scalars (rank zero tensors), vectors (rank one tensors), and matrices (rank two tensors), as well as higher dimensional objects. The Tensor class requires two template arguments: the tensor rank and tensor dimension. For example, here we use tensors of rank one (vectors) with dimension  [2.x.72]  entries.) While this is a bit less flexible than using Vector, the compiler can generate faster code when the length of the vector is known at compile time. Additionally, specifying a Tensor of rank one and dimension  [2.x.73]  guarantees that the tensor will have the right shape (since it is built into the type of the object itself), so the compiler can catch most size-related mistakes for us.
* 

* 
* [1.x.65]
* 
*  The actual definition of the values and gradients of the exact solution class is according to their mathematical definition and does not need much explanation.   
*   The only thing that is worth mentioning is that if we access elements of a base class that is template dependent (in this case the elements of SolutionBase&lt;dim&gt;), then the C++ language forces us to write  [2.x.74] , and similarly for other members of the base class. C++ does not require the  [2.x.75]  qualification if the base class is not template dependent. The reason why this is necessary is complicated; C++ books will explain under the phrase [1.x.66], and there is also a lengthy description in the deal.II FAQs.
* 

* 
* [1.x.67]
* 
*  Likewise, this is the computation of the gradient of the solution.  In order to accumulate the gradient from the contributions of the exponentials, we allocate an object  [2.x.76]  that denotes the mathematical quantity of a tensor of rank  [2.x.77]  and dimension  [2.x.78] . Its default constructor sets it to the vector containing only zeroes, so we need not explicitly care for its initialization.   
*   Note that we could as well have taken the type of the object to be Point&lt;dim&gt; instead of Tensor&lt;1,dim&gt;. Tensors of rank 1 and points are almost exchangeable, and have only very slightly different mathematical meanings. In fact, the Point&lt;dim&gt; class is derived from the Tensor&lt;1,dim&gt; class, which makes up for their mutual exchange ability. Their main difference is in what they logically mean: points are points in space, such as the location at which we want to evaluate a function (see the type of the first argument of this function for example). On the other hand, tensors of rank 1 share the same transformation properties, for example that they need to be rotated in a certain way when we change the coordinate system; however, they do not share the same connotation that points have and are only objects in a more abstract space than the one spanned by the coordinate directions. (In fact, gradients live in `reciprocal' space, since the dimension of their components is not that of a length, but of one over length).
* 

* 
* [1.x.68]
* 
*  For the gradient, note that its direction is along (x-x_i), so we add up multiples of this distance vector, where the factor is given by the exponentials.
* 

* 
* [1.x.69]
* 
*  Besides the function that represents the exact solution, we also need a function which we can use as right hand side when assembling the linear system of discretized equations. This is accomplished using the following class and the following definition of its function. Note that here we only need the value of the function, not its gradients or higher derivatives.
* 

* 
* [1.x.70]
* 
*  The value of the right hand side is given by the negative Laplacian of the solution plus the solution itself, since we wanted to solve Helmholtz's equation:
* 

* 
* [1.x.71]
* 
*  The first contribution is the Laplacian:
* 

* 
* [1.x.72]
* 
*  And the second is the solution itself:
* 

* 
* [1.x.73]
* 
*   [1.x.74]  [1.x.75]
* 

* 
*  Then we need the class that does all the work. Except for its name, its interface is mostly the same as in previous examples.   
*   One of the differences is that we will use this class in several modes: for different finite elements, as well as for adaptive and global refinement. The decision whether global or adaptive refinement shall be used is communicated to the constructor of this class through an enumeration type declared at the top of the class. The constructor then takes a finite element object and the refinement mode as arguments.   
*   The rest of the member functions are as before except for the  [2.x.79]  function: After the solution has been computed, we perform some analysis on it, such as computing the error in various norms. To enable some output, it requires the number of the refinement cycle, and consequently gets it as an argument.
* 

* 
* [1.x.76]
* 
*  Now for the data elements of this class. Among the variables that we have already used in previous examples, only the finite element object differs: The finite elements which the objects of this class operate on are passed to the constructor of this class. It has to store a pointer to the finite element for the member functions to use. Now, for the present class there is no big deal in that, but since we want to show techniques rather than solutions in these programs, we will here point out a problem that often occurs
* 
*  -  and of course the right solution as well.     
*   Consider the following situation that occurs in all the example programs: we have a triangulation object, and we have a finite element object, and we also have an object of type DoFHandler that uses both of the first two. These three objects all have a lifetime that is rather long compared to most other objects: they are basically set at the beginning of the program or an outer loop, and they are destroyed at the very end. The question is: can we guarantee that the two objects which the DoFHandler uses, live at least as long as they are in use? This means that the DoFHandler must have some kind of knowledge on the destruction of the other objects.     
*   We will show here how the library managed to find out that there are still active references to an object and the object is still alive from the point of view of a using object. Basically, the method is along the following line: all objects that are subject to such potentially dangerous pointers are derived from a class called Subscriptor. For example, the Triangulation, DoFHandler, and a base class of the FiniteElement class are derived from Subscriptor. This latter class does not offer much functionality, but it has a built-in counter which we can subscribe to, thus the name of the class. Whenever we initialize a pointer to that object, we can increase its use counter, and when we move away our pointer or do not need it any more, we decrease the counter again. This way, we can always check how many objects still use that object. Additionally, the class requires to know about a pointer that it can use to tell the subscribing object about its invalidation.     
*   If an object of a class that is derived from the Subscriptor class is destroyed, it also has to call the destructor of the Subscriptor class. In this destructor, we tell all the subscribing objects about the invalidation of the object using the stored pointers. The same happens when the object appears on the right hand side of a move expression, i.e., it will no longer contain valid content after the operation. The subscribing class is expected to check the value stored in its corresponding pointer before trying to access the object subscribed to.     
*   This is exactly what the SmartPointer class is doing. It basically acts just like a pointer, i.e. it can be dereferenced, can be assigned to and from other pointers, and so on. On top of that it uses the mechanism described above to find out if the pointer this class is representing is dangling when we try to dereference it. In that case an exception is thrown.     
*   In the present example program, we want to protect the finite element object from the situation that for some reason the finite element pointed to is destroyed while still in use. We therefore use a SmartPointer to the finite element object; since the finite element object is actually never changed in our computations, we pass a const FiniteElement&lt;dim&gt; as template argument to the SmartPointer class. Note that the pointer so declared is assigned at construction time of the solve object, and destroyed upon destruction, so the lock on the destruction of the finite element object extends throughout the lifetime of this HelmholtzProblem object.
* 

* 
* [1.x.77]
* 
*  The second to last variable stores the refinement mode passed to the constructor. Since it is only set in the constructor, we can declare this variable constant, to avoid that someone sets it involuntarily (e.g. in an `if'-statement where == was written as = by chance).
* 

* 
* [1.x.78]
* 
*  For each refinement level some data (like the number of cells, or the L2 error of the numerical solution) will be generated and later printed. The TableHandler can be used to collect all this data and to output it at the end of the run as a table in a simple text or in LaTeX format. Here we don't only use the TableHandler but we use the derived class ConvergenceTable that additionally evaluates rates of convergence:
* 

* 
* [1.x.79]
* 
*   [1.x.80]  [1.x.81]
* 

* 
*   [1.x.82]  [1.x.83]
* 

* 
*  In the constructor of this class, we only set the variables passed as arguments, and associate the DoF handler object with the triangulation (which is empty at present, however).
* 

* 
* [1.x.84]
* 
*   [1.x.85]  [1.x.86]
* 

* 
*  The following function sets up the degrees of freedom, sizes of matrices and vectors, etc. Most of its functionality has been showed in previous examples, the only difference being the renumbering step immediately after first distributing degrees of freedom.   
*   Renumbering the degrees of freedom is not overly difficult, as long as you use one of the algorithms included in the library. It requires only a single line of code. Some more information on this can be found in  [2.x.80] .   
*   Note, however, that when you renumber the degrees of freedom, you must do so immediately after distributing them, since such things as hanging nodes, the sparsity pattern etc. depend on the absolute numbers which are altered by renumbering.   
*   The reason why we introduce renumbering here is that it is a relatively cheap operation but often has a beneficial effect: While the CG iteration itself is independent of the actual ordering of degrees of freedom, we will use SSOR as a preconditioner. SSOR goes through all degrees of freedom and does some operations that depend on what happened before; the SSOR operation is therefore not independent of the numbering of degrees of freedom, and it is known that its performance improves by using renumbering techniques. A little experiment shows that indeed, for example, the number of CG iterations for the fifth refinement cycle of adaptive refinement with the Q1 program used here is 40 without, but 36 with renumbering. Similar savings can generally be observed for all the computations in this program.
* 

* 
* [1.x.87]
* 
*   [1.x.88]  [1.x.89]
* 

* 
*  Assembling the system of equations for the problem at hand is mostly as for the example programs before. However, some things have changed anyway, so we comment on this function fairly extensively.   
*   At the top of the function you will find the usual assortment of variable declarations. Compared to previous programs, of importance is only that we expect to solve problems also with bi-quadratic elements and therefore have to use sufficiently accurate quadrature formula. In addition, we need to compute integrals over faces, i.e.  [2.x.81]  dimensional objects. The declaration of a face quadrature formula is then straightforward:
* 

* 
* [1.x.90]
* 
*  Then we need objects which can evaluate the values, gradients, etc of the shape functions at the quadrature points. While it seems that it should be feasible to do it with one object for both domain and face integrals, there is a subtle difference since the weights in the domain integrals include the measure of the cell in the domain, while the face integral quadrature requires the measure of the face in a lower-dimensional manifold. Internally these two classes are rooted in a common base class which does most of the work and offers the same interface to both domain and interface integrals.     
*   For the domain integrals in the bilinear form for Helmholtz's equation, we need to compute the values and gradients, as well as the weights at the quadrature points. Furthermore, we need the quadrature points on the real cell (rather than on the unit cell) to evaluate the right hand side function. The object we use to get at this information is the FEValues class discussed previously.     
*   For the face integrals, we only need the values of the shape functions, as well as the weights. We also need the normal vectors and quadrature points on the real cell since we want to determine the Neumann values from the exact solution object (see below). The class that gives us this information is called FEFaceValues:
* 

* 
* [1.x.91]
* 
*  Then we need some objects already known from previous examples: An object denoting the right hand side function, its values at the quadrature points on a cell, the cell matrix and right hand side, and the indices of the degrees of freedom on a cell.     
*   Note that the operations we will do with the right hand side object are only querying data, never changing the object. We can therefore declare it  [2.x.82] :
* 

* 
* [1.x.92]
* 
*  Finally we define an object denoting the exact solution function. We will use it to compute the Neumann values at the boundary from it. Usually, one would of course do so using a separate object, in particular since the exact solution is generally unknown while the Neumann values are prescribed. We will, however, be a little bit lazy and use what we already have in information. Real-life programs would to go other ways here, of course.
* 

* 
* [1.x.93]
* 
*  Now for the main loop over all cells. This is mostly unchanged from previous examples, so we only comment on the things that have changed.
* 

* 
* [1.x.94]
* 
*  The first thing that has changed is the bilinear form. It now contains the additional term from the Helmholtz equation:
* 

* 
* [1.x.95]
* 
*  Then there is that second term on the right hand side, the contour integral. First we have to find out whether the intersection of the faces of this cell with the boundary part Gamma2 is nonzero. To this end, we loop over all faces and check whether its boundary indicator equals  [2.x.83] , which is the value that we have assigned to that portions of the boundary composing Gamma2 in the  [2.x.84]  function further below. (The default value of boundary indicators is  [2.x.85] , so faces can only have an indicator equal to  [2.x.86]  if we have explicitly set it.)
* 

* 
* [1.x.96]
* 
*  If we came into here, then we have found an external face belonging to Gamma2. Next, we have to compute the values of the shape functions and the other quantities which we will need for the computation of the contour integral. This is done using the  [2.x.87]  function which we already know from the FEValue class:
* 

* 
* [1.x.97]
* 
*  And we can then perform the integration by using a loop over all quadrature points.               
*   On each quadrature point, we first compute the value of the normal derivative. We do so using the gradient of the exact solution and the normal vector to the face at the present quadrature point obtained from the  [2.x.88]  object. This is then used to compute the additional contribution of this face to the right hand side:
* 

* 
* [1.x.98]
* 
*  Now that we have the contributions of the present cell, we can transfer it to the global matrix and right hand side vector, as in the examples before:
* 

* 
* [1.x.99]
* 
*  Likewise, elimination and treatment of boundary values has been shown previously.     
*   We note, however that now the boundary indicator for which we interpolate boundary values (denoted by the second parameter to  [2.x.89] ) does not represent the whole boundary any more. Rather, it is that portion of the boundary which we have not assigned another indicator (see below). The degrees of freedom at the boundary that do not belong to Gamma1 are therefore excluded from the interpolation of boundary values, just as we want.
* 

* 
* [1.x.100]
* 
*   [1.x.101]  [1.x.102]
* 

* 
*  Solving the system of equations is done in the same way as before:
* 

* 
* [1.x.103]
* 
*   [1.x.104]  [1.x.105]
* 

* 
*  Now for the function doing grid refinement. Depending on the refinement mode passed to the constructor, we do global or adaptive refinement.   
*   Global refinement is simple, so there is not much to comment on.  In case of adaptive refinement, we use the same functions and classes as in the previous example program. Note that one could treat Neumann boundaries differently than Dirichlet boundaries, and one should in fact do so here since we have Neumann boundary conditions on part of the boundaries, but since we don't have a function here that describes the Neumann values (we only construct these values from the exact solution when assembling the matrix), we omit this detail even though doing this in a strictly correct way would not be hard to add.   
*   At the end of the switch, we have a default case that looks slightly strange: an  [2.x.90]  condition. Since the  [2.x.91]  macro raises an error whenever the condition is false, this means that whenever we hit this statement the program will be aborted. This in intentional: Right now we have only implemented two refinement strategies (global and adaptive), but someone might want to add a third strategy (for example adaptivity with a different refinement criterion) and add a third member to the enumeration that determines the refinement mode. If it weren't for the default case of the switch statement, this function would simply run to its end without doing anything. This is most likely not what was intended. One of the defensive programming techniques that you will find all over the deal.II library is therefore to always have default cases that abort, to make sure that values not considered when listing the cases in the switch statement are eventually caught, and forcing programmers to add code to handle them. We will use this same technique in other places further down as well.
* 

* 
* [1.x.106]
* 
*   [1.x.107]  [1.x.108]
* 

* 
*  Finally we want to process the solution after it has been computed. For this, we integrate the error in various (semi-)norms, and we generate tables that will later be used to display the convergence against the continuous solution in a nice format.
* 

* 
* [1.x.109]
* 
*  Our first task is to compute error norms. In order to integrate the difference between computed numerical solution and the continuous solution (described by the Solution class defined at the top of this file), we first need a vector that will hold the norm of the error on each cell. Since accuracy with 16 digits is not so important for these quantities, we save some memory by using  [2.x.92]  instead of  [2.x.93]  values.     
*   The next step is to use a function from the library which computes the error in the L2 norm on each cell.  We have to pass it the DoF handler object, the vector holding the nodal values of the numerical solution, the continuous solution as a function object, the vector into which it shall place the norm of the error on each cell, a quadrature rule by which this norm shall be computed, and the type of norm to be used. Here, we use a Gauss formula with three points in each space direction, and compute the L2 norm.     
*   Finally, we want to get the global L2 norm. This can of course be obtained by summing the squares of the norms on each cell, and taking the square root of that value. This is equivalent to taking the l2 (lower case  [2.x.94] ) norm of the vector of norms on each cell:
* 

* 
* [1.x.110]
* 
*  By same procedure we get the H1 semi-norm. We re-use the  [2.x.95]  vector since it is no longer used after computing the  [2.x.96]  variable above. The global  [2.x.97]  semi-norm error is then computed by taking the sum of squares of the errors on each individual cell, and then the square root of it
* 
*  -  an operation that is conveniently performed by  [2.x.98] 
* 

* 
* [1.x.111]
* 
*  Finally, we compute the maximum norm. Of course, we can't actually compute the true maximum of the error overall* points in the domain, but only the maximum over a finite set of evaluation points that, for convenience, we will still call "quadrature points" and represent by an object of type Quadrature even though we do not actually perform any integration.     
*   There is then the question of what points precisely we want to evaluate at. It turns out that the result we get depends quite sensitively on the "quadrature" points being used. There is also the issue of superconvergence: Finite element solutions are, on some meshes and for polynomial degrees  [2.x.99] , particularly accurate at the node points as well as at Gauss-Lobatto points, much more accurate than at randomly chosen points. (See  [2.x.100]  and the discussion and references in Section 1.2 for more information on this.) In other words, if we are interested in finding the largest difference  [2.x.101] , then we ought to look at points  [2.x.102]  that are specifically not of this "special" kind of points and we should specifically not use `QGauss(fe->degree+1)` to define where we evaluate. Rather, we use a special quadrature rule that is obtained by iterating the trapezoidal rule by the degree of the finite element times two plus one in each space direction. Note that the constructor of the QIterated class takes a one-dimensional quadrature rule and a number that tells it how often it shall repeat this rule in each space direction.     
*   Using this special quadrature rule, we can then try to find the maximal error on each cell. Finally, we compute the global L infinity error from the L infinity errors on each cell with a call to  [2.x.103] 
* 

* 
* [1.x.112]
* 
*  After all these errors have been computed, we finally write some output. In addition, we add the important data to the TableHandler by specifying the key of the column and the value.  Note that it is not necessary to define column keys beforehand
* 
*  -  it is sufficient to just add values, and columns will be introduced into the table in the order values are added the first time.
* 

* 
* [1.x.113]
* 
*   [1.x.114]  [1.x.115]
* 

* 
*  As in previous example programs, the  [2.x.104]  function controls the flow of execution. The basic layout is as in previous examples: an outer loop over successively refined grids, and in this loop first problem setup, assembling the linear system, solution, and post-processing.   
*   The first task in the main loop is creation and refinement of grids. This is as in previous examples, with the only difference that we want to have part of the boundary marked as Neumann type, rather than Dirichlet.   
*   For this, we will use the following convention: Faces belonging to Gamma1 will have the boundary indicator  [2.x.105]  (which is the default, so we don't have to set it explicitly), and faces belonging to Gamma2 will use  [2.x.106]  as boundary indicator.  To set these values, we loop over all cells, then over all faces of a given cell, check whether it is part of the boundary that we want to denote by Gamma2, and if so set its boundary indicator to  [2.x.107] . For the present program, we consider the left and bottom boundaries as Gamma2. We determine whether a face is part of that boundary by asking whether the x or y coordinates (i.e. vector components 0 and 1) of the midpoint of a face equals
* 
*  - , up to some small wiggle room that we have to give since it is instable to compare floating point numbers that are subject to round off in intermediate computations.   
*   It is worth noting that we have to loop over all cells here, not only the active ones. The reason is that upon refinement, newly created faces inherit the boundary indicator of their parent face. If we now only set the boundary indicator for active faces, coarsen some cells and refine them later on, they will again have the boundary indicator of the parent cell which we have not modified, instead of the one we intended. Consequently, we have to change the boundary indicators of faces of all cells on Gamma2, whether they are active or not. Alternatively, we could of course have done this job on the coarsest mesh (i.e. before the first refinement step) and refined the mesh only after that.
* 

* 
* [1.x.116]
* 
*  The next steps are already known from previous examples. This is mostly the basic set-up of every finite element program:
* 

* 
* [1.x.117]
* 
*  The last step in this chain of function calls is usually the evaluation of the computed solution for the quantities one is interested in. This is done in the following function. Since the function generates output that indicates the number of the present refinement step, we pass this number as an argument.
* 

* 
* [1.x.118]
* 
*   [1.x.119]  [1.x.120]
* 

* 
*  After the last iteration we output the solution on the finest grid. This is done using the following sequence of statements which we have already discussed in previous examples. The first step is to generate a suitable filename (called  [2.x.108]  here, since we want to output data in VTK format; we add the prefix to distinguish the filename from that used for other output files further down below). Here, we augment the name by the mesh refinement algorithm, and as above we make sure that we abort the program if another refinement method is added and not handled by the following switch statement:
* 

* 
* [1.x.121]
* 
*  We augment the filename by a postfix denoting the finite element which we have used in the computation. To this end, the finite element base class stores the maximal polynomial degree of shape functions in each coordinate variable as a variable  [2.x.109] , and we use for the switch statement (note that the polynomial degree of bilinear shape functions is really 2, since they contain the term  [2.x.110] ; however, the polynomial degree in each coordinate variable is still only 1). We again use the same defensive programming technique to safeguard against the case that the polynomial degree has an unexpected value, using the  [2.x.111]  idiom in the default branch of the switch statement:
* 

* 
* [1.x.122]
* 
*  Once we have the base name for the output file, we add an extension appropriate for VTK output, open a file, and add the solution vector to the object that will do the actual output:
* 

* 
* [1.x.123]
* 
*  Now building the intermediate format as before is the next step. We introduce one more feature of deal.II here. The background is the following: in some of the runs of this function, we have used biquadratic finite elements. However, since almost all output formats only support bilinear data, the data is written only bilinear, and information is consequently lost.  Of course, we can't change the format in which graphic programs accept their inputs, but we can write the data differently such that we more closely resemble the information available in the quadratic approximation. We can, for example, write each cell as four sub-cells with bilinear data each, such that we have nine data points for each cell in the triangulation. The graphic programs will, of course, display this data still only bilinear, but at least we have given some more of the information we have.     
*   In order to allow writing more than one sub-cell per actual cell, the  [2.x.112]  function accepts a parameter (the default is  [2.x.113] , which is why you haven't seen this parameter in previous examples). This parameter denotes into how many sub-cells per space direction each cell shall be subdivided for output. For example, if you give  [2.x.114] , this leads to 4 cells in 2D and 8 cells in 3D. For quadratic elements, two sub-cells per space direction is obviously the right choice, so this is what we choose. In general, for elements of polynomial order  [2.x.115]  subdivisions, and the order of the elements is determined in the same way as above.     
*   With the intermediate format so generated, we can then actually write the graphical output:
* 

* 
* [1.x.124]
* 
*   [1.x.125]  [1.x.126]
* 

* 
*  After graphical output, we would also like to generate tables from the error computations we have done in  [2.x.116] . There, we have filled a table object with the number of cells for each refinement step as well as the errors in different norms.
* 

* 
*  For a nicer textual output of this data, one may want to set the precision with which the values will be written upon output. We use 3 digits for this, which is usually sufficient for error norms. By default, data is written in fixed point notation. However, for columns one would like to see in scientific notation another function call sets the  [2.x.117] , leading to floating point representation of numbers.
* 

* 
* [1.x.127]
* 
*  For the output of a table into a LaTeX file, the default captions of the columns are the keys given as argument to the  [2.x.118]  functions. To have TeX captions that differ from the default ones you can specify them by the following function calls. Note, that `\\' is reduced to `\' by the compiler such that the real TeX caption is, e.g., ` [2.x.119] -error'.
* 

* 
* [1.x.128]
* 
*  Finally, the default LaTeX format for each column of the table is `c' (centered). To specify a different (e.g. `right') one, the following function may be used:
* 

* 
* [1.x.129]
* 
*  After this, we can finally write the table to the standard output stream  [2.x.120]  (after one extra empty line, to make things look prettier). Note, that the output in text format is quite simple and that captions may not be printed directly above the specific columns.
* 

* 
* [1.x.130]
* 
*  The table can also be written into a LaTeX file.  The (nicely) formatted table can be viewed at after calling `latex filename' and e.g. `xdvi filename', where filename is the name of the file to which we will write output now. We construct the file name in the same way as before, but with a different prefix "error":
* 

* 
* [1.x.131]
* 
*   [1.x.132]  [1.x.133]
* 

* 
*  In case of global refinement, it might be of interest to also output the convergence rates. This may be done by the functionality the ConvergenceTable offers over the regular TableHandler. However, we do it only for global refinement, since for adaptive refinement the determination of something like an order of convergence is somewhat more involved. While we are at it, we also show a few other things that can be done with tables.
* 

* 
* [1.x.134]
* 
*  The first thing is that one can group individual columns together to form so-called super columns. Essentially, the columns remain the same, but the ones that were grouped together will get a caption running across all columns in a group. For example, let's merge the "cycle" and "cells" columns into a super column named "n cells":
* 

* 
* [1.x.135]
* 
*  Next, it isn't necessary to always output all columns, or in the order in which they were originally added during the run. Selecting and re-ordering the columns works as follows (note that this includes super columns):
* 

* 
* [1.x.136]
* 
*  For everything that happened to the ConvergenceTable until this point, it would have been sufficient to use a simple TableHandler. Indeed, the ConvergenceTable is derived from the TableHandler but it offers the additional functionality of automatically evaluating convergence rates. For example, here is how we can let the table compute reduction and convergence rates (convergence rates are the binary logarithm of the reduction rate):
* 

* 
* [1.x.137]
* 
*  Each of these function calls produces an additional column that is merged with the original column (in our example the `L2' and the `H1' column) to a supercolumn.
* 

* 
*  Finally, we want to write this convergence chart again, first to the screen and then, in LaTeX format, to disk. The filename is again constructed as above.
* 

* 
* [1.x.138]
* 
*  The final step before going to  [2.x.121]  is then to close the namespace  [2.x.122]  into which we have put everything we needed for this program:
* 

* 
* [1.x.139]
* 
*   [1.x.140]  [1.x.141]
* 

* 
*  The main function is mostly as before. The only difference is that we solve three times, once for Q1 and adaptive refinement, once for Q1 elements and global refinement, and once for Q2 elements and global refinement.
* 

* 
*  Since we instantiate several template classes below for two space dimensions, we make this more generic by declaring a constant at the beginning of the function denoting the number of space dimensions. If you want to run the program in 1d or 2d, you will then only have to change this one instance, rather than all uses below:
* 

* 
* [1.x.142]
* 
*  Now for the three calls to the main class. Each call is blocked into curly braces in order to destroy the respective objects (i.e. the finite element and the HelmholtzProblem object) at the end of the block and before we go to the next run. This avoids conflicts with variable names, and also makes sure that memory is released immediately after one of the three runs has finished, and not only at the end of the  [2.x.123]  block.
* 

* 
* [1.x.143]
* [1.x.144][1.x.145]
* 

* 
* The program generates two kinds of output. The first are the outputfiles  [2.x.124] , [2.x.125] , and [2.x.126] . We show the latter in a 3d viewhere:
* 

*  [2.x.127] 
* 

* 
* 

* Secondly, the program writes tables not only to disk, but also to thescreen while running. The output looks like the following (recall thatcolumns labeled as " [2.x.128] " actually show the  [2.x.129]  [1.x.146]normof the error, not the full  [2.x.130]  norm):
* 

* 
* [1.x.147]
* 
* 

* One can see the error reduction upon grid refinement, and for thecases where global refinement was performed, also the convergencerates can be seen. The linear and quadratic convergence rates of Q1and Q2 elements in the  [2.x.131]  semi-norm can clearly be seen, asare the quadratic and cubic rates in the  [2.x.132]  norm.
* 

* 
* 

* Finally, the program also generated LaTeX versions of the tables (not shownhere) that is written into a file in a way so that it could becopy-pasted into a LaTeX document.
* 

* [1.x.148][1.x.149]
* 

* What we showed above is how to determine the size of the error [2.x.133]  in a number of different norms. We did this primarilybecause we were interested in testing that our solutionsconverge*.But from an engineering perspective, the question is often morepractical: How fine do I have to make my mesh so that the error is"small enough"? In other words, if in the table above the  [2.x.134] semi-norm has been reduced to `4.121e-03`, is this good enough for meto sign the blueprint and declare that our numerical simulation showedthat the bridge is strong enough?
* In practice, we are rarely in this situation because I can nottypically compare the numerical solution  [2.x.135]  against the exactsolution  [2.x.136]  in situations that matter
* 
*  -  if I knew  [2.x.137] , I would nothave to compute  [2.x.138] . But even if I could, the question to ask ingeneral is then: `4.121e-03`what*? The solution will have physicalunits, say kg-times-meter-squared, and I'm integrating a function withunits square of the above over the domain, and then take the squareroot. So if the domain is two-dimensional, the units of [2.x.139]  are kg-times-meter-cubed. The question is then: Is [2.x.140]  kg-times-meter-cubed small? That depends on whatyou're trying to simulate: If you're an astronomer used to massesmeasured in solar masses and distances in light years, then yes, thisis a fantastically small number. But if you're doing atomic physics,then no: That's not small, and your error is most certainly notsufficiently small; you need a finer mesh.
* In other words, when we look at these sorts of numbers, we generallyneed to compare against a "scale". One way to do that is to not lookat theabsolute* error  [2.x.141]  in whatever norm, but at the
*relative* error  [2.x.142] . If this ratio is  [2.x.143] , thenyou know thaton average*, the difference between  [2.x.144]  and  [2.x.145]  is0.001 per cent
* 
*  -  probably small enough for engineering purposes.
* How do we compute  [2.x.146] ? We just need to do an integration loop overall cells, quadrature points on these cells, and then sum things upand take the square root at the end. But there is a simpler way oftenused: You can call
* [1.x.150]
* which computes  [2.x.147] . Alternatively, if you're particularlylazy and don't feel like creating the `zero_vector`, you could usethat if the mesh is not too coarse, then  [2.x.148] , andwe can compute  [2.x.149]  by calling
* [1.x.151]
* In both cases, one then only has to combine the vector of cellwisenorms into one global norm as we already do in the program, by calling
* [1.x.152]
* 
* 

* 
* [1.x.153][1.x.154]
* 

* [1.x.155][1.x.156]
* 

* Go ahead and run the program with higher order elements ( [2.x.150] ,  [2.x.151] , ...). Youwill notice that assertions in several parts of the code will trigger (forexample in the generation of the filename for the data output). You might have to address these,but it should not be very hard to get the program to work!
* [1.x.157][1.x.158]
* 

* Is Q1 or Q2 better? What about adaptive versus global refinement? A (somewhatunfair but typical) metric to compare them, is to look at the error as afunction of the number of unknowns.
* To see this, create a plot in log-log style with the number of unknowns on the [2.x.152]  axis and the  [2.x.153]  error on the  [2.x.154]  axis. You can add reference lines for [2.x.155]  and  [2.x.156]  and check that global and adaptive refinementfollow those. If one makes the (not completely unreasonable)assumption that with a good linear solver, the computational effort isproportional to the number of unknowns  [2.x.157] , then it is clear that anerror reduction of  [2.x.158]  is substantially better than areduction of the form  [2.x.159] : That is, that adaptiverefinement gives us the desired error level with less computationalwork than if we used global refinement. This is not a particularlysurprising conclusion, but it's worth checking these sorts ofassumptions in practice.
* Of course, a fairer comparison would be to plot runtime (switch to releasemode first!) instead of number of unknowns on the  [2.x.160]  axis. If youplotted run time against the number of unknowns by timing eachrefinement step (e.g., using the Timer class), you will notice thatthe linear solver is not perfect
* 
*  -  its run time grows faster thanproportional to the linear system size
* 
*  -  and picking a betterlinear solver might be appropriate for this kind of comparison.
* 

* [1.x.159][1.x.160] [2.x.161] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-71_0.txt
[0.x.0]*
 [2.x.0] 
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33]
*  [2.x.1] 
* [1.x.34]
* 

* [1.x.35][1.x.36]
* 

* The aim of this tutorial is, quite simply, to introduce the fundamentals of both[automatic](https://en.wikipedia.org/wiki/Automatic_differentiation)and [symbolic differentiation](https://en.wikipedia.org/wiki/Computer_algebra)(respectively abbreviated as ADand SD): Ways in which one can, in source code, describe a function [2.x.2]  and automatically also obtain a representation of derivatives [2.x.3]  (the "Jacobian"), [2.x.4]  (the "Hessian"), etc., without havingto write additional lines of code. Doing this is quite helpful insolving nonlinear or optimization problems where one would like toonly describe the nonlinear equation or the objective function in thecode, without having to also provide their derivatives (which arenecessary for a Newton method for solving a nonlinear problem, or forfinding a minimizer).
* Since AD and SD tools are somewhat independent of finite elements and boundary valueproblems, this tutorial is going to be different to the others that you may haveread beforehand. It will focus specifically on how these frameworks work andthe principles and thinking behind them, and will forgo looking at them in thedirect context of a finite element simulation.
* We will, in fact, look at two different sets of problems that have greatlydifferent levels of complexity, but when framed properly hold sufficientsimilarity that the same AD and SD frameworks can be leveraged. With theseexamples the aim is to build up an understanding of the steps that are requiredto use the AD and SD tools, the differences between them, and hopefully identifywhere they could be immediately be used in order to improve or simplify existingcode.
* It's plausible that you're wondering what AD and SD are, in the first place. Well,that question is easy to answer but without context is not very insightful. Sowe're not going to cover that in this introduction, but will rather defer thisuntil the first introductory example where we lay out the key points as thisexample unfolds. To complement this, we should mention that the core theory forboth frameworks is extensively discussed in the  [2.x.5]  module, soit bears little repeating here.
* Since we have to picksome* sufficiently interesting topic to investigateand identify where AD and SD can be used effectively, the main problem that'simplemented in the second half of the tutorial is one of modeling a coupledconstitutive law, specifically a magneto-active material (with hysteretic effects).As a means of an introduction to that, later in the introduction some groundingtheory for that class of materials will be presented.Naturally, this is not a field (or even a class of materials) that is ofinterest to a wide audience. Therefore, the author wishes to express up frontthat this theory and any subsequent derivations mustn't be considered the focusof this tutorial. Instead, keep in mind the complexity of the problem that arisesfrom the relatively innocuous description of the constitutive law, and what wemight (in the context of a boundary value problem) need to derive from that.We will perform some computations with these constitutive laws at the level of arepresentative continuum point (so, remaining in the  realm of continuummechanics), and will produce some benchmark results around which we can framea final discussion on the topic of computational performance.
* Once we have the foundation upon which we can build further concepts, wewill see how AD in particular can be exploited at a finite element (rather thancontinuum) level: this is a topic that is covered in  [2.x.6] , as well as  [2.x.7] .But before then, let's take a moment to think about why we might want to considerusing these sorts of tools, and what benefits they can potentially offer you.
* 

* [1.x.37][1.x.38]
* 

* The primary driver for using AD or SD is typically that there is some situationthat requires differentiation to be performed, and that doing so is sufficientlychallenging to make the prospect of using an external tool to perform that specifictask appealing. A broad categorization for the circumstances under which AD orSD can be rendered most useful include (but are probably not limited to) thefollowing:
* 
*  - [1.x.39] For a new class of problems where you're trying to  implement a solution quickly, and want to remove some of the intricate details  (in terms of both the mathematics as well as the organizational structure of  the code itself). You might be willing to justify any additional computational  cost, which would be offset by an increased agility in restructuring your code  or modifying the part of the problem that is introducing some complex nonlinearity  with minimal effort.
* 
*  - [1.x.40] It could very well be that some problems just happen to have  a nonlinearity that is incredibly challenging to linearize or formulate by hand.  Having this challenge taken care of for you by a tool that is, for the most part,  robust, reliable, and accurate may alleviate some of the pains in implementing  certain problems. Examples of this include  [2.x.8] , where the  derivative of the nonlinear PDE we solve is not incredibly difficult  to derive, but sufficiently cumbersome that one has to pay attention  in doing so by hand, and where implementing the corresponding finite  element formulation of the Newton step takes more than just the few  lines that it generally takes to implement the bilinear form;   [2.x.9]  (where we actually use AD) is an even more extreme example.
* 
*  - [1.x.41] For materials and simulations that exhibit nonlinear response,  an accurate rather than only approximate material tangent (the term mechanical engineers use for  the derivative of a material law) can be the difference between convergent and  divergent behavior, especially at high external (or coupling) loads.  As the complexity of the problem increases, so do the opportunities to introduce  subtle (or, perhaps, not-so-subtle) errors that produce predictably negative  results.  Additionally, there is a lot to be gained by verifying that the implementation is  completely correct. For example, certain categories of problems are known to exhibit  instabilities, and therefore when you start to lose quadratic convergence in a  nonlinear solver (e.g., Newton's method) then this may not be a huge surprise to  the investigator. However, it is hard (if not impossible) to distinguish between  convergence behavior that is produced as you near an unstable solution and when  you simply have an error in the material or finite element linearization, and  start to drift off the optimal convergence path due to that. Having a  method of verifying the correctness of the implementation of a constitutive law  linearization, for example, is perhaps the only meaningful way that you can  use to catch such errors, assuming that you've got nobody else to scrutinize your code.  Thankfully, with some tactical programming it is quite straight-forward to structure  a code for reuse, such that you can use the same classes in production code and  directly verify them in, for instance, a unit-test framework.
* This tutorial program will have two parts: One where we just introducethe basic ideas of automatic and symbolic differentiation support indeal.II using a simple set of examples; and one where we apply this toa realistic but much more complicated case. For that second half, thenext section will provide some background on magneto-mechanicalmaterials
* 
*  -  you can skip this section if all you want to learnabout is what AD and SD actually are, but you probably want to readover this section if you are interested in how to apply AD and SD forconcrete situations.
* 

* [1.x.42][1.x.43]
* 

* [1.x.44][1.x.45]
* 

* As a prelude to introducing the coupled magneto-mechanical material law that we'll useto model a magneto-active polymer, we'll start with a very concise summary ofthe salient thermodynamics to which these constitutive laws must subscribe.The basis for the theory, as summarized here, is described in copious detail byTruesdell and Toupin  [2.x.10]  and Coleman and Noll  [2.x.11] ,and follows the logic laid out by Holzapfel  [2.x.12] .
* Starting from the first law of thermodynamics, and following a few technicalassumptions, it can be shown the the balance between the kinetic plus internalenergy rates and the power supplied to the system from externalsources is given by the following relationship that equates the rateof change of the energy in an (arbitrary) volume  [2.x.13]  on the left, andthe sum of forces acting on that volume on the right:[1.x.46]Here  [2.x.14]  represents the total time derivative, [2.x.15]  is the material density as measured in the Lagrangian reference frame, [2.x.16]  is the material velocity and  [2.x.17]  its acceleration, [2.x.18]  is the internal energy per unit reference volume, [2.x.19]  is the total Piola stress tensor and  [2.x.20]  isthe time rate of the deformation gradient tensor, [2.x.21]  and  [2.x.22]  are, respectively, the magnetic field vector and themagnetic induction (or magnetic flux density) vector, [2.x.23]  and  [2.x.24]  are the electric field vector and electricdisplacement vector, and [2.x.25]  and  [2.x.26]  represent the referential thermal flux vector and thermalsource.The material differential operator [2.x.27] where  [2.x.28]  is the material position vector.With some rearrangement of terms, invoking the arbitrariness of the integrationvolume  [2.x.29] , the total internal energy density rate  [2.x.30]  can be identified as[1.x.47]The total internal energy includes contributions that arise not only due tomechanical deformation (the first term), and thermal fluxes and sources (thefourth and fifth terms), but also due to the intrinsic energy stored in themagnetic and electric fields themselves (the second and third terms,respectively).
* The second law of thermodynamics, known also as the entropy inequality principle,informs us that certain thermodynamic processes are irreversible. After accountingfor the total entropy and rate of entropy input, the Clausius-Duhem inequalitycan be derived. In local form (and in the material configuration), this reads[1.x.48]The quantity  [2.x.31]  is the absolute temperature, and [2.x.32]  represents the entropy per unit reference volume.
* Using this to replace  [2.x.33]  in the resultstemming from the first law of thermodynamics, we now have the relation[1.x.49]On the basis of Fourier's law, which informs us that heat flows from regionsof high temperature to low temperature, the last term is always positive andcan be ignored.This renders the local dissipation inequality[1.x.50]It is postulated  [2.x.34]  that the Legendre transformation[1.x.51]from which we may define the free energy density function  [2.x.35]  with the statedparameterization, exists and is valid.Taking the material rate of this equation and substituting it into the localdissipation inequality results in the generic expression[1.x.52]Under the assumption of isothermal conditions, and that the electric field doesnot excite the material in a manner that is considered non-negligible, then thisdissipation inequality reduces to[1.x.53]
* [1.x.54][1.x.55]
* 

* When considering materials that exhibit mechanically dissipative behavior,it can be shown that this can be captured within the dissipation inequalitythrough the augmentation of the material free energy density function with additionalparameters that represent internal variables  [2.x.36] . Consequently,we write it as[1.x.56]where  [2.x.37]  represents theinternal variable (which acts like a measure of the deformation gradient)associated with the `i`th mechanical dissipative (viscous) mechanism.As can be inferred from its parameterization, each of these internal parametersis considered to evolve in time.Currently the free energy density function  [2.x.38]  is parameterized in terms ofthe magnetic induction  [2.x.39] . This is the natural parameterization thatcomes as a consequence of the considered balance laws. Should such a class ofmaterials to be incorporated within a finite-element model, it would be ascertainedthat a certain formulation of the magnetic problem, known as the magnetic vectorpotential formulation, would need to be adopted. This has its own set of challenges,so where possible the more simple magnetic scalar potential formulation may bepreferred. In that case, the magnetic problem needs to be parameterized in termsof the magnetic field  [2.x.40] . To make this re-parameterization, we executea final Legendre transformation[1.x.57]At the same time, we may take advantage of the principle of material frameindifference in order to express the energy density function in terms of symmetricdeformation measures:[1.x.58]The upshot of these two transformations (leaving out considerable explicit andhidden details) renders the final expression for the reduced dissipationinequality as[1.x.59](Notice the sign change on the second term on the right hand side, and thetransfer of the time derivative to the magnetic induction vector.)The stress quantity  [2.x.41]  is known as the total Piola-Kirchhoffstress tensor and its energy conjugate  [2.x.42] is the right Cauchy-Green deformation tensor, and [2.x.43]  is the re-parameterizedinternal variable associated with the `i`th mechanical dissipative (viscous)mechanism.
* Expansion of the material rate of the energy density function, and rearrangement of thevarious terms, results in the expression[1.x.60]At this point, its worth noting the use of the[partial derivatives](https://en.wikipedia.org/wiki/Partial_derivative) [2.x.44] . This is an important detail that will befundamental to a certain design choice made within the tutorial.As brief reminder of what this signifies, the partial derivative of amulti-variate function returns the derivative of that function with respectto one of those variables while holding the others constant:[1.x.61]More specific to what's encoded in the dissipation inequality (with the very generalfree energy density function  [2.x.45]  with its parameterization yet to be formalized),if one of the input variables is a function of another, it is also held constantand the chain rule does not propagate any further, while the computing totalderivative would imply judicious use of the chain rule. This can be betterunderstood by comparing the following two statements:
* [1.x.62]
* 
* Returning to the thermodynamics of the problem, we next exploit the arbitrarinessof the quantities  [2.x.46]  and  [2.x.47] ,by application of the Coleman-Noll procedure  [2.x.48] ,  [2.x.49] .This leads to the identification of the kinetic conjugate quantities[1.x.63](Again, note the use of the partial derivatives to define the stress and magneticinduction in this generalized setting.)From what terms remain in the dissipative power (namely those related to themechanical dissipative mechanisms), if they are assumed to be independent ofone another then, for each mechanism `i`,[1.x.64]This constraint must be satisfied through the appropriate choice of free energyfunction, as well as a carefully considered evolution law for the internalvariables.
* In the case that there are no dissipative mechanisms to be captured within theconstitutive model (e.g., if the material to be modelled is magneto-hyperelastic)then the free energy density function [2.x.50]  reduces to a storedenergy density function, and the total stress and magnetic induction can be simplified
* [1.x.65]
* where the operator  [2.x.51]  denotes the total derivative operation.
* For completeness, the linearization of the stress tensor and magnetic inductionare captured within the fourth-order total referential elastic tangent tensor [2.x.52] , the second-order magnetostatic tangent tensor  [2.x.53]  and thethird-order total referential magnetoelastic coupling tensor  [2.x.54] .Irrespective of the parameterization of  [2.x.55]  and  [2.x.56] ,these quantities may be computed by
* [1.x.66]
* For the case of rate-dependent materials, this expands to
* [1.x.67]
* while for rate-independent materials the linearizations are
* [1.x.68]
* The subtle difference between them is the application of a partial derivative duringthe calculation of the first derivatives. We'll see later how this affects the choiceof AD versus SD for this specific application. For now, we'll simply introducethe two specific materials that are implemented within this tutorial.
* [1.x.69][1.x.70]
* 

* The first material that we'll consider is one that is governed by amagneto-hyperelastic constitutive law. This material responds to bothdeformation as well as immersion in a magnetic field, but exhibits notime- or history-dependent behavior (such as dissipation through viscousdamping or magnetic hysteresis, etc.). Thestored energy densityfunction* for such a material is only parameterized in terms of the(current) field variables, but not their time derivatives or past values.
* We'll choose the energy density function, which captures both the energystored in the material due to deformation and magnetization, as well asthe energy stored in the magnetic field itself, to be[1.x.71]with[1.x.72]and for which the variable  [2.x.57]  ( [2.x.58] being the rank-2 identity tensor) represents the spatial dimension and [2.x.59]  is the deformation gradient tensor. To give some briefbackground to the various components of  [2.x.60] , the first two termsbear a great resemblance to the stored energy density function for a(hyperelastic) Neohookean material. The only difference between what'sused here and the Neohookean material is the scaling of the elastic shearmodulus by the magnetic field-sensitive saturation function  [2.x.61]  (see  [2.x.62] , equation29). This function will, in effect, cause the material to stiffen in thepresence of a strong magnetic field. As it is governed by a sigmoid-typefunction, the shear modulus will asymptotically converge on the specifiedsaturation shear modulus. It can also be shown that the last term in [2.x.63]  is the stored energy density function for magnetic field (asderived from first principles), scaled by the relative permeabilityconstant. This definition collectively implies that the material islinearly magnetized, i.e., the magnetization vector and magnetic fieldvector are aligned. (This is certainly not obvious with the magnetic energystated in its current form, but when the magnetic induction and magnetizationare derived from  [2.x.64]  and all magnetic fields are expressed in the [2.x.65] current configuration [2.x.66]  then this correlation becomes clear.)As for the specifics of what the magnetic induction, stress tensor, and thevarious material tangents look like, we'll defer presenting these to thetutorial body where the full, unassisted implementation of the constitutivelaw is defined.
* [1.x.73][1.x.74]
* 

* The second material that we'll formulate is one for amagneto-viscoelastic material with a single dissipative mechanism `i`.Thefree energy density function* that we'll be considering is defined as
* [1.x.75]
* with[1.x.76][1.x.77]and the evolution law[1.x.78]for the internal viscous variable.We've chosen the magnetoelastic part of the energy [2.x.67] to match that of the first material model that we explored, so this partneeds no further explanation. As for the viscous part  [2.x.68] ,this component of the free energy (in conjunction with the evolution law forthe viscous deformation tensor) is taken from  [2.x.69]  (with theadditional scaling by the viscous saturation function described in [2.x.70] ). It is derived in a thermodynamically consistentframework that, at its core, models the movement of polymer chains on amicro-scale level.
* To proceed beyond this point, we'll also need to consider the timediscretization of the evolution law.Choosing the implicit first-order backwards difference scheme, then[1.x.79]where the superscript  [2.x.71]  denotes that the quantity is taken at thecurrent timestep, and  [2.x.72]  denotes quantities taken at the previoustimestep (i.e., a history variable). The timestep size  [2.x.73]  is thedifference between the current time and that of the previous timestep.Rearranging the terms so that all internal variable quantities at thecurrent time are on the left hand side of the equation, we get[1.x.80]that matches  [2.x.74]  equation 54.
* [1.x.81][1.x.82]
* 

* Now that we have shown all of these formulas for the thermodynamics and theorygoverning magneto-mechanics and constitutive models, let us outline what theprogram will do with all of this.We wish to do somethingmeaningful* with the materials laws that we've formulated,and so it makes sense to subject them to some mechanical and magnetic loadingconditions that are, in some way, representative of some conditions that mightbe found either in an application or in a laboratory setting. One way to achievethat aim would be to embed these constitutive laws in a finite element model tosimulate a device. In this instance, though, we'll keep things simple (we arefocusing on the automatic and symbolic differentiation concepts, after all)and will find a concise way to faithfully replicate an industry-standardrheological experiment using an analytical expression for the loading conditions.
* The rheological experiment that we'll reproduce,which idealizes a laboratory experiment that was used to characterizemagneto-active polymers, is detailed in  [2.x.75] (as well as  [2.x.76] , in which it is documented along with thereal-world experiments). The images below provide a visual description ofthe problem set up.
*  [2.x.77] 
* Under the assumptions that an incompressible medium is being tested,and that the deformation profile through the sample thickness is linear,then the displacement at some measurement point  [2.x.78]  withinthe sample, expressed in radial coordinates, is
* [1.x.83]
* where [2.x.79]  and  [2.x.80]  are the radius at
* 
*  -  and angle of
* 
*  -  the sampling point, [2.x.81]  is the (constant) axial deformation, [2.x.82]  is the time-dependenttorsion angle per unit length that will be prescribed using asinusoidally repeating oscillation of fixed amplitude  [2.x.83] .The magnetic field is aligned axially, i.e., in the  [2.x.84]  direction.
* This summarizes everything that we need to fully characterize the idealizedloading at any point within the rheological sample. We'll set up the problemin such a way that we "pick" a representative point with this sample, andsubject it to a harmonic shear deformation at a constant axial deformation(by default, a compressive load) and a constant, axially applied magneticfield. We will record the stress and magnetic induction at this point, andwill output that data to file for post-processing. Although its not necessaryfor this particular problem, we will also be computing the tangents as well.Even though they are not directly used in this particular piece of work, thesesecond derivatives are needed to embed the constitutive law within afinite element model (one possible extension to this work). We'll thereforetake the opportunity to check our hand calculations for correctness usingthe assisted differentiation frameworks.
* [1.x.84][1.x.85]
* 

* In addition to the already mentioned  [2.x.85]  module, the following are a fewreferences that discuss in more detail
* 
*  - magneto-mechanics, and some aspects of automated differentiation frameworks:  [2.x.86] ,  [2.x.87] , and
* 
*  - the automation of finite element frameworks using AD and/or SD:  [2.x.88] ,  [2.x.89] .
*  [2.x.90] 
* 

*  [1.x.86] [1.x.87]
*  We start by including all the necessary deal.II header files and some C++ related ones. This first header will give us access to a data structure that will allow us to store arbitrary data within it.
* 

* 
* [1.x.88]
* 
*  Next come some core classes, including one that provides an implementation for time-stepping.
* 

* 
* [1.x.89]
* 
*  Then some headers that define some useful coordinate transformations and kinematic relationships that are often found in nonlinear elasticity.
* 

* 
* [1.x.90]
* 
*  The following two headers provide all of the functionality that we need to perform automatic differentiation, and use the symbolic computer algebra system that deal.II can utilize. The headers of all automatic differentiation and symbolic differentiation wrapper classes, and any ancillary data structures that are required, are all collected inside these unifying headers.
* 

* 
* [1.x.91]
* 
*  Including this header allows us the capability to write output to a file stream.
* 

* 
* [1.x.92]
* 
*  As per usual, the entire tutorial program is defined within its own unique namespace.
* 

* 
* [1.x.93]
* 
*   [1.x.94]  [1.x.95]
* 

* 
*  Automatic and symbolic differentiation have some magical and mystical qualities. Although their use in a project can be beneficial for a multitude of reasons, the barrier to understanding how to use these frameworks or how they can be leveraged may exceed the patience of the developer that is trying to (reliably) integrate them into their work.   
*   Although it is the wish of the author to successfully illustrate how these tools can be integrated into workflows for finite element modelling, it might be best to first take a step back and start right from the basics. So to start off with, we'll first have a look at differentiating a "simple" mathematical function using both frameworks, so that the fundamental operations (both their sequence and function) can be firmly established and understood with minimal complication. In the second part of this tutorial we will put these fundamentals into practice and build on them further.   
*   Accompanying the description of the algorithmic steps to use the frameworks will be a simplified view as to what theymight* be doing in the background. This description will be very much one designed to aid understanding, and the reader is encouraged to view the  [2.x.91]  module documentation for a far more formal description into how these tools actually work.   
*  
*  [1.x.96]  [1.x.97]
* 

* 
* [1.x.98]
* 
*  In order to convince the reader that these tools are indeed useful in practice, let us choose a function for which it is not too difficult to compute the analytical derivatives by hand. It's just sufficiently complicated to make you think about whether or not you truly want to go through with this exercise, and might also make you question whether you are completely sure that your calculations and implementation for its derivatives are correct. The point, of course, is that differentiation of functions is in a sense relatively formulaic and should be something computers are good at
* 
*  -  if we could build on existing software that understands the rules, we wouldn't have to bother with doing it ourselves.     
*   We choose the two variable trigonometric function  [2.x.92]  for this purpose. Notice that this function is templated on the number type. This is done because we can often (but not always) use special auto-differentiable and symbolic types as drop-in replacements for real or complex valued types, and these will then perform some elementary calculations, such as evaluate a function value along with its derivatives. We will exploit that property and make sure that we need only define our function once, and then it can be re-used in whichever context we wish to perform differential operations on it.
* 

* 
* [1.x.99]
* 
*  Rather than revealing this function's derivatives immediately, we'll forward declare functions that return them and defer their definition to later. As implied by the function names, they respectively return the derivatives  [2.x.93] :
* 

* 
* [1.x.100]
* 
*   [2.x.94] :
* 

* 
* [1.x.101]
* 
*   [2.x.95] :
* 

* 
* [1.x.102]
* 
*   [2.x.96] :
* 

* 
* [1.x.103]
* 
*   [2.x.97] :
* 

* 
* [1.x.104]
* 
*  and, lastly,  [2.x.98] :
* 

* 
* [1.x.105]
* 
*   [1.x.106]  [1.x.107]
* 

* 
*  To begin, we'll use AD as the tool to automatically compute derivatives for us. We will evaluate the function with the arguments `x` and `y`, and expect the resulting value and all of the derivatives to match to within the given tolerance.
* 

* 
* [1.x.108]
* 
*  Our function  [2.x.99]  is a scalar-valued function, with arguments that represent the typical input variables that one comes across in algebraic calculations or tensor calculus. For this reason, the  [2.x.100]  class is the appropriate wrapper class to use to do the computations that we require. (As a point of comparison, if the function arguments represented finite element cell degrees-of-freedom, we'd want to treat them differently.) The spatial dimension of the problem is irrelevant since we have no vector- or tensor-valued arguments to accommodate, so the `dim` template argument is arbitrarily assigned a value of 1. The second template argument stipulates which AD framework will be used (deal.II has support for several external AD frameworks), and what the underlying number type provided by this framework is to be used. This number type influences the maximum order of the differential operation, and the underlying algorithms that are used to compute them. Given its template nature, this choice is a compile-time decision because many (but not all) of the AD libraries exploit compile-time meta-programming to implement these special number types in an efficient manner. The third template parameter states what the result type is; in our case, we're working with `double`s.
* 

* 
* [1.x.109]
* 
*  It is necessary that we pre-register with our  [2.x.101]  class how many arguments (what we will call "independent variables") the function  [2.x.102]  has. Those arguments are `x` and `y`, so obviously there are two of them.
* 

* 
* [1.x.110]
* 
*  We now have sufficient information to create and initialize an instance of the helper class. We can also get the concrete number type that will be used in all subsequent calculations. This is useful, because we can write everything from here on by referencing this type, and if we ever want to change the framework used, or number type (e.g., if we need more differential operations) then we need only adjust the `ADTypeCode` template parameter.
* 

* 
* [1.x.111]
* 
*  The next step is to register the numerical values of the independent variables with the helper class. This is done because the function and its derivatives will be evaluated for exactly these arguments. Since we register them in the order `{x,y}`, the variable `x` will be assigned component number `0`, and `y` will be component `1`
* 

* 
* 
*  -  a detail that will be used in the next few lines.
* 

* 
* [1.x.112]
* 
*  We now ask for the helper class to give to us the independent variables with their auto-differentiable representation. These are termed "sensitive variables", because from this point on any operations that we do with the components `independent_variables_ad` are tracked and recorded by the AD framework, and will be considered when we ask for the derivatives of something that they're used to compute. What the helper returns is a `vector` of auto-differentiable numbers, but we can be sure that the zeroth element represents `x` and the first element `y`. Just to make completely sure that there's no ambiguity of what number type these variables are, we suffix all of the auto-differentiable variables with `ad`.
* 

* 
* [1.x.113]
* 
*  We can immediately pass in our sensitive representation of the independent variables to our templated function that computes  [2.x.103] . This also returns an auto-differentiable number.
* 

* 
* [1.x.114]
* 
*  So now the natural question to ask is what we have actually just computed by passing these special `x_ad` and `y_ad` variables to the function `f`, instead of the original `double` variables `x` and `y`? In other words, how is all of this related to the computation of the derivatives that we were wanting to determine? Or, more concisely: What is so special about this returned `ADNumberType` object that gives it the ability to magically return derivatives?       
*   In essence, how thiscould* be done is the following: This special number can be viewed as a data structure that stores the function value, and the prescribed number of derivatives. For a once-differentiable number expecting two arguments, it might look like this:       
*    [2.x.104]        
*   For our independent variable `x_ad`, the starting value of `x_ad.value` would simply be its assigned value (i.e., the real value of that this variable represents). The derivative `x_ad.derivatives[0]` would be initialized to `1`, since `x` is the zeroth independent variable and  [2.x.105] . The derivative `x.derivatives[1]` would be initialized to zero, since the first independent variable is `y` and  [2.x.106] .       
*   For the function derivatives to be meaningful, we must assume that not only is this function differentiable in an analytical sense, but that it is also differentiable at the evaluation point `x,y`. We can exploit both of these assumptions: when we use this number type in mathematical operations, the AD frameworkcould*
 overload the operations (e.g., `%operator+()`, `%operator*()` as well as `%sin()`, `%exp()`, etc.) such that the returned result has the expected value. At the same time, it would then compute the derivatives through the knowledge of exactly what function is being overloaded and rigorous application of the chain-rule. So, the `%sin()` function (with its argument `a` itself being a function of the independent variables `x` and `y`)might* be defined as follows:       
*    [2.x.107]        
*   All of that could of course also be done for second and even higher order derivatives.       
*   So it is now clear that with the above representation the `ADNumberType` is carrying around some extra data that represents the various derivatives of differentiable functions with respect to the original (sensitive) independent variables. It should therefore be noted that there is computational overhead associated with using them (as we compute extra functions when doing derivative computations) as well as memory overhead in storing these results. So the prescribed number of levels of differential operations should ideally be kept to a minimum to limit computational cost. We could, for instance, have computed the first derivatives ourself and then have used the  [2.x.108]  helper class to determine the gradient of the collection of dependent functions, which would be the second derivatives of the original scalar function.       
*   It is also worth noting that because the chain rule is indiscriminately applied and we only see the beginning and end-points of the calculation `{x,y}`  [2.x.109]  `f(x,y)`, we will only ever be able to query the total derivatives of `f`; the partial derivatives (`a.derivatives[0]` and `a.derivatives[1]` in the above example) are intermediate values and are hidden from us.
* 

* 
*  Okay, since we now at least have some idea as to exactly what `f_ad` represents and what is encoded within it, let's put all of that to some actual use. To gain access to those hidden derivative results, we register the final result with the helper class. After this point, we can no longer change the value of `f_ad` and have those changes reflected in the results returned by the helper class.
* 

* 
* [1.x.117]
* 
*  The next step is to extract the derivatives (specifically, the function gradient and Hessian). To do so we first create some temporary data structures (with the result type `double`) to store the derivatives (noting that all derivatives are returned at once, and not individually)...
* 

* 
* [1.x.118]
* 
*  ... and we then request that the helper class compute these derivatives, and the function value itself. And that's it. We have everything that we were aiming to get.
* 

* 
* [1.x.119]
* 
*  We can convince ourselves that the AD framework is correct by comparing it to the analytical solution. (Or, if you're like the author, you'll be doing the opposite and will rather verify that your implementation of the analytical solution is correct!)
* 

* 
* [1.x.120]
* 
*  Because we know the ordering of the independent variables, we know which component of the gradient relates to which derivative...
* 

* 
* [1.x.121]
* 
*  ... and similar for the Hessian.
* 

* 
* [1.x.122]
* 
*  That's pretty great. There wasn't too much work involved in computing second-order derivatives of this trigonometric function.
* 

* 
*   [1.x.123]  [1.x.124]
* 

* 
*  Since we now know how much "implementation effort" it takes to have the AD framework compute those derivatives for us, let's compare that to the same computed by hand and implemented in several stand-alone functions.
* 

* 
*  Here are the two first derivatives of  [2.x.110] :     
*    [2.x.111] 
* 

* 
* [1.x.125]
* 
*   [2.x.112] 
* 

* 
* [1.x.126]
* 
*  And here are the four second derivatives of  [2.x.113] :     
*    [2.x.114] 
* 

* 
* [1.x.127]
* 
*   [2.x.115] 
* 

* 
* [1.x.128]
* 
*   [2.x.116]  (as expected, on the basis of [Schwarz's theorem](https://en.wikipedia.org/wiki/Symmetry_of_second_derivatives))
* 

* 
* [1.x.129]
* 
*   [2.x.117] 
* 

* 
* [1.x.130]
* 
*  Hmm... there's a lot of places in the above where we could have introduced an error in the above, especially when it comes to applying the chain rule. Although they're no silver bullet, at the very least these AD frameworks can serve as a verification tool to make sure that we haven't made any errors (either by calculation or by implementation) that would negatively affect our results.
* 

* 
*  The point of this example of course is that we might have chosen a relatively simple function  [2.x.118]  for which we can hand-verify that the derivatives the AD framework computed is correct. But the AD framework didn't care that the function was simple: It could have been a much much more convoluted expression, or could have depended on more than two variables, and it would still have been able to compute the derivatives
* 
*  -  the only difference would have been thatwe* wouldn't have been able to come up with the derivatives any more to verify correctness of the AD framework.
* 

* 
*  
*  
* 

* 
*   [1.x.131]  [1.x.132]
* 

* 
*  We'll now repeat the same exercise using symbolic differentiation. The term "symbolic differentiation" is a little bit misleading because differentiation is just one tool that the Computer Algebra System (CAS) (i.e., the symbolic framework) provides. Nevertheless, in the context of finite element modeling and applications it is the most common use of a CAS and will therefore be the one that we'll focus on. Once more, we'll supply the argument values `x` and `y` with which to evaluate our function  [2.x.119]  and its derivatives, and a tolerance with which to test the correctness of the returned results.
* 

* 
* [1.x.133]
* 
*  The first step that we need to take is to form the symbolic variables that represent the function arguments that we wish to differentiate with respect to. Again, these will be the independent variables for our problem and as such are, in some sense, primitive variables that have no dependencies on any other variable. We create these types of (independent) variables by initializing a symbolic type  [2.x.120]  which is a wrapper to a set of classes used by the symbolic framework, with a unique identifier. On this occasion it makes sense that this identifier, a  [2.x.121]  be simply `"x"` for the  [2.x.122]  argument, and likewise `"y"` for the  [2.x.123]  argument to the dependent function. Like before, we'll suffix symbolic variable names with `sd` so that we can clearly see which variables are symbolic (as opposed to numeric) in nature.
* 

* 
* [1.x.134]
* 
*  Using the templated function that computes  [2.x.124] , we can pass these independent variables as arguments to the function. The returned result will be another symbolic type that represents the sequence of operations used to compute  [2.x.125] .
* 

* 
* [1.x.135]
* 
*  At this point it is legitimate to print out the expression `f_sd`, and if we did so  [2.x.126]  we would see `f(x,y) = cos(y/x)` printed to the console.       
*   You might notice that we've constructed our symbolic function `f_sd` with no context as to how we might want to use it: In contrast to the AD approach shown above, what we were returned from calling `f(x_sd, y_sd)` is not the evaluation of the function `f` at some specific point, but is in fact a symbolic representation of the evaluation at a generic, as yet undetermined, point. This is one of the key points that makes symbolic frameworks (the CAS) different from automatic differentiation frameworks. Each of the variables `x_sd` and `y_sd`, and even the composite dependent function `f_sd`, are in some sense respectively "placeholders" for numerical values and a composition of operations. In fact, the individual components that are used to compose the function are also placeholders. The sequence of operations are encoded into in a tree-like data structure (conceptually similar to an [abstract syntax tree](https://en.wikipedia.org/wiki/Abstract_syntax_tree)).       
*   Once we form these data structures we can defer any operations that we might want to do with them until some later time. Each of these placeholders represents something, but we have the opportunity to define or redefine what they represent at any convenient point in time. So for this particular problem it makes sense that we somehow want to associate "x" and "y" withsome* numerical value (with type yet to be determined), but we could conceptually (and if it made sense) assign the ratio "y/x" a value instead of the variables "x" and "y" individually. We could also associate with "x" or "y" some other symbolic function `g(a,b)`. Any of these operations involves manipulating the recorded tree of operations, and substituting the salient nodes on the tree (and that nodes' subtree) with something else. The key word here is "substitution", and indeed there are many functions in the  [2.x.127]  namespace that have this word in their names.       
*   This capability makes the framework entirely generic. In the context of finite element simulations, the types of operations that we would typically perform with our symbolic types are function composition, differentiation, substitution (partial or complete), and evaluation (i.e., conversion of the symbolic type to its numerical counterpart). But should you need it, a CAS is often capable of more than just this: It could be forming anti-derivatives (integrals) of functions, perform simplifications on the expressions that form a function (e.g., replace  [2.x.128]  by  [2.x.129] ; or, more simply: if the function did an operation like `1+2`, a CAS could replace it by `3`), and so forth: Theexpression* that a variable represents is obtained from how the function  [2.x.130]  is implemented, but a CAS can do with it whatever its functionality happens to be.       
*   Specifically, to compute the symbolic representation of the first derivatives of the dependent function with respect to its individual independent variables, we use the  [2.x.131]  function with the independent variable given as its argument. Each call will cause the CAS to go through the tree of operations that compose `f_sd` and differentiate each node of the expression tree with respect to the given symbolic argument.
* 

* 
* [1.x.137]
* 
*  To compute the symbolic representation of the second derivatives, we simply differentiate the first derivatives with respect to the independent variables. So to compute a higher order derivative, we first need to compute the lower order derivative. (As the return type of the call to `differentiate()` is an expression, we could in principal execute double differentiation directly from the scalar by chaining two calls together. But this is unnecessary in this particular case, since we have the intermediate results at hand.)
* 

* 
* [1.x.138]
* 
*  Printing the expressions for the first and second derivatives, as computed by the CAS, using the statements  [2.x.132]  renders the following output:  [2.x.133]  This compares favorably to the analytical expressions for these derivatives that were presented earlier.
* 

* 
*  Now that we have formed the symbolic expressions for the function and its derivatives, we want to evaluate them for the numeric values for the main function arguments `x` and `y`. To accomplish this, we construct asubstitution map*, which maps the symbolic values to their numerical counterparts.
* 

* 
* [1.x.141]
* 
*  The last step in the process is to convert all symbolic variables and operations into numerical values, and produce the numerical result of this operation. To do this we combine the substitution map with the symbolic variable in the step we have already mentioned above: "substitution".       
*   Once we pass this substitution map to the CAS, it will substitute each instance of the symbolic variable (or, more generally, sub-expression) with its numerical counterpart and then propagate these results up the operation tree, simplifying each node on the tree if possible. If the tree is reduced to a single value (i.e., we have substituted all of the independent variables with their numerical counterpart) then the evaluation is complete.       
*   Due to the strongly-typed nature of C++, we need to instruct the CAS to convert its representation of the result into an intrinsic data type (in this case a `double`). This is the "evaluation" step, and through the template type we define the return type of this process. Conveniently, these two steps can be done at once if we are certain that we've performed a full substitution.
* 

* 
* [1.x.142]
* 
*  We can do the same for the first derivatives...
* 

* 
* [1.x.143]
* 
*  ... and the second derivatives. Notice that we can reuse the same substitution map for each of these operations because we wish to evaluate all of these functions for the same values of `x` and `y`. Modifying the values in the substitution map renders the result of same symbolic expression evaluated with different values being assigned to the independent variables. We could also happily have each variable represent a real value in one pass, and a complex value in the next.
* 

* 
* [1.x.144]
* 
*   [1.x.145]  [1.x.146]
* 

* 
*  The function used to drive these initial examples is straightforward. We'll arbitrarily choose some values at which to evaluate the function (although knowing that `x = 0` is not permissible), and then pass these values to the functions that use the AD and SD frameworks.
* 

* 
* [1.x.147]
* 
*   [1.x.148]  [1.x.149]
* 

* 
*  Now that we've introduced the principles behind automatic and symbolic differentiation, we'll put them into action by formulating two coupled magneto-mechanical constitutive laws: one that is rate-independent, and another that exhibits rate-dependent behavior.   
*   As you will recall from the introduction, the material constitutive laws we will consider are far more complicated than the simple example above. This is not just because of the form of the function  [2.x.134]  that we will consider, but in particular because  [2.x.135]  doesn't just depend on two scalar variables, but instead on a whole bunch oftensors*, each with several components. In some cases, these aresymmetric* tensors, for which only a subset of components is in fact independent, and one has to think about what it actually means to compute a derivative such as  [2.x.136]  where  [2.x.137]  is a symmetric tensor. How all of this will work will, hopefully, become clear below. It will also become clear that doing this by hand is going to be, at the very best,exceedingly*
tedious* and, at worst, riddled with hard-to-find bugs.
* 

* 
* [1.x.150]
* 
*   [1.x.151]  [1.x.152]
* 

* 
*  We start with a description of the various material parameters that appear in the description of the energy function  [2.x.138] .     
*   The ConstitutiveParameters class is used to hold these values. Values for all parameters (both constitutive and rheological) are taken from  [2.x.139] , and are given values that produce a constitutive response that is broadly representative of a real, laboratory-made magneto-active polymer, though the specific values used here are of no consequence to the purpose of this program of course.     
*   The first four constitutive parameters respectively represent
* 

* 
* 
*  - the elastic shear modulus  [2.x.140] ,
* 

* 
* 
*  - the elastic shear modulus at magnetic saturation  [2.x.141] ,
* 

* 
* 
*  - the saturation magnetic field strength for the elastic shear modulus  [2.x.142] , and
* 

* 
* 
*  - the Poisson ratio  [2.x.143] .
* 

* 
* [1.x.153]
* 
*  The next four, which only pertain to the rate-dependent material, are parameters for
* 

* 
* 
*  - the viscoelastic shear modulus  [2.x.144] ,
* 

* 
* 
*  - the viscoelastic shear modulus at magnetic saturation  [2.x.145] ,
* 

* 
* 
*  - the saturation magnetic field strength for the viscoelastic shear modulus  [2.x.146] , and
* 

* 
* 
*  - the characteristic relaxation time  [2.x.147] .
* 

* 
* [1.x.154]
* 
*  The last parameter is the relative magnetic permeability  [2.x.148] .
* 

* 
* [1.x.155]
* 
*  The parameters are initialized through the ParameterAcceptor framework, which is discussed in detail in  [2.x.149] .
* 

* 
* [1.x.156]
* 
*   [1.x.157]  [1.x.158]
* 

* 
*  Since we'll be formulating two constitutive laws for the same class of materials, it makes sense to define a base class that ensures a unified interface to them.     
*   The class declaration starts with the constructor that will accept the set of constitutive parameters that, in conjunction with the material law itself, dictate the material response.
* 

* 
* [1.x.159]
* 
*  Instead of computing and returning the kinetic variables or their linearization at will, we'll calculate and store these values within a single method. These cached results will then be returned upon request. We'll defer the precise explanation as to why we'd want to do this to a later stage. What is important for now is to see that this function accepts all of the field variables, namely the magnetic field vector  [2.x.150]  and right Cauchy-Green deformation tensor  [2.x.151] , as well as the time discretizer. These, in addition to the  [2.x.152]  are all the fundamental quantities that are required to compute the material response.
* 

* 
* [1.x.160]
* 
*  The next few functions provide the interface to probe the material response due subject to the applied deformation and magnetic loading.       
*   Since the class of materials can be expressed in terms of a free energy  [2.x.153] , we can compute that...
* 

* 
* [1.x.161]
* 
*  ... as well as the two kinetic quantities:
* 

* 
* 
*  - the magnetic induction vector  [2.x.154] , and
* 

* 
* 
*  - the total Piola-Kirchhoff stress tensor  [2.x.155] 
* 

* 
* [1.x.162]
* 
*  ... and the linearization of the kinetic quantities, which are:
* 

* 
* 
*  - the magnetostatic tangent tensor  [2.x.156] ,
* 

* 
* 
*  - the total referential magnetoelastic coupling tensor  [2.x.157] , and
* 

* 
* 
*  - the total referential elastic tangent tensor  [2.x.158] .
* 

* 
* [1.x.163]
* 
*  We'll also define a method that provides a mechanism for this class instance to do any additional tasks before moving on to the next timestep. Again, the reason for doing this will become clear a little later.
* 

* 
* [1.x.164]
* 
*  In the `protected` part of the class, we store a reference to an instance of the constitutive parameters that govern the material response. For convenience, we also define some functions that return various constitutive parameters (both explicitly defined, as well as calculated).       
*   The parameters related to the elastic response of the material are, in order:
* 

* 
* 
*  - the elastic shear modulus,
* 

* 
* 
*  - the elastic shear modulus at saturation magnetic field,
* 

* 
* 
*  - the saturation magnetic field strength for the elastic shear modulus,
* 

* 
* 
*  - the Poisson ratio,
* 

* 
* 
*  - the Lam&eacute; parameter, and
* 

* 
* 
*  - the bulk modulus.
* 

* 
* [1.x.165]
* 
*  The parameters related to the elastic response of the material are, in order:
* 

* 
* 
*  - the viscoelastic shear modulus,
* 

* 
* 
*  - the viscoelastic shear modulus at magnetic saturation,
* 

* 
* 
*  - the saturation magnetic field strength for the viscoelastic shear modulus, and
* 

* 
* 
*  - the characteristic relaxation time.
* 

* 
* [1.x.166]
* 
*  The parameters related to the magnetic response of the material are, in order:
* 

* 
* 
*  - the relative magnetic permeability, and
* 

* 
* 
*  - the magnetic permeability constant  [2.x.159]  (not really a material constant, but rather a universal constant that we'll group here for simplicity).       
*   We'll also implement a function that returns the timestep size from the time discretizion.
* 

* 
* [1.x.167]
* 
*  In the following, let us start by implementing the several relatively trivial member functions of the class just defined:
* 

* 
* [1.x.168]
* 
*   [1.x.169]  [1.x.170]
* 

* 
*  We'll begin by considering a non-dissipative material, namely one that is governed by a magneto-hyperelastic constitutive law that exhibits stiffening when immersed in a magnetic field. As described in the introduction, the stored energy density function for such a material might be given by [1.x.171] with [1.x.172]     
*   Now on to the class that implements this behavior. Since we expect that this class fully describes a single material, we'll mark it as "final" so that the inheritance tree terminated here. At the top of the class, we define the helper type that we will use in the AD computations for our scalar energy density function. Note that we expect it to return values of type `double`. We also have to specify the number of spatial dimensions, `dim`, so that the link between vector, tensor and symmetric tensor fields and the number of components that they contain may be established. The concrete `ADTypeCode` used for the ADHelper class will be provided as a template argument at the point where this class is actually used.
* 

* 
* [1.x.173]
* 
*  Since the public interface to the base class is pure-`virtual`, here we'll declare that this class will override all of these base class methods.
* 

* 
* [1.x.174]
* 
*  In the `private` part of the class, we need to define some extractors that will help us set independent variables and later get the computed values related to the dependent variables. If this class were to be used in the context of a finite element problem, then each of these extractors is (most likely) related to the gradient of a component of the solution field (in this case, displacement and magnetic scalar potential). As you can probably infer by now, here "C" denotes the right Cauchy-Green tensor and "H" denotes the magnetic field vector.
* 

* 
* [1.x.175]
* 
*  This is an instance of the automatic differentiation helper that we'll set up to do all of the differential calculations related to the constitutive law...
* 

* 
* [1.x.176]
* 
*  ... and the following three member variables will store the output from the  [2.x.160]  The  [2.x.161]  returns the derivatives with respect to all field variables at once, so we'll retain the full gradient vector and Hessian matrix. From that, we'll extract the individual entries that we're actually interested in.
* 

* 
* [1.x.177]
* 
*  When setting up the field component extractors, it is completely arbitrary as to how they are ordered. But it is important that the extractors do not have overlapping indices. The total number of components of these extractors defines the number of independent variables that the  [2.x.162]  needs to track, and with respect to which we'll be taking derivatives. The resulting data structures  [2.x.163]  and  [2.x.164]  must also be sized accordingly. Once the  [2.x.165]  is configured (its input argument being the total number of components of  [2.x.166]  and  [2.x.167] ), we can directly interrogate it as to how many independent variables it uses.
* 

* 
* [1.x.178]
* 
*  As stated before, due to the way that the automatic differentiation libraries work, the  [2.x.168]  will always returns the derivatives of the energy density function with respect to all field variables simultaneously. For this reason, it does not make sense to compute the derivatives in the functions `get_B()`, `get_S()`, etc. because we'd be doing a lot of extra computations that are then simply discarded. So, the best way to deal with that is to have a single function call that does all of the calculations up-front, and then we extract the stored data as its needed. That's what we'll do in the `update_internal_data()` method. As the material is rate-independent, we can ignore the DiscreteTime argument.
* 

* 
* [1.x.179]
* 
*  Since we reuse the  [2.x.169]  data structure at each time step, we need to clear it of all stale information before use.
* 

* 
* [1.x.180]
* 
*  The next step is to set the values for all field components. These define the "point" around which we'll be computing the function gradients and their linearization. The extractors that we created before provide the association between the fields and the registry within the  [2.x.170] 
* 
*  -  they'll be used repeatedly to ensure that we have the correct interpretation of which variable corresponds to which component of `H` or `C`.
* 

* 
* [1.x.181]
* 
*  Now that we've done the initial setup, we can retrieve the AD counterparts of our fields. These are truly the independent variables for the energy function, and are "sensitive" to the calculations that are performed with them. Notice that the AD number are treated as a special number type, and can be used in many templated classes (in this example, as the scalar type for the Tensor and SymmetricTensor class).
* 

* 
* [1.x.182]
* 
*  We can also use them in many functions that are templated on the scalar type. So, for these intermediate values that we require, we can perform tensor operations and some mathematical functions. The resulting type will also be an automatically differentiable number, which encodes the operations performed in these functions.
* 

* 
* [1.x.183]
* 
*  Next we'll compute the scaling function that will cause the shear modulus to change (increase) under the influence of a magnetic field...
* 

* 
* [1.x.184]
* 
*  ... and then we can define the material stored energy density function. We'll see later that this example is sufficiently complex to warrant the use of AD to, at the very least, verify an unassisted implementation.
* 

* 
* [1.x.185]
* 
*  The stored energy density function is, in fact, the dependent variable for this problem, so as a final step in the  "configuration" phase, we register its definition with the  [2.x.171] 
* 

* 
* [1.x.186]
* 
*  Finally, we can retrieve the resulting value of the stored energy density function, as well as its gradient and Hessian with respect to the input fields, and cache them.
* 

* 
* [1.x.187]
* 
*  The following few functions then allow for querying the so-stored value of  [2.x.172] , and to extract the desired components of the gradient vector and Hessian matrix. We again make use of the extractors to express which parts of the total gradient vector and Hessian matrix we wish to retrieve. They only return the derivatives of the energy function, so for our definitions of the kinetic variables and their linearization a few more manipulations are required to form the desired result.
* 

* 
* [1.x.188]
* 
*  Note that for coupled terms the order of the extractor arguments is especially important, as it dictates the order in which the directional derivatives are taken. So, if we'd reversed the order of the extractors in the call to `extract_hessian_component()` then we'd actually have been retrieving part of  [2.x.173] .
* 

* 
* [1.x.189]
* 
*   [1.x.190]  [1.x.191]
* 

* 
*  The second material law that we'll consider will be one that represents a magneto-viscoelastic material with a single dissipative mechanism. We'll consider the free energy density function for such a material to be defined as

* 
* [1.x.192]
*  with [1.x.193] [1.x.194] in conjunction with the evolution law for the internal viscous variable [1.x.195] that was discretized using a first-order backward difference approximation.     
*   Again, let us see how this is implemented in a concrete class. Instead of the AD framework used in the previous class, we will now utilize the SD approach. To support this, the class constructor accepts not only the  [2.x.174]  but also two additional variables that will be used to initialize a  [2.x.175]  We'll give more context to this later.
* 

* 
* [1.x.196]
* 
*  Like for the automatic differentiation helper, the  [2.x.176]  will return a collection of results all at once. So, in order to do that just once, we'll utilize a similar approach to before and do all of the expensive calculations within the `update_internal_data()` function, and cache the results for layer extraction.
* 

* 
* [1.x.197]
* 
*  Since we're dealing with a rate dependent material, we'll have to update the history variable at the appropriate time. That will be the purpose of this function.
* 

* 
* [1.x.198]
* 
*  In the `private` part of the class, we will want to keep track of the internal viscous deformation, so the following two (real-valued, non-symbolic) member variables respectively hold
* 

* 
* 
*  - the value of internal variable time step (and, if embedded within a nonlinear solver framework, Newton step), and
* 

* 
* 
*  - the value of internal variable at the previous timestep.       
*   (We've labeled these variables "Q" so that they're easy to identify; in a sea of calculations it is not necessarily easy to distinguish `Cv` or `C_v` from `C`.)
* 

* 
* [1.x.199]
* 
*  As we'll be using symbolic types, we'll need to define some symbolic variables to use with the framework. (They are all suffixed with "SD" to make it easy to distinguish the symbolic types or expressions from real-valued types or scalars.) This can be done once up front (potentially even as `static` variables) to minimize the overhead associated with creating these variables. For the ultimate in generic programming, we can even describe the constitutive parameters symbolically,potentially* allowing a single class instance to be reused with different inputs for these values too.       
*   These are the symbolic scalars that represent the elastic, viscous, and magnetic material parameters (defined mostly in the same order as they appear in the  [2.x.177]  class). We also store a symbolic expression,  [2.x.178]  that represents the time step size):
* 

* 
* [1.x.200]
* 
*  Next we define some tensorial symbolic variables that represent the independent field variables, upon which the energy density function is parameterized:
* 

* 
* [1.x.201]
* 
*  And similarly we have the symbolic representation of the internal viscous variables (both its current value and its value at the previous timestep):
* 

* 
* [1.x.202]
* 
*  We should also store the definitions of the dependent expressions: Although we'll only compute them once, we require them to retrieve data from the  [2.x.179]  that is declared below. Furthermore, when serializing a material class like this one (not done as a part of this tutorial) we'd either need to serialize these expressions as well or we'd need to reconstruct them upon reloading.
* 

* 
* [1.x.203]
* 
*  The next variable is then the optimizer that is used to evaluate the dependent functions. More specifically, it provides the possibility to accelerate the evaluation of the symbolic dependent expressions. This is a vital tool, because the native evaluation of lengthy expressions (using no method of acceleration, but rather direct evaluation directly of the symbolic expressions) can be very slow. The  [2.x.180]  class provides a mechanism by which to transform the symbolic expression tree into another code path that, for example, shares intermediate results between the various dependent expressions (meaning that these intermediate values only get calculated once per evaluation) and/or compiling the code using a just-in-time compiler (thereby retrieving near-native performance for the evaluation step).       
*   Performing this code transformation is very computationally expensive, so we store the optimizer so that it is done just once per class instance. This also further motivates the decision to make the constitutive parameters themselves symbolic. We could then reuse a single instance of this  [2.x.181]  across several materials (with the same energy function, of course) and potentially multiple continuum points (if embedded within a finite element simulation).       
*   As specified by the template parameter, the numerical result will be of type <tt>double</tt>.
* 

* 
* [1.x.204]
* 
*  During the evaluation phase, we must map the symbolic variables to their real-valued counterparts. The next method will provide this functionality.       
*   The final method of this class will configure the  [2.x.182] 
* 

* 
* [1.x.205]
* 
*  As the resting deformation state is one at which the material is considered to be completely relaxed, the internal viscous variables are initialized with the identity tensor, i.e.  [2.x.183] . The various symbolic variables representing the constitutive parameters, time step size, and field and internal variables all get a unique identifier. The optimizer is passed the two parameters that declare which optimization (acceleration) technique should be applied, as well as which additional steps should be taken by the CAS to help improve performance during evaluation.
* 

* 
* [1.x.206]
* 
*  The substitution map simply pairs all of the following data together:
* 

* 
* 
*  - the constitutive parameters (with values retrieved from the base class),
* 

* 
* 
*  - the time step size (with its value retrieved from the time discretizer),
* 

* 
* 
*  - the field values (with their values being prescribed by an external function that is calling into this  [2.x.184]  instance), and
* 

* 
* 
*  - the current and previous internal viscous deformation (with their values stored within this class instance).
* 

* 
* [1.x.207]
* 
*  Due to the "natural" use of the symbolic expressions, much of the procedure to configure the  [2.x.185]  looks very similar to that which is used to construct the automatic differentiation helper. Nevertheless, we'll detail these steps again to highlight the differences that underlie the two frameworks.     
*   The function starts with expressions that symbolically encode the determinant of the deformation gradient (as expressed in terms of the right Cauchy-Green deformation tensor, our primary field variable), as well as the inverse of  [2.x.186]  itself:
* 

* 
* [1.x.208]
* 
*  Next is the symbolic representation of the saturation function for the elastic part of the free energy density function, followed by the magnetoelastic contribution to the free energy density function. This all has the same structure as we'd seen previously.
* 

* 
* [1.x.209]
* 
*  In addition, we define the magneto-viscoelastic contribution to the free energy density function. The first component required to implement this is a scaling function that will cause the viscous shear modulus to change (increase) under the influence of a magnetic field (see  [2.x.187] , equation 29). Thereafter we can compute the dissipative component of the energy density function; its expression is stated in  [2.x.188]  (equation 28), which is a straight-forward extension of an energy density function formulated in  [2.x.189]  (equation 46).
* 

* 
* [1.x.210]
* 
*  From these building blocks, we can then define the material's total free energy density function:
* 

* 
* [1.x.211]
* 
*  As it stands, to the CAS the variable  [2.x.190]  appears to be independent of  [2.x.191]  Our tensorial symbolic expression  [2.x.192]  just has an identifier associated with it, and there is nothing that links it to the other tensorial symbolic expression  [2.x.193]  So any derivatives taken with respect to  [2.x.194]  will ignore this inherent dependence which, as we can see from the evolution law, is in fact  [2.x.195] . This means that deriving any function  [2.x.196]  with respect to   [2.x.197]  will return partial derivatives  [2.x.198]  as opposed to the total derivative  [2.x.199] .       
*   By contrast, with the current AD libraries the total derivative would always be returned. This implies that the computed kinetic variables would be incorrect for this class of material model, making AD the incorrect tool from which to derive (at the continuum point level) the constitutive law for this dissipative material from an energy density function.       
*   It is this specific level of control that characterizes a defining difference difference between the SD and AD frameworks. In a few lines we'll be manipulating the expression for the internal variable  [2.x.200]  such that it produces the correct linearization.
* 

* 
*  But, first, we'll compute the symbolic expressions for the kinetic variables, i.e., the magnetic induction vector and the Piola-Kirchhoff stress tensor. The code that performs the differentiation quite closely mimics the definition stated in the theory.
* 

* 
* [1.x.212]
* 
*  Since the next step is to linearize the above, it is the appropriate time to inform the CAS of the explicit dependency of  [2.x.201]  on  [2.x.202]  i.e., state that  [2.x.203] . This means that all future differential operations made with respect to  [2.x.204]  will take into account this dependence (i.e., compute total derivatives). In other words, we will transform some expression such that their intrinsic parameterization changes from  [2.x.205]  to  [2.x.206] .       
*   To do this, we consider the time-discrete evolution law. From that, we have the explicit expression for the internal variable in terms of its history as well as the primary field variable. That is what it described in this expression:
* 

* 
* [1.x.213]
* 
*  Next we produce an intermediate substitution map, which will take every instance of  [2.x.207]  (our identifier) found in an expression and replace it with the full expression held in  [2.x.208] 
* 

* 
* [1.x.214]
* 
*  We can the perform this substitution on the two kinetic variables and immediately differentiate the result that appears after that substitution with the field variables. (If you'd like, this could be split up into two steps with the intermediate results stored in a temporary variable.) Again, if you overlook the "complexity" generated by the substitution, these calls that linearize the kinetic variables and produce the three tangent tensors quite closely resembles what's stated in the theory.
* 

* 
* [1.x.215]
* 
*  Now we need to tell the  [2.x.209]  what entries we need to provide numerical values for in order for it to successfully perform its calculations. These essentially act as the input arguments to all dependent functions that the  [2.x.210]  must evaluate. They are, collectively, the independent variables for the problem, the history variables, the time step sie and the constitutive parameters (since we've not hard encoded them in the energy density function).       
*   So what we really want is to provide it a collection of symbols, which one could accomplish in this way:  [2.x.211]  But this is all actually already encoded as the keys of the substitution map. Doing the above would also mean that we need to manage the symbols in two places (here and when constructing the substitution map), which is annoying and a potential source of error if this material class is modified or extended. Since we're not interested in the values at this point, it is alright if the substitution map is filled with invalid data for the values associated with each key entry. So we'll simply create a fake substitution map, and extract the symbols from that. Note that any substitution map passed to the  [2.x.212]  will have to, at the very least, contain entries for these symbols.
* 

* 
* [1.x.217]
* 
*  We then inform the optimizer of what values we want calculated, which in our situation encompasses all of the dependent variables (namely the energy density function and its various derivatives).
* 

* 
* [1.x.218]
* 
*  The last step is to finalize the optimizer. With this call it will determine an equivalent code path that will evaluate all of the dependent functions at once, but with less computational cost than when evaluating the symbolic expression directly. Note: This is an expensive call, so we want execute it as few times as possible. We've done it in the constructor of our class, which achieves the goal of being called only once per class instance.
* 

* 
* [1.x.219]
* 
*  Since the configuration of the  [2.x.213]  was done up front, there's very little to do each time we want to compute kinetic variables or their linearization (derivatives).
* 

* 
* [1.x.220]
* 
*  To update the internal history variable, we first need to compute a few fundamental quantities, which we've seen before. We can also ask the time discretizer for the time step size that was used to iterate from the previous time step to the current one.
* 

* 
* [1.x.221]
* 
*  Now we can update the (real valued) internal viscous deformation tensor, as per the definition given by the evolution law in conjunction with the chosen time discretization scheme.
* 

* 
* [1.x.222]
* 
*  Next we pass the optimizer the numeric values that we wish the independent variables, time step size and (implicit to this call), the constitutive parameters to represent.
* 

* 
* [1.x.223]
* 
*  When making this next call, the call path used to (numerically) evaluate the dependent functions is quicker than dictionary substitution.
* 

* 
* [1.x.224]
* 
*  Having called `update_internal_data()`, it is then valid to extract data from the optimizer. When doing the evaluation, we need the exact symbolic expressions of the data to extracted from the optimizer. The implication of this is that we needed to store the symbolic expressions of all dependent variables for the lifetime of the optimizer (naturally, the same is implied for the input variables).
* 

* 
* [1.x.225]
* 
*  When moving forward in time, the "current" state of the internal variable instantaneously defines the state at the "previous" timestep. As such, we record value of history variable for use as the "past value" at the next time step.
* 

* 
* [1.x.226]
* 
*   [1.x.227]  [1.x.228]
* 

* 
*  Now that we've seen how the AD and SD frameworks can make light(er) work of defining these constitutive laws, we'll implement the equivalent classes by hand for the purpose of verification and to do some preliminary benchmarking of the frameworks versus a native implementation.     
*   At the expense of the author's sanity, what is documented below (hopefully accurately) are the full definitions for the kinetic variables and their tangents, as well as some intermediate computations. Since the structure and design of the constitutive law classes has been outlined earlier, we'll gloss over it and simply delineate between the various stages of calculations in the `update_internal_data()` method definition. It should be easy enough to link the derivative calculations (with their moderately expressive variable names) to their documented definitions that appear in the class descriptions. We will, however, take the opportunity to present two different paradigms for implementing constitutive law classes. The second will provide more flexibility than the first (thereby making it more easily extensible, in the author's opinion) at the expense of some performance.
* 

* 
*   [1.x.229]  [1.x.230]
* 

* 
*  From the stored energy that, as mentioned earlier, is defined as [1.x.231] with [1.x.232] for this magnetoelastic material, the first derivatives that correspond to the magnetic induction vector and total Piola-Kirchhoff stress tensor are [1.x.233]

* 
* [1.x.234]
*  with [1.x.235] [1.x.236] [1.x.237] [1.x.238] [1.x.239] The use of the symmetry operator  [2.x.214]  in the one derivation above helps to ensure that the resulting rank-4 tensor, which holds minor symmetries due to the symmetry of  [2.x.215] , still maps rank-2 symmetric tensors to rank-2 symmetric tensors. See the SymmetricTensor class documentation and the introduction to  [2.x.216]  and for further explanation as to what symmetry means in the context of fourth-order tensors.     
*   The linearization of each of the kinematic variables with respect to their arguments are [1.x.240]

* 
* [1.x.241]
* 

* 
* [1.x.242]
*  with [1.x.243] [1.x.244] [1.x.245]

* 
* [1.x.246]
*      
*   Well, that escalated quickly
* 
*  -  although the the definition of  [2.x.217]  and  [2.x.218]  might have given some hints that the calculating the kinetic fields and their linearization would take some effort, it is likely that there's a little more complexity to the final definitions that perhaps initially thought. Knowing what we now do, it's probably fair to say that we really do not want to compute first and second derivatives of these functions with respect to their arguments
* 
*  -  regardless of well we did in calculus classes, or how good a programmer we may be.     
*   In the class method definition where these are ultimately implemented, we've composed these calculations slightly differently. Some intermediate steps are also retained to give another perspective of how to systematically compute the derivatives. Additionally, some calculations are decomposed less or further to reuse some of the intermediate values and, hopefully, aid the reader to follow the derivative operations.
* 

* 
* [1.x.247]
* 
*  For this class's update method, we'll simply precompute a collection of intermediate values (for function evaluations, derivative calculations, and the like) and "manually" arrange them in the order that's required to maximize their reuse. This means that we have to manage this ourselves, and decide what values must be compute before others, all while keeping some semblance of order or structure in the code itself. It's effective, but perhaps a little tedious. It also doesn't do too much to help future extension of the class, because all of these values remain local to this single method.     
*   Interestingly, this basic technique of precomputing intermediate expressions that are used in more than one place has a name: [common subexpression elimination (CSE)](https://en.wikipedia.org/wiki/Common_subexpression_elimination). It is a strategy used by Computer Algebra Systems to reduce the computational expense when they are tasked with evaluating similar expressions.
* 

* 
* [1.x.248]
* 
*  The saturation function for the magneto-elastic energy.
* 

* 
* [1.x.249]
* 
*  The first derivative of the saturation function, noting that  [2.x.219] .
* 

* 
* [1.x.250]
* 
*  The second derivative of saturation function, noting that  [2.x.220] .
* 

* 
* [1.x.251]
* 
*  Some intermediate quantities attained directly from the field / kinematic variables.
* 

* 
* [1.x.252]
* 
*  First derivatives of the intermediate quantities.
* 

* 
* [1.x.253]
* 
*  Second derivatives of the intermediate quantities.
* 

* 
* [1.x.254]
* 
*  The stored energy density function.
* 

* 
* [1.x.255]
* 
*  The kinetic quantities.
* 

* 
* [1.x.256]
* 
*  The linearization of the kinetic quantities.
* 

* 
* [1.x.257]
* 
*   [1.x.258]  [1.x.259]
* 

* 
*  As mentioned before, the free energy density function for the magneto-viscoelastic material with one dissipative mechanism that we'll be considering is defined as [1.x.260] [1.x.261] [1.x.262] with [1.x.263] [1.x.264] and the evolution law [1.x.265] that itself is parameterized in terms of  [2.x.221] . By design, the magnetoelastic part of the energy  [2.x.222]  is identical to that of the magnetoelastic material presented earlier. So, for the derivatives of the various contributions stemming from this part of the energy, please refer to the previous section. We'll continue to highlight the specific contributions from those terms by superscripting the salient terms with  [2.x.223] , while contributions from the magneto-viscoelastic component are superscripted with  [2.x.224] . Furthermore, the magnetic saturation function  [2.x.225]  for the damping term has the identical form as that of the elastic term (i.e.,  [2.x.226]  ), and so the structure of its derivatives are identical to that seen before; the only change is for the three constitutive parameters that are now associated with the viscous shear modulus  [2.x.227]  rather than the elastic shear modulus  [2.x.228] .     
*   For this magneto-viscoelastic material, the first derivatives that correspond to the magnetic induction vector and total Piola-Kirchhoff stress tensor are [1.x.266] [1.x.267] with the viscous contributions being [1.x.268] [1.x.269] and with [1.x.270] The time-discretized evolution law, [1.x.271] will also dictate how the linearization of the internal variable with respect to the field variables is composed.     
*   Observe that in order to attain thecorrect* expressions for the magnetic induction vector and total Piola-Kirchhoff stress tensor for this dissipative material, we must adhere strictly to the outcome of applying the Coleman-Noll procedure: we must takepartial derivatives*
 of the free energy density function with respect to the field variables. (For our non-dissipative magnetoelastic material, taking either partial or total derivatives would have had the same result, so there was no need to draw your attention to this before.) The crucial part of the operation is to freeze the internal variable  [2.x.229]  while computing the derivatives of  [2.x.230]  with respect to  [2.x.231] 
* 
*  -  the dependence of  [2.x.232]  on  [2.x.233]  is not to be taken into account. When deciding whether to use AD or SD to perform this task the choice is clear
* 
*  -  only the symbolic framework provides a mechanism to do this; as was mentioned before, AD can only return total derivatives so it is unsuitable for the task.     
*   To wrap things up, we'll present the material tangents for this rate-dependent coupled material. The linearization of both kinetic variables with respect to their arguments are [1.x.272] [1.x.273] [1.x.274] where the tangents for the viscous contributions are [1.x.275] [1.x.276]

* 
* [1.x.277]
*  with [1.x.278] and, from the evolution law, [1.x.279] Notice that just the last term of  [2.x.234]  contains the tangent of the internal variable. The linearization of this particular evolution law is linear. For an example of a nonlinear evolution law, for which this linearization must be solved for in an iterative manner, see  [2.x.235] -Theiss2011a.
* 

* 
* [1.x.280]
* 
*  A data structure that is used to store all intermediate calculations. We'll see shortly precisely how this can be leveraged to make the part of the code where we actually perform calculations clean and easy (well, at least easier) to follow and maintain. But for now, we can say that it will allow us to move the parts of the code where we compute the derivatives of intermediate quantities away from where they are used.
* 

* 
* [1.x.281]
* 
*  The next two functions are used to update the state of the field and internal variables, and will be called before we perform any detailed calculations.
* 

* 
* [1.x.282]
* 
*  The remainder of the class interface is dedicated to methods that are used to compute the components required to calculate the free energy density function, and all of its derivatives:
* 

* 
*  The kinematic, or field, variables.
* 

* 
* [1.x.283]
* 
*  A generalized formulation for the saturation function, with the required constitutive parameters passed as arguments to each function.
* 

* 
* [1.x.284]
* 
*  A generalized formulation for the first derivative of saturation function, with the required constitutive parameters passed as arguments to each function.
* 

* 
* [1.x.285]
* 
*  A generalized formulation for the second derivative of saturation function, with the required constitutive parameters passed as arguments to each function.
* 

* 
* [1.x.286]
* 
*  Intermediate quantities attained directly from the field / kinematic variables.
* 

* 
* [1.x.287]
* 
*  First derivatives of the intermediate quantities.
* 

* 
* [1.x.288]
* 
*  Derivative of internal variable with respect to field variables. Notice that we only need this one derivative of the internal variable, as this variable is only differentiated as part of the linearization of the kinetic variables.
* 

* 
* [1.x.289]
* 
*  Second derivatives of the intermediate quantities.
* 

* 
* [1.x.290]
* 
*  Record the applied deformation state as well as the magnetic load. Thereafter, update internal (viscous) variable based on new deformation state.
* 

* 
* [1.x.291]
* 
*  Get the values for the elastic and viscous saturation function based on the current magnetic field...
* 

* 
* [1.x.292]
* 
*  ... as well as their first derivatives...
* 

* 
* [1.x.293]
* 
*  ... and their second derivatives.
* 

* 
* [1.x.294]
* 
*  Intermediate quantities. Note that, since we're fetching these values from a cache that has a lifetime that outlasts this function call, we can alias the result rather than copying the value from the cache.
* 

* 
* [1.x.295]
* 
*  First derivatives of intermediate values, as well as the that of the internal variable with respect to the right Cauchy-Green deformation tensor.
* 

* 
* [1.x.296]
* 
*  Second derivatives of intermediate values.
* 

* 
* [1.x.297]
* 
*  Since the definitions of the linearizations become particularly lengthy, we'll decompose the free energy density function into three additive components:
* 

* 
* 
*  - the "Neo-Hookean"-like term,
* 

* 
* 
*  - the rate-dependent term, and
* 

* 
* 
*  - the term that resembles that of the energy stored in the magnetic field.       
*   To remain consistent, each of these contributions will be individually added to the variables that we want to compute in that same order.       
*   So, first of all this is the energy density function itself:
* 

* 
* [1.x.298]
* 
*  ... followed by the magnetic induction vector and Piola-Kirchhoff stress:
* 

* 
* [1.x.299]
* 
*  ... and lastly the tangents due to the linearization of the kinetic variables.
* 

* 
* [1.x.300]
* 
*  Now that we're done using all of those temporary variables stored in our cache, we can clear it out to free up some memory.
* 

* 
* [1.x.301]
* 
*  The next few functions implement the generalized formulation for the saturation function, as well as its various derivatives.
* 

* 
* [1.x.302]
* 
*  A scaling function that will cause the shear modulus to change (increase) under the influence of a magnetic field.
* 

* 
* [1.x.303]
* 
*  First derivative of scaling function
* 

* 
* [1.x.304]
* 
*  For the cached calculation approach that we've adopted for this material class, the root of all calculations are the field variables, and the immutable ancillary data such as the constitutive parameters and time step size. As such, we need to enter them into the cache in a different manner to the other variables, since they are inputs that are prescribed from outside the class itself. This function simply adds them to the cache directly from the input arguments, checking that there is no equivalent data there in the first place (we expect to call the `update_internal_data()` method only once per time step, or Newton iteration).
* 

* 
* [1.x.305]
* 
*  Set value for  [2.x.236] .
* 

* 
* [1.x.306]
* 
*  Set value for  [2.x.237] .
* 

* 
* [1.x.307]
* 
*  After that, we can fetch them from the cache at any point in time.
* 

* 
* [1.x.308]
* 
*  With the primary variables guaranteed to be in the cache when we need them, we can not compute all intermediate values (either directly, or indirectly) from them.     
*   If the cache does not already store the value that we're looking for, then we quickly calculate it, store it in the cache and return the value just stored in the cache. That way we can return it as a reference and avoid copying the object. The same goes for any values that a compound function might depend on. Said another way, if there is a dependency chain of calculations that come before the one that we're currently interested in doing, then we're guaranteed to resolve the dependencies before we proceed with using any of those values. Although there is a cost to fetching data from the cache, the "resolved dependency" concept might be sufficiently convenient to make it worth looking past the extra cost. If these material laws are embedded within a finite element framework, then the added cost might not even be noticeable.
* 

* 
* [1.x.309]
* 
*   [1.x.310]  [1.x.311]
* 

* 
*  The  [2.x.238]  class is used to drive the numerical experiments that are to be conducted on the coupled materials that we've implemented constitutive laws for.
* 

* 
* [1.x.312]
* 
*  These are  dimensions of the rheological specimen that is to be simulated. They, effectively, define the measurement point for our virtual experiment.
* 

* 
* [1.x.313]
* 
*  The three steady-state loading parameters are respectively
* 

* 
* 
*  - the axial stretch,
* 

* 
* 
*  - the shear strain amplitude, and
* 

* 
* 
*  - the axial magnetic field strength.
* 

* 
* [1.x.314]
* 
*  Moreover, the parameters for the time-dependent rheological loading conditions are
* 

* 
* 
*  - the loading cycle frequency,
* 

* 
* 
*  - the number of load cycles, and
* 

* 
* 
*  - the number of discrete timesteps per cycle.
* 

* 
* [1.x.315]
* 
*  We also declare some self-explanatory parameters related to output data generated for the experiments conducted with rate-dependent and rate-independent materials.
* 

* 
* [1.x.316]
* 
*  The next few functions compute time-related parameters for the experiment...
* 

* 
* [1.x.317]
* 
*  ... while the following two prescribe the mechanical and magnetic loading at any given time...
* 

* 
* [1.x.318]
* 
*  ... and this last one outputs the status of the experiment to the console.
* 

* 
* [1.x.319]
* 
*  The applied magnetic field is always aligned with the axis of rotation of the rheometer's rotor.
* 

* 
* [1.x.320]
* 
*  The applied deformation (gradient) is computed based on the geometry of the rheometer and the sample, the sampling point, and the experimental parameters. From the displacement profile documented in the introduction, the deformation gradient may be expressed in Cartesian coordinates as [1.x.321]
* 

* 
* [1.x.322]
* 
*   [1.x.323]  [1.x.324]
* 

* 
*  This is the function that will drive the numerical experiments.
* 

* 
* [1.x.325]
* 
*  We can take the hand-implemented constitutive law and compare the results that we attain with it to those that we get using AD or SD. In this way, we can verify that they produce identical results (which indicates that either both implementations have a high probability of being correct, or that they're incorrect with identical flaws being present in both). Either way, it is a decent sanity check for the fully self-implemented variants and can certainly be used as a debugging strategy when differences between the results are detected).
* 

* 
* [1.x.326]
* 
*  We'll be outputting the constitutive response of the material to file for post-processing, so here we declare a `stream` that will act as a buffer for this output. We'll use a simple CSV format for the outputted results.
* 

* 
* [1.x.327]
* 
*  Using the DiscreteTime class, we iterate through each timestep using a fixed time step size.
* 

* 
* [1.x.328]
* 
*  We fetch and compute the loading to be applied to the material at this time step...
* 

* 
* [1.x.329]
* 
*  ... then we update the state of the materials...
* 

* 
* [1.x.330]
* 
*  ... and test for discrepancies between the two.
* 

* 
* [1.x.331]
* 
*  The next thing that we will do is collect some results to post-process. All quantities are in the "current configuration" (rather than the "reference configuration", in which all quantities computed by the constitutive laws are framed).
* 

* 
* [1.x.332]
* 
*  Finally, we output the strain-stress and magnetic loading history to file.
* 

* 
* [1.x.333]
* 
*   [1.x.334]  [1.x.335]
* 

* 
*  The purpose of this driver function is to read in all of the parameters from file and, based off of that, create a representative instance of each constitutive law and invoke the function that conducts a rheological experiment with it.
* 

* 
* [1.x.336]
* 
*  We start the actual work by configuring and running the experiment using our rate-independent constitutive law. The automatically differentiable number type is hard-coded here, but with some clever templating it is possible to select which framework to use at run time (e.g., as selected through the parameter file). We'll simultaneously perform the experiments with the counterpary material law that was fully implemented by hand, and check what it computes against our assisted implementation.
* 

* 
* [1.x.337]
* 
*  Next we do the same for the rate-dependent constitutive law. The highest performance option is selected as default if SymEngine is set up to use the LLVM just-in-time compiler which (in conjunction with some aggressive compilation flags) produces the fastest code evaluation path of all of the available option. As a fall-back, the so called "lambda" optimizer (which only requires a C++11 compliant compiler) will be selected. At the same time, we'll ask the CAS to perform common subexpression elimination to minimize the number of intermediate calculations used during evaluation. We'll record how long it takes to execute the "initialization" step inside the constructor for the SD implementation, as this is where the abovementioned transformations occur.
* 

* 
* [1.x.338]
* 
*   [1.x.339]  [1.x.340]
* 

* 
*  The main function only calls the driver functions for the two sets of examples that are to be executed.
* 

* 
* [1.x.341]
* [1.x.342][1.x.343]
* 

* [1.x.344][1.x.345]
* 

* The first exploratory example produces the following output. It is verified thatall three implementations produce identical results.
* [1.x.346]
* 
* [1.x.347][1.x.348]
* 

* To help summarize the results from the virtual experiment itself, below are somegraphs showing the shear stress, plotted against the shear strain, at a selectlocation within the material sample. The plots show the stress-strain curves underthree different magnetic loads, and for the last cycle of the (mechanical)loading profile, when the rate-dependent material reaches a repeatable("steady-state") response. These types of graphs are often referred to as[Lissajous plots](https://en.wikipedia.org/wiki/Lissajous_curve). The areaof the ellipse that the curve takes for viscoelastic materials provides somemeasure of how much energy is dissipated by the material, and its ellipticityindicates the phase shift of the viscous response with respect to the elasticresponse.
*  [2.x.239] 
* It is not surprising to see that the magneto-elastic material response has an unloadingcurve that matches the loading curve
* 
*  -  the material is non-dissipative after all.But here it's clearly noticeable how the gradient of the curve increases as theapplied magnetic field increases. The tangent at any point along this curve isrelated to the instantaneous shear modulus and, due to the way that the energydensity function was defined, we expect that the shear modulus increases as themagnetic field strength increases.We observe much the same behavior for the magneto-viscoelastic material. The majoraxis of the ellipse traced by the loading-unloading curve has a slope that increasesas a greater magnetic load is applied. At the same time, the more energy isdissipated by the material.
* As for the code output, this is what is printed to the console for the partpertaining to the rheological experiment conducted with the magnetoelasticmaterial:
* [1.x.349]
* 
* And this portion of the output pertains to the experiment performed with themagneto-viscoelastic material:
* [1.x.350]
* 
* The timer output is also emitted to the console, so we can compare time takento perform the hand- and assisted- calculations and get some idea of the overheadof using the AD and SD frameworks.Here are the timings taken from the magnetoelastic experiment usingthe AD framework, based on the Sacado component of the Trilinos library:
* [1.x.351]
* With respect to the computations performed using automatic differentiation(as a reminder, this is with two levels of differentiation using the Sacadolibrary in conjunction with dynamic forward auto-differentiable types), weobserve that the assisted computations takes about  [2.x.240]  longer tocompute the desired quantities. This does seem like quite a lot of overheadbut, as mentioned in the introduction, it's entirely subjective andcircumstance-dependent as to whether or not this is acceptable or not:Do you value computer time more than human time for doing thenecessary hand-computations of derivatives, verify their correctness,implement them, and verify the correctness of the implementation? Ifyou develop a research code that will only be run for a relativelysmall number of experiments, you might value your own time more. Ifyou develop a production code that will be run over and over on10,000-core clusters for hours, your considerations might be different.In any case, the one nice featureof the AD approach is the "drop in" capability when functions and classes aretemplated on the scalar type. This means that minimal effort is required tostart working with it.
* In contrast, the timings for magneto-viscoelastic material as implemented usingjust-in-time (JIT) compiled symbolic algebra indicate that, at some non-negligible cost duringinitialization, the calculations themselves are a lot more efficiently executed:
* [1.x.352]
* Since the initialization phase need, most likely, only be executed once perthread, this initial expensive phase can be offset by the repeated use of asingle  [2.x.241]  instance. Even though themagneto-viscoelastic constitutive law has more terms to calculate when comparedto its magnetoelastic counterpart, it still is a whole order of magnitude fasterto execute the computations of the kinetic variables and tangents. And when comparedto the hand computed variant that uses the caching scheme, the calculation timeis nearly equal. So although using the symbolic framework requires a paradigmshift in terms of how one implements and manipulates the symbolic expressions,it can offer good performance and flexibility that the AD frameworks lack.
* On the point of data caching, the added cost of value caching for themagneto-viscoelastic material implementation is, in fact, about a  [2.x.242] increase in the time spent in `update_internal_data()` when compared to theimplementation using intermediate values for the numerical experiments conductedwith this material. Here's a sample output of the timing comparison extracted forthe "hand calculated" variant when the caching data structure is removed:
* [1.x.353]
* 
* With some minor adjustment we can quite easily test the different optimizationschemes for the batch optimizer. So let's compare the computational expenseassociated with the `LLVM` batch optimizer setting versus the alternatives.Below are the timings reported for the `lambda` optimization method (retainingthe use of CSE):
* [1.x.354]
* The primary observation here is that an order of magnitude greater time is spentin the "Assisted computation" section when compared to the `LLVM` approach.
* Last of all we'll test how `dictionary` substitution, in conjunction with CSE,performs. Dictionary substitution simply does all of the evaluation within thenative CAS framework itself, with no transformation of the underlying datastructures taking place. Only the use of CSE, which caches intermediate results,will provide any "acceleration" in this instance. With that in mind, here arethe results from this selection:
* [1.x.355]
* Needless to say, compared to the other two methods, these results took quitesome time to produce... The `dictionary` substitutionmethod is perhaps only really viable for simple expressions or when the numberof calls is sufficiently small.
* [1.x.356][1.x.357]
* 

* Perhaps you've been convinced that these tools have some merit, and can beof immediate help or use to you. The obvious question now is which one touse. Focusing specifically at a continuum point level, where you would beusing these frameworks to compute derivatives of a constitutive law inparticular, we can say the following:
* 
*  - Automatic differentiation probably provides the simplest entry point into  the world of assisted differentiation.
* 
*  - Given a sufficiently generic implementation of a constitutive framework,  AD can often be used as a drop-in replacement for the intrinsic scalar types  and the helper classes can then be leveraged to compute first (and possibly  higher order) derivatives with minimal effort.
* 
*  - As a qualification to the above point, being a "drop-in replacement" does not  mean that you must not be contentious of what the algorithms that these numbers  are being passed through are doing. It is possible to inadvertently perform  an operation that would, upon differentiating, return an incorrect result.  So this is definitely something that one should be aware of.  A concrete example: When computing the eigenvalues of a tensor, if the tensor  is diagonal then a short-cut to the result is simply to return the diagonal  entries directly (as extracted from the input tensor). This is completely  correct in terms of computing the eigenvalues themselves, but not going  through the algorithm that would otherwise compute the eigenvalues for a  non-diagonal tensor has had an unintended side-effect, namely that the  eigenvalues appear (to the AD framework) to be completely decoupled from  one another and their cross-sensitivities are not encoded in the returned  result. Upon differentiating, many entries of the derivative tensor will  be missing. To fix this issue, one has to ensure that the standard eigenvalue  solving algorithm is used so that the sensitivities of the returned eigenvalues  with respect to one another are encoded in the result.
* 
*  - Computations involving AD number types may be expensive. The expense increases  (sometimes quite considerably) as the order of the differential operations  increases. This may be mitigated by computational complexity of surrounding  operations (such as a linear solve, for example), but is ultimately problem  specific.
* 
*  - AD is restricted to the case where only total derivatives are required. If a  differential operation requires a partial derivative with respect to an  independent variable then it is not appropriate to use it.
* 
*  - Each AD library has its own quirks (sad to say but, in the author's experience,  true), so it may take some trial and error to find the appropriate library and  choice of AD number to suit your purposes. The reason for these "quirks"  often boils down to the overall philosophy behind the library (data structures,  the use of template meta-programming, etc.) as well as the mathematical  implementation of the derivative computations (for example, manipulations of  results using logarithmic functions to change basis might restrict the domain  for the input values
* 
*  -  details all hidden from the user, of course).  Furthermore, one library might be able to compute the desired results quicker  than another, so some initial exploration might be beneficial in that regard.
* 
*  - Symbolic differentiation (well, the use of a CAS in general) provides the most  flexible framework with which to perform assisted computations.
* 
*  - The SD framework can do everything that the AD frameworks can, with the  additional benefit of having low-level control over when certain manipulations  and operations are performed.
* 
*  - Acceleration of expression evaluation is possible, potentially leading to  near-native performance of the SD framework compared to some hand implementations  (this comparison being dependent on the overall program design, of course) at  the expense of the initial optimization call.
* 
*  - Clever use of the  [2.x.243]  could minimize the  expense of the costly call that optimizes the dependent expressions.  The possibility to serialize the  [2.x.244]   that often (but not always) this expensive call can be done once and then  reused in a later simulation.
* 
*  - If two or more material laws differ by only their material parameters, for  instance, then a single batch optimizer can be shared between them as long  as those material parameters are considered to be symbolic. The implication  of this is that you can "differentiate once, evaluate in many contexts".
* 
*  - The SD framework may partially be used as a "drop-in replacement" for scalar  types, but one (at the very least) has to add some more framework around it  to perform the value substitution step, converting symbolic types to their  numerical counterparts.
* 
*  - It may not be possible to use SD numbers within some specialized algorithms.  For example, if an algorithm has an exit point or code branch based off of  some concrete, numerical value that the (symbolic) input argument should take,  then obviously this isn't going to work. One either has to reimplement the  algorithm specifically for SD number types (somewhat inconvenient, but  frequently possible as conditionals are supported by the   [2.x.245]  class), or one must use a creative means  around this specific issue (e.g., introduce a symbolic expression that  represents the result returned by this algorithm, perhaps declaring it  to be a  [symbolic function](https://dealii.org/developer/doxygen/deal.II/namespaceDifferentiation_1_1SD.html#a876041f6048705c7a8ad0855cdb1bd7a)  if that makes sense within the context in which it is to be used. This can  later be substituted by its numerical values, and if declared a symbolic  function then its deferred derivatives may also be incorporated into the  calculations as substituted results.).
* 
*  - The biggest drawback to using SD is that using it requires a paradigm shift,  and that one has to frame most problems differently in order to take the  most advantage of it. (Careful consideration of how the data structures  are used and reused is also essential to get it to work effectively.) This may  mean that one needs to play around with it a bit and build up an understanding  of what the sequence of typical operations is and what specifically each step  does in terms of manipulating the underlying data. If one has the time and  inclination to do so, then the benefits of using this tool may be substantial.
* [1.x.358][1.x.359]
* 

* There are a few logical ways in which this program could be extended:
* 
*  - Perhaps the most obvious extension would be to implement and test other constitutive models.  This could still be within the realm of coupled magneto-mechanical problems, perhaps considering  alternatives to the "Neo-Hookean"-type elastic part of the energy functions, changing the  constitutive law for the dissipative energy (and its associated evolution law), or including  magnetic hysteretic effects or damage models for the composite polymer that these material  seek to model.
* 
*  - Of course, the implemented models could be modified or completely replaced with models that are  focused on other aspects of physics, such as electro-active polymers, biomechanical materials,  elastoplastic media, etc.
* 
*  - Implement a different time-discretization scheme for the viscoelastic evolution law.
* 
*  - Instead of deriving everything directly from an energy density function, use the   [2.x.246]  to directly linearize the kinetic quantities.  This would mean that only a once-differentiable auto-differentiable number type  would be required, and would certainly improve the performance greatly.  Such an approach would also offer the opportunity for dissipative materials,  such as the magneto-viscoelastic one consider here, to be implemented in  conjunction with AD. This is because the linearization invokes the total  derivative of the dependent variables with respect to the field variables, which  is exactly what the AD frameworks can provide.
* 
*  - Investigate using other auto-differentiable number types and frameworks (such as  ADOL-C). Since each AD library has its own implementation, the choice of which  to use could result in performance increases and, in the most unfortunate cases,  more stable computations. It can at least be said that for the AD libraries that  deal.II supports, the accuracy of results should be largely unaffected by this decision.
* 
*  - Embed one of these constitutive laws within a finite element simulation.
* With less effort, one could think about re-writing nonlinear problemsolvers such as the one implemented in  [2.x.247]  using AD or SDapproaches to compute the Newton matrix. Indeed, this is done in [2.x.248] .
* 

* [1.x.360][1.x.361] [2.x.249] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-72_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] ,  [2.x.2] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28]
*  [2.x.3] 
* [1.x.29]
* 

* [1.x.30][1.x.31]
* 

* [1.x.32][1.x.33]
* 

* This program solves the same problem as  [2.x.4] , that is, it solvesfor the[minimal surface equation](https://en.wikipedia.org/wiki/Minimal_surface) 
* [1.x.34]
* 
* Among the issues we had identified there (see the[1.x.35] section)was that when wanting to usea Newton iteration, we needed to compute the derivative of theresidual of the equation with regard to the solution  [2.x.5]  (here,because the right hand side is zero, the residual is simply the lefthand side). For the equation we have here, this is cumbersome but notimpossible
* 
*  -  but one can easily imagine much more complicatedequations where just implementing the residual itself correctly is achallenge, let alone doing so for the derivative necessary to computethe Jacobian matrix. We will address this issue in this program: Usingthe automatic differentiation techniques discussed in great detail in [2.x.6] , we will come up with a way how we only have to implement theresidual and get the Jacobian for free.
* In fact, we can even go one step further. While in  [2.x.7]  we havejust taken the equation as a given, the minimal surface equation isactually the product of minimizing an energy. Specifically,the minimal surface equations are the Euler-Lagrange equations thatcorrespond to minimizing the energy  [1.x.36]where theenergy density* is given by  [1.x.37]This is the same as saying that we seek to find the stationary point ofthe variation of the energy functional  [1.x.38]as this is where the equilibrium solution to the boundary value problem lies.
* The key point then is that, maybe, we don't even need to implement theresidual, but that implementing the simpler energy density  [2.x.8] might actually be enough.
* Our goal then is this: Whenusing a Newton iteration, we need to repeatedly solve thelinear partial differential equation 
* [1.x.39]
* so that we can compute the update 
* [1.x.40]
* with the solution  [2.x.9]  of the Newton step. As discussed in  [2.x.10] ,we can compute the derivative  [2.x.11]  by hand andobtain  [1.x.41]
* So here then is what this program is about: It is about techniquesthat can help us with computing  [2.x.12]  without having toimplement it explicitly, either by providing an implementation of [2.x.13]  or an implementation of  [2.x.14] . More precisely, we willimplement three different approaches and compare them in terms ofrun-time but also
* 
*  -  maybe more importantly
* 
*  -  how much human effortit takes to implement them:
* 
*  - The method used in  [2.x.15]  to form the Jacobian matrix.
* 
*  - Computing the Jacobian matrix from an implementation of the  residual  [2.x.16] , using automatic differentiation.
* 
*  - Computing both the residual and Jacobian matrix from an  implementation of the energy functional  [2.x.17] , also using automatic  differentiation.
* For the first of these methods, there are no conceptual changescompared to  [2.x.18] .
* 

* [1.x.42][1.x.43]
* 

* For the second method, let us outline how we will approach the issueusing automatic differentiationto compute the linearization of the residual vector. To this end, letus change notation for a moment and denote by  [2.x.19]  not the residualof the differential equation, but in fact theresidual vector*
* 
*  - i.e., thediscrete residual*. We do so because that is what we
*actually* do when we discretize the problem on a given mesh: We solvethe problem  [2.x.20]  where  [2.x.21]  is the vector of unknowns.
* More precisely, the  [2.x.22] th component of the residual is given by[1.x.44]where  [2.x.23] . Given this, thecontribution for cell  [2.x.24]  is[1.x.45]Its first order Taylor expansion is given as[1.x.46]and consequently we can compute the contribution of cell  [2.x.25]  to theJacobian matrix  [2.x.26]  as  [2.x.27] . Theimportant point here is that on cell  [2.x.28] , we can express[1.x.47]For clarity, we have used  [2.x.29]  and  [2.x.30]  as counting indices to makeclear that they are distinct from each other and from  [2.x.31]  above.Because in this formula,  [2.x.32]  only depends on the coefficients [2.x.33] , we can compute the derivative  [2.x.34]  as a matrix viaautomatic differentiation of  [2.x.35] . By the same argument as wealways use, it is clear that  [2.x.36]  does not actually depend on
*all* unknowns  [2.x.37] , but only on those unknowns for which  [2.x.38]  is ashape function that lives on cell  [2.x.39] , and so in practice, we restrict [2.x.40]  and  [2.x.41]  to that part of the vector and matrix thatcorresponds to thelocal* DoF indices, and then distribute from thelocal cell  [2.x.42]  to the global objects.
* Using all of these realizations, the approach will then be toimplement  [2.x.43]  in the program and let the automatic differentiationmachinery compute the derivatives  [2.x.44]  from that.
* 

* [1.x.48][1.x.49]
* 

* For the final implementation of the assembly process, we will move a levelhigher than the residual: our entire linear system will be determineddirectly from the energy functional that governs the physics of thisboundary value problem. We can take advantage of the fact that we cancalculate the total energy in the domain directly from the localcontributions, i.e.,[1.x.50]In the discrete setting, this means that on each finite element we have[1.x.51]If we implement the cell energy, which depends on the field solution,we can compute its first (discrete) variation[1.x.52]and, thereafter, its second (discrete) variation[1.x.53]So, from the cell contribution to the total energy function, we may expectto have the approximate residual and tangent contributions generatedfor us as long as we can provide an implementation of the local energy [2.x.45] . Again, due to the design of theautomatic differentiation variables used in this tutorial, in practicethese approximations for the contributions to the residual vector andtangent matrix are actually accurate to machine precision.
* 

*  [1.x.54] [1.x.55]
*  The majority of this tutorial is an exact replica of  [2.x.46] . So, in the interest of brevity and maintaining a focus on the changes implemented here, we will only document what's new and simply indicate which sections of code are a repetition of what has come before.
* 

* 
*  
*  
*  [1.x.56]  [1.x.57]
* 

* 
*  There are a few new header files that have been included in this tutorial. The first is the one that provides the declaration of the ParameterAcceptor class.
* 

* 
* [1.x.58]
* 
*  This is the second, which is an all-inclusive header that will allow us to incorporate the automatic differentiation (AD) functionality within this code.
* 

* 
* [1.x.59]
* 
*  And the next three provide some multi-threading capability using the generic  [2.x.47]  framework.
* 

* 
* [1.x.60]
* 
*  We then open a namespace for this program and import everything from the dealii namespace into it, as in previous programs:
* 

* 
* [1.x.61]
* 
*   [1.x.62]  [1.x.63]
* 

* 
*  In this tutorial we will implement three different approaches for assembling the linear system. One mirrors the hand implementation originally provided in  [2.x.48] , while the other two use the Sacado automatic differentiation library that is provided as a part of the Trilinos framework.   
*   To facilitate switching between the three implementations, we have this really basic parameters class that has only two options that are configurable.
* 

* 
* [1.x.64]
* 
*  Selection for the formulation and corresponding AD framework to be used:
* 

* 
* 
*  -  formulation = 0 : Unassisted implementation (full hand linearization).
* 

* 
* 
*  -  formulation = 1 : Automated linearization of the finite element residual.
* 

* 
* 
*  -  formulation = 2 : Automated computation of finite element residual and linearization using a variational formulation.
* 

* 
* [1.x.65]
* 
*  The maximum acceptable tolerance for the linear system residual. We will see that the assembly time becomes appreciable once we use the AD framework, so we have increased the tolerance selected in  [2.x.49]  by one order of magnitude. This way, the computations do not take too long to complete.
* 

* 
* [1.x.66]
* 
*   [1.x.67]  [1.x.68]
* 

* 
*  The class template is essentially the same as in  [2.x.50] . The only functional changes to the class are that:
* 

* 
* 
*  - the run() function now takes in two arguments: one to choose which assembly approach is to be adopted, and one for the tolerance for the permissible final residual is, and
* 

* 
* 
*  - there are now three different assembly functions that implement the three methods of assembling the linear system. We'll provide details on these later on.
* 

* 
*  

* 
* [1.x.69]
* 
*   [1.x.70]  [1.x.71]
* 

* 
*  There are no changes to the boundary conditions applied to the problem.
* 

* 
* [1.x.72]
* 
*   [1.x.73]  [1.x.74]
* 

* 
*   [1.x.75]  [1.x.76]
* 

* 
*  There have been no changes made to the class constructor.
* 

* 
* [1.x.77]
* 
*   [1.x.78]  [1.x.79]
* 

* 
*  There have been no changes made to the function that sets up the class data structures, namely the DoFHandler, the hanging node constraints applied to the problem, and the linear system.
* 

* 
* [1.x.80]
* 
*   [1.x.81]  [1.x.82]
* 

* 
*   [1.x.83]  [1.x.84]
* 

* 
*  The assembly functions are the interesting contributions to this tutorial. The assemble_system_unassisted() method implements exactly the same assembly function as is detailed in  [2.x.51] , but in this instance we use the  [2.x.52]  function to multithread the assembly process. The reason for doing this is quite simple: When using automatic differentiation, we know that there is to be some additional computational overhead incurred. In order to mitigate this performance loss, we'd like to take advantage of as many (easily available) computational resources as possible. The  [2.x.53]  concept makes this a relatively straightforward task. At the same time, for the purposes of fair comparison, we need to do the same to the implementation that uses no assistance when computing the residual or its linearization. (The  [2.x.54]  function is first discussed in  [2.x.55]  and  [2.x.56] , if you'd like to read up on it.)   
*   The steps required to implement the multithreading are the same between the three functions, so we'll use the assemble_system_unassisted() function as an opportunity to focus on the multithreading itself.
* 

* 
* [1.x.85]
* 
*  The  [2.x.57]  expects that we provide two exemplar data structures. The first, `ScratchData`, is to store all large data that is to be reused between threads. The `CopyData` will hold the contributions to the linear system that come from each cell. These independent matrix-vector pairs must be accumulated into the global linear system sequentially. Since we don't need anything on top of what the  [2.x.58]  and  [2.x.59]  classes already provide, we use these exact class definitions for our problem. Note that we only require a single instance of a local matrix, local right-hand side vector, and cell degree of freedom index vector
* 
*  -  the  [2.x.60]  therefore has `1` for all three of its template arguments.
* 

* 
* [1.x.86]
* 
*  We also need to know what type of iterator we'll be working with during assembly. For simplicity, we just ask the compiler to work this out for us using the decltype() specifier, knowing that we'll be iterating over active cells owned by the  [2.x.61] 
* 

* 
* [1.x.87]
* 
*  Here we initialize the exemplar data structures. Since we know that we need to compute the shape function gradients, weighted Jacobian, and the position of the quadrate points in real space, we pass these flags into the class constructor.
* 

* 
* [1.x.88]
* 
*  Now we define a lambda function that will perform the assembly on a single cell. The three arguments are those that will be expected by  [2.x.62]  due to the arguments that we'll pass to that final call. We also capture the  [2.x.63]  pointer, which means that we'll have access to "this" (i.e., the current `MinimalSurfaceProblem<dim>`) class instance, and its private member data (since the lambda function is defined within a MinimalSurfaceProblem<dim> method).     
*   At the top of the function, we initialize the data structures that are dependent on the cell for which the work is being performed. Observe that the reinitialization call actually returns an instance to an FEValues object that is initialized and stored within (and, therefore, reused by) the `scratch_data` object.     
*   Similarly, we get aliases to the local matrix, local RHS vector, and local cell DoF indices from the `copy_data` instance that  [2.x.64]  provides. We then initialize the cell DoF indices, knowing that the local matrix and vector are already correctly sized.
* 

* 
* [1.x.89]
* 
*  For Newton's method, we require the gradient of the solution at the point about which the problem is being linearized.       
*   Once we have that, we can perform assembly for this cell in the usual way.  One minor difference to  [2.x.65]  is that we've used the (rather convenient) range-based loops to iterate over all quadrature points and degrees-of-freedom.
* 

* 
* [1.x.90]
* 
*  The second lambda function that  [2.x.66]  requires is one that performs the task of accumulating the local contributions in the global linear system. That is precisely what this one does, and the details of the implementation have been seen before. The primary point to recognize is that the local contributions are stored in the `copy_data` instance that is passed into this function. This `copy_data` has been filled with data during  [2.x.67]  some call to the `cell_worker`.
* 

* 
* [1.x.91]
* 
*  We have all of the required functions definitions in place, so now we call the  [2.x.68]  to perform the actual assembly.  We pass a flag as the last parameter which states that we only want to perform the assembly on the cells. Internally,  [2.x.69]  then distributes the available work to different threads, making efficient use of the multiple cores almost all of today's processors have to offer.
* 

* 
* [1.x.92]
* 
*  And finally, as is done in  [2.x.70] , we remove hanging nodes from the system and apply zero boundary values to the linear system that defines the Newton updates  [2.x.71] .
* 

* 
* [1.x.93]
* 
*   [1.x.94]  [1.x.95]
* 

* 
*  As outlined in the introduction, what we need to do for this second approach is implement the local contributions  [2.x.72]  from cell  [2.x.73]  to the residual vector, and then let the AD machinery deal with how to compute the derivatives  [2.x.74]  from it.   
*   For the following, recall that [1.x.96] where  [2.x.75] .   
*   Let us see how this is implemented in practice:
* 

* 
* [1.x.97]
* 
*  We'll define up front the AD data structures that we'll be using, utilizing the techniques shown in  [2.x.76] . In this case, we choose the helper class that will automatically compute the linearization of the finite element residual using Sacado forward automatic differentiation types. These number types can be used to compute first derivatives only. This is exactly what we want, because we know that we'll only be linearizing the residual, which means that we only need to compute first-order derivatives. The return values from the calculations are to be of type `double`.     
*   We also need an extractor to retrieve some data related to the field solution to the problem.
* 

* 
* [1.x.98]
* 
*  With this, let us define the lambda function that will be used to compute the cell contributions to the Jacobian matrix and the right hand side:
* 

* 
* [1.x.99]
* 
*  We'll now create and initialize an instance of the AD helper class. To do this, we need to specify how many independent variables and dependent variables there are. The independent variables will be the number of local degrees of freedom that our solution vector has, i.e., the number  [2.x.77]  in the per-element representation of the discretized solution vector  [2.x.78]  that indicates how many solution coefficients are associated with each finite element. In deal.II, this equals  [2.x.79]  The number of dependent variables will be the number of entries in the local residual vector that we will be forming. In this particular problem (like many others that employ the [standard Galerkin method](https://en.wikipedia.org/wiki/Galerkin_method)) the number of local solution coefficients matches the number of local residual equations.
* 

* 
* [1.x.100]
* 
*  Next we inform the helper of the values of the solution, i.e., the actual values for  [2.x.80]  about which we wish to linearize. As this is done on each element individually, we have to extract the solution coefficients from the global solution vector. In other words, we define all of those coefficients  [2.x.81]  where  [2.x.82]  is a local degree of freedom as the independent variables that enter the computation of the vector  [2.x.83]  (the dependent function).       
*   Then we get the complete set of degree of freedom values as represented by auto-differentiable numbers. The operations performed with these variables are tracked by the AD library from this point until the object goes out of scope. So it is  [2.x.84] precisely these variables [2.x.85]  with respect to which we will compute derivatives of the residual entries.
* 

* 
* [1.x.101]
* 
*  Then we do some problem specific tasks, the first being to compute all values, (spatial) gradients, and the like based on "sensitive" AD degree of freedom values. In this instance we are retrieving the solution gradients at each quadrature point. Observe that the solution gradients are now sensitive to the values of the degrees of freedom as they use the  [2.x.86]  as the scalar type and the  [2.x.87]  vector provides the local DoF values.
* 

* 
* [1.x.102]
* 
*  The next variable that we declare will store the cell residual vector contributions. This is rather self-explanatory, save for one [1.x.103] detail: Note that each entry in the vector is hand-initialized with a value of zero. This is a  [2.x.88] highly recommended [2.x.89]  practice, as some AD libraries appear not to safely initialize the internal data structures of these number types. Not doing so could lead to some very hard to understand or detect bugs (appreciate that the author of this program mentions this out of, generally bad, experience). So out of an abundance of caution it's worthwhile zeroing the initial value explicitly. After that, apart from a sign change the residual assembly looks much the same as we saw for the cell RHS vector before: We loop over all quadrature points, ensure that the coefficient now encodes its dependence on the (sensitive) finite element DoF values by using the correct `ADNumberType`, and finally we assemble the components of the residual vector. For complete clarity, the finite element shape functions (and their gradients, etc.) as well as the "JxW" values remain scalar valued, but the  [2.x.90]  and the   [2.x.91]  at each quadrature point are computed in terms of the independent variables.
* 

* 
* [1.x.104]
* 
*  Once we have the full cell residual vector computed, we can register it with the helper class.       
*   Thereafter, we compute the residual values (basically, extracting the real values from what we already computed) and their Jacobian (the linearization of each residual component with respect to all cell DoFs) at the evaluation point. For the purposes of assembly into the global linear system, we have to respect the sign difference between the residual and the RHS contribution: For Newton's method, the right hand side vector needs to be equal to thenegative* residual vector.
* 

* 
* [1.x.105]
* 
*  The remainder of the function equals what we had previously:
* 

* 
* [1.x.106]
* 
*   [1.x.107]  [1.x.108]
* 

* 
*  In this third approach, we compute residual and Jacobian as first and second derivatives of the local energy functional [1.x.109] with the energy density given by [1.x.110]   
*   Let us again see how this is done:
* 

* 
* [1.x.111]
* 
*  In this implementation of the assembly process, we choose the helper class that will automatically compute both the residual and its linearization from the cell contribution to an energy functional using nested Sacado forward automatic differentiation types. The selected number types can be used to compute both first and second derivatives. We require this, as the residual defined as the sensitivity of the potential energy with respect to the DoF values (i.e. its gradient). We'll then need to linearize the residual, implying that second derivatives of the potential energy must be computed. You might want to compare this with the definition of `ADHelper` used int previous function, where we used  [2.x.92] 
* 

* 
* [1.x.112]
* 
*  Let us then again define the lambda function that does the integration on a cell.     
*   To initialize an instance of the helper class, we now only require that the number of independent variables (that is, the number of degrees of freedom associated with the element solution vector) are known up front. This is because the second-derivative matrix that results from an energy functional is necessarily square (and also, incidentally, symmetric).
* 

* 
* [1.x.113]
* 
*  Once more, we register all cell DoFs values with the helper, followed by extracting the "sensitive" variant of these values that are to be used in subsequent operations that must be differentiated
* 
*  -  one of those being the calculation of the solution gradients.
* 

* 
* [1.x.114]
* 
*  We next create a variable that stores the cell total energy. Once more we emphasize that we explicitly zero-initialize this value, thereby ensuring the integrity of the data for this starting value.       
*   The aim for our approach is then to compute the cell total energy, which is the sum of the internal (due to right hand side functions, typically linear in  [2.x.93] ) and external energies. In this particular case, we have no external energies (e.g., from source terms or Neumann boundary conditions), so we'll focus on the internal energy part.       
*   In fact, computing  [2.x.94]  is almost trivial, requiring only the following lines:
* 

* 
* [1.x.115]
* 
*  After we've computed the total energy on this cell, we'll register it with the helper.  Based on that, we may now compute the desired quantities, namely the residual values and their Jacobian at the evaluation point. As before, the Newton right hand side needs to be the negative of the residual:
* 

* 
* [1.x.116]
* 
*  As in the previous two functions, the remainder of the function is as before:
* 

* 
* [1.x.117]
* 
*   [1.x.118]  [1.x.119]
* 

* 
*  The solve function is the same as is used in  [2.x.95] .
* 

* 
* [1.x.120]
* 
*   [1.x.121]  [1.x.122]
* 

* 
*  Nothing has changed since  [2.x.96]  with respect to the mesh refinement procedure and transfer of the solution between adapted meshes.
* 

* 
* [1.x.123]
* 
*   [1.x.124]  [1.x.125]
* 

* 
*  The choice of boundary conditions remains identical to  [2.x.97] ...
* 

* 
* [1.x.126]
* 
*   [1.x.127]  [1.x.128]
* 

* 
*  ... as does the function used to compute the residual during the solution iteration procedure. One could replace this by differentiation of the energy functional if one really wanted, but for simplicity we here simply copy what we already had in  [2.x.98] .
* 

* 
* [1.x.129]
* 
*   [1.x.130]  [1.x.131]
* 

* 
*  The choice of step length (or, under-relaxation factor) for the nonlinear iterations procedure remains fixed at the value chosen and discussed in  [2.x.99] .
* 

* 
* [1.x.132]
* 
*   [1.x.133]  [1.x.134]
* 

* 
*  This last function to be called from `run()` outputs the current solution (and the Newton update) in graphical form as a VTU file. It is entirely the same as what has been used in previous tutorials.
* 

* 
* [1.x.135]
* 
*   [1.x.136]  [1.x.137]
* 

* 
*  In the run function, most remains the same as was first implemented in  [2.x.100] . The only observable changes are that we can now choose (via the parameter file) what the final acceptable tolerance for the system residual is, and that we can choose which method of assembly we wish to utilize. To make the second choice clear, we output to the console some message which indicates the selection. Since we're interested in comparing the time taken to assemble for each of the three methods, we've also added a timer that keeps a track of how much time is spent during assembly. We also track the time taken to solve the linear system, so that we can contrast those numbers to the part of the code which would normally take the longest time to execute.
* 

* 
* [1.x.138]
* 
*   [1.x.139]  [1.x.140]
* 

* 
*  Finally the main function. This follows the scheme of most other main functions, with two obvious exceptions:
* 

* 
* 
*  - We call  [2.x.101]  in order to set up (via a hidden default parameter) the number of threads using the execution of multithreaded tasks.
* 

* 
* 
*  - We also have a few lines dedicates to reading in or initializing the user-defined parameters that will be considered during the execution of the program.
* 

* 
* [1.x.141]
* [1.x.142][1.x.143]
* 

* Since there was no change to the physics of the problem that has first been analyzedin  [2.x.102] , there is nothing to report about that. The only outwardly noticeabledifference between them is that, by default, this program will only run 9 meshrefinement steps (as opposed to  [2.x.103] , which executes 11 refinements).This will be observable in the simulation status that appears between theheader text that prints which assembly method is being used, and the finaltimings. (All timings reported below were obtained in release mode.)
* [1.x.144]
* 
* So what is interesting for us to compare is how long the assembly process takesfor the three different implementations, and to put that into some greater context.Below is the output for the hand linearization (as computed on a circa 2012four core, eight thread laptop
* 
*  -  but we're only really interested in therelative time between the different implementations):
* [1.x.145]
* And for the implementation that linearizes the residual in an automatedmanner using the Sacado dynamic forward AD number type:
* [1.x.146]
* And, lastly, for the implementation that computes both the residual andits linearization directly from an energy functional (using nested Sacadodynamic forward AD numbers):
* [1.x.147]
* 
* It's evident that the more work that is passed off to the automatic differentiationframework to perform, the more time is spent during the assembly process. Accumulatedover all refinement steps, using one level of automatic differentiation resultedin  [2.x.104]  more computational time being spent in the assembly stage whencompared to unassisted assembly, while assembling the discrete linear system took [2.x.105]  longer when deriving directly from the energy functional.Unsurprisingly, the overall time spent solving the linear system remained unchanged.This means that the proportion of time spent in the solve phase to the assembly phaseshifted significantly as the number of times automated differentiation was performedat the finite element level. For many, this might mean that leveraging higher-orderdifferentiation (at the finite element level) in production code leads to anunacceptable overhead, but it may still be useful during the prototyping phase.A good compromise between the two may, therefore, be the automated linearizationof the finite element residual, which offers a lot of convenience at a measurable,but perhaps not unacceptable, cost. Alternatively, one could considernot re-building the Newton matrix in every step
* 
*  -  a topic that isexplored in substantial depth in  [2.x.106] .
* Of course, in practice the actual overhead is very much dependent on the problem being evaluated(e.g., how many components there are in the solution, what the nature of the functionbeing differentiated is, etc.). So the exact results presented here should beinterpreted within the context of this scalar problem alone, and when it comes toother problems, some preliminary investigation by the user is certainly warranted.
* 

* [1.x.148][1.x.149]
* 

* Like  [2.x.107] , there are a few items related to automatic differentiation that couldbe evaluated further:
* 
*  - The use of other AD frameworks should be investigated, with the outlook that  alternative implementations may provide performance benefits.
* 
*  - It is also worth evaluating AD number types other than those that have been  hard-coded into this tutorial. With regard to twice differentiable types  employed at the finite-element level, mixed differentiation modes ("RAD-FAD")  should in principle be more computationally efficient than the single  mode ("FAD-FAD") types employed here. The reason that the RAD-FAD type was not  selected by default is that, at the time of writing, there remain some  bugs in its implementation within the Sacado library that lead to memory leaks.  This is documented in the  [2.x.108]  module.
* 
*  - It might be the case that using reduced precision types (i.e., `float`) as the  scalar types for the AD numbers could render a reduction in computational  expense during assembly. Using `float` as the data type for the  matrix and the residual is not unreasonable, given that the Newton  update is only meant to get us closer to the solution, but not  actuallyto* the solution; as a consequence, it makes sense to  consider using reduced-precision data types for computing these  updates, and then accumulating these updates in a solution vector  that uses the full `double` precision accuracy.
* 
*  - One further method of possibly reducing resources during assembly is to frame  the AD implementations as a constitutive model. This would be similar to the  approach adopted in  [2.x.109] , and pushes the starting point for the automatic  differentiation one level higher up the chain of computations. This, in turn,  means that less operations are tracked by the AD library, thereby reducing the  cost of differentiating (even though one would perform the differentiation at  each cell quadrature point).
* 
*  -  [2.x.110]  is yet another variation of  [2.x.111]  that addresses a very  different part of the problem: Line search and whether it is  necessary to re-build the Newton matrix in every nonlinear  iteration. Given that the results above show that using automatic  differentiation comes at a cost, the techniques in  [2.x.112]  have the  potential to offset these costs to some degree. It would therefore  be quite interesting to combine the current program with the  techniques in  [2.x.113] . For production codes, this would certainly be  the way to go.
* 

* [1.x.150][1.x.151] [2.x.114] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-74_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22]
*  [2.x.2] 
* [1.x.23]
* 

* [1.x.24][1.x.25][1.x.26]
* 

* [1.x.27][1.x.28]
* In this tutorial, we display the usage of the FEInterfaceValues class,which is designed for assembling face terms arising from discontinuous Galerkin (DG) methods.The FEInterfaceValues class provides an easy way to obtain the jumpand the average of shape functions and of the solution across cell faces.This tutorial includes the following topics. [2.x.3]    [2.x.4]  The SIPG method for Poisson's equation, which has already been used in  [2.x.5]  and  [2.x.6] .   [2.x.7]  Assembling of face terms using FEInterfaceValues and the system matrix using  [2.x.8]  which is similar to  [2.x.9] .   [2.x.10]  Adaptive mesh refinement using an error estimator.   [2.x.11]  Two test cases: convergence test for a smooth function and adaptive mesh refinement test for a singular solution. [2.x.12] 
* [1.x.29][1.x.30]
* In this example, we consider Poisson's equation[1.x.31]subject to the boundary condition[1.x.32]For simplicity, we assume that the diffusion coefficient  [2.x.13]  is constant here.Note that if  [2.x.14]  is discontinuous, we need to take this into account when computing jump termson cell faces.
* We denote the mesh by  [2.x.15] , and  [2.x.16]  is a mesh cell.The sets of interior and boundary faces are denoted by  [2.x.17]  and  [2.x.18] respectively. Let  [2.x.19]  and  [2.x.20]  be the two cells sharing a face  [2.x.21] ,and  [2.x.22]  be the outer normal vector of  [2.x.23] . Then the jumpoperator is given by the "here minus there" formula,[1.x.33]and the averaging operator as[1.x.34]respectively. Note that when  [2.x.24] , we define  [2.x.25]  and [2.x.26] .The discretization using the SIPG is given by the following weak formula(more details can be found in  [2.x.27]  and the references therein)
* [1.x.35]
* 
* 

* [1.x.36][1.x.37]
* The penalty parameter is defined as  [2.x.28] , where  [2.x.29]  a local length scale associatedwith the cell face; here we choose an approximation of the length of the cell in the direction normal to the face: [2.x.30] ,where  [2.x.31]  are the two cells adjacent to the face  [2.x.32]  and we wecompute  [2.x.33] .
* In the formula above,  [2.x.34]  is the penalization constant.To ensure the discrete coercivity, the penalization constant has to be large enough  [2.x.35] .People do not really have consensus on which of the formulas proposedin the literature should be used. (This is similar to the situationdiscussed in the "Results" section of  [2.x.36] .)One can just pick a large constant, while other options could be the multiples of  [2.x.37]  or  [2.x.38] . In this code,we follow  [2.x.39]  and use  [2.x.40] .
* 

* [1.x.38][1.x.39]
* In this example, with a slight modification, we use the error estimator by Karakashian and Pascal  [2.x.41] [1.x.40]where
* [1.x.41]
* Here we use  [2.x.42]  instead of  [2.x.43]  for the jump terms of  [2.x.44]  (the first term in  [2.x.45]  and  [2.x.46] ).
* In order to compute this estimator, in each cell  [2.x.47]  we compute
* [1.x.42]
* Then the square of the error estimate per cell is[1.x.43]The factor of  [2.x.48]  results from the fact that the overall errorestimator includes each interior face only once, and so the estimators per cellcount it with a factor of one half for each of the two adjacent cells.Note that we compute  [2.x.49]  instead of  [2.x.50]  to simplify the implementation.The error estimate square per cell is then stored in a global vector, whose  [2.x.51]  norm is equal to  [2.x.52] .
* [1.x.44][1.x.45]
* In the first test problem, we run a convergence test using a smooth manufactured solution with  [2.x.53]  in 2D
* [1.x.46]
* and  [2.x.54] . We compute errors against the manufactured solution and evaluate the convergence rate.
* In the second test, we choose  [2.x.55]  on a L-shaped domain  [2.x.56]  in 2D.The solution is given in the polar coordinates by  [2.x.57] ,which has a singularity at the origin. An error estimator is constructed to detect the region with large errors,according to which the mesh is refined adaptively.
* 

*  [1.x.47] [1.x.48]
*  The first few files have already been covered in previous examples and will thus not be further commented on:
* 

* 
* [1.x.49]
* 
*  Here the discontinuous finite elements and FEInterfaceValues are defined.
* 

* 
* [1.x.50]
* 
*   [1.x.51]  [1.x.52] Here we define two test cases: convergence_rate for a smooth function and l_singularity for the  [2.x.58] 
* 

* 
* [1.x.53]
* 
*  A smooth solution for the convergence test:
* 

* 
* [1.x.54]
* 
*  The corresponding right-hand side of the smooth function:
* 

* 
* [1.x.55]
* 
*  The right-hand side that corresponds to the function  [2.x.59]  where we assume that the diffusion coefficient  [2.x.60] :
* 

* 
* [1.x.56]
* 
*   [1.x.57]  [1.x.58] The following two auxiliary functions are used to compute jump terms for  [2.x.61]  and  [2.x.62]  on a face, respectively.
* 

* 
* [1.x.59]
* 
*  This function computes the penalty  [2.x.63] .
* 

* 
* [1.x.60]
* 
*   [1.x.61]  [1.x.62] In the following, we define "Copy" objects for the  [2.x.64]  which is essentially the same as  [2.x.65] . Note that the "Scratch" object is not defined here because we use  [2.x.66]  instead. (The use of "Copy" and "Scratch" objects is extensively explained in the WorkStream namespace documentation.
* 

* 
* [1.x.63]
* 
*   [1.x.64]  [1.x.65] After these preparations, we proceed with the main class of this program, called `SIPGLaplace`. The overall structure of the class is as in many of the other tutorial programs. Major differences will only come up in the implementation of the assemble functions, since we use FEInterfaceValues to assemble face terms.
* 

* 
* [1.x.66]
* 
*  The remainder of the class's members are used for the following:
* 

* 
* 
*  - Vectors to store error estimator square and energy norm square per cell.
* 

* 
* 
*  - Print convergence rate and errors on the screen.
* 

* 
* 
*  - The fiffusion coefficient  [2.x.67]  is set to 1.
* 

* 
* 
*  - Members that store information about the test case to be computed.
* 

* 
* [1.x.67]
* 
*  The constructor here takes the test case as input and then determines the correct solution and right-hand side classes. The remaining member variables are initialized in the obvious way.
* 

* 
* [1.x.68]
* 
*   [1.x.69]  [1.x.70] The assemble function here is similar to that in  [2.x.68]  and  [2.x.69] . Different from assembling by hand, we just need to focus on assembling on each cell, each boundary face, and each interior face. The loops over cells and faces are handled automatically by  [2.x.70]    
*   The function starts by defining a local (lambda) function that is used to integrate the cell terms:
* 

* 
* [1.x.71]
* 
*  Next, we need a function that assembles face integrals on the boundary:
* 

* 
* [1.x.72]
* 
*  Finally, a function that assembles face integrals on interior faces. To reinitialize FEInterfaceValues, we need to pass cells, face and subface indices (for adaptive refinement) to the reinit() function of FEInterfaceValues:
* 

* 
* [1.x.73]
* 
*  The following lambda function will then copy data into the global matrix and right-hand side.  Though there are no hanging node constraints in DG discretization, we define an empty AffineConstraints object that allows us to use the  [2.x.71]  functionality.
* 

* 
* [1.x.74]
* 
*  Copy data from interior face assembly to the global matrix.
* 

* 
* [1.x.75]
* 
*  With the assembly functions defined, we can now create ScratchData and CopyData objects, and pass them together with the lambda functions above to  [2.x.72]  In addition, we need to specify that we want to assemble on interior faces exactly once.
* 

* 
* [1.x.76]
* 
*   [1.x.77]  [1.x.78] The following two functions are entirely standard and without difficulty.
* 

* 
* [1.x.79]
* 
*   [1.x.80]  [1.x.81] The assembly of the error estimator here is quite similar to that of the global matrix and right-had side and can be handled by the  [2.x.73]  framework. To understand what each of the local (lambda) functions is doing, recall first that the local cell residual is defined as  [2.x.74] :
* 

* 
* [1.x.82]
* 
*  Next compute boundary terms  [2.x.75] :
* 

* 
* [1.x.83]
* 
*  And finally interior face terms  [2.x.76] :
* 

* 
* [1.x.84]
* 
*  Having computed local contributions for each cell, we still need a way to copy these into the global vector that will hold the error estimators for all cells:
* 

* 
* [1.x.85]
* 
*  After all of this set-up, let's do the actual work: We resize the vector into which the results will be written, and then drive the whole process using the  [2.x.77]  function.
* 

* 
* [1.x.86]
* 
*   [1.x.87]  [1.x.88] Next, we evaluate the accuracy in terms of the energy norm. This function is similar to the assembling of the error estimator above. Here we compute the square of the energy norm defined by [1.x.89] Therefore the corresponding error is [1.x.90]
* 

* 
* [1.x.91]
* 
*  Assemble  [2.x.78] .
* 

* 
* [1.x.92]
* 
*  Assemble  [2.x.79] .
* 

* 
* [1.x.93]
* 
*  Assemble  [2.x.80] .
* 

* 
* [1.x.94]
* 
*   [1.x.95]  [1.x.96]
* 

* 
* [1.x.97]
* 
*   [1.x.98]  [1.x.99] We compute three errors in the  [2.x.81]  norm,  [2.x.82]  seminorm, and the energy norm, respectively. These are then printed to screen, but also stored in a table that records how these errors decay with mesh refinement and which can be output in one step at the end of the program.
* 

* 
* [1.x.100]
* 
*   [1.x.101]  [1.x.102]
* 

* 
* [1.x.103]
* 
*  Having run all of our computations, let us tell the convergence table how to format its data and output it to screen:
* 

* 
* [1.x.104]
* 
*   [1.x.105]  [1.x.106] The following  [2.x.83]  function is similar to previous examples as well, and need not be commented on.
* 

* 
* [1.x.107]
* [1.x.108][1.x.109]
* 

* The output of this program consist of the console output andsolutions in vtu format.
* In the first test case, when you run the program, the screen output should look like the following:
* [1.x.110]
* 
* When using the smooth case with polynomial degree 3, the convergencetable will look like this: [2.x.84] 
* Theoretically, for polynomial degree  [2.x.85] , the order of convergence in  [2.x.86] norm and  [2.x.87]  seminorm should be  [2.x.88]  and  [2.x.89] , respectively. Our numericalresults are in good agreement with theory.
* In the second test case, when you run the program, the screen output should look like the following:
* [1.x.111]
* 
* The following figure provides a log-log plot of the errors versusthe number of degrees of freedom for this test case on the L-shapeddomain. In order to interpret it, let  [2.x.90]  be the number of degrees offreedom, then on uniformly refined meshes,  [2.x.91]  is of order [2.x.92]  in 2D. Combining the theoretical results in the previous case,we see that if the solution is sufficiently smooth,we can expect the error in the  [2.x.93]  norm to be of order  [2.x.94] and in  [2.x.95]  seminorm to be  [2.x.96] . It is not a prioriclear that one would get the same kind of behavior as a function of [2.x.97]  on adaptively refined meshes like the ones we use for this secondtest case, but one can certainly hope. Indeed, from the figure, we seethat the SIPG with adaptive mesh refinement produces asymptoticallythe kinds of hoped-for results:
*  [2.x.98] 
* In addition, we observe that the error estimator decreasesat almost the same rate as the errors in the energy norm and  [2.x.99]  seminorm,and one order lower than the  [2.x.100]  error. This suggestsits ability to predict regions with large errors.
* While this tutorial is focused on the implementation, the  [2.x.101]  tutorial program achieves an efficientlarge-scale solver in terms of computing time with matrix-free solution techniques.Note that the  [2.x.102]  tutorial does not work with meshes containing hanging nodes at this moment,because the multigrid interface matrices are not as easily determined,but that is merely the lack of some interfaces in deal.II, nothing fundamental.
* 

* [1.x.112][1.x.113] [2.x.103] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-75_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] ,  [2.x.2] ,  [2.x.3] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32]
*  [2.x.4] 
* [1.x.33]
* 

* 
*  [2.x.5]  As a prerequisite of this program, you need to have the p4estlibrary and the Trilinos library installed. The installation of deal.IItogether with these additional libraries is described in the [1.x.34] file.
* 

* 
* [1.x.35][1.x.36][1.x.37]
* 

* In the finite element context, more degrees of freedom usually yield amore accurate solution but also require more computational effort.
* Throughout previous tutorials, we found ways to effectively distributedegrees of freedom by aligning the grid resolution locally with thecomplexity of the solution (adaptive mesh refinement,  [2.x.6] ). Thisapproach is particularly effective if we do not only adapt the gridalone, but also locally adjust the polynomial degree of the associatedfinite element on each cell (hp-adaptation,  [2.x.7] ).
* In addition, assigning more processes to run your program simultaneouslyhelps to tackle the computational workload in lesser time. Depending onthe hardware architecture of your machine, your program must either beprepared for the case that all processes have access to the same memory(shared memory,  [2.x.8] ), or that processes are hosted on severalindependent nodes (distributed memory,  [2.x.9] ).
* In the high-performance computing segment, memory access turns out to bethe current bottleneck on supercomputers. We can avoid storing matricesaltogether by computing the effect of matrix-vector products on the flywith MatrixFree methods ( [2.x.10] ). They can be used for geometricmultigrid methods ( [2.x.11] ) and also for polynomial multigrid methods tospeed solving the system of equation tremendously.
* This tutorial combines all of these particularities and presents astate-of-the-art way how to solve a simple Laplace problem: utilizingboth hp-adaptation and matrix-free hybrid multigrid methods on machineswith distributed memory.
* 

* [1.x.38][1.x.39]
* 

* For parallel applications in FEM, we partition the grid intosubdomains (aka domain decomposition), which are assigned to processes.This partitioning happens on active cells in deal.II as demonstrated in [2.x.12] . There, each cell has the same finite element and the samenumber of degrees of freedom assigned, and approximately the sameworkload. To balance the workload among all processes, we have tobalance the number of cells on all participating processes.
* With hp-adaptive methods, this is no longer the case: the finite elementtype may vary from cell to cell and consequently also the number ofdegrees of freedom. Matching the number of cells does not yield abalanced workload. In the matrix-free context, the workload can beassumed to be proportional the number of degrees of freedom of eachprocess, since in the best case only the source and the destinationvector have to be loaded.
* One could balance the workload by assigning weights to every cell whichare proportional to the number of degrees of freedom, and balance thesum of all weights between all processes. Assigning individual weightsto each cell can be realized with the class  [2.x.13]  whichwe will use later.
* 

* [1.x.40][1.x.41]
* 

* With hp-adaptive methods, we not only have to decide which cells we wantto refine or coarsen, but we also have the choice how we want to dothat: either by adjusting the grid resolution or the polynomial degreeof the finite element.
* We will again base the decision on which cells to adapt on (aposteriori) computed error estimates of the current solution, e.g.,using the KellyErrorEstimator. We will similarly decide how to adaptwith (a posteriori) computed smoothness estimates: large polynomialdegrees work best on smooth parts of the solution while fine gridresolutions are favorable on irregular parts. In  [2.x.14] , we presented away to calculate smoothness estimates based on the decay of Fouriercoefficients. Let us take here the opportunity and present analternative that follows the same idea, but with Legendre coefficients.
* We will briefly present the idea of this new technique, but limit itsdescription to 1D for simplicity. Suppose  [2.x.15]  is a finiteelement function defined on a cell  [2.x.16]  as[1.x.42]where each  [2.x.17]  is a shape function.We can equivalently represent  [2.x.18]  in the basis of Legendrepolynomials  [2.x.19]  as[1.x.43]Our goal is to obtain a mapping between the finite element coefficients [2.x.20]  and the Legendre coefficients  [2.x.21] . We will accomplish this bywriting the problem as a  [2.x.22] -projection of  [2.x.23]  onto theLegendre basis. Each coefficient  [2.x.24]  can be calculated via[1.x.44]By construction, the Legendre polynomials are orthogonal under the [2.x.25] -inner product on  [2.x.26] . Additionally, we assume that they have beennormalized, so their inner products can be written as[1.x.45]where  [2.x.27]  is the Kronecker delta, and  [2.x.28]  is the Jacobian ofthe mapping from  [2.x.29]  to  [2.x.30] , which (in this tutorial) is assumedto be constant (i.e., the mapping must be affine).
* Hence, combining all these assumptions, the projection matrix forexpressing  [2.x.31]  in the Legendre basis is just  [2.x.32] 
* 
*  -  that is,  [2.x.33]  times the identity matrix. Let  [2.x.34] be the Mapping from  [2.x.35]  to its reference cell  [2.x.36] . The entries inthe right-hand side in the projection system are, therefore,[1.x.46]Recalling the shape function representation of  [2.x.37] , we canwrite this as  [2.x.38] , where [2.x.39]  is the change-of-basis matrix with entries[1.x.47]so the values of  [2.x.40]  can be written  [2.x.41] independently [2.x.42]  of [2.x.43]  by factoring  [2.x.44]  out front after transforming to referencecoordinates. Hence, putting it all together, the projection problem canbe written as[1.x.48]which can be rewritten as simply[1.x.49]
* At this point, we need to emphasize that most finite elementapplications use unstructured meshes for which mapping is almost alwaysnon-affine. Put another way: the assumption that  [2.x.45]  is constantacross the cell is not true for general meshes. Hence, a correctcalculation of  [2.x.46]  requires not only that we calculate thecorresponding transformation matrix  [2.x.47]  for every single cell,but that we also define a set of Legendre-like orthogonal functions on acell  [2.x.48]  which may have an arbitrary and very complex geometry. Thesecond part, in particular, is very computationally expensive. Thecurrent implementation of the FESeries transformation classes relies onthe simplification resulting from having a constant Jacobian to increaseperformance and thus only yields correct results for affine mappings.The transformation is only used for the purpose of smoothness estimationto decide on the type of adaptation, which is not a critical componentof a finite element program. Apart from that, this circumstance does notpose a problem for this tutorial as we only use square-shaped cells.
* Eibner and Melenk  [2.x.49]  argued that a function is analytic,i.e., representable by a power series, if and only if the absolutevalues of the Legendre coefficients decay exponentially with increasingindex  [2.x.50] :[1.x.50]The rate of decay  [2.x.51]  can be interpreted as a measure for thesmoothness of that function. We can get it as the slope of a linearregression fit of the transformation coefficients:[1.x.51]
* We will perform this fit on each cell  [2.x.52]  to get a local estimate forthe smoothness of the finite element approximation. The decay rate [2.x.53]  then acts as the decision indicator for hp-adaptation. For afinite element on a cell  [2.x.54]  with a polynomial degree  [2.x.55] , calculatingthe coefficients for  [2.x.56]  proved to be a reasonable choice toestimate smoothness. You can find a more detailed and dimensionindependent description in  [2.x.57] .
* All of the above is already implemented in the  [2.x.58]  classand the  [2.x.59]  namespace. With the errorestimates and smoothness indicators, we are then left to flag the cellsfor actual refinement and coarsening. Some functions from the [2.x.60]  and  [2.x.61]  namespaces willhelp us with that later.
* 

* [1.x.52][1.x.53]
* 

* Finite element matrices are typically very sparse. Additionally,hp-adaptive methods correspond to matrices with highly variable numbersof nonzero entries per row. Some state-of-the-art preconditioners, likethe algebraic multigrid (AMG) ones as used in  [2.x.62] , behave poorly inthese circumstances.
* We will thus rely on a matrix-free hybrid multigrid preconditioner. [2.x.63]  has already demonstrated the superiority of geometric multigridmethods method when combined with the MatrixFree framework. Theapplication on hp-adaptive FEM requires some additional work thoughsince the children of a cell might have different polynomial degrees. Asa remedy, we perform a p-relaxation to linear elements first (similar toMitchell  [2.x.64] ) and then perform h-relaxation in theusual manner. On the coarsest level, we apply an algebraic multigridsolver. The combination of p-multigrid, h-multigrid, and AMG makes thesolver to a hybrid multigrid solver.
* We will create a custom hybrid multigrid preconditioner with the speciallevel requirements as described above with the help of the existingglobal-coarsening infrastructure via the use ofMGTransferGlobalCoarsening.
* 

* [1.x.54][1.x.55]
* 

* For elliptic equations, each reentrant corner typically invokes asingularity  [2.x.65] . We can use this circumstance to put ourhp-decision algorithms to a test: on all cells to be adapted, we wouldprefer a fine grid near the singularity, and a high polynomial degreeotherwise.
* As the simplest elliptic problem to solve under these conditions, wechose the Laplace equation in a L-shaped domain with the reentrantcorner in the origin of the coordinate system.
* To be able to determine the actual error, we manufacture a boundaryvalue problem with a known solution. On the above mentioned domain, onesolution to the Laplace equation is, in polar coordinates, [2.x.66] :[1.x.56]
* See also  [2.x.67]  or  [2.x.68] . The solution looks as follows:
*  [2.x.69] 
* 

* 
* [1.x.160][1.x.161][1.x.162]
* 

* [1.x.163][1.x.164]
* 

* The deal.II library offers multiple strategies to decide which type ofadaptation to impose on cells: either adjust the grid resolution orchange the polynomial degree. We only presented the [1.x.165] strategy in this tutorial, while  [2.x.70] demonstrated the [1.x.166] equivalent of the same idea.
* See the "possibilities for extensions" section of  [2.x.71]  for anoverview over these strategies, or the corresponding documentationfor a detailed description.
* There, another strategy is mentioned that has not been shown in anytutorial so far: the strategy based on [1.x.167]. Theusage of this method for parallel distributed applications is moretricky than the others, so we will highlight the challenges that comealong with it. We need information about the final state of refinementflags, and we need to transfer the solution across refined meshes. Forthe former, we need to attach the  [2.x.72] function to the  [2.x.73]  signal ina way that it will be called [1.x.168] the [2.x.74]  function. At this stage, allrefinement flags and future FE indices are terminally set and a reliableprediction of the error is possible. The predicted error then needs tobe transferred across refined meshes with the aid of [2.x.75] 
* Try implementing one of these strategies into this tutorial and observethe subtle changes to the results. You will notice that all strategiesare capable of identifying the singularities near the reentrant cornersand will perform  [2.x.76] -refinement in these regions, while preferring [2.x.77] -refinement in the bulk domain. A detailed comparison of thesestrategies is presented in  [2.x.78]  .
* 

* [1.x.169][1.x.170]
* 

* This tutorial focuses solely on matrix-free strategies. All hp-adaptivealgorithms however also work with matrix-based approaches in theparallel distributed context.
* To create a system matrix, you can either use the [2.x.79]  function, or use an [2.x.80]  function similar to the one of  [2.x.81] .You can then pass the system matrix to the solver as usual.
* You can time the results of both matrix-based and matrix-freeimplementations, quantify the speed-up, and convince yourself whichvariant is faster.
* 

* [1.x.171][1.x.172]
* 

* For sake of simplicity, we have restricted ourselves to a single type ofcoarse-grid solver (CG with AMG), smoother (Chebyshev smoother withpoint Jacobi preconditioner), and geometric-coarsening scheme (globalcoarsening) within the multigrid algorithm. Feel free to try outalternatives and investigate their performance and robustness.
* 

* [1.x.173][1.x.174] [2.x.82] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-76_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20]
* 
*  [2.x.2] 
* [1.x.21]
* [1.x.22][1.x.23][1.x.24]
* 

* This tutorial program solves the Euler equations of fluid dynamics, using anexplicit time integrator with the matrix-free framework applied to ahigh-order discontinuous Galerkin discretization in space. The numericalapproach used here is identical to that used in  [2.x.3] , however, we utilizedifferent advanced MatrixFree techniques to reach even a higher throughput.
* The two main features of this tutorial are:
* 
*  - the usage of shared-memory features from MPI-3.0 and
* 
*  - the usage of cell-centric loops, which allow to write to the global vector only  once and, therefore, are ideal for the usage of shared memory.
* Further topics we discuss in this tutorial are the usage and benefits of thetemplate argument VectorizedArrayType (instead of simply usingVectorizedArray<Number>) as well as the possibility to pass lambdas toMatrixFree loops.
* For details on the numerics, we refer to the documentation of  [2.x.4] . Weconcentrate here only on the key differences.
* [1.x.25][1.x.26]
* 

* [1.x.27][1.x.28]
* 

* There exist many shared-memory libraries that are based on threads like TBB,OpenMP, or TaskFlow. Integrating such libraries into existing MPI programsallows one to use shared memory. However, these libraries come with an overheadfor the programmer, since all parallelizable code sections have to be found andtransformed according to the library used, including the difficulty when somethird-party numerical library, like an iterative solver package, only relies onMPI.
* Considering a purely MPI-parallelized FEM application, one can identify thatthe major time and memory benefit of using shared memory would come fromaccessing the part of the solution vector owned by the processes on the samecompute node without the need to make explicit copies and buffering them.Fur this propose, MPI-3.0 provides shared-memory features based on so-calledwindows, where processes can directly access the data of the neighbors on the sameshared-memory domain.
* [1.x.29][1.x.30]
* 

* A few relevant MPI-3.0 commands are worth discussing in detail.A new MPI communicator  [2.x.5] , which consists of processes fromthe communicator  [2.x.6]  that have access to the same shared memory,can be created via:
* [1.x.31]
* 
* The following code snippet shows the simplified allocation routines ofshared memory for the value type  [2.x.7]  and the size [2.x.8] , as well as, how to query pointers to the data belongingto processes in the same shared-memory domain:
* [1.x.32]
* 
* Once the data is not needed anymore, the window has to be freed, which alsofrees the locally-owned data:
* [1.x.33]
* 
* [1.x.34][1.x.35]
* 

* The commands mentioned in the last section are integrated into [2.x.9]  and are used to allocate shared memory ifan optional (second) communicator is provided to the reinit()-functions.
* For example, a vector can be set up with a partitioner (containing the globalcommunicator) and a sub-communicator (containing the processes on the samecompute node):
* [1.x.36]
* 
* Locally owned values and ghost values can be processed as usual. However, nowusers also have read access to the values of the shared-memory neighbors viathe function:
* [1.x.37]
* 
* [1.x.38][1.x.39]
* 

* While  [2.x.10]  provides the option to allocateshared memory and to access the values of shared memory of neighboring processesin a coordinated way, it does not actually exploit the benefits of theusage of shared memory itself.
* The MatrixFree infrastructure, however, does:
* 
*  - On the one hand, within the matrix-free loops  [2.x.11]    [2.x.12]  and  [2.x.13]  only ghost  values that need to be updated  [2.x.14] are [2.x.15]  updated. Ghost values from  shared-memory neighbors can be accessed directly, making buffering, i.e.,  copying of the values into the ghost region of a vector possibly redundant.  To deal with possible race conditions, necessary synchronizations are  performed within MatrixFree. In the case that values have to be buffered,  values are copied directly from the neighboring shared-memory process,  bypassing more expensive MPI operations based on  [2.x.16]  and   [2.x.17] .
* 
*  - On the other hand, classes like FEEvaluation and FEFaceEvaluation can read  directly from the shared memory, so buffering the values is indeed  not necessary in certain cases.
* To be able to use the shared-memory capabilities of MatrixFree, MatrixFreehas to be appropriately configured by providing the user-created sub-communicator:
* [1.x.40]
* 
* 

* [1.x.41][1.x.42]
* 

* [1.x.43][1.x.44]
* 

* "Face-centric loops" (short FCL) visit cells and faces (inner and boundary ones) inseparate loops. As a consequence, each entity is visited only once and fluxesbetween cells are evaluated only once. How to perform face-centric loopswith the help of  [2.x.18]  by providing three functions (one forthe cell integrals, one for the inner, and one for the boundary faces) hasbeen presented in  [2.x.19]  and  [2.x.20] .
* "Cell-centric loops" (short CCL or ECL (for element-centric loops)in the hyper.deal release paper), incontrast, process a cell and in direct succession process all itsfaces (i.e., visit all faces twice). Their benefit has become clear formodern CPU processor architecture in the literature  [2.x.21] ,although this kind of loop implies that fluxes have to be computed twice (foreach side of an interior face). CCL has two primary advantages:
* 
*  - On the one hand, entries in the solution vector are written exactly once  back to main memory in the case of CCL, while in the case of FCL at least once  despite of cache-efficient scheduling of cell and face loops-due to cache  capacity misses.
* 
*  - On the other hand, since each entry of the solution vector is accessed exactly  once, no synchronization between threads is needed while accessing the solution  vector in the case of CCL. This absence of race conditions during writing into  the destination vector makes CCL particularly suitable for shared-memory  parallelization.
* One should also note that although fluxes are computed twice in the case of CCL,this does not automatically translate into doubling of the computation, sincevalues already interpolated to the cell quadrature points can be interpolatedto a face with a simple 1D interpolation.
* [1.x.45][1.x.46]
* 

* For cell-centric loop implementations, the function  [2.x.22] can be used, to which the user can pass a function that should be performed oneach cell.
* To derive an appropriate function, which can be passed in  [2.x.23] one might, in principle, transform/merge the following three functions, which canbe passed to a  [2.x.24] 
* [1.x.47]
* 
* in the following way:
* [1.x.48]
* 
* It should be noted that FEFaceEvaluation is initialized now with two numbers,the cell number and the local face number. The given example onlyhighlights how to transform face-centric loops into cell-centric loops andis by no means efficient, since data is read and written multiple timesfrom and to the global vector as well as computations are performedredundantly. Below, we will discuss advanced techniques that target these issues.
* To be able to use  [2.x.25]  following flags of  [2.x.26] have to be enabled:
* [1.x.49]
* 
* In particular, these flags enable that the internal data structures are set upfor all faces of the cells.
* Currently, cell-centric loops in deal.II only work for uniformly refined meshesand if no constraints are applied (which is the standard case DG is normallyused).
* 

* [1.x.50][1.x.51]
* 

* The examples given above have already used lambdas, which have been provided tomatrix-free loops. The following short examples present how to transform functions betweena version where a class and a pointer to one of its methods are used and avariant where lambdas are utilized.
* In the following code, a class and a pointer to one of its methods, which shouldbe interpreted as cell integral, are passed to  [2.x.27] 
* [1.x.52]
* 
* [1.x.53]
* 
* However, it is also possible to pass an anonymous function via a lambda functionwith the same result:
* [1.x.54]
* 
* [1.x.55][1.x.56]
* 

* The class VectorizedArray<Number> is a key component to achieve the highnode-level performance of the matrix-free algorithms in deal.II.It is a wrapper class around a short vector of  [2.x.28]  entries of type Number andmaps arithmetic operations to appropriate single-instruction/multiple-data(SIMD) concepts by intrinsic functions. The length of the vector can bequeried by  [2.x.29]  and its underlying number type by [2.x.30] 
* In the default case ( [2.x.31] ), the vector length isset at compile time of the library tomatch the highest value supported by the given processor architecture.However, also a second optional template argument can bespecified as  [2.x.32]  explicitlycontrols the  vector length within the capabilities of a particular instructionset. A full list of supported vector lengths is presented in the following table:
*  [2.x.33] 
* This allows users to select the vector length/ISA and, as a consequence, thenumber of cells to be processed at once in matrix-free operator evaluations,possibly reducing the pressure on the caches, an severe issue for very highdegrees (and dimensions).
* A possible further reason to reduce the number of filled lanesis to simplify debugging: instead of having to look at, e.g., 8cells, one can concentrate on a single cell.
* The interface of VectorizedArray also enables the replacement by any type witha matching interface. Specifically, this prepares deal.II for the  [2.x.34] class that is planned to become part of the C++23 standard. The following tablecompares the deal.II-specific SIMD classes and the equivalent C++23 classes:
* 

*  [2.x.35] 
* 

*  [1.x.57] [1.x.58]
*   [1.x.59]  [1.x.60]
* 

* 
*  The same includes as in  [2.x.36] :
* 

* 
* [1.x.61]
* 
*  A new include for categorizing of cells according to their boundary IDs:
* 

* 
* [1.x.62]
* 
*  The same input parameters as in  [2.x.37] :
* 

* 
* [1.x.63]
* 
*  This parameter specifies the size of the shared-memory group. Currently, only the values 1 and  [2.x.38]  is possible, leading to the options that the memory features can be turned off or all processes having access to the same shared-memory domain are grouped together.
* 

* 
* [1.x.64]
* 
*  Here, the type of the data structure is chosen for vectorization. In the default case, VectorizedArray<Number> is used, i.e., the highest instruction-set-architecture extension available on the given hardware with the maximum number of vector lanes is used. However, one might reduce the number of filled lanes, e.g., by writing  [2.x.39]  to only process 4 cells.
* 

* 
* [1.x.65]
* 
*  The following parameters have not changed:
* 

* 
* [1.x.66]
* 
*  Specify max number of time steps useful for performance studies.
* 

* 
* [1.x.67]
* 
*  Runge-Kutta-related functions copied from  [2.x.40]  and slightly modified with the purpose to minimize global vector access:
* 

* 
* [1.x.68]
* 
*  Euler-specific utility functions from  [2.x.41] :
* 

* 
* [1.x.69]
* 
*  General-purpose utility functions from  [2.x.42] :
* 

* 
* [1.x.70]
* 
*   [1.x.71]  [1.x.72]
* 

* 
*  Euler operator from  [2.x.43]  with some changes as detailed below:
* 

* 
* [1.x.73]
* 
*  Instance of SubCommunicatorWrapper containing the sub-communicator, which we need to pass to  [2.x.44]  to be able to exploit MPI-3.0 shared-memory capabilities:
* 

* 
* [1.x.74]
* 
*  New constructor, which creates a sub-communicator. The user can specify the size of the sub-communicator via the global parameter group_size. If the size is set to
* 
*  - , all MPI processes of a shared-memory domain are combined to a group. The specified size is decisive for the benefit of the shared-memory capabilities of MatrixFree and, therefore, setting the  [2.x.45]  is a reasonable choice. By setting, the size to  [2.x.46]  users explicitly disable the MPI-3.0 shared-memory features of MatrixFree and rely completely on MPI-2.0 features, like  [2.x.47]  and  [2.x.48] .
* 

* 
* [1.x.75]
* 
*  New destructor responsible for freeing of the sub-communicator.
* 

* 
* [1.x.76]
* 
*  Modified reinit() function to setup the internal data structures in MatrixFree in a way that it is usable by the cell-centric loops and the MPI-3.0 shared-memory capabilities are used:
* 

* 
* [1.x.77]
* 
*  Categorize cells so that all lanes have the same boundary IDs for each face. This is strictly not necessary, however, allows to write simpler code in  [2.x.49]  without masking, since it is guaranteed that all cells grouped together (in a VectorizedArray) have to perform exactly the same operation also on the faces.
* 

* 
* [1.x.78]
* 
*  Enable MPI-3.0 shared-memory capabilities within MatrixFree by providing the sub-communicator:
* 

* 
* [1.x.79]
* 
*  The following function does an entire stage of a Runge--Kutta update and is
* 

* 
* 
*  - alongside the slightly modified setup
* 
*  - the heart of this tutorial compared to  [2.x.50] .   
*   In contrast to  [2.x.51] , we are not executing the advection step (using  [2.x.52]  and the inverse mass-matrix step (using  [2.x.53]  in sequence, but evaluate everything in one go inside of  [2.x.54]  This function expects a single function that is executed on each locally-owned (macro) cell as parameter so that we need to loop over all faces of that cell and perform needed integration steps on our own.   
*   The following function contains to a large extent copies of the following functions from  [2.x.55]  so that comments related the evaluation of the weak form are skipped here:
* 

* 
* 
*  -  [2.x.56] 
* 

* 
* 
*  -  [2.x.57] 
* 

* 
* 
*  -  [2.x.58] 
* 

* 
* 
*  -  [2.x.59] 
* 

* 
* [1.x.80]
* 
*  Run a cell-centric loop by calling  [2.x.60]  and providing a lambda containing the effects of the cell, face and boundary-face integrals:
* 

* 
* [1.x.81]
* 
*  Loop over all cell batches:
* 

* 
* [1.x.82]
* 
*  Read values from global vector and compute the values at the quadrature points:
* 

* 
* [1.x.83]
* 
*  Buffer the computed values at the quadrature points, since these are overridden by  [2.x.61]  in the next step, however, are needed later on for the face integrals:
* 

* 
* [1.x.84]
* 
*  Apply the cell integral at the cell quadrature points. See also the function  [2.x.62]  from  [2.x.63] :
* 

* 
* [1.x.85]
* 
*  Test with the gradient of the test functions in the quadrature points. We skip the interpolation back to the support points of the element, since we first collect all contributions in the cell quadrature points and only perform the interpolation back as the final step.
* 

* 
* [1.x.86]
* 
*  Loop over all faces of the current cell:
* 

* 
* [1.x.87]
* 
*  Determine the boundary ID of the current face. Since we have set up MatrixFree in a way that all filled lanes have guaranteed the same boundary ID, we can select the boundary ID of the first lane.
* 

* 
* [1.x.88]
* 
*  Interpolate the values from the cell quadrature points to the quadrature points of the current face via a simple 1D interpolation:
* 

* 
* [1.x.89]
* 
*  Check if the face is an internal or a boundary face and select a different code path based on this information:
* 

* 
* [1.x.90]
* 
*  Process and internal face. The following lines of code are a copy of the function  [2.x.64]  from  [2.x.65] :
* 

* 
* [1.x.91]
* 
*  Process a boundary face. These following lines of code are a copy of the function  [2.x.66]  from  [2.x.67] :
* 

* 
* [1.x.92]
* 
*  Evaluate local integrals related to cell by quadrature and add into cell contribution via a simple 1D interpolation:
* 

* 
* [1.x.93]
* 
*  Apply inverse mass matrix in the cell quadrature points. See also the function  [2.x.68]  from  [2.x.69] :
* 

* 
* [1.x.94]
* 
*  Transform values from collocation space to the original Gauss-Lobatto space:
* 

* 
* [1.x.95]
* 
*  Perform Runge-Kutta update and write results back to global vectors:
* 

* 
* [1.x.96]
* 
*  From here, the code of  [2.x.70]  has not changed.
* 

* 
* [1.x.97]
* [1.x.98][1.x.99]
* 

* Running the program with the default settings on a machine with 40 processesproduces the following output:
* [1.x.100]
* 
* and the following visual output:
*  [2.x.71] 
* As a reference, the results of  [2.x.72]  using FCL are:
* [1.x.101]
* 
* By the modifications shown in this tutorial, we were able to achieve a speedup of27% for the Runge-Kutta stages.
* [1.x.102][1.x.103]
* 

* The algorithms are easily extendable to higher dimensions: a high-dimensional[1.x.104]is part of the hyper.deal library. An extension of cell-centric loopsto locally-refined meshes is more involved.
* [1.x.105][1.x.106]
* 

* The solver presented in this tutorial program can also be extended to thecompressible Navier–Stokes equations by adding viscous terms, as alsosuggested in  [2.x.73] . To keep as much of the performance obtained here despitethe additional cost of elliptic terms, e.g. via an interior penalty method, thattutorial has proposed to switch the basis from FE_DGQ to FE_DGQHermite like inthe  [2.x.74]  tutorial program. The reasoning behind this switch is that in thecase of FE_DGQ all values of neighboring cells (i.e.,  [2.x.75]  layers) are needed,whilst in the case of FE_DGQHermite only 2 layers, making the lattersignificantly more suitable for higher degrees. The additional layers have to be,on the one hand, loaded from main memory during flux computation and, one theother hand, have to be communicated. Using the shared-memory capabilitiesintroduced in this tutorial, the second point can be eliminated on a singlecompute node or its influence can be reduced in a hybrid context.
* [1.x.107][1.x.108]
* 

* Cell-centric loops could be used to create block Gauss-Seidel preconditionersthat are multiplicative within one process and additive over processes. Thesetype of preconditioners use during flux computation, in contrast to Jacobi-typepreconditioners, already updated values from neighboring cells. The followingpseudo-code visualizes how this could in principal be achieved:
* [1.x.109]
* 
* For this purpose, one can exploit the cell-data vector capabilities ofMatrixFree and the range-based iteration capabilities of VectorizedArray.
* Please note that in the given example we process  [2.x.76] number of blocks, since each lane corresponds to one block. We consider blocksas updated if all blocks processed by a vector register have been updated. Inthe case of Cartesian meshes this is a reasonable approach, however, forgeneral unstructured meshes this conservative approach might lead to a decrease in theefficiency of the preconditioner. A reduction of cells processed in parallelby explicitly reducing the number of lanes used by  [2.x.77] might increase the quality of the preconditioner, but with the cost that eachiteration might be more expensive. This dilemma leads us to a further"possibility for extension": vectorization within an element.
* 

* [1.x.110][1.x.111] [2.x.78] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-77_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18]
*  [2.x.2] 
* [1.x.19] [2.x.3] 
* [1.x.20][1.x.21][1.x.22]
* 

* The  [2.x.4]  program solved the following, nonlinear equationdescribing the minimal surface problem:
* [1.x.23]
*  [2.x.5]  uses a Newton method, andNewton's method works by repeatedly solving alinearized* problem foran update  [2.x.6] 
* 
*  -  called the "search direction"
* 
*  - , computing a"step length" [2.x.7] , and then combining them to compute the newguess for the solution via
* [1.x.24]
* 
* In the course of the discussions in  [2.x.8] , we found that it isawkward to compute the step length, and so just settled for simplechoice: Always choose  [2.x.9] . This is of course not efficient:We know that we can only realize Newton's quadratic convergence rateif we eventually are able to choose  [2.x.10] , though we may haveto choose it smaller for the first few iterations where we are stilltoo far away to use this long a step length.
* Among the goals of this program is therefore to address thisshortcoming. Since line search algorithms are not entirely trivial toimplement, one does as one should do anyway: Import complicatedfunctionality from an external library. To this end, we will make useof the interfaces deal.II has to one of the big nonlinear solverpackages, namely the[KINSOL](https://computing.llnl.gov/projects/sundials/kinsol)sub-package of the[SUNDIALS](https://computing.llnl.gov/projects/sundials)suite. %SUNDIALS is, at its heart, a package meant to solve complexordinary differential equations (ODEs) and differential-algebraicequations (DAEs), and the deal.II interfaces allow for this via theclasses in the SUNDIALS namespace: Notably the  [2.x.11]  and [2.x.12]  classes. But, because that is an important step in thesolution of ODEs and DAEs with implicit methods, %SUNDIALS also has asolver for nonlinear problems called KINSOL, and deal.II has aninterface to it in the form of the  [2.x.13]  class. This iswhat we will use for the solution of our problem.
* But %SUNDIALS isn't just a convenient way for us to avoid writing aline search algorithm. In general, the solution of nonlinear problemsis quite expensive, and one typically wants to save as much computetime as possible. One way one can achieve this is as follows: Thealgorithm in  [2.x.14]  discretizes the problem and then in everyiteration solves a linear system of the form
* [1.x.25]
* where  [2.x.15]  is the residual vector computed using the current vectorof nodal values  [2.x.16] ,  [2.x.17]  is its derivative (called the"Jacobian"), and  [2.x.18]  is the update vector that corresponds tothe function  [2.x.19]  mentioned above. The construction of [2.x.20]  has been thoroughly discussed in  [2.x.21] , as has the way tosolve the linear system in each Newton iteration. So let us focus onanother aspect of the nonlinear solution procedure: Computing  [2.x.22]  isexpensive, and assembling the matrix  [2.x.23]  even more so. Do weactually need to do that in every iteration? It turns out that in manyapplications, this is not actually necessary: These methods often convergeeven if we replace  [2.x.24]  by an approximation  [2.x.25]  and solve
* [1.x.26]
* instead, then update
* [1.x.27]
* This may require an iteration or two more because our update [2.x.26]  is not quite as good as  [2.x.27] , but itmay still be a win because we don't have to assemble  [2.x.28]  quite asoften.
* What kind of approximation  [2.x.29]  would we like for  [2.x.30] ? Theorysays that as  [2.x.31]  converges to the exact solution  [2.x.32] , we need toensure that  [2.x.33]  needs to converge to  [2.x.34] .In particular, since  [2.x.35] , a valid choice is [2.x.36] . But so is choosing  [2.x.37]  every, say,fifth iteration  [2.x.38]  and for the other iterations, we choose [2.x.39]  equal to the last computed  [2.x.40] . This is what we will dohere: we will just re-use  [2.x.41]  from theprevious iteration, which may again be what we had used in theiteration before that,  [2.x.42] .
* This scheme becomes even more interesting if, for the solution of thelinear system with  [2.x.43] , we don't just have to assemble a matrix, butalso compute a good preconditioner. For example, if we were to use asparse LU decomposition via the SparseDirectUMFPACK class, or used ageometric or algebraic multigrid. In those cases, we would also nothave to update the preconditioner, whose computation may have takenabout as long or longer than the assembly of the matrix in the firstplace. Indeed, with this mindset, we should probably think about usingthebest* preconditioner we can think of, even though theirconstruction is typically quite expensive: We will hope to amortizethe cost of computing this preconditioner by applying it to more thanone just one linear solve.
* The big question is, of course: By what criterion do we decide whetherwe can get away with the approximation  [2.x.44]  based on apreviously computed Jacobian matrix  [2.x.45]  that goes back  [2.x.46] steps, or whether we need to
* 
*  -  at least in this iteration
* 
*  -  actuallyre-compute the Jacobian  [2.x.47]  and the corresponding preconditioner?This is, like the issue with line search, one that requires anon-trivial amount of code that monitors the convergence of theoverall algorithm. Wecould* implement these sorts of thingsourselves, but we probablyshouldn't*: KINSOL already does that forus. It will tell our code when to "update" the Jacobian matrix.
* One last consideration if we were to use an iterative solver instead ofthe sparse direct one mentioned above: Not only is it possible to getaway with replacing  [2.x.48]  by some approximation  [2.x.49]  whensolving for the update  [2.x.50] , but one can also ask whether itis necessary to solve the linear system
* [1.x.28]
* to high accuracy. The thinking goes like this: While our current solution [2.x.51]  is still far away from  [2.x.52] , why would we solve this linearsystem particularly accurately? The update [2.x.53]  is likely still going to be far awayfrom the exact solution, so why spend much time on solving the linear systemto great accuracy? This is the kind of thinking that underlies algorithmssuch as the "Eisenstat-Walker trick"  [2.x.54]  in which one is givena tolerance to which the linear system above in iteration  [2.x.55]  has to besolved, with this tolerance dependent on the progress in the overallnonlinear solver. As before, one could try to implement this oneself,but KINSOL already provides this kind of information for us
* 
*  -  though wewill not use it in this program since we use a direct solver that requiresno solver tolerance and just solves the linear system exactly up toround-off.
* As a summary of all of these considerations, we could say thefollowing: There is no need to reinvent the wheel. Just like deal.IIprovides a vast amount of finite-element functionality, %SUNDIALS'KINSOL package provides a vast amount of nonlinear solverfunctionality, and we better use it.
* 

* [1.x.29][1.x.30]
* 

* KINSOL, like many similar packages, works in a pretty abstract way. Atits core, it sees a nonlinear problem of the form
* [1.x.31]
* and constructs a sequence of iterates  [2.x.56]  which, in general, arevectors of the same length as the vector returned by the function [2.x.57] . To do this, there are a few things it needs from the user:
* 
*  - A way to resize a given vector to the correct size.
* 
*  - A way to evaluate, for a given vector  [2.x.58] , the function  [2.x.59] . This  function is generally called the "residual" operation because the  goal is of course to find a point  [2.x.60]  for which  [2.x.61] ;  if  [2.x.62]  returns a nonzero vector, then this is the  [1.x.32]  (i.e., the "rest", or whatever is "left over"). The function  that will do this is in essence the same as the computation of  the right hand side vector in  [2.x.63] , but with an important difference:  There, the right hand side denoted thenegative* of the residual,  so we have to switch a sign.
* 
*  - A way to compute the matrix  [2.x.64]  if that is necessary in the  current iteration, along with possibly a preconditioner or other  data structures (e.g., a sparse decomposition via  SparseDirectUMFPACK if that's what we choose to use to solve a  linear system). This operation will generally be called the  "setup" operation.
* 
*  - A way to solve a linear system  [2.x.65]  with whatever  matrix  [2.x.66]  was last computed. This operation will generally  be called the "solve" operation.
* All of these operations need to be provided to KINSOL by [2.x.67] objects that take the appropriate set of arguments and that generallyreturn an integer that indicates success (a zero return value) orfailure (a nonzero return value). Specifically, the objects we willaccess are the [2.x.68]  [2.x.69]  [2.x.70]  and [2.x.71] member variables. (See the documentation of these variables for theirdetails.) In our implementation, we will use[lambda functions](https://en.cppreference.com/w/cpp/language/lambda)to implement these "callbacks" that in turn can call member functions;KINSOL will then call these callbacks whenever its internal algorithmsthink it is useful.
* 

* [1.x.33][1.x.34]
* 

* The majority of the code of this tutorial program is as in  [2.x.72] ,and we will not comment on it in much detail. There is really just oneaspect one has to pay some attention to, namely how to compute  [2.x.73] given a vector  [2.x.74]  on the one hand, and  [2.x.75]  given a vector  [2.x.76] separately. At first, this seems trivial: We just take the`assemble_system()` function and in the one case throw out all codethat deals with the matrix and in the other case with the right handside vector. There: Problem solved.
* But it isn't quite as simple. That's because the two are notindependent if we have nonzero Dirichlet boundary values, as we dohere. The linear system we want to solve contains both interior andboundary degrees of freedom, and when eliminating those degrees offreedom from those that are truly "free", using for example [2.x.77]  we need to know thematrix when assembling the right hand side vector.
* Of course, this completely contravenes the original intent: Tonot*
assemble the matrix if we can get away without it. We solve thisproblem as follows:
* 
*  - We set the starting guess for the solution vector,  [2.x.78] , to one  where boundary degrees of freedom already have their correct values.
* 
*  - This implies that all updates can have zero updates for these  degrees of freedom, and we can build both residual vectors  [2.x.79]   and Jacobian matrices  [2.x.80]  that corresponds to linear systems whose  solutions are zero in these vector components. For this special  case, the assembly of matrix and right hand side vectors is  independent, and can be broken into separate functions.
* There is an assumption here that whenever KINSOL asks for a linearsolver with the (approximation of the) Jacobian, that this will be forfor an update  [2.x.81]  (which has zero boundary values), a multipleof which will be added to the solution (which already has the rightboundary values).  This may not be true and if so, we might have torethink our approach. That said, it turns out that in practice this isexactly what KINSOL does when using a Newton method, and so ourapproach is successful.
* 

*  [1.x.35] [1.x.36]
*   [1.x.37]  [1.x.38]
* 

* 
*  This program starts out like most others with well known include files. Compared to the  [2.x.82]  program from which most of what we do here is copied, the only difference is the include of the header files from which we import the SparseDirectUMFPACK class and the actual interface to KINSOL:
* 

* 
*  

* 
* [1.x.39]
* 
*   [1.x.40]  [1.x.41]
* 

* 
*  Similarly, the main class of this program is essentially a copy of the one in  [2.x.83] . The class does, however, split the computation of the Jacobian (system) matrix (and its factorization using a direct solver) and residual into separate functions for the reasons outlined in the introduction. For the same reason, the class also has a pointer to a factorization of the Jacobian matrix that is reset every time we update the Jacobian matrix.   
*   (If you are wondering why the program uses a direct object for the Jacobian matrix but a pointer for the factorization: Every time KINSOL requests that the Jacobian be updated, we can simply write `jacobian_matrix=0;` to reset it to an empty matrix that we can then fill again. On the other hand, the SparseDirectUMFPACK class does not have any way to throw away its content or to replace it with a new factorization, and so we use a pointer: We just throw away the whole object and create a new one whenever we have a new Jacobian matrix to factor.)   
*   Finally, the class has a timer variable that we will use to assess how long the different parts of the program take so that we can assess whether KINSOL's tendency to not rebuild the matrix and its factorization makes sense. We will discuss this in the "Results" section below.
* 

* 
* [1.x.42]
* 
*   [1.x.43]  [1.x.44]
* 

* 
*  The classes implementing boundary values are a copy from  [2.x.84] :
* 

* 
* [1.x.45]
* 
*   [1.x.46]  [1.x.47]
* 

* 
*   [1.x.48]  [1.x.49]
* 

* 
*  The following few functions are also essentially copies of what  [2.x.85]  already does, and so there is little to discuss.
* 

* 
* [1.x.50]
* 
*   [1.x.51]  [1.x.52]
* 

* 
*  The following function is then responsible for assembling and factorizing the Jacobian matrix. The first half of the function is in essence the `assemble_system()` function of  [2.x.86] , except that it does not deal with also forming a right hand side vector (i.e., the residual) since we do not always have to do these operations at the same time.   
*   We put the whole assembly functionality into a code block enclosed by curly braces so that we can use a  [2.x.87]  variable to measure how much time is spent in this code block, excluding everything that happens in this function after the matching closing brace `}`.
* 

* 
* [1.x.53]
* 
*  The second half of the function then deals with factorizing the so-computed matrix. To do this, we first create a new SparseDirectUMFPACK object and by assigning it to the member variable `jacobian_matrix_factorization`, we also destroy whatever object that pointer previously pointed to (if any). Then we tell the object to factorize the Jacobian.     
*   As above, we enclose this block of code into curly braces and use a timer to assess how long this part of the program takes.     
*   (Strictly speaking, we don't actually need the matrix any more after we are done here, and could throw the matrix object away. A code intended to be memory efficient would do this, and only create the matrix object in this function, rather than as a member variable of the surrounding class. We omit this step here because using the same coding style as in previous tutorial programs breeds familiarity with the common style and helps make these tutorial programs easier to read.)
* 

* 
* [1.x.54]
* 
*   [1.x.55]  [1.x.56]
* 

* 
*  The second part of what `assemble_system()` used to do in  [2.x.88]  is computing the residual vector, i.e., the right hand side vector of the Newton linear systems. We have broken this out of the previous function, but the following function will be easy to understand if you understood what `assemble_system()` in  [2.x.89]  did. Importantly, however, we need to compute the residual not linearized around the current solution vector, but whatever we get from KINSOL. This is necessary for operations such as line search where we want to know what the residual  [2.x.90]  is for different values of  [2.x.91] ; KINSOL in those cases simply gives us the argument to the function  [2.x.92]  and we then compute the residual  [2.x.93]  at this point.   
*   The function prints the norm of the so-computed residual at the end as a way for us to follow along the progress of the program.
* 

* 
* [1.x.57]
* 
*   [1.x.58]  [1.x.59]
* 

* 
*  Next up is the function that implements the solution of a linear system with the Jacobian matrix. Since we have already factored the matrix when we built the matrix, solving a linear system comes down to applying the inverse matrix to the given right hand side vector: This is what the  [2.x.94]  function does that we use here. Following this, we have to make sure that we also address the values of hanging nodes in the solution vector, and this is done using  [2.x.95]    
*   The function takes an additional, but unused, argument `tolerance` that indicates how accurately we have to solve the linear system. The meaning of this argument is discussed in the introduction in the context of the "Eisenstat Walker trick", but since we are using a direct rather than an iterative solver, we are not using this opportunity to solve linear systems only inexactly.
* 

* 
* [1.x.60]
* 
*   [1.x.61]  [1.x.62]
* 

* 
*  The following three functions are again simply copies of the ones in  [2.x.96] :
* 

* 
* [1.x.63]
* 
*   [1.x.64]  [1.x.65]
* 

* 
*  The only function thatreally* is interesting in this program is the one that drives the overall algorithm of starting on a coarse mesh, doing some mesh refinement cycles, and on each mesh using KINSOL to find the solution of the nonlinear algebraic equation we obtain from discretization on this mesh. The `refine_mesh()` function above makes sure that the solution on one mesh is used as the starting guess on the next mesh. We also use a TimerOutput object to measure how much time every operation on each mesh costs, and reset the timer at the beginning of each cycle.   
*   As discussed in the introduction, it is not necessary to solve problems on coarse meshes particularly accurately since these will only solve as starting guesses for the next mesh. As a consequence, we will use a target tolerance of  [2.x.97]  for the  [2.x.98] th mesh refinement cycle.   
*   All of this is encoded in the first part of this function:
* 

* 
* [1.x.66]
* 
*  This is where the fun starts. At the top we create the KINSOL solver object and feed it with an object that encodes a number of additional specifics (of which we only change the nonlinear tolerance we want to reach; but you might want to look into what other members the  [2.x.99]  class has and play with them).
* 

* 
* [1.x.67]
* 
*  Then we have to describe the operations that were already mentioned in the introduction. In essence, we have to teach KINSOL how to (i) resize a vector to the correct size, (ii) compute the residual vector, (iii) compute the Jacobian matrix (during which we also compute its factorization), and (iv) solve a linear system with the Jacobian.           
*   All four of these operations are represented by member variables of the  [2.x.100]  class that are of type  [2.x.101]  i.e., they are objects to which we can assign a pointer to a function or, as we do here, a "lambda function" that takes the appropriate arguments and returns the appropriate information. By convention, KINSOL wants that functions doing something nontrivial return an integer where zero indicates success. It turns out that we can do all of this in just 25 lines of code.           
*   (If you're not familiar what "lambda functions" are, take a look at  [2.x.102]  or at the [wikipedia page](https://en.wikipedia.org/wiki/Anonymous_function) on the subject. The idea of lambda functions is that one wants to define a function with a certain set of arguments, but (i) not make it a named functions because, typically, the function is used in only one place and it seems unnecessary to give it a global name; and (ii) that the function has access to some of the variables that exist at the place where it is defined, including member variables. The syntax of lambda functions is awkward, but ultimately quite useful.)           
*   At the very end of the code block we then tell KINSOL to go to work and solve our problem. The member functions called from the 'residual', 'setup_jacobian', and 'solve_jacobian_system' functions will then print output to screen that allows us to follow along with the progress of the program.
* 

* 
* [1.x.68]
* 
*  The rest is then just house-keeping: Writing data to a file for visualizing, and showing a summary of the timing collected so that we can interpret how long each operation has taken, how often it was executed, etc:
* 

* 
* [1.x.69]
* [1.x.70][1.x.71]
* 

* When running the program, you get output that looks like this:
* [1.x.72]
* 
* The way this should be interpreted is most easily explained by looking atthe first few lines of the output on the first mesh:
* [1.x.73]
* What is happening is this:
* 
*  - In the first residual computation, KINSOL computes the residual to see whether  the desired tolerance has been reached. The answer is no, so it requests the  user program to compute the Jacobian matrix (and the function then also  factorizes the matrix via SparseDirectUMFPACK).
* 
*  - KINSOL then instructs us to solve a linear system of the form   [2.x.103]  with this matrix and the previously computed  residual vector.
* 
*  - It is then time to determine how far we want to go in this direction,  i.e., do line search. To this end, KINSOL requires us to compute the  residual vector  [2.x.104]  for different step lengths   [2.x.105] . For the first step above, it finds an acceptable  [2.x.106]   after two tries, the second time around it takes three tries.
* 
*  - Having found a suitable updated solution  [2.x.107] , the process is  repeated except now KINSOL is happy with the current Jacobian matrix  and does not instruct us to re-build the matrix and its factorization,  and instead asks us to solve a linear system with that same matrix.
* The program also writes the solution to a VTU file at the endof each mesh refinement cycle, and it looks as follows: [2.x.108] 
* 

* The key takeaway messages of this program are the following:
* 
*  - The solution is the same as the one we computed in  [2.x.109] , i.e., the  interfaces to %SUNDIALS' KINSOL package really did what they were supposed  to do. This should not come as a surprise, but the important point is that  we don't have to spend the time implementing the complex algorithms that  underlie advanced nonlinear solvers ourselves.
* 
*  - KINSOL is able to avoid all sorts of operations such as rebuilding the  Jacobian matrix when that is not actually necessary. Comparing the  number of linear solves in the output above with the number of times  we rebuild the Jacobian and compute its factorization should make it  clear that this leads to very substantial savings in terms of compute  times, without us having to implement the intricacies of algorithms  that determine when we need to rebuild this information.
* [1.x.74][1.x.75][1.x.76]
* 

* For all but the small problems we consider here, a sparse direct solverrequires too much time and memory
* 
*  -  we need an iterative solver likewe use in many other programs. The trade-off between constructing anexpensive preconditioner (say, a geometric or algebraic multigrid method)is different in the current case, however: Since we can re-use the samematrix for numerous linear solves, we can do the same for the preconditionerand putting more work into building a good preconditioner can more easilybe justified than if we used it only for a single linear solve as onedoes for many other situations.
* But iterative solvers also afford other opportunities. For example (and asdiscussed briefly in the introduction), we may not need to solve tovery high accuracy (small tolerances) in early nonlinear iterations as longas we are still far away from the actual solution. This was the basis of theEisenstat-Walker trick mentioned there.
* KINSOL provides the function that does the linear solution with a targettolerance that needs to be reached. We ignore it in the program abovebecause the direct solver we use does not need a tolerance and insteadsolves the linear system exactly (up to round-off, of course), but iterativesolvers could make use of this kind of information
* 
*  -  and, in fact, should.
* 

* [1.x.77][1.x.78] [2.x.110] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-78_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21]
* [1.x.22][1.x.23][1.x.24]
* 

* The Black-Scholes equation is a partial differential equation that falls a bitout of the ordinary scheme. It describes what the fair price of a "Europeancall" stock option is. Without going into too much detail, a stock "option" isa contract one can buy from a bank that allows me, but not requires me, to buya specific stock at a fixed price  [2.x.2]  at a fixed future time  [2.x.3]  in thefuture. The question one would then want to answer as a buyer of such anoption is "How much do I think such a contract is worth?", or as the seller"How much do I need to charge for this contract?", both as a function of thetime  [2.x.4]  before the contract is up at time  [2.x.5]  and as a function of the stockprice  [2.x.6] . Fischer Black and Myron Scholes derived a partial differentialequation for the fair price  [2.x.7]  for such options under the assumption thatstock prices exhibit random price fluctuations with a given level of"volatility" plus a background exponential price increase (which one can thinkof as the inflation rate that simply devalues all money over time). For theirwork, Black and Scholes received the Nobel Prize in Economic Sciences in 1997,making this the first tutorial program dealing with a problem for which someonehas gotten a Nobel Prize  [2.x.8] .
* The equation reads as follows:
* [1.x.25]
* where
* [1.x.26]
* 
* The way we should interpret this equation is that it is a time-dependent partialdifferential equation of one "space" variable [2.x.9]  as the price of the stock, and  [2.x.10]  is the price of the option at time [2.x.11]  if the stock price at that time were  [2.x.12] .
* [1.x.27][1.x.28]
* 

* There are a number of oddities in this equation that are worth discussing beforemoving on to its numerical solution. First, the "spatial" domain [2.x.13]  is unbounded, and thus  [2.x.14]  can be unbounded in value.This is because there may be a practical upper bound for stock prices, but not aconceptual one. The boundary conditions  [2.x.15]  as [2.x.16]  can then be interpreted as follows: What is the value ofan option that allows me to buy a stock at price  [2.x.17]  if the stock price (todayor at time  [2.x.18] ) is  [2.x.19] ? One would expect that it is  [2.x.20]  plussome adjustment for inflation, or, if we really truly consider huge values of [2.x.21] , we can neglect  [2.x.22]  and arrive at the statement that the boundary values atthe infinite boundary should be of the form  [2.x.23]  as stated above.
* In practice, for us to use a finite element method to solve this, we are goingto need to bound  [2.x.24] . Since this equation describes prices, and it doesn'tmake sense to talk about prices being negative, we will set the lower bound of [2.x.25]  to be 0. Then, for an upper bound, we will choose a very large number,one that  [2.x.26]  is not very likely to ever get to. We will call this  [2.x.27] .So,  [2.x.28] .
* Second, after truncating the domain, we need to ask what boundary values weshould pose at this now finite boundary. To take care of this, we use "put-call"parity  [2.x.29] . A "pull option" is one in which we areallowed, but not required, tosell* a stock at price  [2.x.30]  to someone at a futuretime  [2.x.31] . This says
* [1.x.29]
* where  [2.x.32]  is the value of the call option, and  [2.x.33]  is the value of theput option. Since we expect  [2.x.34]  as  [2.x.35] ,this says
* [1.x.30]
* and we can use this as a reasonable boundary condition at our finite point [2.x.36] .
* The second complication of the Block-Scholes equation is that we are given afinal condition, and not an initial condition. This is because we know what theoption is worth at time  [2.x.37] : If the stock price at  [2.x.38]  is  [2.x.39] , then we haveno incentive to use our option of buying a price  [2.x.40]  because we can buy that stockfor cheaper on the open market. So  [2.x.41]  for  [2.x.42] . On the other hand, ifat time  [2.x.43]  we have  [2.x.44] , then we can buy the stock at price  [2.x.45]  via the optionand immediately sell it again on the market for price  [2.x.46] , giving me a profit of [2.x.47] . In other words,  [2.x.48]  for  [2.x.49] . So, we only knowvalues for  [2.x.50]  at theend time* but not the initial time
* 
*  -  in fact, findingout what a fair price at the current time (conventionally taken to be  [2.x.51] ) iswhat solving these equations is all about.
* This means that this is not an equation that is posed going forward intime, but in fact goingbackward* in time. Thus it makes sense to solve thisproblem in reverse by making the change of variables  [2.x.52]  where now  [2.x.53] denotes the time before the strike time  [2.x.54] .
* With all of this, we finally end up with the following problem:
* [1.x.31]
* 
* Conceptually, this is an advection-diffusion-reaction problem for the variable [2.x.55] : There is both a second-order derivative diffusion term, a first-orderderivative advection term, and a zeroth-order reaction term.We can expect this problem to be a little bit forgiving in practice because forrealistic values of the coefficients, it is diffusive dominated. But, because ofthe advective terms in the problem, we will have to be careful with meshrefinement and time step choice. There is also the issue that the diffusion term is written in a non-conservative form and so integration by parts is not immediately obvious. This will be discussed in the next section.
* [1.x.32][1.x.33]
* 

* We will solve this problem using an IMEX method. In particular, we first discretizein time with the theta method and will later pick different values of theta forthe advective and diffusive terms.Let  [2.x.56]  approximate  [2.x.57] :
* [1.x.34]
* Here,  [2.x.58]  is the time step size. Given this timediscretization, we can proceed to discretize space by multiplying with testfunctions and then integrating by parts. Because there are some interestingdetails in this due to the advective and non-advective terms in this equation,this process will be explained in detail.
* So, we begin by multiplying by test functions,  [2.x.59] :
* [1.x.35]
* 
* 

* As usual, (1) becomes  [2.x.60]  and (4) becomes [2.x.61] , where [2.x.62] , and where we have taken theliberty of denoting by  [2.x.63]  not only the function  [2.x.64]  but also the vector ofnodal values after discretization.
* The interesting parts come from (2) and (3).
* 

* For (2), we have:
* [1.x.36]
* 
* There are two integrals here, that are more or less the same, with thedifferences being a slightly different coefficient in front of the integral,and a different time step for V. Therefore, we will outline this integral in thegeneral case, and account for the differences at the end. So, consider thegeneral integral, which we will solve using integration by parts:
* [1.x.37]
* 
* So, after adding in the constants and exchanging  [2.x.65]  for  [2.x.66]  whereapplicable, we arrive at the following for (2):
* [1.x.38]
* But, because the matrix  [2.x.67]  involves an advective term, we will choose [2.x.68]  there
* 
*  -  in other words, we use an explicit Euler method to treatadvection. Conversely, since the matrix  [2.x.69]  involves the diffusive term,we will choose  [2.x.70]  there
* 
*  -  i.e., we treat diffusion using the secondorder Crank-Nicolson method.
* So, we arrive at the following:
* [1.x.39]
* 
* Now, to handle (3). For this, we will again proceed by considering the generalcase like above.
* [1.x.40]
* 
* So, again after adding in the constants and exchanging  [2.x.71]  for  [2.x.72]  whereapplicable, we arrive at the following for (3):
* [1.x.41]
* Just as before, we will use  [2.x.73]  for the matrix  [2.x.74]  and [2.x.75]  for the matrix  [2.x.76] . So, we arrive at thefollowing for (3):
* [1.x.42]
* 
* Now, putting everything together, we obtain the following discrete form for theBlack-Scholes Equation:
* [1.x.43]
* So, altogether we have:
* [1.x.44]
* 
* As usual, we can write this with the unknown quantities on the left and theknown ones on the right. This leads to the following linear system that wouldhave to be solved in each time step:
* [1.x.45]
* 
* 

* 
* 

* [1.x.46][1.x.47]
* For this program, we will use the Method of Manufactured Solutions (MMS) to test that it is working correctly. This means that we will choose our solution to be  a certain function similar to  [2.x.77] . For our case, we will use:
* [1.x.48]
* This means that, using our PDE, we arrive at the following problem:
* [1.x.49]
* Where,  [2.x.78] .This set-up now has right hand sides for the equation itself and for theboundary conditions at  [2.x.79]  that we did not have before, along with "final"conditions (or, with  [2.x.80] -time "initial conditions") that do not match thereal situation. We will implement this in such a way in the code that it is easyto exchange
* 
*  -  the introduction of the changes above is just meant to enable the use of a manufactured solution.
* If the program is working correctly, then it should produce (**) as thesolution. This does mean that we need to modify our variational form somewhat toaccount for the non-zero right hand side.
* First, we define the following:
* [1.x.50]
* So, we arrive at the new equation:
* [1.x.51]
* 
* We then solve this equation as outlined above.
* 

*  [1.x.52] [1.x.53]
*   [1.x.54]  [1.x.55]
* 

* 
*  The program starts with the usual include files, all of which you should have seen before by now:
* 

* 
* [1.x.56]
* 
*  Then the usual placing of all content of this program into a namespace and the importation of the deal.II namespace into the one we will work in. We also define an identifier to allow for the MMS code to be run when  [2.x.81]  is defined. Otherwise, the program solves the original problem:
* 

* 
* [1.x.57]
* 
*   [1.x.58]  [1.x.59]
* 

* 
*  This section creates a class for the known solution when testing using the MMS. Here we are using  [2.x.82]  for the solution. We need to include the solution equation and the gradient for the H1 seminorm calculation.
* 

* 
* [1.x.60]
* 
*   [1.x.61]  [1.x.62]
* 

* 
*  In the following classes and functions, we implement the right hand side and boundary values that define this problem and for which we need function objects. The right hand side is chosen as discussed at the end of the introduction.   
*   First, we handle the initial condition.
* 

* 
* [1.x.63]
* 
*  Next, we handle the left boundary condition.
* 

* 
* [1.x.64]
* 
*  Then, we handle the right boundary condition.
* 

* 
* [1.x.65]
* 
*  Finally, we handle the right hand side.
* 

* 
* [1.x.66]
* 
*   [1.x.67]  [1.x.68]
* 

* 
*  The next piece is the declaration of the main class of this program. This is very similar to the  [2.x.83]  tutorial, with some modifications. New matrices had to be added to calculate the A and B matrices, as well as the  [2.x.84]  vector mentioned in the introduction. We also define the parameters used in the problem.
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  -  [2.x.85] : The imposed upper bound on the spatial domain. This is the maximum allowed stock price.
* 

* 
* 
*  -  [2.x.86] : The upper bound on the time domain. This is when the option expires.\n
* 

* 
* 
*  -  [2.x.87] : The volatility of the stock price.\n
* 

* 
* 
*  -  [2.x.88] : The risk free interest rate.\n
* 

* 
* 
*  -  [2.x.89] : The agreed upon price that the buyer will have the option of purchasing  the stocks at the expiration time.   
*   Some slight differences between this program and  [2.x.90]  are the creation of the  [2.x.91] , which is described in the introduction. We then also need to store the current time, the size of the time step, and the number of the current time step. Next, we will store the output into a  [2.x.92]  variable because we will be layering the solution at each time on top of one another to create the solution manifold. Then, we have a variable that stores the current cycle and number of cycles that we will run when calculating the solution. The cycle is one full solution calculation given a mesh. We refine the mesh once in between each cycle to exhibit the convergence properties of our program. Finally, we store the convergence data into a convergence table.   
*   As far as member functions are concerned, we have a function that calculates the convergence information for each cycle, called  [2.x.93] . This is just like what is done in  [2.x.94] .
* 

* 
* [1.x.69]
* 
*   [1.x.70]  [1.x.71]
* 

* 
*  Now, we get to the implementation of the main class. We will set the values for the various parameters used in the problem. These were chosen because they are fairly normal values for these parameters. Although the stock price has no upper bound in reality (it is in fact infinite), we impose an upper bound that is twice the strike price. This is a somewhat arbitrary choice to be twice the strike price, but it is large enough to see the interesting parts of the solution.
* 

* 
* [1.x.72]
* 
*   [1.x.73]  [1.x.74]
* 

* 
*  The next function sets up the DoFHandler object, computes the constraints, and sets the linear algebra objects to their correct sizes. We also compute the mass matrix here by calling a function from the library. We will compute the other 3 matrices next, because these need to be computed 'by hand'.   
*   Note, that the time step is initialized here because the maturity time was needed to compute the time step.
* 

* 
* [1.x.75]
* 
*  Below is the code to create the Laplace matrix with non-constant coefficients. This corresponds to the matrix D in the introduction. This non-constant coefficient is represented in the  [2.x.95]  variable.
* 

* 
* [1.x.76]
* 
*  Now we will create the A matrix. Below is the code to create the matrix A as discussed in the introduction. The non constant coefficient is again represented in  the  [2.x.96]  variable.
* 

* 
* [1.x.77]
* 
*  Finally we will create the matrix B. Below is the code to create the matrix B as discussed in the introduction. The non constant coefficient is again represented in the  [2.x.97]  variable.
* 

* 
* [1.x.78]
* 
*   [1.x.79]  [1.x.80]
* 

* 
*  The next function is the one that solves the actual linear system for a single time step. The only interesting thing here is that the matrices we have built are symmetric positive definite, so we can use the conjugate gradient method.
* 

* 
* [1.x.81]
* 
*   [1.x.82]  [1.x.83]
* 

* 
*  This is simply the function to stitch the solution pieces together. For this, we create a new layer at each time, and then add the solution vector for that timestep. The function then stitches this together with the old solutions using 'build_patches'.
* 

* 
* [1.x.84]
* 
*   [1.x.85]  [1.x.86]
* 

* 
*  It is somewhat unnecessary to have a function for the global refinement that we do. The reason for the function is to allow for the possibility of an adaptive refinement later.
* 

* 
* [1.x.87]
* 
*   [1.x.88]  [1.x.89]
* 

* 
*  This is where we calculate the convergence and error data to evaluate the effectiveness of the program. Here, we calculate the  [2.x.98] ,  [2.x.99]  and  [2.x.100]  norms.
* 

* 
* [1.x.90]
* 
*   [1.x.91]  [1.x.92]
* 

* 
*  This next part is building the convergence and error tables. By this, we need to set the settings for how to output the data that was calculated during  [2.x.101] . First, we will create the headings and set up the cells properly. During this, we will also prescribe the precision of our results. Then we will write the calculated errors based on the  [2.x.102] ,  [2.x.103] , and  [2.x.104]  norms to the console and to the error LaTeX file.
* 

* 
* [1.x.93]
* 
*  Next, we will make the convergence table. We will again write this to the console and to the convergence LaTeX file.
* 

* 
* [1.x.94]
* 
*   [1.x.95]  [1.x.96]
* 

* 
*  Now we get to the main driver of the program. This is where we do all the work of looping through the time steps and calculating the solution vector each time. Here at the top, we set the initial refinement value and then create a mesh. Then we refine this mesh once. Next, we set up the data_out_stack object to store our solution. Finally, we start a for loop to loop through the cycles. This lets us recalculate a solution for each successive mesh refinement. At the beginning of each iteration, we need to reset the time and time step number. We introduce an if statement to accomplish this because we don't want to do this on the first iteration.
* 

* 
* [1.x.97]
* 
*  Next, we run the main loop which runs until we exceed the maturity time. We first compute the right hand side of the equation, which is described in the introduction. Recall that it contains the term  [2.x.105] . We put these terms into the variable system_rhs, with the help of a temporary vector:
* 

* 
* [1.x.98]
* 
*  The second piece is to compute the contributions of the source terms. This corresponds to the term  [2.x.106] . The following code calls  [2.x.107]  to compute the vectors  [2.x.108] , where we set the time of the right hand side (source) function before we evaluate it. The result of this all ends up in the forcing_terms variable:
* 

* 
* [1.x.99]
* 
*  Next, we add the forcing terms to the ones that come from the time stepping, and also build the matrix  [2.x.109]  that we have to invert in each time step. The final piece of these operations is to eliminate hanging node constrained degrees of freedom from the linear system:
* 

* 
* [1.x.100]
* 
*  There is one more operation we need to do before we can solve it: boundary values. To this end, we create a boundary value object, set the proper time to the one of the current time step, and evaluate it as we have done many times before. The result is used to also set the correct boundary values in the linear system:
* 

* 
* [1.x.101]
* 
*  With this out of the way, all we have to do is solve the system, generate graphical data on the last cycle, and create the convergence table data.
* 

* 
* [1.x.102]
* 
*   [1.x.103]  [1.x.104]
* 

* 
*  Having made it this far, there is, again, nothing much to discuss for the main function of this program: it looks like all such functions since  [2.x.110] .
* 

* 
* [1.x.105]
* [1.x.106][1.x.107]
* 

* 
* Below is the output of the program:
* [1.x.108]
* 
* What is more interesting is the output of the convergence tables. They areoutputted into the console, as well into a LaTeX file. The convergence tablesare shown above. Here, you can see that the the solution has a convergence rateof  [2.x.111]  with respect to the  [2.x.112] -norm, and the solution has a convergence rateof  [2.x.113]  with respect to the  [2.x.114] -norm.
* 

* Below is the visualization of the solution.
*  [2.x.115] 
* 

* [1.x.109][1.x.110] [2.x.116] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-79_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] ,  [2.x.2] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32]
* [1.x.33][1.x.34][1.x.35]
* 

* Topology Optimization of Elastic Media is a technique used to optimize astructure that is bearing some load. Ideally, we would like to minimize themaximum stress placed on a structure by selecting a region  [2.x.3]  where material isplaced. In other words,[1.x.36][1.x.37][1.x.38]
* Here,  [2.x.4]  is the stresswithin the body that is caused by the external forces  [2.x.5] , where we have for simplicity assumedthat the material is linear-elastic and so  [2.x.6]  is the stress-strain tensor and [2.x.7]  is thesmall-deformation strain as a function of the displacement  [2.x.8] 
* 
*  -  see [2.x.9]  and  [2.x.10]  for more on linear elasticity. In the formulation above, [2.x.11]  is the maximal amount of material we are willing to provide tobuild the object. The last of the constraints is the partial differentialequation that relates stress  [2.x.12]  and forces  [2.x.13]  and is simply thesteady-state force balance.
* That said, the infinity norm above creates a problem: As a function of locationof material, this objective function is necessarily not differentiable, makingprospects of optimization rather bleak. So instead, a common approach intopology optimization is to find an approximate solution by optimizing a relatedproblem: We would like to minimize the strain energy. This is ameasure of the potential energy stored in an object due to its deformation, butalso works as a measure of total deformation over the structure.
* [1.x.39][1.x.40][1.x.41]
* The value of the objective function is calculated using a finite element method,where the solution is the displacements. This is placed inside of a nonlinearsolver loop that solves for a vector denoting placement of material.
* [1.x.42][1.x.43]
* 

* In actual practice, we can only build objects in which the material is eitherpresent, or not present, at any given point
* 
*  -  i.e., we would have an indicatorfunction  [2.x.14]  that describes the material-filledregion and that we want to find through the optimization problem. In this case,the optimization problem becomes combinatorial, and very expensive to solve.Instead, we use an approach called Solid Isotropic Material with Penalization,or SIMP.  [2.x.15] 
* The SIMP method is based on an idea of allowing the material to exist in alocation with a density  [2.x.16]  between 0 and 1. A density of 0 suggests thematerial is not there, and it is not a part of the structure, while a density of1 suggests the material is present. Values between 0 and 1 do not reflect adesign we can create in the real-world, but allow us to turn the combinatorialproblem into a continuous one. One then looks at density values  [2.x.17] ,with the constraint that  [2.x.18] . The minimum value [2.x.19] , typically chosen to be around  [2.x.20] , avoids the possibilityof having an infinite strain energy, but is small enough to provide accurateresults.
* The straightforward application of the effect of this "density" on theelasticity of the media would be to simply multiply the stiffness tensor  [2.x.21] of the medium by the given density, that is,  [2.x.22] . However, thisapproach often gives optimal solutions where density values are far from both 0and 1. As one wants to find a real-world solution, meaning the material eitheris present or it is not, a penalty is applied to these in-between values. Asimple and effective way to do this is to multiply the stiffness tensor by thedensity raised to some integer power penalty parameter  [2.x.23] , so that [2.x.24] . This makes density values farther away from 0 or 1 lesseffective. It has been shown that using  [2.x.25]  is sufficiently high to create'black-and-white' solutions: that is, one gets optimal solutions in whichmaterial is either present or not present at all points.
* More material should always provide a structure with a lower strain energy, and so theinequality constraint can be viewed as an equality where the total volume usedis the maximum volume.
* Using this density idea also allows us to reframe the volume constraint on theoptimization problem. Use of SIMP then turns the optimization problem into thefollowing:
* [1.x.44][1.x.45][1.x.46][1.x.47]The final constraint, the balance of linear momentum (which we will refer to as the elasticity equation), gives a method for finding  [2.x.26]  and  [2.x.27]  given the density  [2.x.28] .
* [1.x.48][1.x.49]
* The elasticity equation in the time independent limit reads[1.x.50]In the situations we will care about, we will assume that the medium has a linear material responseand in that case, we have that[1.x.51]In everything we will do below, we will always consider the displacementfield  [2.x.29]  as the only solution variable, rather than considering [2.x.30]  and  [2.x.31]  as solution variables (as is done in mixedformulations).
* Furthermore, we will make the assumption that the material is linear isotropic,in which case the stress-strain tensor can be expressed in terms of the Lam&eacute;parameters  [2.x.32]  such that
* [1.x.52]
* See  [2.x.33]  for how this transformation works.
* Integrating the objective function by parts gives[1.x.53]into which the linear elasticity equation can then be substituted, giving[1.x.54]Because we are assuming no body forces, this simplifies further to[1.x.55]which is the final form of the governing equation that we'll be consideringfrom this point forward.
* [1.x.56][1.x.57]
* 

* Typically, the solutions to topology optimization problems aremesh-dependent, and as such the problem is ill-posed. This is becausefractal structures are often formed as the mesh is refined further. As the mesh gainsresolution, the optimal solution typically gains smaller and smaller structures.There are a few competing workarounds to this issue, but the most popular forfirst order optimization is the sensitivity filter, while second orderoptimization methods tend to prefer use of a density filter.
* As the filters affect the gradient and Hessian of the strain energy (i.e., theobjective function), the choice of filter has an effect on the solution of theproblem. The density filter as part of a second order method works byintroducing an unfiltered density, which we refer to as  [2.x.34] , and thenrequiring that the density be a convolution of the unfiltered density:[1.x.58]Here,  [2.x.35]  is an operator so that  [2.x.36]  is some kind of average ofthe values of  [2.x.37]  in the area around  [2.x.38] 
* 
*  -  i.e., it is a smoothedversion of  [2.x.39] .
* This prevents checkerboarding; the radius of the filter allows the user todefine an effective minimal beam width for the optimal structures we seek tofind.
*  [2.x.40] 
* 

* These pictures show that what we find here is in accordance with what onetypically sees in other publications on the topic  [2.x.41] . Maybe more interestingly, theresult looks like a truss bridge (except that we apply the load at the top ofthe trusses, rather than the bottom as in real truss bridges, akin to a "decktruss" bridge), suggesting that the designs that have been used in bridge-building for centuries are indeed based on ideas we can now show to be optimalin some sense.
* 

* [1.x.203][1.x.204]
* 

* The results shown above took around 75 iterations to find, which is quiteconcerning given the expense in solving the large linear systems in eachiteration. Looking at the evolution, it does look as though the convergence hasmoments of happening quickly and moments of happening slowly. We believe this tobe due to both a lack of precision on when and how to decrease the boundaryvalues, as well as our choice of merit function being sub-optimal. In the future,a LOQO barrier update replacing the monotone reduction, as well as a MarkovFilter in place of a merit function will decrease the number of necessaryiterations significantly.
* The barrier decrease is most sensitive in the middle of the convergence, whichis problematic, as it seems like we need it to decrease quickly, then slowly,then quickly again.
* Secondly, the linear solver used here is just the sparse direct solver based onthe SparseDirectUMFPACK class. This works reasonably well on small problems,but the formulation of the optimization problem detailed above has quite a largenumber of variables and so the linear problem is not only large but also has alot of nonzero entries in many rows, even on meshes that overall are stillrelatively coarse. As a consequence, the solver time dominates thecomputations, and more sophisticated approaches at solving the linear systemare necessary.
* 

* [1.x.205][1.x.206] [2.x.42] 
* [0.x.10]

include/deal.II-translator/A-tutorial/step-8_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17]
* [1.x.18][1.x.19][1.x.20]
* 

* 
* In real life, most partial differential equations are really systemsof equations. Accordingly, the solutions are usuallyvector-valued. The deal.II library supports such problems (see theextensive documentation in the  [2.x.2]  module), and we will showthat that is mostly rather simple. The only more complicated problemsare in assembling matrix and right hand side, but these are easilyunderstood as well.
*  [2.x.3] 
* In this tutorial program we will want to solve the[1.x.21].They are an extension to Laplace's equation with a vector-valued solution thatdescribes the displacement in each space direction of a rigid bodywhich is subject to a force. Of course, the force is alsovector-valued, meaning that in each point it has a direction and anabsolute value.
* One can write the elasticity equations in a number of ways. The one that showsthe symmetry with the Laplace equation in the most obvious way is to write itas[1.x.22]where  [2.x.4]  is the vector-valued displacement at each point, [2.x.5]  the force, and  [2.x.6]  is a rank-4 tensor (i.e., it has fourindices) that encodes the stress-strain relationship
* 
*  -  in essence,it represents the[1.x.23] inHookes law that relates the displacement to the forces.  [2.x.7]  will, in manycases, depend on  [2.x.8]  if the body whose deformation we want tosimulate is composed of different materials.
* While the form of the equations above is correct, it is not the waythey are usually derived. In truth, the gradient of the displacement [2.x.9]  (a matrix) has no physical meaning whereas itssymmetrized version,[1.x.24]does and is typically called the "strain". (Here and in the following, [2.x.10] . We will also use the[1.x.25] that whenever the same index appears twice in an equation,summation over this index is implied; we will, however, not distinguishbetween upper and lower indices.)With this definition of the strain, the elasticity equationsthen read as[1.x.26]which you can think of as the more natural generalization of the Laplaceequation to vector-valued problems. (The form shown first is equivalent tothis form because the tensor  [2.x.11]  has certain symmetries, namely that [2.x.12] , and consequently  [2.x.13] .)
* One can of course alternatively write these equations in component form:[1.x.27]
* In many cases, one knows that the material under consideration isisotropic, in which case by introduction of the two coefficients [2.x.14]  and  [2.x.15]  the coefficient tensor reduces to[1.x.28]
* The elastic equations can then be rewritten in much simpler a form:[1.x.29]and the respective bilinear form is then[1.x.30]or also writing the first term a sum over components:[1.x.31]
*  [2.x.16]  As written, the equations above are generally considered to be the rightdescription for the displacement of three-dimensional objects if thedisplacement is small and we can assume that [1.x.32] is valid. Inthat case, the indices  [2.x.17]  above all run over the set  [2.x.18]  (or,in the C++ source, over  [2.x.19] ). However, as is, the program runs in 2d,and while the equations above also make mathematical sense in that case, theywould only describe a truly two-dimensional solid. In particular, they are notthe appropriate description of an  [2.x.20]  cross-section of a body infinite inthe  [2.x.21]  direction; this is in contrast to many other two-dimensional equationsthat can be obtained by assuming that the body has infinite extent in [2.x.22] -direction and that the solution function does not depend on the  [2.x.23] coordinate. On the other hand, there are equations for two-dimensional modelsof elasticity; see for example the Wikipedia article on [1.x.33], [1.x.34] and [1.x.35].
* But let's get back to the original problem.How do we assemble the matrix for such an equation? A very long answerwith a number of different alternatives is given in the documentation of the [2.x.24]  module. Historically, the solution shown below was the onlyone available in the early years of the library. It turns out to also be thefastest. On the other hand, if a few per cent of compute time do not matter,there are simpler and probably more intuitive ways to assemble the linearsystem than the one discussed below but that weren't available until severalyears after this tutorial program was first written; if you are interested inthem, take a look at the  [2.x.25]  module.
* Let us go back to the question of how to assemble the linear system. The firstthing we need is some knowledge about how the shape functions work in the caseof vector-valued finite elements. Basically, this comes down to the following:let  [2.x.26]  be the number of shape functions for the scalar finite element ofwhich we build the vector element (for example, we will use bilinear functionsfor each component of the vector-valued finite element, so the scalar finiteelement is the  [2.x.27]  element which we have used in previousexamples already, and  [2.x.28]  in two space dimensions). Further, let  [2.x.29]  be thenumber of shape functions for the vector element; in two space dimensions, weneed  [2.x.30]  shape functions for each component of the vector, so  [2.x.31] . Then,the  [2.x.32] th shape function of the vector element has the form[1.x.36]where  [2.x.33]  is the  [2.x.34] th unit vector,  [2.x.35]  is the function that tellsus which component of  [2.x.36]  is the one that is nonzero (foreach vector shape function, only one component is nonzero, and all others arezero).  [2.x.37]  describes the space dependence of the shapefunction, which is taken to be the  [2.x.38] -th shape function of the scalarelement. Of course, while  [2.x.39]  is in the range  [2.x.40] , the functions [2.x.41]  and  [2.x.42]  have the ranges  [2.x.43]  (in 2D) and  [2.x.44] ,respectively.
* For example (though this sequence of shape functions is notguaranteed, and you should not rely on it),the following layout could be used by the library:[1.x.37]
* where here[1.x.38][1.x.39]
* In all but very rare cases, you will not need to know which shape function [2.x.45]  of the scalar element belongs to a shape function  [2.x.46] of the vector element. Let us therefore define[1.x.40]by which we can write the vector shape function as[1.x.41]You can now safely forget about the function  [2.x.47] , at least for the restof this example program.
* Now using this vector shape functions, we can write the discrete finiteelement solution as[1.x.42]with scalar coefficients  [2.x.48] . If we define an analog function  [2.x.49]  astest function, we can write the discrete problem as follows: Find coefficients [2.x.50]  such that[1.x.43]
* If we insert the definition of the bilinear form and the representation of [2.x.51]  and  [2.x.52]  into this formula:[1.x.44]
* We note that here and in the following, the indices  [2.x.53]  run over spatialdirections, i.e.  [2.x.54] , and that indices  [2.x.55]  run over degreesof freedom.
* The local stiffness matrix on cell  [2.x.56]  therefore has the following entries:[1.x.45]where  [2.x.57]  now are local degrees of freedom and therefore  [2.x.58] .In these formulas, we always take some component of the vector shape functions [2.x.59] , which are of course given as follows (see their definition):[1.x.46]with the Kronecker symbol  [2.x.60] . Due to this, we can delete some ofthe sums over  [2.x.61]  and  [2.x.62] :[1.x.47]
* 
* Likewise, the contribution of cell  [2.x.63]  to the right hand side vector is[1.x.48]
* 
* This is the form in which we will implement the local stiffness matrix andright hand side vectors.
* As a final note: in the  [2.x.64]  example program, we willrevisit the elastic problem laid out here, and will show how to solve it in%parallel on a cluster of computers. The resulting program will thus be able tosolve this problem to significantly higher accuracy, and more efficiently ifthis is required. In addition, in  [2.x.65] ,  [2.x.66] " [2.x.67] ", as well as a few other of the later tutorial programs, we willrevisit some vector-valued problems and show a few techniques that may make itsimpler to actually go through all the stuff shown above, with [2.x.68]  etc.
* 

*  [1.x.49] [1.x.50]
*   [1.x.51]  [1.x.52]
* 

* 
*  As usual, the first few include files are already known, so we will not comment on them further.
* 

* 
* [1.x.53]
* 
*  In this example, we need vector-valued finite elements. The support for these can be found in the following include file:
* 

* 
* [1.x.54]
* 
*  We will compose the vector-valued finite elements from regular Q1 elements which can be found here, as usual:
* 

* 
* [1.x.55]
* 
*  This again is C++:
* 

* 
* [1.x.56]
* 
*  The last step is as in previous programs. In particular, just like in  [2.x.69] , we pack everything that's specific to this program into a namespace of its own.
* 

* 
* [1.x.57]
* 
*   [1.x.58]  [1.x.59]
* 

* 
*  The main class is, except for its name, almost unchanged with respect to the  [2.x.70]  example.   
*   The only change is the use of a different class for the  [2.x.71]  variable: Instead of a concrete finite element class such as FE_Q, we now use a more generic one, FESystem. In fact, FESystem is not really a finite element itself in that it does not implement shape functions of its own. Rather, it is a class that can be used to stack several other elements together to form one vector-valued finite element. In our case, we will compose the vector-valued element of  [2.x.72]  objects, as shown below in the constructor of this class.
* 

* 
* [1.x.60]
* 
*   [1.x.61]  [1.x.62]
* 

* 
*  Before going over to the implementation of the main class, we declare and define the function which describes the right hand side. This time, the right hand side is vector-valued, as is the solution, so we will describe the changes required for this in some more detail.   
*   To prevent cases where the return vector has not previously been set to the right size we test for this case and otherwise throw an exception at the beginning of the function. Note that enforcing that output arguments already have the correct size is a convention in deal.II, and enforced almost everywhere. The reason is that we would otherwise have to check at the beginning of the function and possibly change the size of the output vector. This is expensive, and would almost always be unnecessary (the first call to the function would set the vector to the right size, and subsequent calls would only have to do redundant checks). In addition, checking and possibly resizing the vector is an operation that can not be removed if we can't rely on the assumption that the vector already has the correct size; this is in contract to the Assert call that is completely removed if the program is compiled in optimized mode.   
*   Likewise, if by some accident someone tried to compile and run the program in only one space dimension (in which the elastic equations do not make much sense since they reduce to the ordinary Laplace equation), we terminate the program in the second assertion. The program will work just fine in 3d, however.
* 

* 
* [1.x.63]
* 
*  The rest of the function implements computing force values. We will use a constant (unit) force in x-direction located in two little circles (or spheres, in 3d) around points (0.5,0) and (-0.5,0), and y-force in an area around the origin; in 3d, the z-component of these centers is zero as well.     
*   For this, let us first define two objects that denote the centers of these areas. Note that upon construction of the Point objects, all components are set to zero.
* 

* 
* [1.x.64]
* 
*  If  [2.x.73]  is in a circle (sphere) of radius 0.2 around one of these points, then set the force in x-direction to one, otherwise to zero:
* 

* 
* [1.x.65]
* 
*  Likewise, if  [2.x.74]  is in the vicinity of the origin, then set the y-force to one, otherwise to zero:
* 

* 
* [1.x.66]
* 
*   [1.x.67]  [1.x.68]
* 

* 
*   [1.x.69]  [1.x.70]
* 

* 
*  Following is the constructor of the main class. As said before, we would like to construct a vector-valued finite element that is composed of several scalar finite elements (i.e., we want to build the vector-valued element so that each of its vector components consists of the shape functions of a scalar element). Of course, the number of scalar finite elements we would like to stack together equals the number of components the solution function has, which is  [2.x.75]  since we consider displacement in each space direction. The FESystem class can handle this: we pass it the finite element of which we would like to compose the system of, and how often it shall be repeated:
* 

* 
*  

* 
* [1.x.71]
* 
*  In fact, the FESystem class has several more constructors which can perform more complex operations than just stacking together several scalar finite elements of the same type into one; we will get to know these possibilities in later examples.
* 

* 
*  
*  
*  [1.x.72]  [1.x.73]
* 

* 
*  Setting up the system of equations is identical to the function used in the  [2.x.76]  example. The DoFHandler class and all other classes used here are fully aware that the finite element we want to use is vector-valued, and take care of the vector-valuedness of the finite element themselves. (In fact, they do not, but this does not need to bother you: since they only need to know how many degrees of freedom there are per vertex, line and cell, and they do not ask what they represent, i.e. whether the finite element under consideration is vector-valued or whether it is, for example, a scalar Hermite element with several degrees of freedom on each vertex).
* 

* 
* [1.x.74]
* 
*   [1.x.75]  [1.x.76]
* 

* 
*  The big changes in this program are in the creation of matrix and right hand side, since they are problem-dependent. We will go through that process  [2.x.77] by-step, since it is a bit more complicated than in previous examples.   
*   The first parts of this function are the same as before, however: setting up a suitable quadrature formula, initializing an FEValues object for the (vector-valued) finite element we use as well as the quadrature object, and declaring a number of auxiliary arrays. In addition, we declare the ever same two abbreviations:  [2.x.78]  and  [2.x.79] . The number of degrees of freedom per cell we now obviously ask from the composed finite element rather than from the underlying scalar Q1 element. Here, it is  [2.x.80]  times the number of degrees of freedom per cell of the Q1 element, though this is not explicit knowledge we need to care about:
* 

* 
* [1.x.77]
* 
*  As was shown in previous examples as well, we need a place where to store the values of the coefficients at all the quadrature points on a cell. In the present situation, we have two coefficients, lambda and mu.
* 

* 
* [1.x.78]
* 
*  Well, we could as well have omitted the above two arrays since we will use constant coefficients for both lambda and mu, which can be declared like this. They both represent functions always returning the constant value 1.0. Although we could omit the respective factors in the assemblage of the matrix, we use them here for purpose of demonstration.
* 

* 
* [1.x.79]
* 
*  Like the two constant functions above, we will call the function right_hand_side just once per cell to make things simpler.
* 

* 
* [1.x.80]
* 
*  Now we can begin with the loop over all cells:
* 

* 
* [1.x.81]
* 
*  Next we get the values of the coefficients at the quadrature points. Likewise for the right hand side:
* 

* 
* [1.x.82]
* 
*  Then assemble the entries of the local stiffness matrix and right hand side vector. This follows almost one-to-one the pattern described in the introduction of this example.  One of the few comments in place is that we can compute the number  [2.x.81] , i.e. the index of the only nonzero vector component of shape function  [2.x.82]  using the  [2.x.83]  function call below.         
*   (By accessing the  [2.x.84]  variable of the return value of the  [2.x.85]  function, you might already have guessed that there is more in it. In fact, the function returns a  [2.x.86]  int, unsigned int [2.x.87]  of which the first element is  [2.x.88]  and the second is the value  [2.x.89]  also noted in the introduction, i.e.  the index of this shape function within all the shape functions that are nonzero in this component, i.e.  [2.x.90]  in the diction of the introduction. This is not a number that we are usually interested in, however.)         
*   With this knowledge, we can assemble the local matrix contributions:
* 

* 
* [1.x.83]
* 
*  The first term is  [2.x.91] . Note that  [2.x.92]  returns the gradient of the only nonzero component of the i-th shape function at quadrature point q_point. The component  [2.x.93]  of the gradient, which is the derivative of this only nonzero vector component of the i-th shape function with respect to the comp(i)th coordinate is accessed by the appended brackets.
* 

* 
* [1.x.84]
* 
*  The second term is  [2.x.94] . We need not access a specific component of the gradient, since we only have to compute the scalar product of the two gradients, of which an overloaded version of <tt>operator*</tt> takes care, as in previous examples.                         
*   Note that by using the <tt>?:</tt> operator, we only do this if <tt>component_i</tt> equals <tt>component_j</tt>, otherwise a zero is added (which will be optimized away by the compiler).
* 

* 
* [1.x.85]
* 
*  Assembling the right hand side is also just as discussed in the introduction:
* 

* 
* [1.x.86]
* 
*  The transfer from local degrees of freedom into the global matrix and right hand side vector does not depend on the equation under consideration, and is thus the same as in all previous examples.
* 

* 
* [1.x.87]
* 
*   [1.x.88]  [1.x.89]
* 

* 
*  The solver does not care about where the system of equations comes, as long as it stays positive definite and symmetric (which are the requirements for the use of the CG solver), which the system indeed is. Therefore, we need not change anything.
* 

* 
* [1.x.90]
* 
*   [1.x.91]  [1.x.92]
* 

* 
*  The function that does the refinement of the grid is the same as in the  [2.x.95]  example. The quadrature formula is adapted to the linear elements again. Note that the error estimator by default adds up the estimated obtained from all components of the finite element solution, i.e., it uses the displacement in all directions with the same weight. If we would like the grid to be adapted to the x-displacement only, we could pass the function an additional parameter which tells it to do so and do not consider the displacements in all other directions for the error indicators. However, for the current problem, it seems appropriate to consider all displacement components with equal weight.
* 

* 
* [1.x.93]
* 
*   [1.x.94]  [1.x.95]
* 

* 
*  The output happens mostly as has been shown in previous examples already. The only difference is that the solution function is vector valued. The DataOut class takes care of this automatically, but we have to give each component of the solution vector a different name.   
*   To do this, the  [2.x.96]  function wants a vector of strings. Since the number of components is the same as the number of dimensions we are working in, we use the  [2.x.97]  statement below.   
*   We note that some graphics programs have restriction on what characters are allowed in the names of variables. deal.II therefore supports only the minimal subset of these characters that is supported by all programs. Basically, these are letters, numbers, underscores, and some other characters, but in particular no whitespace and minus/hyphen. The library will throw an exception otherwise, at least if in debug mode.   
*   After listing the 1d, 2d, and 3d case, it is good style to let the program die if we run upon a case which we did not consider. Remember that the Assert macro generates an exception if the condition in the first parameter is not satisfied. Of course, the condition  [2.x.98]  can never be satisfied, so the program will always abort whenever it gets to the default statement:
* 

* 
* [1.x.96]
* 
*  After setting up the names for the different components of the solution vector, we can add the solution vector to the list of data vectors scheduled for output. Note that the following function takes a vector of strings as second argument, whereas the one which we have used in all previous examples accepted a string there. (In fact, the function we had used before would convert the single string into a vector with only one element and forwards that to the other function.)
* 

* 
* [1.x.97]
* 
*   [1.x.98]  [1.x.99]
* 

* 
*  The  [2.x.99]  function does the same things as in  [2.x.100] , for example. This time, we use the square [-1,1]^d as domain, and we refine it globally four times before starting the first iteration.   
*   The reason for refining is a bit accidental: we use the QGauss quadrature formula with two points in each direction for integration of the right hand side; that means that there are four quadrature points on each cell (in 2D). If we only refine the initial grid once globally, then there will be only four quadrature points in each direction on the domain. However, the right hand side function was chosen to be rather localized and in that case, by pure chance, it happens that all quadrature points lie at points where the right hand side function is zero (in mathematical terms, the quadrature points happen to be at points outside the [1.x.100] of the right hand side function). The right hand side vector computed with quadrature will then contain only zeroes (even though it would of course be nonzero if we had computed the right hand side vector exactly using the integral) and the solution of the system of equations is the zero vector, i.e., a finite element function that is zero everywhere. In a sense, we should not be surprised that this is happening since we have chosen an initial grid that is totally unsuitable for the problem at hand.   
*   The unfortunate thing is that if the discrete solution is constant, then the error indicators computed by the KellyErrorEstimator class are zero for each cell as well, and the call to  [2.x.101]  will not flag any cells for refinement (why should it if the indicated error is zero for each cell?). The grid in the next iteration will therefore consist of four cells only as well, and the same problem occurs again.   
*   The conclusion needs to be: while of course we will not choose the initial grid to be well-suited for the accurate solution of the problem, we must at least choose it such that it has the chance to capture the important features of the solution. In this case, it needs to be able to see the right hand side. Thus, we refine globally four times. (Any larger number of global refinement steps would of course also work.)
* 

* 
* [1.x.101]
* 
*   [1.x.102]  [1.x.103]
* 

* 
*  After closing the  [2.x.102]  namespace in the last line above, the following is the main function of the program and is again exactly like in  [2.x.103]  (apart from the changed class names, of course).
* 

* 
* [1.x.104]
* [1.x.105][1.x.106]
* 

* 
* There is not much to be said about the results of this program, other thanthat they look nice. All images were made using VisIt from theoutput files that the program wrote to disk. The first two pictures showthe  [2.x.104] - and  [2.x.105] -displacements as scalar components:
*  [2.x.106] 
* 

* You can clearly see the sources of  [2.x.107] -displacement around  [2.x.108]  and [2.x.109] , and of  [2.x.110] -displacement at the origin.
* What one frequently would like to do is to show the displacement as a vectorfield, i.e., vectors that for each point illustrate the direction and magnitudeof displacement. Unfortunately, that's a bit more involved. To understand whythis is so, remember that we have just defined our finite element as acollection of two  components (in  [2.x.111]  dimensions). Nowhere havewe said that this is not just a pressure and a concentration (two scalarquantities) but that the two components actually are the parts of avector-valued quantity, namely the displacement. Absent this knowledge, theDataOut class assumes that all individual variables we print are separatescalars, and VisIt and Paraview then faithfully assume that this is indeed what it is. Inother words, once we have written the data as scalars, there is nothing inthese programs that allows us to paste these two scalar fields back together as avector field. Where we would have to attack this problem is at the root,namely in  [2.x.112] . We won't do so here butinstead refer the reader to the  [2.x.113]  program where we show how to do thisfor a more general situation. That said, we couldn't help generating the dataanyway that would show how this would look if implemented as discussed in [2.x.114] . The vector field then looks like this (VisIt and Paraviewrandomly select a fewhundred vertices from which to draw the vectors; drawing them from eachindividual vertex would make the picture unreadable):
*  [2.x.115] 
* 

* We note that one may have intuitively expected thesolution to be symmetric about the  [2.x.116] - and  [2.x.117] -axes since the  [2.x.118] - and [2.x.119] -forces are symmetric with respect to these axes. However, the forceconsidered as a vector is not symmetric and consequently neither isthe solution.
* 

* [1.x.107][1.x.108] [2.x.120] 
* [0.x.1]

include/deal.II-translator/A-tutorial/step-9_0.txt
[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18]
* [1.x.19][1.x.20][1.x.21]
* 

* 
* In this example, our aims are the following: [2.x.2]    [2.x.3] solve the advection equation  [2.x.4] ;   [2.x.5] show how we can use multiple threads to get results quicker if we have a    multi-processor machine;   [2.x.6] develop a simple refinement criterion. [2.x.7] While the second aim is difficult to describe in general terms withoutreference to the code, we will discuss the other two aims in thefollowing. The use of multiple threads will then be detailed at therelevant places within the program. We will, however, follow thegeneral discussion of the WorkStream approach detailed in the [2.x.8]  "Parallel computing with multiple processors accessing shared memory"documentation module.
* 

* [1.x.22][1.x.23]
* 

* In the present example program, we want to numerically approximate thesolution of the advection equation[1.x.24]where  [2.x.9]  is a vector field that describes the advection direction andspeed (which may be dependent on the space variables if [2.x.10] ),  [2.x.11]  is a sourcefunction, and  [2.x.12]  is the solution. The physical process that thisequation describes is that of a given flow field  [2.x.13] , with whichanother substance is transported, the density or concentration ofwhich is given by  [2.x.14] . The equation does not contain diffusion of thissecond species within its carrier substance, but there are sourceterms.
* It is obvious that at the inflow, the above equation needs to beaugmented by boundary conditions:[1.x.25]where  [2.x.15]  describes the inflow portion of the boundary and isformally defined by[1.x.26]and  [2.x.16]  being the outward normal to the domain at point [2.x.17] . This definition is quite intuitive, sinceas  [2.x.18]  points outward, the scalar product with  [2.x.19]  can onlybe negative if the transport direction  [2.x.20]  points inward, i.e. atthe inflow boundary. The mathematical theory states that we must notpose any boundary condition on the outflow part of the boundary.
* Unfortunately, the equation stated above cannot be solved in a stable way usingthe standard finite element method. The problem is thatsolutions to this equation possess insufficient regularityperpendicular to the transport direction: while they are smooth alongthe streamlines defined by the "wind field" [2.x.21] , they may be discontinuous perpendicular to thisdirection. This is easy to understand: what the equation  [2.x.22]  means is in essence that the [1.x.27]. But the equation has no implicationsfor the derivatives in the perpendicular direction, and consequentlyif  [2.x.23]  is discontinuous at a point on the inflow boundary, then thisdiscontinuity will simply be transported along the streamline of thewind field that starts at this boundary point.These discontinuities lead to numerical instabilities thatmake a stable solution by a standard continuous finite element discretizationimpossible.
* A standard approach to address this difficulty is the  [2.x.24] "streamline-upwindPetrov-Galerkin" [2.x.25]  (SUPG) method, sometimes also called thestreamline diffusion method. A good explanation of the method can befound in  [2.x.26]  . Formally, this method replaces the stepin which we derive the the weak form of the differential equation fromthe strong form: Instead of multiplying the equation by a testfunction  [2.x.27]  and integrating over the domain, we instead multiplyby  [2.x.28] , where  [2.x.29]  is aparameter that is chosen in the range of the (local) mesh width  [2.x.30] ;good results are usually obtained by setting  [2.x.31] .(Why this is called "streamline diffusion" will be explained below;for the moment, let us simply take for granted that this is how wederive a stable discrete formulation.)The value for  [2.x.32]  here is small enoughthat we do not introduce excessive diffusion, but large enough that theresulting problem is well-posed.
* Using the test functions as defined above, an initial weak form of theproblem would ask for finding a function  [2.x.33]  so that for all testfunctions  [2.x.34]  we have[1.x.28]However, we would like to include inflow boundary conditions  [2.x.35] weakly into this problem, and this can be done by requiring that inaddition to the equation above we also have[1.x.29]for all test functions  [2.x.36]  that live on the boundary and that arefrom a suitable test space. It turns out that a suitable space of testfunctions happens to be  [2.x.37]  times the traces ofthe functions  [2.x.38]  in the test space we already use for thedifferential equation in the domain. Thus, we require that for alltest functions  [2.x.39]  we have[1.x.30]Without attempting a justification (see again the literature on the finiteelement method in general, and the streamline diffusion method inparticular), we can combine the equations for the differentialequation and the boundary values in the followingweak formulation ofour stabilized problem: find a discrete function  [2.x.40]  such thatfor all discrete test functions  [2.x.41]  there holds[1.x.31]
* 

* One would think that this leads to a system matrixto be inverted of the form[1.x.32]with basis functions  [2.x.42] .  However, this is apitfall that happens to every numerical analyst at least once(including the author): we have here expanded the solution [2.x.43] , but if we do so, we will have to solve theproblem[1.x.33]where  [2.x.44]  is the vector of expansion coefficients, i.e., we have tosolve the transpose problem of what we might have expected naively.
* This is a point we made in the introduction of  [2.x.45] . There, we argued thatto avoid this very kind of problem, one should get in the habit of alwaysmultiplying with test functions [1.x.34] instead of from the rightto obtain the correct matrix right away. In order to obtain the formof the linear system that we need, it is therefore best to rewrite the weakformulation to[1.x.35]and then to obtain[1.x.36]as system matrix. We will assemble this matrix in the program.
* 

* [1.x.37][1.x.38]
* 

* Looking at the bilinear form mentioned above, we see that the discretesolution has to satisfy an equation of which the left hand side inweak form has a domain term of the kind[1.x.39]or if we split this up, the form[1.x.40]If we wanted to see what strong form of the equation that wouldcorrespond to, we need to integrate the second term. This yields thefollowing formulation, where for simplicity we'll ignore boundaryterms for now:[1.x.41]Let us assume for a moment that the wind field  [2.x.46]  isdivergence-free, i.e., that  [2.x.47] . Then applyingthe product rule to the derivative of the term in square brackets onthe right and using the divergence-freeness will give us the following:[1.x.42]That means that the strong form of the equation would be of the sort[1.x.43]What is important to recognize now is that  [2.x.48]  is the [2.x.49] derivative in direction  [2.x.50]  [2.x.51] . So, if we denote this by [2.x.52]  (in the same way aswe often write  [2.x.53]  forthe derivative in normal direction at the boundary), then the strongform of the equation is[1.x.44]In other words, the unusual choice of test function is equivalent tothe addition of term to the strong form that corresponds to a secondorder (i.e., diffusion) differential operator in the direction of the windfield  [2.x.54] , i.e., in "streamline direction". A fuller account wouldalso have to explore the effect of the test function on boundaryvalues and why it is necessary to also use the same test function forthe right hand side, but the discussion above might make clear wherethe name "streamline diffusion" for the method originates from.
* 

* [1.x.45][1.x.46]
* 

* A "Galerkin method" is one where one obtains the weak formulation bymultiplying the equation by a test function  [2.x.55]  (and then integratingover  [2.x.56] ) where the functions  [2.x.57]  are from the same space as thesolution  [2.x.58]  (though possibly with different boundary values). Butthis is not strictly necessary: One could also imagine choosing thetest functions from a different set of functions, as long as thatdifferent set has "as many dimensions" as the original set offunctions so that we end up with as many independent equations asthere are degrees of freedom (where all of this needs to beappropriately defined in the infinite-dimensional case). Methods thatmake use of this possibility (i.e., choose the set of test functionsdifferently than the set of solutions) are called "Petrov-Galerkin"methods. In the current case, the test functions all have the form [2.x.59]  where  [2.x.60]  is from the set of solutions.
* 

* [1.x.47][1.x.48]
* 

* [Upwind methods](https://en.wikipedia.org/wiki/Upwind_scheme) have along history in the derivation of stabilized schemes for advectionequations. Generally, the idea is that instead of looking at afunction "here", we look at it a small distance further "upstream" or "upwind",i.e., where the information "here" originally came from. This mightsuggest not considering  [2.x.61] , butsomething like  [2.x.62] . Or, equivalently uponintegration, we could evaluate  [2.x.63]  and instead consider  [2.x.64] a bit downstream:  [2.x.65] . This would be cumbersomefor a variety of reasons: First, we would have to define what  [2.x.66] should be if  [2.x.67]  happens to be outside [2.x.68] ; second, computing integrals numerically would be much moreawkward since we no longer evaluate  [2.x.69]  and  [2.x.70]  at the same quadraturepoints. But since we assume that  [2.x.71]  is small, we can do a Taylorexpansion:[1.x.49]This form for the test function should by now look familiar.
* 

* [1.x.50][1.x.51]
* 

* As the resulting matrix is no longer symmetric positive definite, we cannotuse the usual Conjugate Gradient method (implemented in theSolverCG class) to solve the system. Instead, we use the GMRES (GeneralizedMinimum RESidual) method (implemented in SolverGMRES) that is suitablefor problems of the kind we have here.
* 

* [1.x.52][1.x.53]
* 

* For the problem which we will solve in this tutorial program, we usethe following domain and functions (in  [2.x.72]  space dimensions):[1.x.54]
* For  [2.x.73] , we extend  [2.x.74]  and  [2.x.75]  by simply duplicatingthe last of the components shown above one more time.
* With all of this, the following comments are in order: [2.x.76]  [2.x.77]  The advection field  [2.x.78]  transports the solution roughly indiagonal direction from lower left to upper right, but with a wigglestructure superimposed. [2.x.79]  The right hand side adds to the field generated by the inflowboundary conditions a blob in the lower left corner, which is thentransported along. [2.x.80]  The inflow boundary conditions impose a weighted sinusoidalstructure that is transported along with the flow field. Since [2.x.81]  on the boundary, the weighting term never gets very large. [2.x.82] 
* 

* [1.x.55][1.x.56]
* 

* In all previous examples with adaptive refinement, we have used anerror estimator first developed by Kelly et al., which assigns to eachcell  [2.x.83]  the following indicator:[1.x.57]where  [2.x.84]  denotes the jump of the normal derivativesacross a face  [2.x.85]  of the cell  [2.x.86] . It can beshown that this error indicator uses a discrete analogue of the secondderivatives, weighted by a power of the cell size that is adjusted tothe linear elements assumed to be in use here:[1.x.58]which itself is related to the error size in the energy norm.
* The problem with this error indicator in the present case is that itassumes that the exact solution possesses second derivatives. This isalready questionable for solutions to Laplace's problem in some cases,although there most problems allow solutions in  [2.x.87] . If solutionsare only in  [2.x.88] , then the second derivatives would be singular insome parts (of lower dimension) of the domain and the error indicatorswould not reduce there under mesh refinement. Thus, the algorithmwould continuously refine the cells around these parts, i.e. wouldrefine into points or lines (in 2d).
* However, for the present case, solutions are usually not even in  [2.x.89] (and this missing regularity is not the exceptional case as forLaplace's equation), so the error indicator described above is notreally applicable. We will thus develop an indicator that is based ona discrete approximation of the gradient. Although the gradient oftendoes not exist, this is the only criterion available to us, at leastas long as we use continuous elements as in the presentexample. To start with, we note that given two cells  [2.x.90] ,  [2.x.91]  ofwhich the centers are connected by the vector  [2.x.92] , we canapproximate the directional derivative of a function  [2.x.93]  as follows:[1.x.59]where  [2.x.94]  and  [2.x.95]  denote  [2.x.96]  evaluated at the centers of therespective cells. We now multiply the above approximation by [2.x.97]  and sum over all neighbors  [2.x.98]  of  [2.x.99] :[1.x.60]If the vectors  [2.x.100]  connecting  [2.x.101]  with its neighbors spanthe whole space (i.e. roughly:  [2.x.102]  has neighbors in all directions),then the term in parentheses in the left hand side expression forms aregular matrix, which we can invert to obtain an approximation of thegradient of  [2.x.103]  on  [2.x.104] :[1.x.61]We will denote the approximation on the right hand side by [2.x.105] , and we will use the following quantity as refinementcriterion:[1.x.62]which is inspired by the following (not rigorous) argument:[1.x.63]
* 
* 

*  [1.x.64] [1.x.65]
*  Just as in previous examples, we have to include several files of which the meaning has already been discussed:
* 

* 
* [1.x.66]
* 
*  The following two files provide classes and information for multithreaded programs. In the first one, the classes and functions are declared which we need to do assembly in parallel (i.e. the  [2.x.106]  namespace). The second file has a class MultithreadInfo which can be used to query the number of processors in your system, which is often useful when deciding how many threads to start in parallel.
* 

* 
* [1.x.67]
* 
*  The next new include file declares a base class  [2.x.107]  not unlike the  [2.x.108]  class, but with the difference that  [2.x.109]  returns a Tensor instead of a scalar.
* 

* 
* [1.x.68]
* 
*  This is C++, as we want to write some output to disk:
* 

* 
* [1.x.69]
* 
*  The last step is as in previous programs:
* 

* 
* [1.x.70]
* 
*   [1.x.71]  [1.x.72]
* 

* 
*  Next we declare a class that describes the advection field. This, of course, is a vector field with as many components as there are space dimensions. One could now use a class derived from the  [2.x.110]  base class, as we have done for boundary values and coefficients in previous examples, but there is another possibility in the library, namely a base class that describes tensor valued functions. This is more convenient than overriding  [2.x.111]  with a method that knows about multiple function components: at the end of the day we need a Tensor, so we may as well just use a class that returns a Tensor.
* 

* 
* [1.x.73]
* 
*  In previous examples, we have used assertions that throw exceptions in several places. However, we have never seen how such exceptions are declared. This can be done as follows:
* 

* 
* [1.x.74]
* 
*  The syntax may look a little strange, but is reasonable. The format is basically as follows: use the name of one of the macros  [2.x.112]  denotes the number of additional parameters which the exception object shall take. In this case, as we want to throw the exception when the sizes of two vectors differ, we need two arguments, so we use  [2.x.113] . The first parameter then describes the name of the exception, while the following declare the data types of the parameters. The last argument is a sequence of output directives that will be piped into the  [2.x.114]  object, thus the strange format with the leading  [2.x.115]  operator and the like. Note that we can access the parameters which are passed to the exception upon construction (i.e. within the  [2.x.116]  call) by using the names  [2.x.117] , where  [2.x.118]  is the number of arguments as defined by the use of the respective macro  [2.x.119] .     
*   To learn how the preprocessor expands this macro into actual code, please refer to the documentation of the exception classes. In brief, this macro call declares and defines a class  [2.x.120]  inheriting from ExceptionBase which implements all necessary error output functions.
* 

* 
* [1.x.75]
* 
*  The following two functions implement the interface described above. The first simply implements the function as described in the introduction, while the second uses the same trick to avoid calling a virtual function as has already been introduced in the previous example program. Note the check for the right sizes of the arguments in the second function, which should always be present in such functions; it is our experience that many if not most programming errors result from incorrectly initialized arrays, incompatible parameters to functions and the like; using assertion as in this case can eliminate many of these problems.
* 

* 
* [1.x.76]
* 
*  Besides the advection field, we need two functions describing the source terms ( [2.x.121] ) and the boundary values. As described in the introduction, the source is a constant function in the vicinity of a source point, which we denote by the constant static variable  [2.x.122] . We set the values of this center using the same template tricks as we have shown in the  [2.x.123]  example program. The rest is simple and has been shown previously.
* 

* 
* [1.x.77]
* 
*  The only new thing here is that we check for the value of the  [2.x.124]  parameter. As this is a scalar function, it is obvious that it only makes sense if the desired component has the index zero, so we assert that this is indeed the case.  [2.x.125]  is a global predefined exception (probably the one most often used, we therefore made it global instead of local to some class), that takes three parameters: the index that is outside the allowed range, the first element of the valid range and the one past the last (i.e. again the half-open interval so often used in the C++ standard library):
* 

* 
* [1.x.78]
* 
*  Finally for the boundary values, which is just another class derived from the  [2.x.126]  base class:
* 

* 
* [1.x.79]
* 
*   [1.x.80]  [1.x.81]
* 

* 
*  Here comes the main class of this program. It is very much like the main classes of previous examples, so we again only comment on the differences.
* 

* 
* [1.x.82]
* 
*  The next set of functions will be used to assemble the matrix. However, unlike in the previous examples, the  [2.x.127]  function will not do the work itself, but rather will delegate the actual assembly to helper functions  [2.x.128]  and  [2.x.129] . The rationale is that matrix assembly can be parallelized quite well, as the computation of the local contributions on each cell is entirely independent of other cells, and we only have to synchronize when we add the contribution of a cell to the global matrix.     
*   The strategy for parallelization we choose here is one of the possibilities mentioned in detail in the  [2.x.130]  module in the documentation. Specifically, we will use the WorkStream approach discussed there. Since there is so much documentation in this module, we will not repeat the rationale for the design choices here (for example, if you read through the module mentioned above, you will understand what the purpose of the  [2.x.131]  and  [2.x.132]  structures is). Rather, we will only discuss the specific implementation.     
*   If you read the page mentioned above, you will find that in order to parallelize assembly, we need two data structures
* 
*  -  one that corresponds to data that we need during local integration ("scratch data", i.e., things we only need as temporary storage), and one that carries information from the local integration to the function that then adds the local contributions to the corresponding elements of the global matrix. The former of these typically contains the FEValues and FEFaceValues objects, whereas the latter has the local matrix, local right hand side, and information about which degrees of freedom live on the cell for which we are assembling a local contribution. With this information, the following should be relatively self-explanatory:
* 

* 
* [1.x.83]
* 
*  FEValues and FEFaceValues are expensive objects to set up, so we include them in the scratch object so that as much data is reused between cells as possible.
* 

* 
* [1.x.84]
* 
*  We also store a few vectors that we will populate with values on each cell. Setting these objects up is, in the usual case, cheap; however, they require memory allocations, which can be expensive in multithreaded applications. Hence we keep them here so that computations on a cell do not require new allocations.
* 

* 
* [1.x.85]
* 
*  Finally, we need objects that describe the problem's data:
* 

* 
* [1.x.86]
* 
*  The following functions again are the same as they were in previous examples, as are the subsequent variables:
* 

* 
* [1.x.87]
* 
*   [1.x.88]  [1.x.89]
* 

* 
*  Now, finally, here comes the class that will compute the difference approximation of the gradient on each cell and weighs that with a power of the mesh size, as described in the introduction. This class is a simple version of the  [2.x.133]  class in the library, that uses similar techniques to obtain finite difference approximations of the gradient of a finite element field, or of higher derivatives.   
*   The class has one public static function  [2.x.134]  that is called to compute a vector of error indicators, and a few private functions that do the actual work on all active cells. As in other parts of the library, we follow an informal convention to use vectors of floats for error indicators rather than the common vectors of doubles, as the additional accuracy is not necessary for estimated values.   
*   In addition to these two functions, the class declares two exceptions which are raised when a cell has no neighbors in each of the space directions (in which case the matrix described in the introduction would be singular and can't be inverted), while the other one is used in the more common case of invalid parameters to a function, namely a vector of wrong size.   
*   Two other comments: first, the class has no non-static member functions or variables, so this is not really a class, but rather serves the purpose of a  [2.x.135]  in C++. The reason that we chose a class over a namespace is that this way we can declare functions that are private. This can be done with namespaces as well, if one declares some functions in header files in the namespace and implements these and other functions in the implementation file. The functions not declared in the header file are still in the namespace but are not callable from outside. However, as we have only one file here, it is not possible to hide functions in the present case.   
*   The second comment is that the dimension template parameter is attached to the function rather than to the class itself. This way, you don't have to specify the template parameter yourself as in most other cases, but the compiler can figure its value out itself from the dimension of the DoFHandler object that one passes as first argument.   
*   Before jumping into the fray with the implementation, let us also comment on the parallelization strategy. We have already introduced the necessary framework for using the WorkStream concept in the declaration of the main class of this program above. We will use it again here. In the current context, this means that we have to define  [2.x.136]   [2.x.137] classes for scratch and copy objects, [2.x.138]   [2.x.139] a function that does the local computation on one cell, and [2.x.140]   [2.x.141] a function that copies the local result into a global object. [2.x.142]   [2.x.143]  Given this general framework, we will, however, deviate from it a bit. In particular, WorkStream was generally invented for cases where each local computation on a cell [1.x.90] to a global object
* 
*  -  for example, when assembling linear systems where we add local contributions into a global matrix and right hand side. WorkStream is designed to handle the potential conflict of multiple threads trying to do this addition at the same time, and consequently has to provide for some way to ensure that only one thread gets to do this at a time. Here, however, the situation is slightly different: we compute contributions from every cell individually, but then all we need to do is put them into an element of an output vector that is unique to each cell. Consequently, there is no risk that the write operations from two cells might conflict, and the elaborate machinery of WorkStream to avoid conflicting writes is not necessary. Consequently, what we will do is this: We still need a scratch object that holds, for example, the FEValues object. However, we only create a fake, empty copy data structure. Likewise, we do need the function that computes local contributions, but since it can already put the result into its final location, we do not need a copy-local-to-global function and will instead give the  [2.x.144]  function an empty function object
* 
*  -  the equivalent to a NULL function pointer.
* 

* 
* [1.x.91]
* 
*   [1.x.92]  [1.x.93]
* 

* 
*  
*   Now for the implementation of the main class. Constructor, destructor and the function  [2.x.145]  follow the same pattern that was used previously, so we need not comment on these three function:
* 

* 
* [1.x.94]
* 
*  In the following function, the matrix and right hand side are assembled. As stated in the documentation of the main class above, it does not do this itself, but rather delegates to the function following next, utilizing the WorkStream concept discussed in  [2.x.146]  .   
*   If you have looked through the  [2.x.147]  module, you will have seen that assembling in parallel does not take an incredible amount of extra code as long as you diligently describe what the scratch and copy data objects are, and if you define suitable functions for the local assembly and the copy operation from local contributions to global objects. This done, the following will do all the heavy lifting to get these operations done on multiple threads on as many cores as you have in your system:
* 

* 
* [1.x.95]
* 
*  As already mentioned above, we need to have scratch objects for the parallel computation of local contributions. These objects contain FEValues and FEFaceValues objects (as well as some arrays), and so we will need to have constructors and copy constructors that allow us to create them. For the cell terms we need the values and gradients of the shape functions, the quadrature points in order to determine the source density and the advection field at a given point, and the weights of the quadrature points times the determinant of the Jacobian at these points. In contrast, for the boundary integrals, we don't need the gradients, but rather the normal vectors to the cells. This determines which update flags we will have to pass to the constructors of the members of the class:
* 

* 
* [1.x.96]
* 
*  Now, this is the function that does the actual work. It is not very different from the  [2.x.148]  functions of previous example programs, so we will again only comment on the differences. The mathematical stuff closely follows what we have said in the introduction.   
*   There are a number of points worth mentioning here, though. The first one is that we have moved the FEValues and FEFaceValues objects into the ScratchData object. We have done so because the alternative would have been to simply create one every time we get into this function
* 
*  -  i.e., on every cell. It now turns out that the FEValues classes were written with the explicit goal of moving everything that remains the same from cell to cell into the construction of the object, and only do as little work as possible in  [2.x.149]  whenever we move to a new cell. What this means is that it would be very expensive to create a new object of this kind in this function as we would have to do it for every cell
* 
*  -  exactly the thing we wanted to avoid with the FEValues class. Instead, what we do is create it only once (or a small number of times) in the scratch objects and then re-use it as often as we can.   
*   This begs the question of whether there are other objects we create in this function whose creation is expensive compared to its use. Indeed, at the top of the function, we declare all sorts of objects. The  [2.x.150] ,  [2.x.151]  do not cost much to create, so there is no harm here. However, allocating memory in creating the  [2.x.152]  and similar variables below typically costs a significant amount of time, compared to just accessing the (temporary) values we store in them. Consequently, these would be candidates for moving into the  [2.x.153]  class. We will leave this as an exercise.
* 

* 
* [1.x.97]
* 
*  We define some abbreviations to avoid unnecessarily long lines:
* 

* 
* [1.x.98]
* 
*  We declare cell matrix and cell right hand side...
* 

* 
* [1.x.99]
* 
*  ... an array to hold the global indices of the degrees of freedom of the cell on which we are presently working...
* 

* 
* [1.x.100]
* 
*  ... then initialize the  [2.x.154]  object...
* 

* 
* [1.x.101]
* 
*  ... obtain the values of right hand side and advection directions at the quadrature points...
* 

* 
* [1.x.102]
* 
*  ... set the value of the streamline diffusion parameter as described in the introduction...
* 

* 
* [1.x.103]
* 
*  ... and assemble the local contributions to the system matrix and right hand side as also discussed above:
* 

* 
* [1.x.104]
* 
*  Alias the AssemblyScratchData object to keep the lines from getting too long:
* 

* 
* [1.x.105]
* 
*  Besides the cell terms which we have built up now, the bilinear form of the present problem also contains terms on the boundary of the domain. Therefore, we have to check whether any of the faces of this cell are on the boundary of the domain, and if so assemble the contributions of this face as well. Of course, the bilinear form only contains contributions from the  [2.x.155]  part of the boundary, but to find out whether a certain part of a face of the present cell is part of the inflow boundary, we have to have information on the exact location of the quadrature points and on the direction of flow at this point; we obtain this information using the FEFaceValues object and only decide within the main loop whether a quadrature point is on the inflow boundary.
* 

* 
* [1.x.106]
* 
*  Ok, this face of the present cell is on the boundary of the domain. Just as for the usual FEValues object which we have used in previous examples and also above, we have to reinitialize the FEFaceValues object for the present face:
* 

* 
* [1.x.107]
* 
*  For the quadrature points at hand, we ask for the values of the inflow function and for the direction of flow:
* 

* 
* [1.x.108]
* 
*  Now loop over all quadrature points and see whether this face is on the inflow or outflow part of the boundary. The normal vector points out of the cell: since the face is at the boundary, the normal vector points out of the domain, so if the advection direction points into the domain, its scalar product with the normal vector must be negative (to see why this is true, consider the scalar product definition that uses a cosine):
* 

* 
* [1.x.109]
* 
*  If the face is part of the inflow boundary, then compute the contributions of this face to the global matrix and right hand side, using the values obtained from the FEFaceValues object and the formulae discussed in the introduction:
* 

* 
* [1.x.110]
* 
*  The final piece of information the copy routine needs is the global indices of the degrees of freedom on this cell, so we end by writing them to the local array:
* 

* 
* [1.x.111]
* 
*  The second function we needed to write was the one that copies the local contributions the previous function computed (and put into the AssemblyCopyData object) into the global matrix and right hand side vector objects. This is essentially what we always had as the last block of code when assembling something on every cell. The following should therefore be pretty obvious:
* 

* 
* [1.x.112]
* 
*  Here comes the linear solver routine. As the system is no longer symmetric positive definite as in all the previous examples, we cannot use the Conjugate Gradient method anymore. Rather, we use a solver that is more general and does not rely on any special properties of the matrix: the GMRES method. GMRES, like the conjugate gradient method, requires a decent preconditioner: we use a Jacobi preconditioner here, which works well enough for this problem.
* 

* 
* [1.x.113]
* 
*  The following function refines the grid according to the quantity described in the introduction. The respective computations are made in the class  [2.x.156] .
* 

* 
* [1.x.114]
* 
*  This function is similar to the one in step 6, but since we use a higher degree finite element we save the solution in a different way. Visualization programs like VisIt and Paraview typically only understand data that is associated with nodes: they cannot plot fifth-degree basis functions, which results in a very inaccurate picture of the solution we computed. To get around this we save multiple  [2.x.157] patches [2.x.158]  per cell: in 2D we save 64 bilinear `cells' to the VTU file for each cell, and in 3D we save 512. The end result is that the visualization program will use a piecewise linear interpolation of the cubic basis functions: this captures the solution detail and, with most screen resolutions, looks smooth. We save the grid in a separate step with no extra patches so that we have a visual representation of the cell faces.   
*   Version 9.1 of deal.II gained the ability to write higher degree polynomials (i.e., write piecewise bicubic visualization data for our piecewise bicubic solution) VTK and VTU output: however, not all recent versions of ParaView and VisIt (as of 2018) can read this format, so we use the older, more general (but less efficient) approach here.
* 

* 
* [1.x.115]
* 
*  VTU output can be expensive, both to compute and to write to disk. Here we ask ZLib, a compression library, to compress the data in a way that maximizes throughput.
* 

* 
* [1.x.116]
* 
*  ... as is the main loop (setup
* 
*  -  solve
* 
*  -  refine), aside from the number of cycles and the initial grid:
* 

* 
* [1.x.117]
* 
*   [1.x.118]  [1.x.119]
* 

* 
*  Now for the implementation of the  [2.x.159]  class. Let us start by defining constructors for the  [2.x.160]  class used by the  [2.x.161]  function:
* 

* 
* [1.x.120]
* 
*  We allocate a vector to hold iterators to all active neighbors of a cell. We reserve the maximal number of active neighbors in order to avoid later reallocations. Note how this maximal number of active neighbors is computed here.
* 

* 
* [1.x.121]
* 
*  Next comes the implementation of the  [2.x.162]  class. The first function does not much except for delegating work to the other function, but there is a bit of setup at the top.   
*   Before starting with the work, we check that the vector into which the results are written has the right size. Programming mistakes in which one forgets to size arguments correctly at the calling site are quite common. Because the resulting damage from not catching such errors is often subtle (e.g., corruption of data somewhere in memory, or non-reproducible results), it is well worth the effort to check for such things.
* 

* 
* [1.x.122]
* 
*  Here comes the function that estimates the local error by computing the finite difference approximation of the gradient. The function first computes the list of active neighbors of the present cell and then computes the quantities described in the introduction for each of the neighbors. The reason for this order is that it is not a one-liner to find a given neighbor with locally refined meshes. In principle, an optimized implementation would find neighbors and the quantities depending on them in one step, rather than first building a list of neighbors and in a second step their contributions but we will gladly leave this as an exercise. As discussed before, the worker function passed to  [2.x.163]  works on "scratch" objects that keep all temporary objects. This way, we do not need to create and initialize objects that are expensive to initialize within the function that does the work every time it is called for a given cell. Such an argument is passed as the second argument. The third argument would be a "copy-data" object (see  [2.x.164]  for more information) but we do not actually use any of these here. Since  [2.x.165]  insists on passing three arguments, we declare this function with three arguments, but simply ignore the last one.   
*   (This is unsatisfactory from an aesthetic perspective. It can be avoided by using an anonymous (lambda) function. If you allow, let us here show how. First, assume that we had declared this function to only take two arguments by omitting the unused last one. Now,  [2.x.166]  still wants to call this function with three arguments, so we need to find a way to "forget" the third argument in the call. Simply passing  [2.x.167]  the pointer to the function as we do above will not do this
* 
*  -  the compiler will complain that a function declared to have two arguments is called with three arguments. However, we can do this by passing the following as the third argument to  [2.x.168]   [2.x.169]  This is not much better than the solution implemented below: either the routine itself must take three arguments or it must be wrapped by something that takes three arguments. We don't use this since adding the unused argument at the beginning is simpler.   
*   Now for the details:
* 

* 
* [1.x.124]
* 
*  We need space for the tensor  [2.x.170] , which is the sum of outer products of the y-vectors.
* 

* 
* [1.x.125]
* 
*  First initialize the  [2.x.171]  object, as well as the  [2.x.172]  tensor:
* 

* 
* [1.x.126]
* 
*  Now, before we go on, we first compute a list of all active neighbors of the present cell. We do so by first looping over all faces and see whether the neighbor there is active, which would be the case if it is on the same level as the present cell or one level coarser (note that a neighbor can only be once coarser than the present cell, as we only allow a maximal difference of one refinement over a face in deal.II). Alternatively, the neighbor could be on the same level and be further refined; then we have to find which of its children are next to the present cell and select these (note that if a child of a neighbor of an active cell that is next to this active cell, needs necessarily be active itself, due to the one-refinement rule cited above).     
*   Things are slightly different in one space dimension, as there the one-refinement rule does not exist: neighboring active cells may differ in as many refinement levels as they like. In this case, the computation becomes a little more difficult, but we will explain this below.     
*   Before starting the loop over all neighbors of the present cell, we have to clear the array storing the iterators to the active neighbors, of course.
* 

* 
* [1.x.127]
* 
*  First define an abbreviation for the iterator to the face and the neighbor
* 

* 
* [1.x.128]
* 
*  Then check whether the neighbor is active. If it is, then it is on the same level or one level coarser (if we are not in 1D), and we are interested in it in any case.
* 

* 
* [1.x.129]
* 
*  If the neighbor is not active, then check its children.
* 

* 
* [1.x.130]
* 
*  To find the child of the neighbor which bounds to the present cell, successively go to its right child if we are left of the present cell (n==0), or go to the left child if we are on the right (n==1), until we find an active cell.
* 

* 
* [1.x.131]
* 
*  As this used some non-trivial geometrical intuition, we might want to check whether we did it right, i.e., check whether the neighbor of the cell we found is indeed the cell we are presently working on. Checks like this are often useful and have frequently uncovered errors both in algorithms like the line above (where it is simple to involuntarily exchange  [2.x.173]  or the like) and in the library (the assumptions underlying the algorithm above could either be wrong, wrongly documented, or are violated due to an error in the library). One could in principle remove such checks after the program works for some time, but it might be a good things to leave it in anyway to check for changes in the library or in the algorithm above.                   
*   Note that if this check fails, then this is certainly an error that is irrecoverable and probably qualifies as an internal error. We therefore use a predefined exception class to throw here.
* 

* 
* [1.x.132]
* 
*  If the check succeeded, we push the active neighbor we just found to the stack we keep:
* 

* 
* [1.x.133]
* 
*  If we are not in 1d, we collect all neighbor children `behind' the subfaces of the current face and move on:
* 

* 
* [1.x.134]
* 
*  OK, now that we have all the neighbors, lets start the computation on each of them. First we do some preliminaries: find out about the center of the present cell and the solution at this point. The latter is obtained as a vector of function values at the quadrature points, of which there are only one, of course. Likewise, the position of the center is the position of the first (and only) quadrature point in real space.
* 

* 
* [1.x.135]
* 
*  Now loop over all active neighbors and collect the data we need.
* 

* 
* [1.x.136]
* 
*  Then get the center of the neighbor cell and the value of the finite element function at that point. Note that for this information we have to reinitialize the  [2.x.174]  object for the neighbor cell.
* 

* 
* [1.x.137]
* 
*  Compute the vector  [2.x.175]  connecting the centers of the two cells. Note that as opposed to the introduction, we denote by  [2.x.176]  the normalized difference vector, as this is the quantity used everywhere in the computations.
* 

* 
* [1.x.138]
* 
*  Then add up the contribution of this cell to the Y matrix...
* 

* 
* [1.x.139]
* 
*  ... and update the sum of difference quotients:
* 

* 
* [1.x.140]
* 
*  If now, after collecting all the information from the neighbors, we can determine an approximation of the gradient for the present cell, then we need to have passed over vectors  [2.x.177]  which span the whole space, otherwise we would not have all components of the gradient. This is indicated by the invertibility of the matrix.     
*   If the matrix is not invertible, then the present cell had an insufficient number of active neighbors. In contrast to all previous cases (where we raised exceptions) this is, however, not a programming error: it is a runtime error that can happen in optimized mode even if it ran well in debug mode, so it is reasonable to try to catch this error also in optimized mode. For this case, there is the  [2.x.178]  macro: it checks the condition like the  [2.x.179]  macro, but not only in debug mode; it then outputs an error message, but instead of aborting the program as in the case of the  [2.x.180]  macro, the exception is thrown using the  [2.x.181]  command of C++. This way, one has the possibility to catch this error and take reasonable counter actions. One such measure would be to refine the grid globally, as the case of insufficient directions can not occur if every cell of the initial grid has been refined at least once.
* 

* 
* [1.x.141]
* 
*  If, on the other hand, the matrix is invertible, then invert it, multiply the other quantity with it, and compute the estimated error using this quantity and the correct powers of the mesh width:
* 

* 
* [1.x.142]
* 
*  The last part of this function is the one where we write into the element of the output vector what we have just computed. The address of this vector has been stored in the scratch data object, and all we have to do is know how to get at the correct element inside this vector
* 
*  -  but we can ask the cell we're on the how-manyth active cell it is for this:
* 

* 
* [1.x.143]
* 
*   [1.x.144]  [1.x.145]
* 

* 
*  The  [2.x.182]  function is similar to the previous examples. The primary difference is that we use MultithreadInfo to set the maximum number of threads (see the documentation module  [2.x.183]  "Parallel computing with multiple processors accessing shared memory" for more information). The number of threads used is the minimum of the environment variable DEAL_II_NUM_THREADS and the parameter of  [2.x.184] . If no value is given to  [2.x.185] , the default value from the Intel Threading Building Blocks (TBB) library is used. If the call to  [2.x.186]  is omitted, the number of threads will be chosen by TBB independently of DEAL_II_NUM_THREADS.
* 

* 
* [1.x.146]
* [1.x.147][1.x.148]
* 

* 
* The results of this program are not particularly spectacular. Theyconsist of the console output, some grid files, and the solution oneach of these grids. First for the console output:
* [1.x.149]
* 
* Quite a number of cells are used on the finest level to resolve the features ofthe solution. Here are the fourth and tenth grids: [2.x.187] and the fourth and tenth solutions: [2.x.188] and both the grid and solution zoomed in: [2.x.189] 
* The solution is created by that part that is transported along the wigglyadvection field from the left and lower boundaries to the top right, and thepart that is created by the source in the lower left corner, and the results ofwhich are also transported along. The grid shown above is well-adapted toresolve these features. The comparison between plots shows that, even though weare using a high-order approximation, we still need adaptive mesh refinement tofully resolve the wiggles.
* 

* [1.x.150][1.x.151] [2.x.190] 
* [0.x.1]

include/deal.II-translator/A-tutorial/tutorial_0.txt
[0.x.0]*
  [2.x.0] 
*  New to deal.II? You might want to start with tutorial  [2.x.1]  and work your way up to  [2.x.2] . At that point you can explore what features you are interested in and look at the large collection of programs listed below.
*  The deal.II tutorial contains a collection of programs, each more or less built atop of previous ones, which demonstrate various aspects of the library. Each such example has the following structure:  [2.x.3]     [2.x.4]  [1.x.0] What the program does, including        the mathematical model, and        what programming techniques are new.    [2.x.5]  [1.x.1] An extensively documented listing of the        source code.    [2.x.6]  [1.x.2] The output of the program, with comments and        interpretation.    [2.x.7]  [1.x.3] The source code stripped of        all comments.  [2.x.8]  You can browse the available tutorial programs  [2.x.9]     [2.x.10]  as [1.x.4][1.x.5] that shows how the major      concepts of each tutorial programs builds on previous ones (though each      program may also use minor pieces from other programs not specifically
*       connected in the graph).    [2.x.11]  as [1.x.6][1.x.7] that provides a short     synopsis of each program.    [2.x.12]  or [1.x.8][1.x.9].  [2.x.13] 
*  The programs are in the  [2.x.14]  directory of your local deal.II installation. After compiling the library itself, if you go into one of the tutorial directories, you can configure the program by typing  [2.x.15]  and run it using  [2.x.16] . The latter command also compiles the program if that has not already been done. The CMakeLists.txt files in the different directories are based on the [1.x.10].
* 

* 
*  [2.x.17]  Some of the tutorial programs also jointly form   the [1.x.11]. More, often more complex but less well documented,   deal.II-based programs than the ones that form the tutorial can also be   found in the  [2.x.18]  .
* 

*  [1.x.12]  [2.x.19]  TutorialConnectionGraph [1.x.13]
*  The following graph shows the connections between tutorial programs and how their major components build on each other. Click on any of the boxes to go to one of the programs. If you hover your mouse pointer over a box, a brief description of the program should appear.

* 
* [1.x.14]
*  [1.x.15]<br />

* 
* [1.x.16]
*  [1.x.17] [1.x.18]
*   [2.x.20] 
* 

*  [1.x.19] [1.x.20]
*  [1.x.21][1.x.22]  [2.x.21]  [1.x.23][1.x.24]  [2.x.22]  [1.x.25][1.x.26]  [2.x.23] 
* 

*  [1.x.27][1.x.28]  [2.x.24] 
* 

* 
*  [1.x.29][1.x.30]  [2.x.25] 
* 

* 
*  [1.x.31][1.x.32]  [2.x.26] 

* 
* [0.x.1]

