include/deal.II-translator/optimization/line_minimization_0.txt
[0.x.0]*
 A namespace for various algorithms related to minimization a over line.

* 
* [0.x.1]*
   Given  [2.x.0]  and  [2.x.1]  together with values of function    [2.x.2]  and  [2.x.3]  and the gradient  [2.x.4] , return the local   minimizer of the quadratic interpolation function.     The return type is optional to fit with similar functions that may   not have a solution for given parameters.  
* [0.x.2]*
   Given  [2.x.5]  and  [2.x.6]  together with values of function    [2.x.7]  and  [2.x.8]  and its gradients ( [2.x.9] ) at   those points, return the local minimizer of the cubic interpolation   function (that is, the location where the cubic interpolation function   attains its minimum value).     The return type is optional as the real-valued solution might not exist.  
* [0.x.3]*
   Find the minimizer of a cubic polynomial that goes through the   points  [2.x.10] ,  [2.x.11]  and  [2.x.12]    and has derivatve  [2.x.13]  at  [2.x.14] .     The return type is optional as the real-valued solution might not exist.  
* [0.x.4]*
   Return the minimizer of a polynomial using function values  [2.x.15]  ,    [2.x.16]  , and  [2.x.17]  at three points  [2.x.18]  ,  [2.x.19]  , and    [2.x.20]  as well as the derivatives at two points  [2.x.21]  and  [2.x.22]    The returned point should be within the bounds  [2.x.23]  .     This function will first try to perform a cubic_fit(). If its unsuccessful,   or if the minimum is not within the provided  [2.x.24]  a quadratic_fit()   will be performed. The function will fallback to a bisection method if   quadratic_fit() fails as well.  
* [0.x.5]*
   Same as poly_fit(), but performing a cubic fit with three points (see   cubic_fit_three_points() ).  
* [0.x.6]*
   Perform a line search in  [2.x.25]  with strong Wolfe conditions   [1.x.0]   using the one dimensional function  [2.x.26]  in conjunction with a function  [2.x.27]    to choose a new point from the interval based on the function values and   derivatives at its ends.   The parameter  [2.x.28]  is a trial estimate of the first step.   Interpolation can be done using either poly_fit() or   poly_fit_three_points(), or any other function that has a similar   signature.     The function implements Algorithms 2.6.2 and 2.6.4 on pages 34-35 in  
* [1.x.1]
*    These are minor variations of  Algorithms 3.5 and 3.6 on pages 60-61 in  
* [1.x.2]
*    It consists of a bracketing phase and a zoom phase, where  [2.x.29]  is used.     Two examples of use might be as follows:   In the first example, we wish to find the minimum of the function    [2.x.30] . To find the approximate solution using line search   with a polynomial fit to the curve one would perform the following steps:    
* [1.x.3]
*      In the second example, we wish to perform line search in the context of a   non-linear finite element problem. What follows below is a non-optimized   implementation of the back-tracking algorithm, which may be useful when   the load-step size is too large. The following illustrates the basic steps   necessary to utilize the scheme within the context of a global nonlinear   solver:    
* [1.x.4]
*         [2.x.31]  func A one dimensional function which returns value and derivative   at the given point.    [2.x.32]  f0 The function value at the origin.    [2.x.33]  g0 The function derivative at the origin.    [2.x.34]  interpolate A function which determines how interpolation is done   during the zoom phase. It takes values and derivatives at the current   interval/bracket ( [2.x.35] ,  [2.x.36] ) as well as up to 5 values and   derivatives at previous steps. The returned value is to be provided within   the given bounds.    [2.x.37]  a1 Initial trial step for the bracketing phase.    [2.x.38]  eta A parameter in the second Wolfe condition (curvature condition).    [2.x.39]  mu A parameter in the first Wolfe condition (sufficient decrease).    [2.x.40]  a_max The maximum allowed step size.    [2.x.41]  max_evaluations The maximum allowed number of function evaluations.    [2.x.42]  debug_output A flag to output extra debug information into the    [2.x.43]  static object.    [2.x.44]  The function returns the step size and the number of times function    [2.x.45]  was called.  
* [0.x.7]    Assert((f_lo < f_hi) && w1(a_lo, f_lo), ExcInternalError());    Assert(((a_hi
* 
*  - a_lo) g_lo < 0) && !w2(g_lo), ExcInternalError());    Assert((w1(a_hi, f_hi) || f_hi >= f_lo), ExcInternalError());   
* [0.x.8]

include/deal.II-translator/optimization/solver_bfgs_0.txt
[0.x.0]*
 Implement the limited memory BFGS minimization method.
*  This class implements a method to minimize a given function for which only the values of the function and its derivatives, but not its second derivatives are available. The BFGS method is a variation of the Newton method for function minimization in which the Hessian matrix is only approximated. In particular, the Hessian is updated using the formula of Broyden, Fletcher, Goldfarb, and Shanno (BFGS):

* 
* [1.x.0]
*  for a symmetric positive definite  [2.x.0] . Limited memory variant is implemented via the two-loop recursion.

* 
* [0.x.1]*
   Number type.  
* [0.x.2]*
   Standardized data struct to pipe additional data to the solver.  
* [0.x.3]*
     Constructor.    
* [0.x.4]*
     Maximum history size.    
* [0.x.5]*
     Print extra debug output to deallog.    
* [0.x.6]*
   Constructor.  
* [0.x.7]*
   Solve the unconstrained minimization problem   [1.x.1]   starting from initial state  [2.x.1]      The function  [2.x.2]  takes two arguments indicating the values of  [2.x.3]    and of the gradient  [2.x.4] . When called, it needs to update the gradient  [2.x.5]  at the given   location  [2.x.6]  and return the value of the function being minimized, i.e.,    [2.x.7] .  
* [0.x.8]*
   Connect a slot to perform a custom line-search.     Given the value of function  [2.x.8]  the current value of unknown  [2.x.9]    the gradient  [2.x.10]  and the search direction  [2.x.11]    return the size  [2.x.12]  of the step  [2.x.13] ,   and update  [2.x.14]   [2.x.15]  and  [2.x.16]  accordingly.  
* [0.x.9]*
   Connect a slot to perform a custom preconditioning.     The preconditioner is applied inside the two loop recursion to   vector `g` using the history of position increments `s` and   gradient increments `y`.     One possibility is to use the oldest `s,y` pair:  
* [1.x.2]
*      No preconditioning is performed if the code using this class has not   attached anything to the signal.  
* [0.x.10]*
   Additional data to the solver.  
* [0.x.11]*
   Signal used to perform line search.  
* [0.x.12]*
   Signal used to perform preconditioning.  
* [0.x.13]

