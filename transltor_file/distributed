include/deal.II-translator/distributed/cell_data_transfer_0.txt
[0.x.0]*
     Transfer data that is associated with each active cell (like error     indicators) while refining and/or coarsening a distributed triangulation     and handle the necessary communication.         This class therefore does for cell-related information what      [2.x.0]  does for the values of degrees of     freedom defined on a  [2.x.1]          This class has been designed to operate on any kind of datatype that is     serializable. A non-distributed container (like Vector or  [2.x.2]      has to be provided, which holds the cell-wise data in the same order as     active cells are traversed. In other words, each entry corresponds to the     cell with the same index  [2.x.3]  and the     container has to be of size  [2.x.4]          [1.x.0]         The following code snippet demonstrates how to transfer cell-related data     across refinement/coarsening of the registered triangulation.        
* [1.x.1]
*              [1.x.2]         This class can be used to serialize and later deserialize a distributed     mesh with attached data to separate files.         For serialization, the following code snippet saves not only the     triangulation itself, but also the cell-wise data attached:    
* [1.x.3]
*          Later, during deserialization, both triangulation and data can be     restored as follows:    
* [1.x.4]
*         
*  [2.x.5]  If you use more than one object to transfer data via the      [2.x.6]  and      [2.x.7]  interface     with the aim of serialization, the calls to the corresponding     prepare_for_serialization() and deserialize() functions need to happen in     the same order, respectively. Classes relying on this interface are e.g.      [2.x.8]       [2.x.9]  and  [2.x.10]         
*  [2.x.11]  See the documentation of  [2.x.12]  for     matching code snippets for both transfer and serialization.        
*  [2.x.13]     
* [0.x.1]*
       An alias that defines the data type of provided container template.      
* [0.x.2]*
       Constructor.              [2.x.14]  triangulation The triangulation on which all operations will         happen. At the time when this constructor is called, the refinement         in question has not happened yet.        [2.x.15]  transfer_variable_size_data Specify whether your VectorType         container stores values that differ in size. A varying amount of data         may be packed per cell, if for example the underlying ValueType of         the VectorType container is a container itself.        [2.x.16]  refinement_strategy %Function deciding how data will be         stored on refined cells from its parent cell.        [2.x.17]  coarsening_strategy %Function deciding which data to store         on a cell whose children will get coarsened into.      
* [0.x.3]*
       Prepare the current object for coarsening and refinement.             It registers the data transfer of  [2.x.18]  on the underlying triangulation.        [2.x.19]  includes data to be interpolated onto the new (refined and/or       coarsened) grid. See documentation of this class for more information       on how to use this functionality.             This function can be called only once for the specified container       until data transfer has been completed. If multiple vectors shall be       transferred via this class, use the function below.      
* [0.x.4]*
       Same as the function above, only for a list of vectors.      
* [0.x.5]*
       Prepare the serialization of the given vector.             The serialization is done by  [2.x.20]  See documentation       of this class for more information on how to use this functionality.             This function can be called only once for the specified container       until data transfer has been completed. If multiple vectors shall be       transferred via this class, use the function below.      
* [0.x.6]*
       Same as the function above, only for a list of vectors.      
* [0.x.7]*
       Unpack the information previously stored in this object before       the mesh was refined or coarsened onto the current set of cells.      
* [0.x.8]*
       Same as the function above, only for a list of vectors.      
* [0.x.9]*
       Execute the deserialization of the stored information.       This needs to be done after calling  [2.x.21]       
* [0.x.10]*
       Same as the function above, only for a list of vectors.      
* [0.x.11]*
       Pointer to the triangulation to work with.      
* [0.x.12]*
       Specifies if size of data to transfer varies from cell to cell.      
* [0.x.13]*
       %Function deciding how data will be stored on refined cells from its       parent cell.      
* [0.x.14]*
       %Function deciding on how to process data from children to be stored on       the parent cell.      
* [0.x.15]*
       A vector that stores pointers to all the vectors we are supposed to       copy over from the old to the new mesh.      
* [0.x.16]*
       The handle that triangulation has assigned to this object       with which we can access our memory offset and our pack function.      
* [0.x.17]*
       Registers the pack_callback() function to the triangulation       and stores the returning handle.      
* [0.x.18]*
       A callback function used to pack the data on the current mesh into       objects that can later be retrieved after refinement, coarsening and       repartitioning.      
* [0.x.19]*
       A callback function used to unpack the data on the current mesh that       has been packed up previously on the mesh before refinement,       coarsening and repartitioning.      
* [0.x.20]

include/deal.II-translator/distributed/cell_data_transfer.templates_0.txt
[0.x.0]

include/deal.II-translator/distributed/cell_weights_0.txt
[0.x.0]*
   Anytime a  [2.x.0]  is repartitioned, either upon request   or by refinement/coarsening, cells will be distributed amongst all   subdomains to achieve an equally balanced workload. If the workload per   cell varies, which is in general the case for DoFHandler objects with   hp-capabilities, we can take that into account by introducing individual   weights for different cells.     This class allows computing these weights for load balancing by   consulting the FiniteElement that is associated with each cell of   a DoFHandler. One can choose from predefined weighting algorithms provided   by this class or provide a custom one.     If the associated DoFHandler has not been initialized yet, i.e., its    [2.x.1]  is empty, all cell weights will be evaluated as zero.     This class offers two different ways of connecting the chosen weighting   function to the corresponding signal of the linked    [2.x.2]  The recommended way involves creating an   object of this class which will automatically take care of registering the   weighting function upon creation and de-registering it once destroyed. An   object of this class needs to exist for every DoFHandler associated with   the Triangulation we work on to achieve satisfying work balancing results.   The connected weighting function may be changed anytime using the    [2.x.3]  function. The following code snippet demonstrates how   to achieve each cell being weighted by its current number of degrees of   freedom. We chose a factor of `1000` that corresponds to the initial weight   each cell is assigned to upon creation.  
* [1.x.0]
*      On the other hand, you are also able to take care of handling the signal   connection manually by using the static member function of this class. In   this case, an analogous code example looks as follows.  
* [1.x.1]
*      The use of this class is demonstrated in  [2.x.4] .    
*  [2.x.5]  See  [2.x.6]  for more information on   weighting and load balancing.    
*  [2.x.7]  Be aware that this class connects the weight function to the   Triangulation during this class's constructor. If the Triangulation   associated with the DoFHandler changes during the lifetime of the   latter via  [2.x.8]  an assertion will be triggered in   the weight_callback() function. Use  [2.x.9]  to deregister the   weighting function on the old Triangulation and connect it to the new one.    
*  [2.x.10]   
* [0.x.1]*
     An alias that defines the characteristics of a function that can be used     for weighting cells during load balancing.         Such weighting functions take as arguments an iterator to a cell and the     future finite element that will be assigned to it after repartitioning.     They return an unsigned integer which is interpreted as the cell's     weight or, in other words, the additional computational load associated     with it.    
* [0.x.2]*
     Constructor.          [2.x.11]  dof_handler The DoFHandler which will be used to        determine each cell's finite element.      [2.x.12]  weighting_function The function that determines each        cell's weight during load balancing.    
* [0.x.3]*
     Destructor.         Disconnects the function previously connected to the weighting signal.    
* [0.x.4]*
     Connect a different  [2.x.13]  to the Triangulation     associated with the  [2.x.14]          Disconnects the function previously connected to the weighting signal.    
* [0.x.5]*
     Converts a  [2.x.15]  to a different type that qualifies as     a callback function, which can be connected to a weighting signal of a     Triangulation.         This function does [1.x.2] connect the converted function to the     Triangulation associated with the  [2.x.16]     
* [0.x.6]*
      [2.x.17]  Selection of weighting functions      [2.x.18]     
* [0.x.7]*
     Choose a constant weight  [2.x.19]  on each cell.    
* [0.x.8]*
     The pair of floating point numbers  [2.x.20]  provided via      [2.x.21]  determines the weight  [2.x.22]  of each cell  [2.x.23]  with      [2.x.24]  degrees of freedom in the following way: [1.x.3]         The right hand side will be rounded to the nearest integer since cell     weights are required to be integers.    
* [0.x.9]*
     The container  [2.x.25]  provides pairs of floating point numbers      [2.x.26]  that determine the weight  [2.x.27]  of each cell      [2.x.28]  with  [2.x.29]  degrees of freedom in the following way: [1.x.4]         The right hand side will be rounded to the nearest integer since cell     weights are required to be integers.    
* [0.x.10]*
      [2.x.30]     
* [0.x.11]*
     A connection to the corresponding cell_weight signal of the Triangulation     which is attached to the DoFHandler.    
* [0.x.12]*
     A callback function that will be connected to the cell_weight signal of     the  [2.x.31]  to which the  [2.x.32]  is attached. Ultimately     returns the weight for each cell, determined by the  [2.x.33]      provided as a parameter. Returns zero if  [2.x.34]  has not been     initialized yet.    
* [0.x.13]

include/deal.II-translator/distributed/fully_distributed_tria_0.txt
[0.x.0]*
   A namespace for the fully distributed triangulation.    
*  [2.x.0]   
* [0.x.1]*
     A distributed triangulation with a distributed coarse grid.         The motivation for  [2.x.1]  has its     origins in the following observations about complex geometries and/or     about given meshes created by an external mesh generator. We regard     complex geometries as geometries that can be meshed only with a     non-negligible number of coarse cells (>10,000):
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - storing the coarse-grid information on every process is too expensive       from a memory point of view (as done by        [2.x.2]  Normally, a process only needs a       small section of the global triangulation, i.e., a small section of the       coarse grid such that a partitioning of the coarse grid is indeed       essential. The cells stored on each process consist of the        [2.x.3]  "locally owned cells" and the        [2.x.4]  "ghost cells".
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - the distribution of the active cells
* 
*  - on the finest level
* 
*  - among all       processes by simply partitioning a space-filling curve might not lead       to an optimal result for triangulations that originate from large       coarse grids: e.g. partitions that belong to the same process might       be discontinuous, leading to increased communication (within a       node and beyond). Graph-based partitioning algorithms might be a sound       alternative to the space filling curve used by        [2.x.5]          To be able to construct a fully partitioned triangulation that     distributes the coarse grid and gives flexibility regarding partitioning,     the following ingredients are required:
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - a locally relevant coarse-grid triangulation       (vertices, cell definition; including a layer of ghost cells)
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - a mapping of the locally relevant coarse-grid triangulation into the       global coarse-grid triangulation
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - information about which cell should be refined as well as information       regarding the subdomain_id, the level_subdomain_id, manifold_id,       and boundary_id of each cell.         The ingredients listed above are bundled in the struct      [2.x.6]  The user has to fill this     data structure
* 
*  - in a pre-processing step
* 
*  - before actually creating the     triangulation. Predefined functions to create      [2.x.7]  can be found in the namespace      [2.x.8]          Once the  [2.x.9]  `construction_data` has     been constructed, the triangulation `tria` can be created by calling     `tria.create_triangulation(construction_data);`.        
*  [2.x.10]  This triangulation supports: 1D/2D/3D, hanging nodes,           geometric multigrid, and periodicity.        
*  [2.x.11]  You can create a triangulation with hanging nodes and multigrid           levels with create_triangulation(). However, once it has been           created, it cannot be altered anymore, i.e. you cannot coarsen or           refine afterwards.        
*  [2.x.12]  Currently only simple periodicity conditions (i.e. without offsets           and rotation matrices
* 
*  - see also the documentation of            [2.x.13]  are supported.    
* [0.x.2]*
       Constructor.              [2.x.14]  mpi_communicator The MPI communicator to be used for the                               triangulation.      
* [0.x.3]*
       Destructor.      
* [0.x.4]*
        [2.x.15]   [2.x.16]             
*  [2.x.17]  This is the function to be used instead of        [2.x.18]  for some of the other       triangulations of deal.II.      
* [0.x.5]*
      
*  [2.x.19]  This function is not implemented for this class  and throws             an assertion. Instead, use             the other create_triangulation() function to create the             triangulation.      
* [0.x.6]*
       Implementation of the same function as in the base class.              [2.x.20]  other_tria The triangulation to be copied. It can be a serial              Triangulation or a  [2.x.21]  Both              can have been refined already.            
*  [2.x.22]  This function uses the partitioner registered with             set_partitioner().      
* [0.x.7]*
       Register a partitioner, which is used within the method       copy_triangulation.              [2.x.23]  partitioner A partitioning function, which takes as input argument                          a reference to the triangulation to be partitioned                          and the number of partitions to be created.                          The function needs to set subdomain                          ids for each active cell of the given triangulation,                          with values between zero (inclusive)                          and the second argument to the function (exclusive).        [2.x.24]  settings See the description of the Settings enumerator.            
*  [2.x.25]  As a default,  [2.x.26]  is used             as partitioner and data structures on multigrid levels are not             set up.      
* [0.x.8]*
       Coarsen and refine the mesh according to refinement and coarsening       flags set.            
*  [2.x.27]  Not implemented yet.      
* [0.x.9]*
       Override the implementation of prepare_coarsening_and_refinement from       the base class.            
*  [2.x.28]  Not implemented yet.      
* [0.x.10]*
       Return true if the triangulation has hanging nodes.            
*  [2.x.29]  Not implemented yet.      
* [0.x.11]*
       Return the local memory consumption in bytes.      
* [0.x.12]*
       Save the triangulation into the given file. This file needs to be       reachable from all nodes in the computation on a shared network file       system. See the SolutionTransfer class on how to store solution vectors       into this file. Additional cell-based data can be saved using       register_data_attach().      
* [0.x.13]*
       Load the triangulation saved with save() back in. The mesh       must be empty before calling this function.             You need to load with the same number of MPI processes that       you saved with, hence autopartition is disabled.             Cell-based data that was saved with register_data_attach() can be read       in with notify_ready_to_unpack() after calling load().      
* [0.x.14]*
       Go through all active cells that are locally owned and record how they       will change in the private member vector local_cell_relations.             As no adaptive mesh refinement is supported at the moment for this       class, all cells will be flagged with the CellStatus CELL_PERSIST.       These relations will currently only be used for serialization.             The stored vector will have a size equal to the number of locally owned       active cells and will be ordered by the occurrence of those cells.      
* [0.x.15]*
       store the Settings.      
* [0.x.16]*
       Partitioner used in copy_triangulation().      
* [0.x.17]*
       Sorted list of pairs of coarse-cell ids and their indices.      
* [0.x.18]*
       List of the coarse-cell id for each coarse cell (stored at       cell->index()).      
* [0.x.19]*
       Boolean indicating that the function create_triangulation() was called       for internal usage.      
* [0.x.20]*
       Boolean indicating that the function       prepare_coarsening_and_refinement() was called for internal usage.      
* [0.x.21]

include/deal.II-translator/distributed/grid_refinement_0.txt
[0.x.0]*
         Compute the global max and min of the criteria vector. These are         returned only on the processor with rank zero, all others get a pair         of zeros.        
* [0.x.1]*
           Compute a threshold value so that exactly n_target_cells have a           value that is larger.          
* [0.x.2]*
           Compute a threshold value so that the error accumulated over all           criteria[i] so that               criteria[i] > threshold           is larger than target_error.          
* [0.x.3]*
     This namespace provides a collection of functions that aid in refinement     and coarsening of triangulations. Despite the name of the namespace, the     functions do not actually [1.x.0] the triangulation, but only     [1.x.1]. In other words, they     perform the "mark" part of the typical "solve-estimate-mark-refine"     cycle of the adaptive finite element loop.         In contrast to the functions in namespace  [2.x.0]      the functions in the current namespace are intended for distributed     meshes, i.e., objects of type  [2.x.1]         
*  [2.x.2]     
* [0.x.4]*
       Like  [2.x.3]  but for       parallel distributed triangulations.             The vector of criteria needs to be a vector of refinement criteria       for all cells active on the current triangulation, i.e.,       it needs to be of length  [2.x.4]  (and not        [2.x.5] ). In other words,       the vector needs to include entries for ghost and artificial       cells. However, the current       function will only look at the indicators that correspond to those       cells that are actually locally owned, and ignore the indicators for       all other cells. The function will then coordinate among all       processors that store part of the triangulation so that at the end       a fraction  [2.x.6]  of all  [2.x.7]        active cells are refined, rather than a fraction of the        [2.x.8]  on each processor individually.       In other words, it may be that on some processors, no cells are       refined at all.             The same is true for the fraction of cells that is coarsened.              [2.x.9]  tria The triangulation whose cells this function is       supposed to mark for coarsening and refinement.              [2.x.10]  criteria The refinement criterion for each mesh cell active       on the current triangulation. Entries may not be negative.              [2.x.11]  top_fraction_of_cells The fraction of cells to be refined.       If this number is zero, no cells will be refined. If it equals one,       the result will be flagging for global refinement.              [2.x.12]  bottom_fraction_of_cells The fraction of cells to be       coarsened. If this number is zero, no cells will be coarsened.              [2.x.13]  max_n_cells This argument can be used to specify a maximal       number of cells. If this number is going to be exceeded upon       refinement, then refinement and coarsening fractions are going to be       adjusted in an attempt to reach the maximum number of cells. Be aware       though that through proliferation of refinement due to        [2.x.14]  this number is only an indicator. The       default value of this argument is to impose no limit on the number of       cells.      
* [0.x.5]*
       Like  [2.x.15]  but       for parallel distributed triangulations.             The vector of criteria needs to be a vector of refinement criteria       for all cells active on the current triangulation, i.e.,       it needs to be of length  [2.x.16]  (and not        [2.x.17] ). In other words,       the vector needs to include entries for ghost and artificial       cells. However, the current       function will only look at the indicators that correspond to those       cells that are actually locally owned, and ignore the indicators for       all other cells. The function will then coordinate among all       processors that store part of the triangulation so that at the end       the smallest fraction of  [2.x.18]  (not        [2.x.19]  on each processor       individually)       is refined that together make up a total of  [2.x.20]        of the total error. In other words, it may be that on some       processors, no cells are refined at all.             The same is true for the fraction of cells that is coarsened.              [2.x.21]  tria The triangulation whose cells this function is       supposed to mark for coarsening and refinement.              [2.x.22]  criteria The refinement criterion computed on each mesh cell       active on the current triangulation. Entries may not be negative.              [2.x.23]  top_fraction_of_error The fraction of the total estimate       which should be refined. If this number is zero, no cells will be       refined. If it equals one, the result will be flagging for global       refinement.              [2.x.24]  bottom_fraction_of_error The fraction of the estimate       coarsened. If this number is zero, no cells will be coarsened.              [2.x.25]  norm_type To determine thresholds, combined errors on       subsets of cells are calculated as norms of the criteria on these       cells. Different types of norms can be used for this purpose, from       which  [2.x.26]  and        [2.x.27]  are currently supported.      
* [0.x.6]

include/deal.II-translator/distributed/p4est_wrappers_0.txt
[0.x.0]*
     A structure whose explicit specializations contain alias to the     relevant p4est_* and p8est_* types. Using this structure, for example     by saying  [2.x.0]  we can write code in a     dimension independent way, either referring to p4est_connectivity_t or     p8est_connectivity_t, depending on template argument.    
* [0.x.1]*
     A structure whose explicit specializations contain pointers to the     relevant p4est_* and p8est_* functions. Using this structure, for     example by saying  [2.x.1]  we can write code     in a dimension independent way, either calling p4est_quadrant_compare     or p8est_quadrant_compare, depending on template argument.    
* [0.x.2]*
     This struct templatizes the p4est iterate structs and function     prototypes, which are used to execute callback functions for faces,     edges, and corners that require local neighborhood information, i.e.     the neighboring cells    
* [0.x.3]*
     Initialize the  [2.x.2]  children of the     cell p4est_cell.    
* [0.x.4]*
     Initialize quadrant to represent a coarse cell.    
* [0.x.5]*
     Return whether q1 and q2 are equal    
* [0.x.6]*
     Return whether q1 is an ancestor of q2    
* [0.x.7]*
     Return whether the children of a coarse cell are stored locally    
* [0.x.8]*
     Deep copy a p4est connectivity object.    
* [0.x.9]

include/deal.II-translator/distributed/repartitioning_policy_tools_0.txt
[0.x.0]*
 A namespace with repartitioning policies. These classes return vectors of of the new owners of the active locally owned and ghost cells of a Triangulation object. The returned vectors can be used, e.g., in  [2.x.0]  to create a  [2.x.1]  based on a given Triangulation and the predescribed partition, which can be used to set up a  [2.x.2]  objects.
*  These policies can be also used in context of  [2.x.3]  to prescribe arbitrary partitioning in multgrid levels of global coarsening multigrid schmeme.

* 
* [0.x.1]*
   A base class of a repartitioning policy.  
* [0.x.2]*
     Return a vector of the new owners of the active locally owned and ghost     cells.    
* [0.x.3]*
   A dummy policy that simply returns an empty vector, which is interpreted   in  [2.x.4]    in a way that the triangulation is not repartitioned.  
* [0.x.4]*
   A policy that partitions coarse grids based on a base triangulation   according to a first-child policy. The triangulation to be partitioned   should be able to be obtained by a sequence of (global) coarsening steps.  
* [0.x.5]*
     Constructor taking the base (fine) triangulation.    
* [0.x.6]*
     Number of coarse cells.    
* [0.x.7]*
     Number of global levels.    
* [0.x.8]*
     Index set constructed from the triangulation passed to the constructor.     It contains all the cells that would be owned by the current process     if the levels would be partitioned according to a first-child policy.    
* [0.x.9]*
   A policy that allows to specify a minimal number of cells per process. If   a threshold is reached, processes might be left without cells.  
* [0.x.10]*
     Constructor taking the minimum number of cells per process.    
* [0.x.11]*
     Minimum number of cells per process.    
* [0.x.12]

include/deal.II-translator/distributed/shared_tria_0.txt
[0.x.0]*
     This class provides a parallel triangulation for which every processor     knows about every cell of the global mesh (unlike for the      [2.x.0]  class) but in which cells are     automatically partitioned when run with MPI so that each processor     "owns" a subset of cells. The use of this class is demonstrated in      [2.x.1] .         Different from the  [2.x.2]  and      [2.x.3]  classes, this implies     that the entire mesh is stored on each processor. While this is clearly     a memory bottleneck that limits the use of this class to a few dozen     or hundreds of MPI processes, the partitioning of the mesh can be used     to partition work such as assembly or postprocessing between     participating processors, and it can also be used to partition which     processor stores which parts of matrices and vectors. As a consequence,     using this class is often a gentler introduction to parallelizing a     code than the more involved  [2.x.4]  class     in which processors only know their own part of the mesh, but nothing     about cells owned by other processors with the exception of a single     layer of ghost cells around their own part of the domain.         As a consequence of storing the entire mesh on each processor, active     cells need to be flagged for refinement or coarsening consistently on     all processors if you want to adapt them, regardless of being classified     as locally owned, ghost or artificial.         The class is also useful in cases where compute time and memory     considerations dictate that the program needs to be run in parallel,     but where algorithmic concerns require that every processor knows     about the entire mesh. An example could be where an application     has to have both volume and surface meshes that can then both     be partitioned independently, but where it is difficult to ensure     that the locally owned set of surface mesh cells is adjacent to the     locally owned set of volume mesh cells and the other way around. In     such cases, knowing the [1.x.0] of both meshes ensures that     assembly of coupling terms can be implemented without also     implementing overly complicated schemes to transfer information about     adjacent cells from processor to processor.         The partitioning of cells between processors is done internally     based on a number of different possibilities. By passing appropriate     flags to the constructor of this class (see the      [2.x.5]      enum), it is possible to select different ways of partitioning the mesh,     including ways that are dictated by the application and not by the     desire to minimize the length of the interface between subdomains owned     by processors (as is done by the METIS and Zoltan packages, both of     which are options for partitioning). The DoFHandler class knows how to     enumerate degrees of freedom in ways appropriate for the partitioned     mesh.        
*  [2.x.6]     
* [0.x.1]*
       Configuration flags for distributed Triangulations to be set in the       constructor. Settings can be combined using bitwise OR.             The constructor requires that exactly one of        [2.x.7] ,        [2.x.8]  and        [2.x.9]  is set. If        [2.x.10]  is chosen, it will use        [2.x.11]  (if available), then        [2.x.12]  (if available) and finally        [2.x.13] .      
* [0.x.2]*
         Choose the partitioner depending on the enabled         dependencies that were found when configuring deal.II.  In         particular, if the Trilinos package Zoltan was found, then         use the  [2.x.14]  strategy. If Zoltan was not         found but the METIS package was found, then use the         partition_metis strategy. If neither of these were found,         then use the partition_zorder partitioning strategy.        
* [0.x.3]*
         Use METIS partitioner to partition active cells.        
* [0.x.4]*
         Partition active cells with the same scheme used in the         p4est library.                 The term "Z-order" originates in the fact that cells are         sorted using a space filling curve which in 2d connects the         four children of a cell in the order bottom left, bottom         right, top left, top right (i.e., with a curve that looks         like a reverse "Z"), and does so recursively on all levels         of a triangulation. This is also the order in which         children are enumerated by the GeometryInfo class. The         "Z-order" is also sometimes called "Morton ordering", see         https://en.wikipedia.org/wiki/Z-order_curve .                  [2.x.15]           [2.x.16]  "Z order glossary entry".        
* [0.x.5]*
         Use Zoltan to partition active cells.        
* [0.x.6]*
         Partition cells using a custom, user defined function. This is         accomplished by connecting the post_refinement signal to the         triangulation whenever it is first created and passing the user         defined function through the signal using  [2.x.17] .         Here is an example:        
* [1.x.1]
*                  An equivalent code using lambda functions would look like this:        
* [1.x.2]
*                 
*  [2.x.18]  If you plan to use a custom partition with geometric multigrid,         you must manually partition the level cells in addition to the active         cells.        
* [0.x.7]*
         This flag needs to be set to use the geometric multigrid         functionality. This option requires additional computation and         communication.                 Note: This flag should always be set alongside a flag for an         active cell partitioning method.        
* [0.x.8]*
       Constructor.             The flag  [2.x.19]  can be used to enable artificial       cells. If enabled, this class will behave similarly       to  [2.x.20]  and        [2.x.21]  in the sense that there will       be locally owned cells, a single layer of ghost cells, and       artificial cells. However, one should not forget that in contrast to       those parallel triangulations all cells are duplicated on all       processes, leading in most cases to significantly more artificial       cells.             If artificial cells are disabled, all non-locally owned cells are       considered ghost cells. This might lead to very expensive ghost-value       update steps. While in the case of artificial cells, ghost-value       updates lead to communication only with the direct process neighbors in       a point-to-point fashion, these degenerate to an operation in which       every process communicates with every other process (an "all-to-all"       communication) if no artificial cells are available. If such       ghost-value updates are the bottleneck in your code, you may want to       consider enabling artificial cells.      
* [0.x.9]*
       Destructor.      
* [0.x.10]*
       Return if multilevel hierarchy is supported and has been constructed.      
* [0.x.11]*
       Coarsen and refine the mesh according to refinement and coarsening       flags set.             This step is equivalent to the  [2.x.22]  class with an       addition of calling  [2.x.23]  at       the end.      
* [0.x.12]*
       Create a triangulation.             This function also partitions triangulation based on the MPI       communicator provided to the constructor.      
* [0.x.13]*
        [2.x.24]   [2.x.25]             
*  [2.x.26]  Not implemented yet.      
* [0.x.14]*
       Copy  [2.x.27]  to this triangulation.             This function also partitions triangulation based on the MPI       communicator provided to the constructor.            
*  [2.x.28]  This function can not be used with  [2.x.29]        since it only stores those cells that it owns, one layer of ghost cells       around the ones it locally owns, and a number of artificial cells.      
* [0.x.15]*
       Read the data of this object from a stream for the purpose of       serialization. Throw away the previous content.             This function first does the same work as in        [2.x.30]  then partitions the triangulation based on       the MPI communicator provided to the constructor.      
* [0.x.16]*
       Return a vector of length  [2.x.31]  where each       element stores the subdomain id of the owner of this cell. The       elements of the vector are obviously the same as the subdomain ids       for locally owned and ghost cells, but are also correct for       artificial cells that do not store who the owner of the cell is in       their subdomain_id field.      
* [0.x.17]*
       Return a vector of length  [2.x.32]  where each       element stores the level subdomain id of the owner of this cell. The       elements of the vector are obviously the same as the level subdomain       ids for locally owned and ghost cells, but are also correct for       artificial cells that do not store who the owner of the cell is in       their level_subdomain_id field.      
* [0.x.18]*
       Return allow_artificial_cells , namely true if artificial cells are       allowed.      
* [0.x.19]*
       Settings      
* [0.x.20]*
       A flag to decide whether or not artificial cells are allowed.      
* [0.x.21]*
       This function calls  [2.x.33]  () and if       requested in the constructor of the class marks artificial cells.      
* [0.x.22]*
       A vector containing subdomain IDs of cells obtained by partitioning       using either zorder, METIS, or a user-defined partitioning scheme.       In case allow_artificial_cells is false, this vector is       consistent with IDs stored in cell->subdomain_id() of the       triangulation class. When allow_artificial_cells is true, cells which       are artificial will have cell->subdomain_id() ==  [2.x.34]              The original partition information is stored to allow using sequential       DoF distribution and partitioning functions with semi-artificial       cells.      
* [0.x.23]*
       A vector containing level subdomain IDs of cells obtained by       partitioning each level.             The original partition information is stored to allow using sequential       DoF distribution and partitioning functions with semi-artificial       cells.      
* [0.x.24]*
     Dummy class the compiler chooses for parallel shared triangulations if     we didn't actually configure deal.II with the MPI library. The     existence of this class allows us to refer to      [2.x.35]  objects throughout the library even if     it is disabled.         Since the constructor of this class is deleted, no such objects     can actually be created as this would be pointless given that     MPI is not available.    
* [0.x.25]*
       Constructor. Deleted to make sure that objects of this type cannot be       constructed (see also the class documentation).      
* [0.x.26]*
       Return if multilevel hierarchy is supported and has been constructed.      
* [0.x.27]*
       A dummy function to return empty vector.      
* [0.x.28]*
       A dummy function to return empty vector.      
* [0.x.29]*
       A dummy function which always returns true.      
* [0.x.30]*
       A dummy vector.      
* [0.x.31]*
       A dummy vector.      
* [0.x.32]*
       This class temporarily modifies the subdomain ID of all active cells to       their respective "true" owner.             The modification only happens on  [2.x.36]        objects with artificial cells, and persists for the lifetime of an       instantiation of this class.             The TemporarilyRestoreSubdomainIds class should only be used for       temporary read-only purposes. For example, whenever your implementation       requires to treat artificial cells temporarily as locally relevant to       access their dof indices.             This class has effect only if artificial cells are allowed. Without       artificial cells, the current subdomain IDs already correspond to the       true subdomain IDs. See the  [2.x.37]  "glossary"       for more information about artificial cells.      
* [0.x.33]*
         Constructor.                 Stores the subdomain ID of all active cells if the provided         Triangulation is of type  [2.x.38]                  Replaces them by their true subdomain ID equivalent.        
* [0.x.34]*
         Destructor.                 Returns the subdomain ID of all active cells on the          [2.x.39]  into their previous state.        
* [0.x.35]*
         The modified  [2.x.40]         
* [0.x.36]*
         A vector that temporarily stores the subdomain IDs on all active         cells before they have been modified on the          [2.x.41]         
* [0.x.37]

include/deal.II-translator/distributed/solution_transfer_0.txt
[0.x.0]*
     Transfer a discrete FE function (like a solution vector) by     interpolation while refining and/or coarsening a distributed grid and     handles the necessary communication.        
*  [2.x.0]  It is important to note, that if you use more than one     SolutionTransfer object at the same time, that the calls to prepare_*()     and interpolate()/deserialize() need to be in the same order.         [1.x.0] In a parallel computation PETSc or     Trilinos vector may contain ghost elements or not. For reading in     information with prepare_for_coarsening_and_refinement() or     prepare_for_serialization() you need to supply vectors with ghost     elements, so that all locally_active elements can be read. On the other     hand, ghosted vectors are generally not writable, so for calls to     interpolate() or deserialize() you need to supply distributed vectors     without ghost elements. More precisely, during interpolation the     current algorithm writes into all locally active degrees of freedom.         [1.x.1] Here VectorType is your favorite     vector type, e.g.  [2.x.1]       [2.x.2]  or corresponding block vectors.    
* [1.x.2]
*          As the grid is distributed, it is important to note that the old     solution(s) must be copied to one that also provides access to the     locally relevant DoF values (these values required for the interpolation     process):    
* [1.x.3]
*          Different from PETSc and Trilinos vectors,      [2.x.3]  allows writing into ghost elements.     For a ghosted vector the interpolation step can be accomplished via    
* [1.x.4]
*          [1.x.5]         This class can be used to serialize and later deserialize a distributed     mesh with solution vectors to a file. If you use more than one     DoFHandler and therefore more than one SolutionTransfer object, they     need to be serialized and deserialized in the same order.         If vector has the locally relevant DoFs, serialization works as     follows:    
* [1.x.6]
*      For deserialization the vector needs to be a distributed vector     (without ghost elements):    
* [1.x.7]
*              [1.x.8]         Since data on DoFHandler objects with hp-capabilities is associated with     many different FiniteElement objects, each cell's data has to be     processed with its corresponding `future_fe_index`. Further, if     refinement is involved, data will be packed on the parent cell with its     `future_fe_index` and unpacked later with the same index on its children.     If cells get coarsened into one, data will be packed on the children with     the least dominant finite element of their common subspace, and unpacked     on the parent with this particular finite element (consult      [2.x.4]  for more information).         Transferring a solution across refinement works exactly like in the     non-hp-case. However, when considering serialization, we also have to     store the active FE indices in an additional step. A code snippet     demonstrating serialization with the      [2.x.5]  class with DoFHandler objects     with hp-capabilities is provided in the following. Here VectorType is     your favorite vector type, e.g.  [2.x.6]       [2.x.7]  or corresponding block vectors.         If vector has the locally relevant DoFs, serialization works as follows:    
* [1.x.9]
*          For deserialization the vector needs to be a distributed vector     (without ghost elements):    
* [1.x.10]
*              [1.x.11]         In essence, this class implements the same steps as does      [2.x.8]  (though the implementation is entirely     separate). Consequently, the same issue with hanging nodes and     coarsening can happen with this class as happens with      [2.x.9]  See there for an extended discussion.        
*  [2.x.10]     
* [0.x.1]*
       Constructor.              [2.x.11]  dof The DoFHandler on which all operations will happen.         At the time when this constructor is called, the DoFHandler still         points to the Triangulation before the refinement in question         happens.      
* [0.x.2]*
       Destructor.      
* [0.x.3]*
       Prepare the current object for coarsening and refinement. It       stores the dof indices of each cell and stores the dof values of the       vectors in  [2.x.12]  in each cell that'll be coarsened.  [2.x.13]        includes all vectors that are to be interpolated onto the new       (refined and/or coarsened) grid.      
* [0.x.4]*
       Same as the previous function but for only one discrete function to be       interpolated.      
* [0.x.5]*
       Interpolate the data previously stored in this object before the mesh       was refined or coarsened onto the current set of cells. Do so for       each of the vectors provided to       prepare_for_coarsening_and_refinement() and write the result into the       given set of vectors.      
* [0.x.6]*
       Same as the previous function. It interpolates only one function. It       assumes the vectors having the right sizes (i.e.       <tt>in.size()==n_dofs_old</tt>, <tt>out.size()==n_dofs_refined</tt>)             Multiple calling of this function is NOT allowed. Interpolating       several functions can be performed in one step by using       <tt>interpolate (all_in, all_out)</tt>      
* [0.x.7]*
       Prepare the serialization of the given vector. The serialization is       done by  [2.x.14]  The given vector needs all information       on the locally active DoFs (it must be ghosted). See documentation of       this class for more information.      
* [0.x.8]*
       Same as the function above, only for a list of vectors.      
* [0.x.9]*
       Execute the deserialization of the given vector. This needs to be       done after calling  [2.x.15]  The given vector must be a       fully distributed vector without ghost elements. See documentation of       this class for more information.      
* [0.x.10]*
       Same as the function above, only for a list of vectors.      
* [0.x.11]*
       Pointer to the degree of freedom handler to work with.      
* [0.x.12]*
       A vector that stores pointers to all the vectors we are supposed to       copy over from the old to the new mesh.      
* [0.x.13]*
       The handle that the Triangulation has assigned to this object       with which we can access our memory offset and our pack function.      
* [0.x.14]*
       A callback function used to pack the data on the current mesh into       objects that can later be retrieved after refinement, coarsening and       repartitioning.      
* [0.x.15]*
       A callback function used to unpack the data on the current mesh that       has been packed up previously on the mesh before refinement,       coarsening and repartitioning.      
* [0.x.16]*
       Registers the pack_callback() function to the        [2.x.16]  that has been assigned to the       DoFHandler class member and stores the returning handle.      
* [0.x.17]*
        [2.x.17]  Use  [2.x.18]        without the DoFHandlerType template instead.      
* [0.x.18]

include/deal.II-translator/distributed/tria_0.txt
[0.x.0]*
     This class acts like the  [2.x.0]  class, but it     distributes the mesh across a number of different processors when using     MPI. The class's interface does not add a lot to the      [2.x.1]  class but there are a number of difficult     algorithms under the hood that ensure we always have a load-balanced,     fully distributed mesh. Use of this class is explained in  [2.x.2] ,      [2.x.3] , the      [2.x.4]      documentation module, as well as the      [2.x.5] .     See there for more information. This class satisfies the      [2.x.6]  "MeshType concept".        
*  [2.x.7]  This class does not support anisotropic refinement, because it     relies on the p4est library that does not support this. Attempts to     refine cells anisotropically will result in errors.        
*  [2.x.8]  There is currently no support for distributing 1d triangulations.             [1.x.0]         Refining and coarsening a distributed triangulation is a complicated     process because cells may have to be migrated from one processor to     another. On a single processor, materializing that part of the global     mesh that we want to store here from what we have stored before     therefore may involve several cycles of refining and coarsening the     locally stored set of cells until we have finally gotten from the     previous to the next triangulation. This process is described in more     detail in the      [2.x.9] .     Unfortunately, in this process, some information can get lost relating     to flags that are set by user code and that are inherited from mother     to child cell but that are not moved along with a cell if that cell is     migrated from one processor to another.         An example are boundary indicators. Assume, for example, that you start     with a single cell that is refined once globally, yielding four     children. If you have four processors, each one owns one cell. Assume     now that processor 1 sets the boundary indicators of the external     boundaries of the cell it owns to 42. Since processor 0 does not own     this cell, it doesn't set the boundary indicators of its ghost cell     copy of this cell. Now, assume we do several mesh refinement cycles and     end up with a configuration where this processor suddenly finds itself     as the owner of this cell. If boundary indicator 42 means that we need     to integrate Neumann boundary conditions along this boundary, then     processor 0 will forget to do so because it has never set the boundary     indicator along this cell's boundary to 42.         The way to avoid this dilemma is to make sure that things like setting     boundary indicators or material ids is done immediately every time a     parallel triangulation is refined. This is not necessary for sequential     triangulations because, there, these flags are inherited from mother to     child cell and remain with a cell even if it is refined and the     children are later coarsened again, but this does not hold for     distributed triangulations. It is made even more difficult by the fact     that in the process of refining a parallel distributed triangulation,     the triangulation may call      [2.x.10]  multiple times     and this function needs to know about boundaries. In other words, it is     [1.x.1] enough to just set boundary indicators on newly created     faces only [1.x.2] calling      [2.x.11]      it actually has to happen while that function is still running.         The way to do this is by writing a function that sets boundary     indicators and that will be called by the  [2.x.12]  class.     The triangulation does not provide a pointer to itself to the function     being called, nor any other information, so the trick is to get this     information into the function. C++ provides a nice mechanism for this     that is best explained using an example:    
* [1.x.3]
*          The object passed as argument to  [2.x.13]  is an object     that can be called like a function with no arguments. It does so by     wrapping a function that does, in fact, take an argument but this one     argument is stored as a reference to the coarse grid triangulation when     the lambda function is created. After each refinement step, the     triangulation will then call the object so created which will in turn     call  [2.x.14]  with the reference to the coarse     grid as argument.         This approach can be generalized. In the example above, we have used a     global function that will be called. However, sometimes it is necessary     that this function is in fact a member function of the class that     generates the mesh, for example because it needs to access run-time     parameters. This can be achieved as follows: assuming the      [2.x.15]  function has been declared as a (non-     static, but possibly private) member function of the      [2.x.16]  class, then the following will work:    
* [1.x.4]
*      The lambda function above again is an object that can     be called like a global function with no arguments, and this object in     turn calls the current object's member function      [2.x.17]  with a reference to the triangulation to     work on. Note that     because the  [2.x.18]  function is declared as      [2.x.19] , it is necessary that the      [2.x.20]  function is also declared      [2.x.21] .         [1.x.5]For reasons that have to do with the way the      [2.x.22]  is implemented, functions that     have been attached to the post-refinement signal of the triangulation     are called more than once, sometimes several times, every time the     triangulation is actually refined.            
*  [2.x.23]     
* [0.x.1]*
       An alias that is used to identify cell iterators. The concept of       iterators is discussed at length in the        [2.x.24]  "iterators documentation module".             The current alias identifies cells in a triangulation. You can find       the exact type it refers to in the base class's own alias, but it       should be TriaIterator<CellAccessor<dim,spacedim> >. The TriaIterator       class works like a pointer that when you dereference it yields an       object of type CellAccessor. CellAccessor is a class that identifies       properties that are specific to cells in a triangulation, but it is       derived (and consequently inherits) from TriaAccessor that describes       what you can ask of more general objects (lines, faces, as well as       cells) in a triangulation.            
*  [2.x.25]       
* [0.x.2]*
       An alias that is used to identify        [2.x.26]  "active cell iterators".       The concept of iterators is discussed at length in the        [2.x.27]  "iterators documentation module".             The current alias identifies active cells in a triangulation. You       can find the exact type it refers to in the base class's own alias,       but it should be TriaActiveIterator<CellAccessor<dim,spacedim> >. The       TriaActiveIterator class works like a pointer to active objects that       when you dereference it yields an object of type CellAccessor.       CellAccessor is a class that identifies properties that are specific       to cells in a triangulation, but it is derived (and consequently       inherits) from TriaAccessor that describes what you can ask of more       general objects (lines, faces, as well as cells) in a triangulation.            
*  [2.x.28]       
* [0.x.3]*
       Configuration flags for distributed Triangulations to be set in the       constructor. Settings can be combined using bitwise OR.      
* [0.x.4]*
         Default settings, other options are disabled.        
* [0.x.5]*
         If set, the deal.II mesh will be reconstructed from the coarse mesh         every time a repartitioning in p4est happens. This can be a bit more         expensive, but guarantees the same memory layout and therefore cell         ordering in the deal.II mesh. As assembly is done in the deal.II         cell ordering, this flag is required to get reproducible behavior         after snapshot/resume.        
* [0.x.6]*
         This flags needs to be set to use the geometric multigrid         functionality. This option requires additional computation and         communication.        
* [0.x.7]*
         Setting this flag will disable automatic repartitioning of the cells         after a refinement cycle. It can be executed manually by calling         repartition().        
* [0.x.8]*
       Constructor.              [2.x.29]  mpi_communicator The MPI communicator to be used for       the triangulation.              [2.x.30]  smooth_grid Degree and kind of mesh smoothing to be applied to       the mesh. See the  [2.x.31]  class for a description of       the kinds of smoothing operations that can be applied.              [2.x.32]  settings See the description of the Settings enumerator.       Providing  [2.x.33]  enforces        [2.x.34]        for smooth_grid.            
*  [2.x.35]  This class does not currently support the        [2.x.36]  argument provided by the base       class.            
*  [2.x.37]  While it is possible to pass all of the mesh smoothing flags       listed in the base class to objects of this type, it is not always       possible to honor all of these smoothing options if they would       require knowledge of refinement/coarsening flags on cells not locally       owned by this processor. As a consequence, for some of these flags,       the ultimate number of cells of the parallel triangulation may depend       on the number of processors into which it is partitioned. On the       other hand, if no smoothing flags are passed, if you always mark the       same cells of the mesh, you will always get the exact same refined       mesh independent of the number of processors into which the       triangulation is partitioned.      
* [0.x.9]*
       Destructor.      
* [0.x.10]*
       Reset this triangulation into a virgin state by deleting all data.             Note that this operation is only allowed if no subscriptions to this       object exist any more, such as DoFHandler objects using it.      
* [0.x.11]*
       Return if multilevel hierarchy is supported and has been constructed.      
* [0.x.12]*
       Transfer data across forests.             Besides the actual  [2.x.38]  which has been already refined       and repartitioned, this function also needs information about its       previous state, i.e. the locally owned intervals in p4est's       sc_array of each processor. This information needs to be memcopyied       out of the old p4est object and has to be provided via the parameter        [2.x.39]              Data has to be previously packed with        [2.x.40]       
* [0.x.13]*
       Implementation of the same function as in the base class.            
*  [2.x.41]  This function can be used to copy a serial Triangulation to a        [2.x.42]  but only if the serial       Triangulation has never been refined.      
* [0.x.14]*
       Create a triangulation as documented in the base class.             This function also sets up the various data structures necessary to       distribute a mesh across a number of processors. This will be       necessary once the mesh is being refined, though we will always keep       the entire coarse mesh that is generated by this function on all       processors.      
* [0.x.15]*
        [2.x.43]   [2.x.44]             
*  [2.x.45]  Not implemented yet.      
* [0.x.16]*
       Coarsen and refine the mesh according to refinement and coarsening       flags set.             Since the current processor only has control over those cells it owns       (i.e. the ones for which <code>cell- [2.x.46]  ==       this- [2.x.47]  refinement and coarsening       flags are only respected for those locally owned cells. Flags may be       set on other cells as well (and may often, in fact, if you call        [2.x.48]  but will       be largely ignored: the decision to refine the global mesh will only       be affected by flags set on locally owned cells.            
*  [2.x.49]  This function by default partitions the mesh in such a way that       the number of cells on all processors is roughly equal. If you want       to set weights for partitioning, e.g. because some cells are more       expensive to compute than others, you can use the signal cell_weight       as documented in the  [2.x.50]  class. This function will       check whether a function is connected to the signal and if so use it.       If you prefer to repartition the mesh yourself at user-defined       intervals only, you can create your triangulation object by passing       the  [2.x.51]        flag to the constructor, which ensures that calling the current       function only refines and coarsens the triangulation, but doesn't       partition it. You can then call the repartition() function manually.       The usage of the cell_weights signal is identical in both cases, if a       function is connected to the signal it will be used to balance the       calculated weights, otherwise the number of cells is balanced.      
* [0.x.17]*
       Override the implementation of prepare_coarsening_and_refinement from       the base class. This is necessary if periodic boundaries are enabled       and the level difference over vertices over the periodic boundary       must not be more than 2:1.      
* [0.x.18]*
       Manually repartition the active cells between processors. Normally       this repartitioning will happen automatically when calling       execute_coarsening_and_refinement() (or refine_global()) unless the        [2.x.52]  is set in the constructor. Setting the       flag and then calling repartition() gives the same result.             If you want to transfer data (using SolutionTransfer or manually with       register_data_attach() and notify_ready_to_unpack()), you need to set       it up twice: once when calling execute_coarsening_and_refinement(),       which will handle coarsening and refinement but obviously won't ship       any data between processors, and a second time when calling       repartition().  Here, no coarsening and refinement will be done but       information will be packed and shipped to different processors. In       other words, you probably want to treat a call to repartition() in       the same way as execute_coarsening_and_refinement() with respect to       dealing with data movement (SolutionTransfer, etc.).            
*  [2.x.53]  If no function is connected to the cell_weight signal described       in the  [2.x.54]  class, this function will balance the       number of cells on each processor. If one or more functions are       connected, it will calculate the sum of the weights and balance the       weights across processors. The only requirement on the weights is       that every cell's weight is positive and that the sum over all       weights on all processors can be formed using a 64-bit integer.       Beyond that, it is your choice how you want to interpret the weights.       A common approach is to consider the weights proportional to the cost       of doing computations on a cell, e.g., by summing the time for       assembly and solving. In practice, determining this cost is of course       not trivial since we don't solve on isolated cells, but on the entire       mesh. In such cases, one could, for example, choose the weight equal       to the number of unknowns per cell (in the context of hp-finite       element methods), or using a heuristic that estimates the cost on       each cell depending on whether, for example, one has to run some       expensive algorithm on some cells but not others (such as forming       boundary integrals during the assembly only on cells that are       actually at the boundary, or computing expensive nonlinear terms only       on some cells but not others, e.g., in the elasto-plastic problem in        [2.x.55] ).      
* [0.x.19]*
       Return true if the triangulation has hanging nodes.             In the context of parallel distributed triangulations, every       processor stores only that part of the triangulation it locally owns.       However, it also stores the entire coarse mesh, and to guarantee the       2:1 relationship between cells, this may mean that there are hanging       nodes between cells that are not locally owned or ghost cells (i.e.,       between ghost cells and artificial cells, or between artificial and       artificial cells; see        [2.x.56]  "the glossary").       One is not typically interested in this case, so the function returns       whether there are hanging nodes between any two cells of the "global"       mesh, i.e., the union of locally owned cells on all processors.      
* [0.x.20]*
       Return the local memory consumption in bytes.      
* [0.x.21]*
       Return the local memory consumption contained in the p4est data       structures alone. This is already contained in memory_consumption()       but made available separately for debugging purposes.      
* [0.x.22]*
       A collective operation that produces a sequence of output files with       the given file base name that contain the mesh in VTK format.             More than anything else, this function is useful for debugging the       interface between deal.II and p4est.      
* [0.x.23]*
       Produce a check sum of the triangulation.  This is a collective       operation and is mostly useful for debugging purposes.      
* [0.x.24]*
       Save the refinement information from the coarse mesh into the given       file. This file needs to be reachable from all nodes in the       computation on a shared network file system. See the SolutionTransfer       class on how to store solution vectors into this file. Additional       cell-based data can be saved using        [2.x.57]       
* [0.x.25]*
       Load the refinement information saved with save() back in. The mesh       must contain the same coarse mesh that was used in save() before       calling this function.             You do not need to load with the same number of MPI processes that       you saved with. Rather, if a mesh is loaded with a different number       of MPI processes than used at the time of saving, the mesh is       repartitioned appropriately. Cell-based data that was saved with        [2.x.58]  can       be read in with        [2.x.59]        after calling load().             If you use p4est version > 0.3.4.2 the  [2.x.60]  flag tells       p4est to ignore the partitioning that the triangulation had when it       was saved and make it uniform upon loading. If  [2.x.61]  is       set to false, the triangulation is only repartitioned if needed (i.e.       if a different number of MPI processes is encountered).      
* [0.x.26]*
       Load the refinement information from a given parallel forest. This       forest might be obtained from the function call to        [2.x.62]       
* [0.x.27]*
       Return a permutation vector for the order the coarse cells are handed       off to p4est. For example the value of the  [2.x.63] th element in this       vector is the index of the deal.II coarse cell (counting from       begin(0)) that corresponds to the  [2.x.64] th tree managed by p4est.      
* [0.x.28]*
       Return a permutation vector for the mapping from the coarse deal       cells to the p4est trees. This is the inverse of       get_p4est_tree_to_coarse_cell_permutation.      
* [0.x.29]*
       This returns a pointer to the internally stored p4est object (of type       p4est_t or p8est_t depending on  [2.x.65]               [2.x.66]  If you modify the p4est object, internal data structures       can become inconsistent.      
* [0.x.30]*
       In addition to the action in the base class Triangulation, this       function joins faces in the p4est forest for periodic boundary       conditions. As a result, each pair of faces will differ by at most one       refinement level and ghost neighbors will be available across these       faces.             The vector can be filled by the function        [2.x.67]              For more information on periodic boundary conditions see        [2.x.68]         [2.x.69]  and  [2.x.70] .            
*  [2.x.71]  Before this function can be used the Triangulation has to be       initialized and must not be refined. Calling this function more than       once is possible, but not recommended: The function destroys and       rebuilds the p4est forest each time it is called.      
* [0.x.31]*
       store the Settings.      
* [0.x.32]*
       A flag that indicates whether the triangulation has actual content.      
* [0.x.33]*
       A data structure that holds the connectivity between trees. Since       each tree is rooted in a coarse grid cell, this data structure holds       the connectivity between the cells of the coarse grid.      
* [0.x.34]*
       A data structure that holds the local part of the global       triangulation.      
* [0.x.35]*
       A data structure that holds some information about the ghost cells of       the triangulation.      
* [0.x.36]*
       Go through all p4est trees and record the relations between locally       owned p4est quadrants and active deal.II cells in the private member       vector local_cell_relations.             The vector contains an active cell iterator for every locally owned       p4est quadrant, as well as a CellStatus flag to describe their       relation.             The stored vector will be ordered by the occurrence of quadrants in       the corresponding local sc_array of the parallel_forest. p4est requires       this specific ordering for its transfer functions. Therefore, the size       of this vector will be equal to the number of locally owned quadrants       in the parallel_forest object.             These relations will be established for example in the mesh refinement       process: after adapting the parallel_forest, but before applying these       changes to this triangulation, we will record how cells will change in       the refinement process. With this information, we can prepare all       buffers for data transfer accordingly.      
* [0.x.37]*
       Two arrays that store which p4est tree corresponds to which coarse       grid cell and vice versa. We need these arrays because p4est goes       with the original order of coarse cells when it sets up its forest,       and then applies the Morton ordering within each tree. But if coarse       grid cells are badly ordered this may mean that individual parts of       the forest stored on a local machine may be split across coarse grid       cells that are not geometrically close. Consequently, we apply a       hierarchical preordering according to        [2.x.72]  to ensure that the part of the       forest stored by p4est is located on geometrically close coarse grid       cells.      
* [0.x.38]*
       Return a pointer to the p4est tree that belongs to the given       dealii_coarse_cell_index()      
* [0.x.39]*
       The function that computes the permutation between the two data       storage schemes.      
* [0.x.40]*
       Take the contents of a newly created triangulation we are attached to       and copy it to p4est data structures.             This function exists in 2d and 3d variants.      
* [0.x.41]*
       Copy the local part of the refined forest from p4est into the       attached triangulation.      
* [0.x.42]*
       Internal function notifying all registered slots to provide their       weights before repartitioning occurs. Called from       execute_coarsening_and_refinement() and repartition().              [2.x.73]  A vector of unsigned integers representing the weight or       computational load of every cell after the refinement/coarsening/       repartition cycle. Note that the number of entries does not need to       be equal to either n_active_cells() or n_locally_owned_active_cells(),       because the triangulation is not updated yet. The weights are sorted       in the order that p4est will encounter them while iterating over       them.      
* [0.x.43]*
       This method returns a bit vector of length tria.n_vertices()       indicating the locally active vertices on a level, i.e., the vertices       touched by the locally owned level cells for use in geometric       multigrid (possibly including the vertices due to periodic boundary       conditions) are marked by true.             Used by  [2.x.74]       
* [0.x.44]*
     Specialization of the general template for the 1d case. There is     currently no support for distributing 1d triangulations. Consequently,     all this class does is throw an exception.    
* [0.x.45]*
       dummy settings      
* [0.x.46]*
       Constructor. The argument denotes the MPI communicator to be used for       the triangulation.      
* [0.x.47]*
       Destructor.      
* [0.x.48]*
       Return a permutation vector for the order the coarse cells are       handed of to p4est. For example the first element i in this vector       denotes that the first cell in hierarchical ordering is the ith deal       cell starting from begin(0).      
* [0.x.49]*
       This function is not implemented, but needs to be present for the       compiler.      
* [0.x.50]*
       This function is not implemented, but needs to be present for the       compiler.      
* [0.x.51]*
       This function is not implemented, but needs to be present for the       compiler.      
* [0.x.52]*
       This function is not implemented, but needs to be present for the       compiler.      
* [0.x.53]*
       Dummy arrays. This class isn't usable but the compiler wants to see       these variables at a couple places anyway.      
* [0.x.54]*
       This method, which is only implemented for dim = 2 or 3,       needs a stub because it is used in dof_handler_policy.cc      
* [0.x.55]*
       Like above, this method, which is only implemented for dim = 2 or 3,       needs a stub because it is used in dof_handler_policy.cc      
* [0.x.56]*
     Dummy class the compiler chooses for parallel distributed     triangulations if we didn't actually configure deal.II with the p4est     library. The existence of this class allows us to refer to      [2.x.75]  objects throughout the library     even if it is disabled.         Since the constructor of this class is deleted, no such objects     can actually be created as this would be pointless given that     p4est is not available.    
* [0.x.57]*
       Dummy settings to allow defining the deleted constructor.      
* [0.x.58]*
       Constructor. Deleted to make sure that objects of this type cannot be       constructed (see also the class documentation).      
* [0.x.59]*
       Dummy replacement to allow for better error messages when compiling       this class.      
* [0.x.60]*
       Dummy replacement to allow for better error messages when compiling       this class.      
* [0.x.61]*
       Dummy replacement to allow for better error messages when compiling       this class.      
* [0.x.62]*
       Dummy replacement to allow for better error messages when compiling       this class.      
* [0.x.63]*
     This class temporarily modifies the refine and coarsen flags of all     active cells to match the p4est oracle.         The modification only happens on  [2.x.76]      objects, and persists for the lifetime of an instantiation of this     class.         The TemporarilyMatchRefineFlags class should only be used in     combination with the  [2.x.77]      signal. At this stage, the p4est oracle already has been refined, but     the triangulation is still unchanged. After the modification, all     refine and coarsen flags describe how the traingulation will actually     be refined.         The use of this class is demonstrated in  [2.x.78] .    
* [0.x.64]*
       Constructor.             Stores the refine and coarsen flags of all active cells if the       provided Triangulation is of type        [2.x.79]              Adjusts them to be consistent with the p4est oracle.      
* [0.x.65]*
       Destructor.             Returns the refine and coarsen flags of all active cells on the        [2.x.80]  into their previous state.      
* [0.x.66]*
       The modified  [2.x.81]       
* [0.x.67]*
       A vector that temporarily stores the refine flags before they have       been modified on the  [2.x.82]       
* [0.x.68]*
       A vector that temporarily stores the coarsen flags before they have       been modified on the  [2.x.83]       
* [0.x.69]

include/deal.II-translator/distributed/tria_base_0.txt
[0.x.0]*
   This class describes the interface for all triangulation classes that   work in parallel, namely  [2.x.0]     [2.x.1]  and    [2.x.2]      It is, consequently, a class that can be used to test whether a   pointer of reference to a triangulation object refers to a   sequential triangulation, or whether the triangulation is in fact   parallel. In other words, one could write a function like this:  
* [1.x.0]
*      All parallel triangulations share certain traits, such as the fact that   they communicate via    [2.x.3]  "MPI communicators"   or that they have    [2.x.4]  "locally owned",    [2.x.5]  "ghost", and possibly    [2.x.6]  "artificial cells".   This class provides   a number of member functions that allows querying some information   about the triangulation that is independent of how exactly a   parallel triangulation is implemented (i.e., which of the various   classes derived from the current one it actually is).  
* [0.x.1]*
     Constructor.    
* [0.x.2]*
     Destructor.    
* [0.x.3]*
     Return MPI communicator used by this triangulation.    
* [0.x.4]*
     Return if multilevel hierarchy is supported and has been constructed.    
* [0.x.5]*
     Implementation of the same function as in the base class.        
*  [2.x.7]  This function copies the cells, but not the communicator,     of the source triangulation. In other words, the resulting     triangulation will operate on the communicator it was constructed     with.    
* [0.x.6]*
     Return the number of active cells in the triangulation that are locally     owned, i.e. that have a subdomain_id equal to     locally_owned_subdomain(). Note that there may be more active cells in     the triangulation stored on the present processor, such as for example     ghost cells, or cells further away from the locally owned block of     cells but that are needed to ensure that the triangulation that stores     this processor's set of active cells still remains balanced with     respect to the 2:1 size ratio of adjacent cells.         As a consequence of the remark above, the result of this function is     always smaller or equal to the result of the function with the same     name in the ::Triangulation base class, which includes the active ghost     and artificial cells (see also      [2.x.8]      and      [2.x.9] ).    
* [0.x.7]*
     Return the sum over all processors of the number of active cells owned     by each processor. This equals the overall number of active cells in     the triangulation.    
* [0.x.8]*
     Return the local memory consumption in bytes.    
* [0.x.9]*
     Return the global maximum level. This may be bigger than the number      [2.x.10]  (a function in this class's base     class) returns if the current processor only stores cells in parts of     the domain that are not very refined, but if other processors store     cells in more deeply refined parts of the domain.    
* [0.x.10]*
     Return the subdomain id of those cells that are owned by the current     processor. All cells in the triangulation that do not have this     subdomain id are either owned by another processor or have children     that only exist on other processors.    
* [0.x.11]*
     Return a set of MPI ranks of the processors that have at least one     ghost cell adjacent to the cells of the local processor. In other     words, this is the set of subdomain_id() for all ghost cells.         The returned sets are symmetric, that is if  [2.x.11]  is contained in the     list of processor  [2.x.12]  then  [2.x.13]  will also be contained in the list of     processor  [2.x.14]     
* [0.x.12]*
     Return a set of MPI ranks of the processors that have at least one     level ghost cell adjacent to our cells used in geometric multigrid. In     other words, this is the set of level_subdomain_id() for all level     ghost cells.         The returned sets are symmetric, that is if  [2.x.15]  is contained in the     list of processor  [2.x.16]  then  [2.x.17]  will also be contained in the list of     processor  [2.x.18]         
*  [2.x.19]  The level ghost owners can only be determined if the multigrid     ownership has been assigned (by setting the     construct_multigrid_hierarchy flag at construction time), otherwise the     returned set will be empty.    
* [0.x.13]*
     Return partitioner for the global indices of the cells on the active     level of the triangulation.    
* [0.x.14]*
     Return partitioner for the global indices of the cells on the given  [2.x.20]      level of the triangulation.    
* [0.x.15]*
     Return a map that, for each vertex, lists all the processors whose     subdomains are adjacent to that vertex.          [2.x.21]  Use  [2.x.22]      instead of      [2.x.23]     
* [0.x.16]*
      [2.x.24]   [2.x.25]         
*  [2.x.26]  This function involves a global communication gathering all current       IDs from all processes.    
* [0.x.17]*
      [2.x.27]   [2.x.28]         
*  [2.x.29]  This function involves a global communication gathering all current       IDs from all processes.    
* [0.x.18]*
     When vertices have been moved locally, for example using code like    
* [1.x.1]
*      then this function can be used to update the location of vertices     between MPI processes.         All the vertices that have been moved and might be in the ghost layer     of a process have to be reported in the  [2.x.30]      argument. This ensures that that part of the information that has to     be send between processes is actually sent. Additionally, it is quite     important that vertices on the boundary between processes are     reported on exactly one process (e.g. the one with the highest id).     Otherwise we could expect undesirable results if multiple processes     move a vertex differently. A typical strategy is to let processor  [2.x.31]      move those vertices that are adjacent to cells whose owners include     processor  [2.x.32]  but no other processor  [2.x.33]  with  [2.x.34] ; in other words,     for vertices at the boundary of a subdomain, the processor with the     lowest subdomain id "owns" a vertex.        
*  [2.x.35]  It only makes sense to move vertices that are either located on     locally owned cells or on cells in the ghost layer. This is because     you can be sure that these vertices indeed exist on the finest mesh     aggregated over all processors, whereas vertices on artificial cells     but not at least in the ghost layer may or may not exist on the     globally finest mesh. Consequently, the  [2.x.36]      argument may not contain vertices that aren't at least on ghost     cells.        
*  [2.x.37]  This function moves vertices in such a way that on every     processor, the vertices of every locally owned and ghost cell is     consistent with the corresponding location of these cells on other     processors. On the other hand, the locations of artificial cells will     in general be wrong since artificial cells may or may not exist on     other processors and consequently it is not possible to determine     their location in any way. This is not usually a problem since one     never does anything on artificial cells. However, it may lead to     problems if the mesh with moved vertices is refined in a later step.     If that's what you want to do, the right way to do it is to save the     offset applied to every vertex, call this function, and before     refining or coarsening the mesh apply the opposite offset and call     this function again.          [2.x.38]  vertex_locally_moved A bitmap indicating which vertices have     been moved. The size of this array must be equal to      [2.x.39]  and must be a subset of those vertices     flagged by  [2.x.40]           [2.x.41]  This function is used, for example, in      [2.x.42]     
* [0.x.19]*
     MPI communicator to be used for the triangulation. We create a unique     communicator for this class, which is a duplicate of the one passed to     the constructor.    
* [0.x.20]*
     The subdomain id to be used for the current processor. This is the MPI     rank.    
* [0.x.21]*
     The total number of subdomains (or the size of the MPI communicator).    
* [0.x.22]*
     A structure that contains information about the distributed     triangulation.    
* [0.x.23]*
       Number of locally owned active cells of this MPI rank.      
* [0.x.24]*
       The total number of active cells (sum of  [2.x.43]        n_locally_owned_active_cells).      
* [0.x.25]*
       The global number of levels computed as the maximum number of levels       taken over all MPI ranks, so <tt>n_levels()<=n_global_levels =       max(n_levels() on proc i)</tt>.      
* [0.x.26]*
       A set containing the subdomain_id (MPI rank) of the owners of the       ghost cells on this processor.      
* [0.x.27]*
       A set containing the MPI ranks of the owners of the level ghost cells       on this processor (for all levels).      
* [0.x.28]*
       Partitioner for the global active cell indices.      
* [0.x.29]*
       Partitioner for the global level cell indices for each level.      
* [0.x.30]*
     Update the number_cache variable after mesh creation or refinement.    
* [0.x.31]*
      [2.x.44]   [2.x.45]     
* [0.x.32]*
     Reset global active cell indices and global level cell indices.    
* [0.x.33]*
   A base class for distributed triangulations, i.e., triangulations that   do not store all cells on all processors. This implies that not   every detail of a triangulation may be known on each processor.   In particular, you have to expect that triangulations of classes   derived from this one only store some of the active cells (namely,   the    [2.x.46]  "locally owned cells"),   along with    [2.x.47]  "ghost cells"   and possibly    [2.x.48]  "artificial cells".   In contrast to the classes   derived from  [2.x.49]  it is certain that the   classes derived from the current class will not store the entire   triangulation as long as it has a large enough number of cells. (The   difference to  [2.x.50]  is that the    [2.x.51]  is derived from    [2.x.52]  but not from the current class.) The   distinction is not large in practice: Everything that is difficult for   parallel distributed triangulation is generally also difficult for any   other kind of parallel triangulation classes; however, this intermediate   base class allows to further differentiate between the different kinds of   classes providing parallel mesh functionality.     This class can, then, be used to test whether a   pointer or reference to a triangulation object refers to any kind of   parallel triangulation, or whether the triangulation is in fact   parallel distributed. In other words, one could write a function like   this:  
* [1.x.2]
*   
* [0.x.34]*
     Constructor.    
* [0.x.35]*
     Reset this triangulation into a virgin state by deleting all data.         Note that this operation is only allowed if no subscriptions to this     object exist any more, such as DoFHandler objects using it.    
* [0.x.36]*
     Save the triangulation into the given file. This file needs to be     reachable from all nodes in the computation on a shared network file     system. See the SolutionTransfer class on how to store solution vectors     into this file. Additional cell-based data can be saved using     register_data_attach().    
* [0.x.37]*
     Load the triangulation saved with save() back in. Cell-based data that     was saved with register_data_attach() can be read in with     notify_ready_to_unpack() after calling load().    
* [0.x.38]*
     Register a function that can be used to attach data of fixed size     to cells. This is useful for two purposes: (i) Upon refinement and     coarsening of a triangulation ( [2.x.53]  e.g. in      [2.x.54]      one needs to be able to store one or more data vectors per cell that     characterizes the solution values on the cell so that this data can     then be transferred to the new owning processor of the cell (or     its parent/children) when the mesh is re-partitioned; (ii) when     serializing a computation to a file, it is necessary to attach     data to cells so that it can be saved ( [2.x.55]  e.g. in      [2.x.56]  along with the cell's     other information and, if necessary, later be reloaded from disk     with a different subdivision of cells among the processors.         The way this function works is that it allows any number of interest     parties to register their intent to attach data to cells. One example     of classes that do this is  [2.x.57]      where each  [2.x.58]  object that works     on the current Triangulation object then needs to register its intent.     Each of these parties registers a callback function (the first     argument here,  [2.x.59]  that will be called whenever the     triangulation's execute_coarsening_and_refinement() or save()     functions are called.         The current function then returns an integer handle that corresponds     to the number of data set that the callback provided here will attach.     While this number could be given a precise meaning, this is     not important: You will never actually have to do anything with     this number except return it to the notify_ready_to_unpack() function.     In other words, each interested party (i.e., the caller of the current     function) needs to store their respective returned handle for later use     when unpacking data in the callback provided to     notify_ready_to_unpack().         Whenever  [2.x.60]  is then called by     execute_coarsening_and_refinement() or load() on a given cell, it     receives a number of arguments. In particular, the first     argument passed to the callback indicates the cell for which     it is supposed to attach data. This is always an active cell.         The second, CellStatus, argument provided to the callback function     will tell you if the given cell will be coarsened, refined, or will     persist as is. (This status may be different than the refinement     or coarsening flags set on that cell, to accommodate things such as     the "one hanging node per edge" rule.). These flags need to be     read in context with the p4est quadrant they belong to, as their     relations are gathered in local_cell_relations.         Specifically, the values for this argument mean the following:
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - `CELL_PERSIST`: The cell won't be refined/coarsened, but might be     moved to a different processor. If this is the case, the callback     will want to pack up the data on this cell into an array and store     it at the provided address for later unpacking wherever this cell     may land.
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - `CELL_REFINE`: This cell will be refined into 4 or 8 cells (in 2d     and 3d, respectively). However, because these children don't exist     yet, you cannot access them at the time when the callback is     called. Thus, in local_cell_relations, the corresponding     p4est quadrants of the children cells are linked to the deal.II     cell which is going to be refined. To be specific, only the very     first child is marked with `CELL_REFINE`, whereas the others will be     marked with `CELL_INVALID`, which indicates that these cells will be     ignored by default during the packing or unpacking process. This     ensures that data is only transferred once onto or from the parent     cell. If the callback is called with `CELL_REFINE`, the callback     will want to pack up the data on this cell into an array and store     it at the provided address for later unpacking in a way so that     it can then be transferred to the children of the cell that will     then be available. In other words, if the data the callback     will want to pack up corresponds to a finite element field, then     the prolongation from parent to (new) children will have to happen     during unpacking.
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - `CELL_COARSEN`: The children of this cell will be coarsened into the     given cell. These children still exist, so if this is the value     given to the callback as second argument, the callback will want     to transfer data from the children to the current parent cell and     pack it up so that it can later be unpacked again on a cell that     then no longer has any children (and may also be located on a     different processor). In other words, if the data the callback     will want to pack up corresponds to a finite element field, then     it will need to do the restriction from children to parent at     this point.
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - `CELL_INVALID`: See `CELL_REFINE`.        
*  [2.x.61]  If this function is used for serialization of data       using save() and load(), then the cell status argument with which       the callback is called will always be `CELL_PERSIST`.         The callback function is expected to return a memory chunk of the     format  [2.x.62]  representing the packed data on a     certain cell.         The second parameter  [2.x.63]  indicates whether     the returned size of the memory region from the callback function     varies by cell (<tt>=true</tt>) or stays constant on each one     throughout the whole domain (<tt>=false</tt>).        
*  [2.x.64]  The purpose of this function is to register intent to       attach data for a single, subsequent call to       execute_coarsening_and_refinement() and notify_ready_to_unpack(),       save(), load(). Consequently, notify_ready_to_unpack(), save(),       and load() all forget the registered callbacks once these       callbacks have been called, and you will have to re-register       them with a triangulation if you want them to be active for       another call to these functions.    
* [0.x.39]*
     This function is the opposite of register_data_attach(). It is called     [1.x.3] the execute_coarsening_and_refinement() or save()/load()     functions are done when classes and functions that have previously     attached data to a triangulation for either transfer to other     processors, across mesh refinement, or serialization of data to     a file are ready to receive that data back. The important part about     this process is that the triangulation cannot do this right away from     the end of execute_coarsening_and_refinement() or load() via a     previously attached callback function (as the register_data_attach()     function does) because the classes that eventually want the data     back may need to do some setup between the point in time where the     mesh has been recreated and when the data can actually be received.     An example is the  [2.x.65]  class     that can really only receive the data once not only the mesh is     completely available again on the current processor, but only     after a DoFHandler has been reinitialized and distributed     degrees of freedom. In other words, there is typically a significant     amount of set up that needs to happen in user space before the classes     that can receive data attached to cell are ready to actually do so.     When they are, they use the current function to tell the triangulation     object that now is the time when they are ready by calling the     current function.         The supplied callback function is then called for each newly locally     owned cell. The first argument to the callback is an iterator that     designates the cell; the second argument indicates the status of the     cell in question; and the third argument localizes a memory area by     two iterators that contains the data that was previously saved from     the callback provided to register_data_attach().         The CellStatus will indicate if the cell was refined, coarsened, or     persisted unchanged. The  [2.x.66]  argument to the callback     will then either be an active,     locally owned cell (if the cell was not refined), or the immediate     parent if it was refined during execute_coarsening_and_refinement().     Therefore, contrary to during register_data_attach(), you can now     access the children if the status is `CELL_REFINE` but no longer for     callbacks with status `CELL_COARSEN`.         The first argument to this function, `handle`, corresponds to     the return value of register_data_attach(). (The precise     meaning of what the numeric value of this handle is supposed     to represent is neither important, nor should you try to use     it for anything other than transmit information between a     call to register_data_attach() to the corresponding call to     notify_ready_to_unpack().)    
* [0.x.40]*
     Save additional cell-attached data into the given file. The first     arguments are used to determine the offsets where to write buffers to.         Called by  [2.x.67] .    
* [0.x.41]*
     Load additional cell-attached data from the given file, if any was saved.     The first arguments are used to determine the offsets where to read     buffers from.         Called by  [2.x.68] .    
* [0.x.42]*
     A function to record the CellStatus of currently active cells that     are locally owned. This information is mandatory to transfer data     between meshes during adaptation or serialization, e.g., using      [2.x.69]          Relations will be stored in the private member local_cell_relations. For     an extensive description of CellStatus, see the documentation for the     member function register_data_attach().    
* [0.x.43]*
     Auxiliary data structure for assigning a CellStatus to a deal.II cell     iterator. For an extensive description of the former, see the     documentation for the member function register_data_attach().    
* [0.x.44]*
     Vector of pairs, each containing a deal.II cell iterator and its     respective CellStatus. To update its contents, use the     update_cell_relations() member function.    
* [0.x.45]*
     A structure that stores information about the data that has been, or     will be, attached to cells via the register_data_attach() function     and later retrieved via notify_ready_to_unpack().    
* [0.x.46]*
       number of functions that get attached to the Triangulation through       register_data_attach() for example SolutionTransfer.      
* [0.x.47]*
       number of functions that need to unpack their data after a call from       load()      
* [0.x.48]*
       These callback functions will be stored in the order in which they       have been registered with the register_data_attach() function.      
* [0.x.49]*
     This class in the private scope of  [2.x.70]      is dedicated to the data transfer across repartitioned meshes     and to/from the file system.         It is designed to store all data buffers intended for transfer.    
* [0.x.50]*
       Prepare data transfer by calling the pack callback functions on each       cell       in  [2.x.71]              All registered callback functions in  [2.x.72]  will write       into the fixed size buffer, whereas each entry of  [2.x.73]        will write its data into the variable size buffer.      
* [0.x.51]*
       Unpack the CellStatus information on each entry of        [2.x.74]              Data has to be previously transferred with execute_transfer()       or read from the file system via load().      
* [0.x.52]*
       Unpack previously transferred data on each cell registered in        [2.x.75]  with the provided  [2.x.76]  function.             The parameter  [2.x.77]  corresponds to the position where the        [2.x.78]  function is allowed to read from the memory. Its       value needs to be in accordance with the corresponding pack_callback       function that has been registered previously.             Data has to be previously transferred with execute_transfer()       or read from the file system via load().      
* [0.x.53]*
       Transfer data to file system.             The data will be written in a separate file, whose name       consists of the stem  [2.x.79]  and an attached identifier       <tt>_fixed.data</tt> for fixed size data and <tt>_variable.data</tt>       for variable size data.             All processors write into these files simultaneously via MPIIO.       Each processor's position to write to will be determined       from the provided input parameters.             Data has to be previously packed with pack_data().      
* [0.x.54]*
       Transfer data from file system.             The data will be read from separate file, whose name       consists of the stem  [2.x.80]  and an attached identifier       <tt>_fixed.data</tt> for fixed size data and <tt>_variable.data</tt>       for variable size data.       The  [2.x.81]  and  [2.x.82]        parameters are required to gather the memory offsets for each       callback.             All processors read from these files simultaneously via MPIIO.       Each processor's position to read from will be determined       from the provided input arguments.             After loading, unpack_data() needs to be called to finally       distribute data across the associated triangulation.      
* [0.x.55]*
       Clears all containers and associated data, and resets member       values to their default state.             Frees memory completely.      
* [0.x.56]*
       Flag that denotes if variable size data has been packed.      
* [0.x.57]*
       Cumulative size in bytes that those functions that have called       register_data_attach() want to attach to each cell. This number       only pertains to fixed-sized buffers where the data attached to       each cell has exactly the same size.             The last entry of this container corresponds to the data size       packed per cell in the fixed size buffer (which can be accessed       calling <tt>sizes_fixed_cumulative.back()</tt>).      
* [0.x.58]*
       Consecutive buffers designed for the fixed size transfer       functions of p4est.      
* [0.x.59]*
       Consecutive buffers designed for the variable size transfer       functions of p4est.      
* [0.x.60]

