include/deal.II-translator/lac/affine_constraints_0.txt
[0.x.0]*
     This struct contains all the information we need to store about each of     the global entries (global_row): are they obtained directly by some local     entry (local_row) or some constraints (constraint_position). This is not     directly used in the user code, but accessed via the GlobalRowsFromLocal.         The actions performed here correspond to reshaping the constraint     information from global degrees of freedom to local ones (i.e.,     cell-related DoFs), and also transforming the constraint information from     compressed row storage (each local dof that is constrained has a list of     constraint entries associated to it) into compressed column storage based     on the cell-related DoFs (we have a list of global degrees of freedom,     and to each we have a list of local rows where the entries come from). To     increase the speed, we additionally store whether an entry is generated     directly from the local degrees of freedom or whether it comes from a     constraint.    
* [0.x.1]*
     This class represents a cache for constraints that are encountered on a     local level. The functionality is similar to      [2.x.0]  > >, but tuned so that     frequent memory allocation for each entry is avoided. The data is put     into a  [2.x.1]  > and the row length is kept     fixed at row_length. Both the number of rows and the row length can     change is this structure is filled. In that case, the data is     rearranged. This is not directly used in the user code, but accessed     via the GlobalRowsFromLocal.    
* [0.x.2]*
     A data structure that collects all the global rows from a local     contribution (cell) and their origin (direct/constraint). This     is basically a vector consisting of "Distributing" structs     using access via the DataCache. The structure provides some     specialized sort and insert functions.         In case there are no constraints, this is basically a list of pairs     `<uint,uint>` with the first index being the global index and the second     index the local index. The list is sorted with respect to the global     index.         In case there are constraints, a global dof might get a contribution also     because it gets data from a constrained dof. This means that a global dof     might also have indirect contributions from a local dof via a constraint,     besides the direct ones.         The actions performed here correspond to reshaping the constraint     information from global degrees of freedom to local ones (i.e.,     cell-related DoFs), and also transforming the constraint information from     compressed row storage (each local dof that is constrained has a list of     constraint entries associated to it) into compressed column storage based     on the cell-related DoFs (we have a list of global degrees of freedom,     and to each we have a list of local rows where the entries come from). To     increase the speed, we additionally store whether an entry is generated     directly from the local degrees of freedom or whether it comes from a     constraint.    
* [0.x.3]*
       Constructor.      
* [0.x.4]*
       Return the number of global indices in the struct.      
* [0.x.5]*
       Return the number of constraints that are associated to the       counter_index-th entry in the list.      
* [0.x.6]*
       Return the global row of the counter_index-th entry in the list.      
* [0.x.7]*
       Return the global row of the counter_index-th entry in the list.      
* [0.x.8]*
       Return the local row in the cell matrix associated with the       counter_index-th entry in the list. Return invalid_size_type for       constrained rows.      
* [0.x.9]*
       Return a reference instead of the value as in the function above.      
* [0.x.10]*
       Return the local row in the cell matrix associated with the       counter_index-th entry in the list in the index_in_constraint-th       position of constraints.      
* [0.x.11]*
       Return the value of the constraint in the counter_index-th entry in       the list in the index_in_constraint-th position of constraints.      
* [0.x.12]*
       Return whether there is one row with indirect contributions (i.e.,       there has been at least one constraint with non-trivial       ConstraintLine).      
* [0.x.13]*
       Append an entry that is constrained. This means that there is one less       nontrivial row.      
* [0.x.14]*
       Return the number of constrained dofs in the structure. Constrained       dofs do not contribute directly to the matrix, but are needed in order       to set matrix diagonals and resolve inhomogeneities.      
* [0.x.15]*
       Return the number of constrained dofs in the structure that have an       inhomogeneity.      
* [0.x.16]*
       This function tells the structure that the ith constraint is       inhomogeneous. inhomogeneous constraints contribute to right hand       sides, so to have fast access to them, put them before homogeneous       constraints.      
* [0.x.17]*
       The local row where constraint number i was detected, to find that row       easily when the GlobalRowsToLocal has been set up.      
* [0.x.18]*
       A vector that contains all the global ids and the corresponding local       ids as well as a pointer to that data where we store how to resolve       constraints.      
* [0.x.19]*
       A data structure that holds the actual data from the constraints.      
* [0.x.20]*
       A number that states how many rows there are, constraints       disregarded.      
* [0.x.21]*
       A number that represents the number of rows with       inhomogeneous constraints.      
* [0.x.22]*
     Scratch data that is used during calls to distribute_local_to_global and     add_entries_local_to_global. In order to avoid frequent memory     allocation, we keep the data alive from one call to the next in a static     variable. Since we want to allow for different number types in matrices,     this is a template.         Since each thread gets its private version of scratch data out of a     ThreadLocalStorage, no conflicting access can occur. For this to be     valid, we need to make sure that no call within     distribute_local_to_global is made that by itself can spawn tasks.     Otherwise, we might end up in a situation where several threads fight for     the data.         Access to the scratch data is only through an accessor class which     handles the access as well as marks the data as used.    
* [0.x.23]*
       Constructor, does nothing.      
* [0.x.24]*
       Copy constructor, does nothing      
* [0.x.25]*
       Stores whether the data is currently in use.      
* [0.x.26]*
       Temporary array for column indices      
* [0.x.27]*
       Temporary array for column values      
* [0.x.28]*
       Temporary array for block start indices      
* [0.x.29]*
       Temporary array for vector indices      
* [0.x.30]*
       Temporary array for vector values      
* [0.x.31]*
       Data array for reorder row/column indices.      
* [0.x.32]*
       Data array for reorder row/column indices.      
* [0.x.33]*
 This class implements dealing with linear (possibly inhomogeneous) constraints on degrees of freedom. The concept and origin of such constraints is extensively described in the  [2.x.2]  module. The class is meant to deal with a limited number of constraints relative to the total number of degrees of freedom, for example a few per cent up to maybe 30 per cent; and with a linear combination of [1.x.0] other degrees of freedom where [1.x.1] is also relatively small (no larger than at most around the average number of entries per row of a linear system). It is  [2.x.3] not [2.x.4]  meant to describe full rank linear systems.
*  The algorithms used in the implementation of this class are described in some detail in the  [2.x.5]  "hp-paper". There is also a significant amount of documentation on how to use this class in the  [2.x.6]  module.
* 

*  [1.x.2]
*  Each "line" in objects of this class corresponds to one constrained degree of freedom, with the number of the line being [1.x.3], entered by using add_line() or add_lines(). The entries in this line are pairs of the form ([1.x.4],[1.x.5]), which are added by add_entry() or add_entries(). The organization is essentially a SparsityPattern, but with only a few lines containing nonzero elements, and  therefore no data wasted on the others. For each line, which has been added by the mechanism above, an elimination of the constrained degree of freedom of the form [1.x.6] is performed, where [1.x.7] is optional and set by set_inhomogeneity(). Thus, if a constraint is formulated for instance as a zero mean value of several degrees of freedom, one of the degrees has to be chosen to be eliminated.
*  Note that the constraints are linear in the [1.x.8], and that there might be a constant (non-homogeneous) term in the constraint. This is exactly the form we need for hanging node constraints, where we need to constrain one degree of freedom in terms of others. There are other conditions of this form possible, for example for implementing mean value conditions as is done in the  [2.x.7]  tutorial program. The name of the class stems from the fact that these constraints can be represented in matrix form as [1.x.9] [1.x.10] = [1.x.11], and this object then describes the matrix [1.x.12] and the vector [1.x.13]. The most frequent way to create/fill objects of this type is using the  [2.x.8]  function. The use of these objects is first explained in  [2.x.9] .
*  Objects of the present type are organized in lines (rows), but only those lines are stored where constraints are present. New constraints are added by adding new lines using the add_line() function, and then populating it using the add_entry() function to a given line, or add_entries() to add more than one entry at a time. The right hand side element, if nonzero, can be set using the set_inhomogeneity() function. After all constraints have been added, you need to call close(), which compresses the storage format and sorts the entries.
* 

* 
*  [2.x.10]  Many of the algorithms this class implements are discussed in the  [2.x.11] . The algorithms are also related to those shown in [1.x.14], with the difference that the algorithms shown there completely eliminated constrained degrees of freedom, whereas we usually keep them as part of the linear system.
* 

* 
*  [2.x.12] 

* 
*  [2.x.13] 

* 
* [0.x.34]*
   Declare the type for container size.  
* [0.x.35]*
   An enum that describes what should happen if the two AffineConstraints   objects involved in a call to the merge() function happen to have   constraints on the same degrees of freedom.  
* [0.x.36]*
     Throw an exception if the two objects concerned have conflicting     constraints on the same degree of freedom.    
* [0.x.37]*
     In an operation  [2.x.14]  and      [2.x.15]  have constraints on the same degree of freedom, take     the one from  [2.x.16] .    
* [0.x.38]*
     In an operation  [2.x.17]  and      [2.x.18]  have constraints on the same degree of freedom, take     the one from  [2.x.19] .    
* [0.x.39]*
   Constructor. The supplied IndexSet defines which indices might be   constrained inside this AffineConstraints container. In a calculation   with a DoFHandler object based on  [2.x.20]    or  [2.x.21]  one should use the set of locally   relevant dofs (see    [2.x.22] ).     The given IndexSet allows the AffineConstraints container to save   memory by just not caring about degrees of freedom that are not of   importance to the current processor. Alternatively, if no such   IndexSet is provided, internal data structures for [1.x.15] possible   indices will be created, leading to memory consumption on every   processor that is proportional to the [1.x.16] size of the   problem, not just proportional to the size of the portion of the   overall problem that is handled by the current processor.  
* [0.x.40]*
   Copy constructor  
* [0.x.41]*
   Move constructor  
* [0.x.42]*
   Copy operator. Like for many other large objects, this operator   is deleted to avoid its inadvertent use in places such as   accidentally declaring a  [2.x.23]  object as a   function argument by value, rather than by reference.     However, you can use the copy_from() function to explicitly   copy AffineConstraints objects.  
* [0.x.43]*
   Move assignment operator  
* [0.x.44]*
   Copy the given object to the current one.     This function exists because  [2.x.24]  is explicitly   disabled.  
* [0.x.45]*
   clear() the AffineConstraints object and supply an IndexSet with lines   that may be constrained. This function is only relevant in the   distributed case to supply a different IndexSet. Otherwise this routine   is equivalent to calling clear(). See the constructor for details.  
* [0.x.46]*
   Determines if we can store a constraint for the given  [2.x.25]  This   routine only matters in the distributed case and checks if the IndexSet   allows storage of this line. Always returns true if not in the   distributed case.  
* [0.x.47]*
   Return the index set describing locally relevant lines if any are   present. Note that if no local lines were given, this represents an empty   IndexSet, whereas otherwise it contains the global problem size and the   local range.  
* [0.x.48]*
   This function copies the content of  [2.x.26]  with DoFs that are   element of the IndexSet  [2.x.27]  Elements that are not present in the   IndexSet are ignored. All DoFs will be transformed to local index space   of the filter, both the constrained DoFs and the other DoFs these entries   are constrained to. The local index space of the filter is a contiguous   numbering of all (global) DoFs that are elements in the filter.     If, for example, the filter represents the range <tt>[10,20)</tt>, and   the constraints object  [2.x.28]  includes the global indices   <tt>{7,13,14}</tt>, the indices <tt>{3,4}</tt> are added to the calling   constraints object (since 13 and 14 are elements in the filter and element   13 is the fourth element in the index, and 14 is the fifth).     This function provides an easy way to create a AffineConstraints for   certain vector components in a vector-valued problem from a full   AffineConstraints, i.e. extracting a diagonal subblock from a larger   AffineConstraints. The block is specified by the IndexSet argument.  
* [0.x.49]*
    [2.x.29]  Adding constraints    [2.x.30]   
* [0.x.50]*
   Add a new line to the matrix. If the line already exists, then the   function simply returns without doing anything.  
* [0.x.51]*
   Call the first add_line() function for every index  [2.x.31]  for   which  [2.x.32]  is true.     This function essentially exists to allow adding several constraints of   the form [1.x.17]=0 all at once, where the set of indices   [1.x.18] for which these constraints should be added are given by the   argument of this function. On the other hand, just as if the single-   argument add_line() function were called repeatedly, the constraints can   later be modified to include linear dependencies using the add_entry()   function as well as inhomogeneities using set_inhomogeneity().  
* [0.x.52]*
   Call the first add_line() function for every index  [2.x.33]  that   appears in the argument.     This function essentially exists to allow adding several constraints of   the form [1.x.19]=0 all at once, where the set of indices   [1.x.20] for which these constraints should be added are given by the   argument of this function. On the other hand, just as if the single-   argument add_line() function were called repeatedly, the constraints can   later be modified to include linear dependencies using the add_entry()   function as well as inhomogeneities using set_inhomogeneity().  
* [0.x.53]*
   Call the first add_line() function for every index  [2.x.34]  that   appears in the argument.     This function essentially exists to allow adding several constraints of   the form [1.x.21]=0 all at once, where the set of indices   [1.x.22] for which these constraints should be added are given by the   argument of this function. On the other hand, just as if the single-   argument add_line() function were called repeatedly, the constraints can   later be modified to include linear dependencies using the add_entry()   function as well as inhomogeneities using set_inhomogeneity().  
* [0.x.54]*
   Add an entry to a given line. In other words, this function adds   a term  [2.x.35]  to the constraints for the  [2.x.36] th degree of freedom.     If an entry with the same indices as the one this function call denotes   already exists, then this function simply returns provided that the value   of the entry is the same. Thus, it does no harm to enter a constraint   twice.      [2.x.37]  constrained_dof_index The index  [2.x.38]  of the degree of freedom     that is being constrained.    [2.x.39]  column The index  [2.x.40]  of the degree of freedom being entered     into the constraint for degree of freedom  [2.x.41] .    [2.x.42]  weight The factor  [2.x.43]  that multiplies  [2.x.44] .  
* [0.x.55]*
   Add a whole series of entries, denoted by pairs of column indices and   weight values, to a line of constraints. This function is equivalent to   calling the preceding function several times, but is faster.  
* [0.x.56]*
   Set an inhomogeneity to the constraint for a degree of freedom. In other   words, it adds a constant  [2.x.45]  to the constraint for degree of freedom    [2.x.46] . For this to work, you need to call add_line() first for the given   degree of freedom.      [2.x.47]  constrained_dof_index The index  [2.x.48]  of the degree of freedom     that is being constrained.    [2.x.49]  value The right hand side value  [2.x.50]  for the constraint on     the degree of freedom  [2.x.51] .  
* [0.x.57]*
   Close the filling of entries. Since the lines of a matrix of this type   are usually filled in an arbitrary order and since we do not want to use   associative constrainers to store the lines, we need to sort the lines   and within the lines the columns before usage of the matrix. This is done   through this function.     Also, zero entries are discarded, since they are not needed.     After closing, no more entries are accepted. If the object was already   closed, then this function returns immediately.     This function also resolves chains of constraints. For example, degree of   freedom 13 may be constrained to  [2.x.52]    while degree of freedom 7 is itself constrained as  [2.x.53] . Then, the resolution will be that  [2.x.54] . Note, however, that   cycles in this graph of constraints are not allowed, i.e., for example    [2.x.55]  may not itself be constrained, directly or indirectly, to  [2.x.56]    again.  
* [0.x.58]*
   Merge the constraints represented by the object given as argument into   the constraints represented by this object. Both objects may or may not   be closed (by having their function close() called before). If this   object was closed before, then it will be closed afterwards as well.   Note, however, that if the other argument is closed, then merging may be   significantly faster.     Using the default value of the second arguments, the constraints in each   of the two objects (the old one represented by this object and the   argument) may not refer to the same degree of freedom, i.e. a degree of   freedom that is constrained in one object may not be constrained in the   second. If this is nevertheless the case, an exception is thrown.   However, this behavior can be changed by providing a different value for   the second argument.     By default, merging two AffineConstraints objects that are initialized   with different IndexSet objects is not allowed.   This behavior can be altered by setting  [2.x.57]    appropriately.     Merging a AffineConstraints that is initialized with an IndexSet   and one that is not initialized with an IndexSet is not yet implemented.  
* [0.x.59]*
   Shift all entries of this matrix down  [2.x.58]  rows and over  [2.x.59]    columns. If this object is initialized with an IndexSet, local_lines are   shifted as well.     This function is useful if you are building block matrices, where all   blocks are built by the same DoFHandler object, i.e. the matrix size is   larger than the number of degrees of freedom. Since several matrix rows   and columns correspond to the same degrees of freedom, you'd generate   several constraint objects, then shift them, and finally merge() them   together again.  
* [0.x.60]*
   Clear all entries of this matrix. Reset the flag determining whether new   entries are accepted or not.     This function may be called also on objects which are empty or already   cleared.  
* [0.x.61]*
    [2.x.60]   
* [0.x.62]*
    [2.x.61]  Querying constraints    [2.x.62]   
* [0.x.63]*
   Return number of constraints stored in this matrix.  
* [0.x.64]*
   Return whether the degree of freedom with number  [2.x.63]  is a   constrained one.     Note that if close() was called before, then this function is   significantly faster, since then the constrained degrees of freedom are   sorted and we can do a binary search, while before close() was called, we   have to perform a linear search through all entries.  
* [0.x.65]*
   Return whether the dof is constrained, and whether it is constrained to   only one other degree of freedom with weight one. The function therefore   returns whether the degree of freedom would simply be eliminated in favor   of exactly one other degree of freedom.     The function returns  [2.x.64]  if either the degree of freedom is not   constrained at all, or if it is constrained to more than one other degree   of freedom, or if it is constrained to only one degree of freedom but   with a weight different from one.  
* [0.x.66]*
   Return whether the two given degrees of freedom are linked by an equality   constraint that either constrains index1 to be so that    [2.x.65]  or constrains index2 so that    [2.x.66] .  
* [0.x.67]*
   Return the maximum number of other dofs that one dof is constrained to.   For example, in 2d a hanging node is constrained only to its two   neighbors, so the returned value would be 2. However, for higher order   elements and/or higher dimensions, or other types of constraints, this   number is no more obvious.     The name indicates that within the system matrix, references to a   constrained node are indirected to the nodes it is constrained to.  
* [0.x.68]*
   Return <tt>true</tt> in case the dof is constrained and there is a non-   trivial inhomogeneous values set to the dof.  
* [0.x.69]*
   Return <tt>false</tt> if all constraints in the AffineConstraints are   homogeneous ones, and <tt>true</tt> if there is at least one   inhomogeneity.  
* [0.x.70]*
   Return a pointer to the vector of entries if a line is constrained,   and a zero pointer in case the dof is not constrained.  
* [0.x.71]*
   Return the value of the inhomogeneity stored in the constrained dof  [2.x.67]    line_n. Unconstrained dofs also return a zero value.  
* [0.x.72]*
   Print the constraints represented by the current object to the   given stream.     For each constraint of the form   [1.x.23]   this function will write a sequence of lines that look like this:  
* [1.x.24]
*    The last line is only shown if the inhomogeneity (here: 2.75) is   nonzero.     A block of lines such as the one above is repeated for each   constrained degree of freedom.  
* [0.x.73]*
   Write the graph of constraints in 'dot' format. 'dot' is a program that   can take a list of nodes and produce a graphical representation of the   graph of constrained degrees of freedom and the degrees of freedom they   are constrained to.     The output of this function can be used as input to the 'dot' program   that can convert the graph into a graphical representation in postscript,   png, xfig, and a number of other formats.     This function exists mostly for debugging purposes.  
* [0.x.74]*
   Determine an estimate for the memory consumption (in bytes) of this   object.  
* [0.x.75]*
   Add the constraint indices associated to the indices in the given vector.   After a call to this function, the indices vector contains the initial   elements and all the associated constrained indices. This function sorts   the elements and suppresses duplicates.  
* [0.x.76]*
    [2.x.68]   
* [0.x.77]*
    [2.x.69]  Eliminating constraints from linear systems after their creation    [2.x.70]   
* [0.x.78]*
   Condense a sparsity pattern. The name of the function mimics the name of   the function we use to condense linear systems, but it is a bit of a   misnomer for the current context. This is because in the context of   linear systems, we eliminate certain rows and columns of the linear   system, i.e., we "reduce" or "condense" the linear system. On the other   hand, in the current context, the functions does not remove nonzero   entries from the sparsity pattern. Rather, it adds those nonzero entry   locations to the sparsity pattern that will later be needed for the   process of condensation of constrained degrees of freedom from a linear   system.     Since this function adds new nonzero entries to the sparsity pattern, the   given sparsity pattern must not be compressed. The current object must be   closed. The sparsity pattern is compressed at the end of the function.  
* [0.x.79]*
   Same function as above, but condenses square block sparsity patterns.  
* [0.x.80]*
   Same function as above, but condenses square compressed sparsity   patterns.  
* [0.x.81]*
   Same function as above, but condenses square compressed sparsity   patterns.  
* [0.x.82]*
   Condense a given matrix, i.e., eliminate the rows and columns of the   matrix that correspond to constrained degrees of freedom.     See the general documentation of this class for more detailed   information.  
* [0.x.83]*
   Same function as above, but condenses square block sparse matrices.  
* [0.x.84]*
   Condense the given vector in-place. The  [2.x.71]  may be a   Vector<float>, Vector<number>, BlockVector<tt><...></tt>, a PETSc or   Trilinos vector wrapper class, or any other type having the same   interface. Note that this function does not take any inhomogeneity into   account and throws an exception in case there are any inhomogeneities.   Use the function using both a matrix and vector for that case.    
*  [2.x.72]  This function does not work for MPI vectors. Use condense() with   two vector arguments instead.  
* [0.x.85]*
   The function copies and condenses values from  [2.x.73]  into  [2.x.74]    output. In a serial code it is equivalent to calling condense (vec). If   called in parallel,  [2.x.75]  is supposed to contain ghost elements   while  [2.x.76]  should not.  
* [0.x.86]*
   Condense a given matrix and a given vector by eliminating rows and   columns of the linear system that correspond to constrained degrees of   freedom. The sparsity pattern associated with the matrix needs to be   condensed and compressed.  This function is the appropriate choice for   applying inhomogeneous constraints.     The current object must be closed to call this function.     See the general documentation of this class for more detailed   information.  
* [0.x.87]*
   Same function as above, but condenses square block sparse matrices and   vectors.  
* [0.x.88]*
   Set the values of all constrained DoFs in a vector to zero.  The  [2.x.77]    VectorType may be a Vector<float>, Vector<number>,   BlockVector<tt><...></tt>, a PETSc or Trilinos vector wrapper class, or   any other type having the same interface.  
* [0.x.89]*
    [2.x.78]   
* [0.x.90]*
    [2.x.79]  Eliminating constraints from linear systems during their creation    [2.x.80]   
* [0.x.91]*
   This function takes a vector of local contributions ( [2.x.81]    corresponding to the degrees of freedom indices given in  [2.x.82]    local_dof_indices and distributes them to the global vector. In other   words, this function implements a   [scatter   operation](https://en.wikipedia.org/wiki/Gather-scatter_(vector_addressing)).   In most   cases, these local contributions will be the result of an integration   over a cell or face of a cell. However, as long as  [2.x.83]  and  [2.x.84]    local_dof_indices have the same number of elements, this function is   happy with whatever it is given.     In contrast to the similar function in the DoFAccessor class, this   function also takes care of constraints, i.e. if one of the elements of    [2.x.85]  belongs to a constrained node, then rather than   writing the corresponding element of  [2.x.86]  into  [2.x.87]    global_vector, the element is distributed to the entries in the global   vector to which this particular degree of freedom is constrained.     Thus, by using this function to distribute local contributions to the   global object, one saves the call to the condense function after the   vectors and matrices are fully assembled. On the other hand, by   consequence, the function does not only write into the entries enumerated   by the  [2.x.88]  array, but also (possibly) others as   necessary.     Note that this function will apply all constraints as if they were   homogeneous. For correctly setting inhomogeneous constraints, use the   similar function with a matrix argument or the function with both matrix   and vector arguments.    
*  [2.x.89]  This function in itself is thread-safe, i.e., it works properly   also when several threads call it simultaneously. However, the function   call is only thread-safe if the underlying global vector allows for   simultaneous access and the access is not to rows with the same global   index at the same time. This needs to be made sure from the caller's   site. There is no locking mechanism inside this method to prevent data   races.      [2.x.90]  local_vector Vector of local contributions.    [2.x.91]  local_dof_indices Local degrees of freedom indices   corresponding to the vector of local contributions.    [2.x.92]   global_vector The global vector to which all local   contributions will be added.  
* [0.x.92]*
   This function takes a vector of local contributions ( [2.x.93]    corresponding to the degrees of freedom indices given in  [2.x.94]    local_dof_indices and distributes them to the global vector. In other   words, this function implements a   [scatter   operation](https://en.wikipedia.org/wiki/Gather-scatter_(vector_addressing)).   In most   cases, these local contributions will be the result of an integration   over a cell or face of a cell. However, as long as  [2.x.95]  and  [2.x.96]    local_dof_indices have the same number of elements, this function is   happy with whatever it is given.     In contrast to the similar function in the DoFAccessor class, this   function also takes care of constraints, i.e. if one of the elements of    [2.x.97]  belongs to a constrained node, then rather than   writing the corresponding element of  [2.x.98]  into  [2.x.99]    global_vector, the element is distributed to the entries in the global   vector to which this particular degree of freedom is constrained.     Thus, by using this function to distribute local contributions to the   global object, one saves the call to the condense function after the   vectors and matrices are fully assembled. On the other hand, by   consequence, the function does not only write into the entries enumerated   by the  [2.x.100]  array, but also (possibly) others as   necessary. This includes writing into diagonal elements of the matrix if   the corresponding degree of freedom is constrained.     The fourth argument <tt>local_matrix</tt> is intended to be used in case   one wants to apply inhomogeneous constraints on the vector only. Such a   situation could be where one wants to assemble of a right hand side   vector on a problem with inhomogeneous constraints, but the global matrix   has been assembled previously. A typical example of this is a time   stepping algorithm where the stiffness matrix is assembled once, and the   right hand side updated every time step. Note that, however, the entries   in the columns of the local matrix have to be exactly the same as those   that have been written into the global matrix. Otherwise, this function   will not be able to correctly handle inhomogeneities.    
*  [2.x.101]  This function in itself is thread-safe, i.e., it works properly   also when several threads call it simultaneously. However, the function   call is only thread-safe if the underlying global vector allows for   simultaneous access and the access is not to rows with the same global   index at the same time. This needs to be made sure from the caller's   site. There is no locking mechanism inside this method to prevent data   races.  
* [0.x.93]*
   Same as the previous function, except that it uses two (possibly) different   index sets to correctly handle inhomogeneities when the local matrix is   computed from a combination of two neighboring elements, for example for an   edge integral term in DG. Note that in the case that these two elements   have different polynomial degree, the local matrix is rectangular.     <tt>local_dof_indices_row</tt> is the set of row indices and   <tt>local_dof_indices_col</tt> is the set of column indices of the local   matrix. <tt>diagonal=false</tt> says whether the two index sets are equal   or not.     If both index sets are equal, <tt>diagonal</tt> must be set to true or we   simply use the previous function. If both index sets are different   (diagonal=false) the <tt>global_vector</tt> is modified to handle   inhomogeneities but no entries from <tt>local_vector</tt> are added. Note   that the edge integrals for inner edged for DG do not contribute any values   to the right hand side.  
* [0.x.94]*
   Enter a single value into a result vector, obeying constraints.  
* [0.x.95]*
   This function takes a pointer to a vector of local contributions ( [2.x.102]    local_vector) corresponding to the degrees of freedom indices given in  [2.x.103]    local_dof_indices and distributes them to the global vector. In other   words, this function implements a   [scatter   operation](https://en.wikipedia.org/wiki/Gather-scatter_(vector_addressing)).   In most   cases, these local contributions will be the result of an integration   over a cell or face of a cell. However, as long as the entries in  [2.x.104]    local_dof_indices indicate reasonable global vector entries, this   function is happy with whatever it is given.     If one of the elements of  [2.x.105]  belongs to a constrained   node, then rather than writing the corresponding element of  [2.x.106]    local_vector into  [2.x.107]  the element is distributed to the   entries in the global vector to which this particular degree of freedom   is constrained.     Thus, by using this function to distribute local contributions to the   global object, one saves the call to the condense function after the   vectors and matrices are fully assembled. Note that this function   completely ignores inhomogeneous constraints.    
*  [2.x.108]  This function in itself is thread-safe, i.e., it works properly   also when several threads call it simultaneously. However, the function   call is only thread-safe if the underlying global vector allows for   simultaneous access and the access is not to rows with the same global   index at the same time. This needs to be made sure from the caller's   site. There is no locking mechanism inside this method to prevent data   races.  
* [0.x.96]*
   This function takes a matrix of local contributions ( [2.x.109]    corresponding to the degrees of freedom indices given in  [2.x.110]    local_dof_indices and distributes them to the global matrix. In other   words, this function implements a   [scatter   operation](https://en.wikipedia.org/wiki/Gather-scatter_(vector_addressing)).   In most   cases, these local contributions will be the result of an integration   over a cell or face of a cell. However, as long as  [2.x.111]  and  [2.x.112]    local_dof_indices have the same number of elements, this function is   happy with whatever it is given.     In contrast to the similar function in the DoFAccessor class, this   function also takes care of constraints, i.e. if one of the elements of    [2.x.113]  belongs to a constrained node, then rather than   writing the corresponding element of  [2.x.114]  into  [2.x.115]    global_matrix, the element is distributed to the entries in the global   matrix to which this particular degree of freedom is constrained.     With this scheme, we never write into rows or columns of constrained   degrees of freedom. In order to make sure that the resulting matrix can   still be inverted, we need to do something with the diagonal elements   corresponding to constrained nodes. Thus, if a degree of freedom in  [2.x.116]    local_dof_indices is constrained, we distribute the corresponding entries   in the matrix, but also add the absolute value of the diagonal entry of   the local matrix to the corresponding entry in the global matrix.   Assuming the discretized operator is positive definite, this guarantees   that the diagonal entry is always non-zero, positive, and of the same   order of magnitude as the other entries of the matrix. On the other hand,   when solving a source problem  [2.x.117]  the exact value of the diagonal   element is not important, since the value of the respective degree of   freedom will be overwritten by the distribute() call later on anyway.    
*  [2.x.118]  The procedure described above adds an unforeseeable number of   artificial eigenvalues to the spectrum of the matrix. Therefore, it is   recommended to use the equivalent function with two local index vectors   in such a case.     By using this function to distribute local contributions to the global   object, one saves the call to the condense function after the vectors and   matrices are fully assembled.    
*  [2.x.119]  This function in itself is thread-safe, i.e., it works properly   also when several threads call it simultaneously. However, the function   call is only thread-safe if the underlying global matrix allows for   simultaneous access and the access is not to rows with the same global   index at the same time. This needs to be made sure from the caller's   site. There is no locking mechanism inside this method to prevent data   races.  
* [0.x.97]*
   This function does almost the same as the function above but can treat   general rectangular matrices. The main difference to achieve this is that   the diagonal entries in constrained rows are left untouched instead of   being filled with arbitrary values.     Since the diagonal entries corresponding to eliminated degrees of freedom   are not set, the result may have a zero eigenvalue, if applied to a   square matrix. This has to be considered when solving the resulting   problems. For solving a source problem  [2.x.120] , it is possible to set the   diagonal entry after building the matrix by a piece of code of the form    
* [1.x.25]
*      The value of one which is used here is arbitrary, but in the context of   Krylov space methods uncritical, since it corresponds to an invariant   subspace. If the other matrix entries are smaller or larger by a factor   close to machine accuracy, it may be advisable to adjust it.     For solving eigenvalue problems, this will only add one spurious zero   eigenvalue (with a multiplicity that is possibly greater than one).   Taking this into account, nothing else has to be changed.  
* [0.x.98]*
   This function does almost the same as the function above for general   rectangular matrices but uses different AffineConstraints objects on the   row and column indices. The convention is that row indices are constrained   according to the calling AffineConstraints  [2.x.121] , whereas   column indices are constrained according to the given AffineConstraints    [2.x.122] . This function allows to handle the   case where rows and columns of a matrix are represented by different   function spaces with their own enumeration of indices, as e.g. in mixed   finite element problems with separate DoFHandler objects or for flux   matrices between different levels in multigrid methods.     Like the other method with separate slots for row and column indices,   this method does not add diagonal entries to eliminated degrees of   freedom. See there for a more elaborate description.  
* [0.x.99]*
   This function simultaneously writes elements into matrix and vector,   according to the constraints specified by the calling AffineConstraints.   In other words, it performs the   [scatter   operation](https://en.wikipedia.org/wiki/Gather-scatter_(vector_addressing))   of the corresponding functions for matrices and vectors at the same time.   This function can correctly handle inhomogeneous constraints as well. For   the parameter use_inhomogeneities_for_rhs see the documentation in    [2.x.123]    module.    
*  [2.x.124]  This function in itself is thread-safe, i.e., it works properly   also when several threads call it simultaneously. However, the function   call is only thread-safe if the underlying global matrix and vector allow   for simultaneous access and the access is not to rows with the same   global index at the same time. This needs to be made sure from the   caller's site. There is no locking mechanism inside this method to   prevent data races.  
* [0.x.100]*
   Do a similar operation as the distribute_local_to_global() function that   distributes writing entries into a matrix for constrained degrees of   freedom, except that here we don't write into a matrix but only allocate   sparsity pattern entries.     As explained in the    [2.x.125]  "hp-paper"   and in  [2.x.126] , first allocating a sparsity pattern and later coming back   and allocating additional entries for those matrix entries that will be   written to due to the elimination of constrained degrees of freedom   (using  [2.x.127]  ), can be a very expensive procedure.   It is cheaper to allocate these entries right away without having to do a   second pass over the sparsity pattern object. This function does exactly   that.     Because the function only allocates entries in a sparsity pattern, all it   needs to know are the degrees of freedom that couple to each other.   Unlike the previous function, no actual values are written, so the second   input argument is not necessary here.     The third argument to this function, keep_constrained_entries determines   whether the function shall allocate entries in the sparsity pattern at   all for entries that will later be set to zero upon condensation of the   matrix. These entries are necessary if the matrix is built unconstrained,   and only later condensed. They are not necessary if the matrix is built   using the distribute_local_to_global() function of this class which   distributes entries right away when copying a local matrix into a global   object. The default of this argument is true, meaning to allocate the few   entries that may later be set to zero.     By default, the function adds entries for all pairs of indices given in   the first argument to the sparsity pattern (unless   keep_constrained_entries is false). However, sometimes one would like to   only add a subset of all of these pairs. In that case, the last argument   can be used which specifies a boolean mask which of the pairs of indices   should be considered. If the mask is false for a pair of indices, then no   entry will be added to the sparsity pattern for this pair, irrespective   of whether one or both of the indices correspond to constrained degrees   of freedom.     This function is not typically called from user code, but is used in the    [2.x.128]  function when passed an   AffineConstraints object.    
*  [2.x.129]  This function in itself is thread-safe, i.e., it works properly   also when several threads call it simultaneously. However, the function   call is only thread-safe if the underlying global sparsity pattern allows   for simultaneous access and the access is not to rows with the same   global index at the same time. This needs to be made sure from the   caller's site. There is no locking mechanism inside this method to   prevent data races.  
* [0.x.101]*
   Similar to the other function, but for non-quadratic sparsity patterns.  
* [0.x.102]*
   This function imports values from a global vector ( [2.x.130]  by   applying the constraints to a vector of local values, expressed in   iterator format.  In most cases, the local values will be identified by   the local dof values on a cell. However, as long as the entries in  [2.x.131]    local_dof_indices indicate reasonable global vector entries, this   function is happy with whatever it is given.     If one of the elements of  [2.x.132]  belongs to a constrained   node, then rather than writing the corresponding element of  [2.x.133]    global_vector into  [2.x.134]  the constraints are resolved as the   respective distribute function does, i.e., the local entry is constructed   from the global entries to which this particular degree of freedom is   constrained.     In contrast to the similar function get_dof_values in the DoFAccessor   class, this function does not need the constrained values to be correctly   set (i.e., distribute to be called).  
* [0.x.103]*
    [2.x.135]   
* [0.x.104]*
    [2.x.136]  Dealing with constraints after solving a linear system    [2.x.137]   
* [0.x.105]*
   Given a vector, set all constrained degrees of freedom to values so   that the constraints are satisfied. For example, if the current object   stores the constraint  [2.x.138] , then this   function will read the values of  [2.x.139]  and  [2.x.140]  from the given vector   and set the element  [2.x.141]  according to this constraints. Similarly, if   the current object stores the constraint  [2.x.142] , then this   function will set the 42nd element of the given vector to 208.    
*  [2.x.143]  If this function is called with a parallel vector  [2.x.144]  then the   vector must not contain ghost elements.  
* [0.x.106]*
    [2.x.145]   
* [0.x.107]*
   This class represents one constraint in an AffineConstraints object.  
* [0.x.108]*
     A data type in which we store the list of entries that make up the     homogeneous part of a constraint.    
* [0.x.109]*
     Global DoF index of this line. Since only very few lines are stored,     we can not assume a specific order and have to store the index     explicitly.    
* [0.x.110]*
     Row numbers and values of the entries in this line.         For the reason why we use a vector instead of a map and the     consequences thereof, the same applies as what is said for      [2.x.146]     
* [0.x.111]*
     Value of the inhomogeneity.    
* [0.x.112]*
     Default constructor.    
* [0.x.113]*
     Copy constructor.    
* [0.x.114]*
     Copy assignment.    
* [0.x.115]*
     This operator is a bit weird and unintuitive: it compares the line     numbers of two lines. We need this to sort the lines; in fact we could     do this using a comparison predicate.  However, this way, it is easier,     albeit unintuitive since two lines really have no god-given order     relation.    
* [0.x.116]*
     This operator is likewise weird: it checks whether the line indices of     the two operands are equal, irrespective of the fact that the contents     of the line may be different.    
* [0.x.117]*
     Determine an estimate for the memory consumption (in bytes) of this     object.    
* [0.x.118]*
     Write and read the data of this object from a stream for the purpose     of serialization using the [BOOST serialization     library](https://www.boost.org/doc/libs/1_74_0/libs/serialization/doc/index.html).    
* [0.x.119]*
   Alias for the iterator type that is used in the LineRange container.  
* [0.x.120]*
   Alias for the return type used by get_lines().  
* [0.x.121]*
   Return a range object containing (const) iterators to all line entries   stored in the AffineConstraints container. Such a range is useful to   initialize range-based for loops as supported by C++11.      [2.x.147]  A range object for the half open range <code>[this->begin(),   this->end())</code> of line entries.  
* [0.x.122]*
   Check if the current object is consistent on all processors   in a distributed computation.     This method checks if all processors agree on the constraints for their   local lines as given by  [2.x.148]  This method is a collective   operation and will return  [2.x.149]  only if all processors are consistent.     Please supply the owned DoFs per processor as returned by    [2.x.150]   [2.x.151]  as    [2.x.152]  and the result of    [2.x.153]  as    [2.x.154]  The former is used to determine ownership of the   specific DoF, while the latter is used as the set of rows that need to be   checked.     If  [2.x.155]  is set to  [2.x.156]  additional debug information is written   to  [2.x.157]     
*  [2.x.158]  This method exchanges all constraint information of locally active   lines and is as such slow for large computations and should probably   only be used in debug mode. We do not check all lines returned by   get_local_lines() but only the locally active ones, as we allow processors   to not know about some locally relevant rows.      [2.x.159]  Whether all AffineConstraints objects are consistent. Returns   the same value on all processors.  
* [0.x.123]*
   Exception    
*  [2.x.160]   
* [0.x.124]*
   Exception    
*  [2.x.161]   
* [0.x.125]*
   Exception    
*  [2.x.162]   
* [0.x.126]*
   Exception    
*  [2.x.163]   
* [0.x.127]*
   Exception    
*  [2.x.164]   
* [0.x.128]*
   Exception.    
*  [2.x.165]   
* [0.x.129]*
   Exception    
*  [2.x.166]   
* [0.x.130]*
   Exception    
*  [2.x.167]   
* [0.x.131]*
   Exception    
*  [2.x.168]   
* [0.x.132]*
   Exception    
*  [2.x.169]   
* [0.x.133]*
   Store the lines of the matrix.  Entries are usually appended in an   arbitrary order and insertion into a vector is done best at the end, so   the order is unspecified after all entries are inserted. Sorting of the   entries takes place when calling the <tt>close()</tt> function.     We could, instead of using a vector, use an associative array, like a map   to store the lines. This, however, would mean a much more fragmented heap   since it allocates many small objects, and would additionally make usage   of this matrix much slower.  
* [0.x.134]*
   A list of size_type that contains the position of the ConstraintLine of a   constrained degree of freedom, or  [2.x.170]  if the   degree of freedom is not constrained. The  [2.x.171]    return value returns thus whether there is a constraint line for a given   degree of freedom index. Note that this class has no notion of how many   degrees of freedom there really are, so if we check whether there is a   constraint line for a given degree of freedom, then this vector may   actually be shorter than the index of the DoF we check for.     This field exists since when adding a new constraint line we have to   figure out whether it already exists. Previously, we would simply walk   the unsorted list of constraint lines until we either hit the end or   found it. This algorithm is O(N) if N is the number of constraints, which   makes it O(N^2) when inserting all constraints. For large problems with   many constraints, this could easily take 5-10 per cent of the total run   time. With this field, we can save this time since we find any constraint   in O(1) time or get to know that it a certain degree of freedom is not   constrained.     To make things worse, traversing the list of existing constraints   requires reads from many different places in memory. Thus, in large 3d   applications, the add_line() function showed up very prominently in the   overall compute time, mainly because it generated a lot of cache misses.   This should also be fixed by using the O(1) algorithm to access the   fields of this array.     The field is useful in a number of other contexts as well, e.g. when one   needs random access to the constraints as in all the functions that apply   constraints on the fly while add cell contributions into vectors and   matrices.  
* [0.x.135]*
   This IndexSet is used to limit the lines to save in the AffineConstraints   to a subset. This is necessary, because the lines_cache vector would   become too big in a distributed calculation.  
* [0.x.136]*
   Store whether the arrays are sorted.  If so, no new entries can be added.  
* [0.x.137]*
   Internal function to calculate the index of line  [2.x.172]  in the vector   lines_cache using local_lines.  
* [0.x.138]*
   This function actually implements the local_to_global function for   standard (non-block) matrices.  
* [0.x.139]*
   This function actually implements the local_to_global function for block   matrices.  
* [0.x.140]*
   This function actually implements the local_to_global function for   standard (non-block) sparsity types.  
* [0.x.141]*
   This function actually implements the local_to_global function for block   sparsity types.  
* [0.x.142]*
   Internal helper function for distribute_local_to_global function.     Creates a list of affected global rows for distribution, including the   local rows where the entries come from. The list is sorted according to   the global row indices.  
* [0.x.143]*
   Internal helper function for add_entries_local_to_global function.     Creates a list of affected rows for distribution without any additional   information, otherwise similar to the other make_sorted_row_list()   function.  
* [0.x.144]*
   Internal helper function for distribute_local_to_global function.  
* [0.x.145]*
     A "traits" class that can be used to determine whether a given type is a     block matrix type or not. For example,    
* [1.x.26]
*      has the value `false`, whereas    
* [1.x.27]
*      is true. This is sometimes useful in template contexts where we may want     to do things differently depending on whether a template type denotes a     regular or a block matrix type.          [2.x.173]       [2.x.174]  "Block (linear algebra)"    
* [0.x.146]*
       Overload returning true if the class is derived from BlockMatrixBase,       which is what block matrices do (with the exception of       BlockSparseMatrixEZ).      
* [0.x.147]*
       Overload for BlockSparseMatrixEZ, which is the only block matrix not       derived from BlockMatrixBase at the time of writing this class.      
* [0.x.148]*
       Catch all for all other potential types that are then apparently not       block matrices.      
* [0.x.149]*
       A statically computable value that indicates whether the template       argument to this class is a block matrix (in fact whether the type is       derived from BlockMatrixBase<T> or is one of the other block matrix       types).      
* [0.x.150]*
     A class that can be used to determine whether a given type is a block     sparsity pattern type or not. In this, it matches the IsBlockMatrix     class.          [2.x.175]       [2.x.176]  "Block (linear algebra)"    
* [0.x.151]*
       Overload returning true if the class is derived from       BlockSparsityPatternBase, which is what block sparsity patterns do.      
* [0.x.152]*
       Catch all for all other potential types that are then apparently not       block sparsity patterns.      
* [0.x.153]*
       A statically computable value that indicates whether the template       argument to this class is a block sparsity pattern (in fact whether the       type is derived from BlockSparsityPatternBase<T>).      
* [0.x.154]

include/deal.II-translator/lac/affine_constraints.templates_0.txt
[0.x.0]*
     This class is an accessor class to scratch data that is used     during calls to distribute_local_to_global and     add_entries_local_to_global. In order to avoid frequent memory     allocation, we keep the data alive from one call to the next in     a static variable. Since we want to allow for different number     types in matrices, this is a template.         Since each thread gets its private version of scratch data out of the     ThreadLocalStorage, no conflicting access can occur. For this to be     valid, we need to make sure that no call within     distribute_local_to_global is made that by itself can spawn tasks.     Otherwise, we might end up in a situation where several threads fight for     the data.    
* [0.x.1]*
       Constructor. Takes the scratch data object for the current       thread out of the provided object and marks it as used.      
* [0.x.2]*
       Destructor. Mark scratch data as available again.      
* [0.x.3]*
       Dereferencing operator.      
* [0.x.4]*
       Dereferencing operator.      
* [0.x.5]

include/deal.II-translator/lac/arpack_solver_0.txt
[0.x.0]*
 Interface for using ARPACK. ARPACK is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems.  Here we interface to the routines  [2.x.0]  of ARPACK. If the operator is specified to be symmetric we use the symmetric interface  [2.x.1]  of ARPACK instead.  The package is designed to compute a few eigenvalues and corresponding eigenvectors of a general n by n matrix A. It is most appropriate for large sparse matrices A.
*  In this class we make use of the method applied to the generalized eigenspectrum problem  [2.x.2] , for  [2.x.3] ; where  [2.x.4]  is a system matrix,  [2.x.5]  is a mass matrix, and  [2.x.6]  are a set of eigenvalues and eigenvectors respectively.
*  The ArpackSolver can be used in application codes with serial objects in the following way:

* 
* [1.x.0]
*  for the generalized eigenvalue problem  [2.x.7] , where the variable  [2.x.8]  tells ARPACK the number of eigenvector/eigenvalue pairs to solve for. Here,  [2.x.9]  is a vector that will contain the eigenvalues computed,  [2.x.10]  a vector that will contain the eigenvectors computed, and  [2.x.11]  is an inverse operation for the matrix  [2.x.12] . Shift and invert transformation around zero is applied.
*  Through the AdditionalData the user can specify some of the parameters to be set.
*  For further information on how the ARPACK routines  [2.x.13] ,  [2.x.14]  work and also how to set the parameters appropriately please take a look into the ARPACK manual.
* 

* 
*  [2.x.15]  Whenever you eliminate degrees of freedom using AffineConstraints, you generate spurious eigenvalues and eigenvectors. If you make sure that the diagonals of eliminated matrix rows are all equal to one, you get a single additional eigenvalue. But beware that some functions in deal.II set these diagonals to rather arbitrary (from the point of view of eigenvalue problems) values. See also  [2.x.16]  " [2.x.17] " for an example.

* 
* [0.x.1]*
   Declare the type for container size.  
* [0.x.2]*
   An enum that lists the possible choices for which eigenvalues to compute   in the solve() function.  
* [0.x.3]*
     The algebraically largest eigenvalues.    
* [0.x.4]*
     The algebraically smallest eigenvalues.    
* [0.x.5]*
     The eigenvalue with the largest magnitudes.    
* [0.x.6]*
     The eigenvalue with the smallest magnitudes.    
* [0.x.7]*
     The eigenvalues with the largest real parts.    
* [0.x.8]*
     The eigenvalues with the smallest real parts.    
* [0.x.9]*
     The eigenvalues with the largest imaginary parts.    
* [0.x.10]*
     The eigenvalues with the smallest imaginary parts.    
* [0.x.11]*
     Compute half of the eigenvalues from the high end of the spectrum and     the other half from the low end. If the number of requested     eigenvectors is odd, then the extra eigenvector comes from the high end     of the spectrum.    
* [0.x.12]*
   Standardized data struct to pipe additional data to the solver.  
* [0.x.13]*
     Constructor. By default, set the number of Arnoldi vectors (Lanczos     vectors if the problem is symmetric) to 15. Set the solver to find the     eigenvalues of largest magnitude for a non-symmetric problem).    
* [0.x.14]*
     Number of Arnoldi/Lanczos vectors. This number should be less than the     size of the problem but greater than 2 times the number of eigenvalues     (or n_eigenvalues if it is set) plus one.    
* [0.x.15]*
     Specify the eigenvalues of interest.    
* [0.x.16]*
     Specify if the problem is symmetric or not.    
* [0.x.17]*
   Access to the object that controls convergence.  
* [0.x.18]*
   Constructor.  
* [0.x.19]*
   Set initial vector for building Krylov space.  
* [0.x.20]*
   Set shift  [2.x.18]  for shift-and-invert spectral transformation.     If this function is not called, the shift is assumed to be zero.  
* [0.x.21]*
   Solve the generalized eigensprectrum problem  [2.x.19]  by calling   the  [2.x.20]  or    [2.x.21]  functions of ARPACK.     The function returns a vector of eigenvalues of length [1.x.1] and a   vector of eigenvectors of length [1.x.2] in the symmetric case   and of length [1.x.3] in the non-symmetric case. In the symmetric case   all eigenvectors are real. In the non-symmetric case complex eigenvalues   always occur as complex conjugate pairs. Therefore the eigenvector for an   eigenvalue with nonzero complex part is stored by putting the real and   the imaginary parts in consecutive real-valued vectors. The eigenvector   of the complex conjugate eigenvalue does not need to be stored, since it   is just the complex conjugate of the stored eigenvector. Thus, if the last   n-th eigenvalue has a nonzero imaginary part, Arpack needs in total n+1   real-valued vectors to store real and imaginary parts of the eigenvectors.      [2.x.22]  A The operator for which we want to compute eigenvalues. Actually,   this parameter is entirely unused.      [2.x.23]  B The inner product of the underlying space, typically the mass   matrix. For constrained problems, it can be a partial mass matrix, like   for instance the velocity mass matrix of a Stokes problem. Only its   function  [2.x.24]  is used.      [2.x.25]  inverse This is the possibly shifted inverse that is actually used   instead of  [2.x.26]  is   used.      [2.x.27]  eigenvalues is a vector of complex numbers in which the   eigenvalues are returned.      [2.x.28]  eigenvectors is a [1.x.4] vector of eigenvectors, containing   the real parts of all eigenvectors and the imaginary parts of the   eigenvectors corresponding to complex conjugate eigenvalue pairs.   Therefore, its length should be [1.x.5] in the symmetric case and   [1.x.6] in the non-symmetric case. In the non-symmetric case the storage   scheme leads for example to the following pattern. Suppose that the first   two eigenvalues are real and the third and fourth are a complex conjugate   pair. Asking for three eigenpairs results in [1.x.7]. Note that we get the same pattern if we ask for   four eigenpairs in this example, since the fourth eigenvector is simply the   complex conjugate of the third one.      [2.x.29]  n_eigenvalues The purpose of this parameter is not clear, but it   is safe to set it to the size of  [2.x.30]  or greater.   Leave it at its default zero, which will be reset to the size of    [2.x.31]  internally.  
* [0.x.22]*
   Reference to the object that controls convergence of the iterative   solver.  
* [0.x.23]*
   Store a copy of the flags for this particular solver.  
* [0.x.24]*
   Store an initial vector  
* [0.x.25]*
   Real part of the shift  
* [0.x.26]*
   Imaginary part of the shift  
* [0.x.27]*
   Exceptions.  
* [0.x.28]

include/deal.II-translator/lac/blas_extension_templates_0.txt
[0.x.0]

include/deal.II-translator/lac/block_indices_0.txt
[0.x.0]*
 BlockIndices represents a range of indices (such as the range  [2.x.0]  of valid indices for elements of a vector) and how this one range is broken down into smaller but contiguous "blocks" (such as the velocity and pressure parts of a solution vector). In particular, it provides the ability to translate between global indices and the indices [1.x.0] a block. This class is used, for example, in the BlockVector, BlockSparsityPattern, and BlockMatrixBase classes.
*  The information that can be obtained from this class falls into two groups. First, it is possible to query the global size of the index space (through the total_size() member function), and the number of blocks and their sizes (via size() and the block_size() functions).
*  Secondly, this class manages the conversion of global indices to the local indices within this block, and the other way around. This is required, for example, when you address a global element in a block vector and want to know within which block this is, and which index within this block it corresponds to. It is also useful if a matrix is composed of several blocks, where you have to translate global row and column indices to local ones.
* 

* 
*  [2.x.1]   [2.x.2]   [2.x.3]  "Block (linear algebra)"

* 
* [0.x.1]*
   Declare the type for container size.  
* [0.x.2]*
   Default constructor. Initialize for zero blocks.  
* [0.x.3]*
   Constructor. Initialize the number of entries in each block  [2.x.4]  as   <tt>block_sizes[i]</tt>. The number of blocks will be the size of  [2.x.5]    block_sizes.  
* [0.x.4]*
   Move constructor. Initialize a new object by stealing the internal data of   another BlockIndices object.  
* [0.x.5]*
   Copy constructor.  
* [0.x.6]*
   Specialized constructor for a structure with blocks of equal size.  
* [0.x.7]*
   Reinitialize the number of blocks and assign each block the same number   of elements.  
* [0.x.8]*
   Reinitialize the number of indices within each block from the given   argument. The number of blocks will be adjusted to the size of   <tt>block_sizes</tt> and the size of block  [2.x.6]  is set to   <tt>block_sizes[i]</tt>.  
* [0.x.9]*
   Add another block of given size to the end of the block structure.  
* [0.x.10]*
    [2.x.7]  Size information  
* [0.x.11]*
   Number of blocks in index field.  
* [0.x.12]*
   Return the total number of indices accumulated over all blocks, that is,   the dimension of the vector space of the block vector.  
* [0.x.13]*
   The size of the  [2.x.8]  block.  
* [0.x.14]*
   String representation of the block sizes. The output is of the form   `[nb->b1,b2,b3|s]`, where `nb` is n_blocks(), `s` is total_size() and   `b1` etc. are the values returned by block_size() for each of the blocks.  
* [0.x.15]*
    [2.x.9]  Index conversion     Functions in this group assume an object, which was created after sorting   by block, such that each block forms a set of consecutive indices in the   object. If applied to other objects, the numbers obtained from these   functions are meaningless.  
* [0.x.16]*
   Return the block and the index within that block for the global index  [2.x.10]    i. The first element of the pair is the block, the second the index   within it.  
* [0.x.17]*
   Return the global index of  [2.x.11]  in block  [2.x.12]   
* [0.x.18]*
   The start index of the ith block.  
* [0.x.19]*
   Copy operator.  
* [0.x.20]*
   Move assignment operator. Move another BlockIndices object onto the   current one by transferring its contents.  
* [0.x.21]*
   Compare whether two objects are the same, i.e. whether the number of   blocks and the sizes of all blocks are equal.  
* [0.x.22]*
   Swap the contents of these two objects.  
* [0.x.23]*
   Determine an estimate for the memory consumption (in bytes) of this   object.  
* [0.x.24]*
   Number of blocks. While this value could be obtained through   <tt>start_indices.size()-1</tt>, we cache this value for faster access.  
* [0.x.25]*
   Global starting index of each vector. The last and redundant value is the   total number of entries.  
* [0.x.26]*
 Operator for logging BlockIndices. Writes the number of blocks, the size of each block and the total size of the index field.
*   [2.x.13] 

* 
* [0.x.27]*
 Global function  [2.x.14]  which overloads the default implementation of the C++ standard library which uses a temporary object. The function simply exchanges the data of the two objects.
*   [2.x.15]  BlockIndices

* 
* [0.x.28]

include/deal.II-translator/lac/block_linear_operator_0.txt
[0.x.0]*
 A class to store the concept of a block linear operator.
*  This class increases the interface of LinearOperator (which encapsulates the   [2.x.0]  interface) by three additional functions:

* 
* [1.x.0]
*  that describe the underlying block structure (of an otherwise opaque) linear operator.
*  Objects of type BlockLinearOperator can be created similarly to LinearOperator with a wrapper function:

* 
* [1.x.1]
* 
*  Alternatively, there are several helper functions available for creating instances from multiple independent matrices of possibly different types. Here is an example of a block diagonal matrix created from a FullMatrix and a SparseMatrixEZ:
* 

* 
* [1.x.2]
* 
* 

*  A BlockLinearOperator can be sliced to a LinearOperator at any time. This removes all information about the underlying block structure (because above  [2.x.1]  objects are no longer available)
* 
*  - the linear operator interface, however, remains intact.
* 

* 
*  [2.x.2]  This class makes heavy use of  [2.x.3]  objects and lambda functions. This flexibility comes with a run-time penalty. Only use this object to encapsulate object with medium to large individual block sizes, and small block structure (as a rule of thumb, matrix blocks greater than  [2.x.4] ).
* 

* 

* 
*  [2.x.5] 

* 
* [0.x.1]*
   Create an empty BlockLinearOperator object.      [2.x.6]  member objects of this class and its base   class LinearOperator are initialized with default variants that throw an   exception upon invocation.  
* [0.x.2]*
   Default copy constructor.  
* [0.x.3]*
   Templated copy constructor that creates a BlockLinearOperator object from   an object  [2.x.7]  for which the conversion function    [2.x.8]  is defined.  
* [0.x.4]*
   Create a BlockLinearOperator from a two-dimensional array  [2.x.9]  of   LinearOperator. This constructor calls the corresponding block_operator()   specialization.  
* [0.x.5]*
   Create a block-diagonal BlockLinearOperator from a one-dimensional array    [2.x.10]  of LinearOperator. This constructor calls the corresponding   block_operator() specialization.  
* [0.x.6]*
   Default copy assignment operator.  
* [0.x.7]*
   Templated copy assignment operator for an object  [2.x.11]  for which the   conversion function  [2.x.12]  is defined.  
* [0.x.8]*
   Copy assignment from a two-dimensional array  [2.x.13]  of LinearOperator.   This assignment operator calls the corresponding block_operator()   specialization.  
* [0.x.9]*
   Copy assignment from a one-dimensional array  [2.x.14]  of LinearOperator   that creates a block-diagonal BlockLinearOperator. This assignment   operator calls the corresponding block_operator() specialization.  
* [0.x.10]*
   Return the number of blocks in a column (i.e, the number of "block rows",   or the number  [2.x.15] , if interpreted as a  [2.x.16]  block system).  
* [0.x.11]*
   Return the number of blocks in a row (i.e, the number of "block columns",   or the number  [2.x.17] , if interpreted as a  [2.x.18]  block system).  
* [0.x.12]*
   Access the block with the given coordinates. This    [2.x.19]  object returns a LinearOperator representing   the  [2.x.20] -th block of the BlockLinearOperator.  
* [0.x.13]*
     A dummy class for BlockLinearOperators that do not require any     extensions to facilitate the operations of the block matrix or its     subblocks.         This is the Payload class typically associated with deal.II's native     BlockSparseMatrix. To use either  [2.x.21]  or      [2.x.22]  one must initialize a     BlockLinearOperator with their associated BlockPayload.            
*  [2.x.23]     
* [0.x.14]*
       Type of payload held by each subblock      
* [0.x.15]*
       Default constructor             Since this class does not do anything in particular and needs no       special configuration, we have only one generic constructor that can       be called under any conditions.      
* [0.x.16]*
  [2.x.24]  Creation of a BlockLinearOperator

* 
* [0.x.17]*
  [2.x.25]  BlockLinearOperator
*  A function that encapsulates a  [2.x.26]  into a BlockLinearOperator.
*  All changes made on the block structure and individual blocks of  [2.x.27]  block_matrix after the creation of the BlockLinearOperator object are reflected by the operator object.
* 

* 
*  [2.x.28] 

* 
* [0.x.18]*
  [2.x.29]  BlockLinearOperator
*  A variant of above function that encapsulates a given collection  [2.x.30]  of LinearOperators into a block structure. Here, it is assumed that Range and Domain are block vectors, i.e., derived from  [2.x.31] . The individual linear operators in  [2.x.32]  must act on the underlying vector type of the block vectors, i.e., on  [2.x.33]  yielding a result in  [2.x.34] 
*  The list  [2.x.35]  is best passed as an initializer list. Consider for example a linear operator block (acting on Vector<double>)

* 
* [1.x.3]
*  The corresponding block_operator invocation takes the form

* 
* [1.x.4]
* 
* 

* 
*  [2.x.36] 

* 
* [0.x.19]*
  [2.x.37]  BlockLinearOperator
*  This function extracts the diagonal blocks of  [2.x.38]  (either a block matrix type or a BlockLinearOperator) and creates a BlockLinearOperator with the diagonal. Off-diagonal elements are initialized as null_operator (with correct reinit_range_vector and reinit_domain_vector methods).
*  All changes made on the individual diagonal blocks of  [2.x.39]  after the creation of the BlockLinearOperator object are reflected by the operator object.
* 

* 
*  [2.x.40] 

* 
* [0.x.20]*
  [2.x.41]  BlockLinearOperator
*  A variant of above function that builds up a block diagonal linear operator from an array  [2.x.42]  of diagonal elements (off-diagonal blocks are assumed to be 0).
*  The list  [2.x.43]  is best passed as an initializer list. Consider for example a linear operator block (acting on Vector<double>) <code>diag(op_a0, op_a1, ..., op_am)</code>. The corresponding block_operator invocation takes the form

* 
* [1.x.5]
* 
* 

* 
*  [2.x.44] 

* 
* [0.x.21]*
  [2.x.45]  BlockLinearOperator
*  A variant of above function that only takes a single LinearOperator argument  [2.x.46]  and creates a blockdiagonal linear operator with  [2.x.47]  copies of it.
* 

* 
*  [2.x.48] 

* 
* [0.x.22]*
  [2.x.49]  Manipulation of a BlockLinearOperator

* 
* [0.x.23]*
  [2.x.50]  LinearOperator  [2.x.51]  BlockLinearOperator
*  This function implements forward substitution to invert a lower block triangular matrix. As arguments, it takes a BlockLinearOperator  [2.x.52]  block_operator representing a block lower triangular matrix, as well as a BlockLinearOperator  [2.x.53]  representing inverses of diagonal blocks of  [2.x.54] 
*  Let us assume we have a linear system with the following block structure:
* 

* 
* [1.x.6]
* 
*  First of all,  [2.x.55] . Then, we can use x0 to recover x1:

* 
* [1.x.7]
*  and therefore:

* 
* [1.x.8]
* 
* 

* 
*  [2.x.56]  We are not using all blocks of the BlockLinearOperator arguments: Just the lower triangular block matrix of  [2.x.57]  is used as well as the diagonal of  [2.x.58] 
* 

* 
*  [2.x.59] 

* 
* [0.x.24]*
  [2.x.60]  LinearOperator  [2.x.61]  BlockLinearOperator
*  This function implements back substitution to invert an upper block triangular matrix. As arguments, it takes a BlockLinearOperator  [2.x.62]  block_operator representing an upper block triangular matrix, as well as a BlockLinearOperator  [2.x.63]  representing inverses of diagonal blocks of  [2.x.64] 
*  Let us assume we have a linear system with the following block structure:
* 

* 
* [1.x.9]
* 
*  First of all,  [2.x.65] . Then, we can use xn to recover x(n-1):

* 
* [1.x.10]
*  and therefore:

* 
* [1.x.11]
* 
* 

* 
*  [2.x.66]  We are not using all blocks of the BlockLinearOperator arguments: Just the upper triangular block matrix of  [2.x.67]  is used as well as the diagonal of  [2.x.68] 
* 

* 
*  [2.x.69] 

* 
* [0.x.25]

include/deal.II-translator/lac/block_matrix_base_0.txt
[0.x.0]!  [2.x.0]  Matrix1 [2.x.1] 

* 
* [0.x.1]*
 Namespace in which iterators in block matrices are implemented.

* 
* [0.x.2]*
   Base class for block matrix accessors, implementing the stepping through   a matrix.  
* [0.x.3]*
     Declare type for container size.    
* [0.x.4]*
     Typedef the value type of the matrix we point into.    
* [0.x.5]*
     Initialize data fields to default values.    
* [0.x.6]*
     Block row of the element represented by this object.    
* [0.x.7]*
     Block column of the element represented by this object.    
* [0.x.8]*
     Block row into which we presently point.    
* [0.x.9]*
     Block column into which we presently point.    
* [0.x.10]*
   Accessor classes in block matrices.  
* [0.x.11]*
   Block matrix accessor for non const matrices.  
* [0.x.12]*
     Declare type for container size.    
* [0.x.13]*
     Type of the matrix used in this accessor.    
* [0.x.14]*
     Typedef the value type of the matrix we point into.    
* [0.x.15]*
     Constructor. Since we use accessors only for read access, a const     matrix pointer is sufficient.         Place the iterator at the beginning of the given row of the matrix, or     create the end pointer if  [2.x.2]  equals the total number of rows in the     matrix.    
* [0.x.16]*
     Row number of the element represented by this object.    
* [0.x.17]*
     Column number of the element represented by this object.    
* [0.x.18]*
     Value of the entry at the current position.    
* [0.x.19]*
     Set new value.    
* [0.x.20]*
     The matrix accessed.    
* [0.x.21]*
     Iterator of the underlying matrix class.    
* [0.x.22]*
     Move ahead one element.    
* [0.x.23]*
     Compare this accessor with another one for equality.    
* [0.x.24]*
   Block matrix accessor for constant matrices, implementing the stepping   through a matrix.  
* [0.x.25]*
     Declare type for container size.    
* [0.x.26]*
     Type of the matrix used in this accessor.    
* [0.x.27]*
     Typedef the value type of the matrix we point into.    
* [0.x.28]*
     Constructor. Since we use accessors only for read access, a const     matrix pointer is sufficient.         Place the iterator at the beginning of the given row of the matrix, or     create the end pointer if  [2.x.3]  equals the total number of rows in the     matrix.    
* [0.x.29]*
     Initialize const accessor from non const accessor.    
* [0.x.30]*
     Row number of the element represented by this object.    
* [0.x.31]*
     Column number of the element represented by this object.    
* [0.x.32]*
     Value of the entry at the current position.    
* [0.x.33]*
     The matrix accessed.    
* [0.x.34]*
     Iterator of the underlying matrix class.    
* [0.x.35]*
     Move ahead one element.    
* [0.x.36]*
     Compare this accessor with another one for equality.    
* [0.x.37]*
 Blocked matrix class. The behavior of objects of this type is almost as for the usual matrix objects, with most of the functions being implemented in both classes. The main difference is that the matrix represented by this object is composed of an array of matrices (e.g. of type SparseMatrix<number>) and all accesses to the elements of this object are relayed to accesses of the base matrices. The actual type of the individual blocks of this matrix is the type of the template argument, and can, for example be the usual SparseMatrix or  [2.x.4] 
*  In addition to the usual matrix access and linear algebra functions, there are functions block() which allow access to the different blocks of the matrix. This may, for example, be of help when you want to implement Schur complement methods, or block preconditioners, where each block belongs to a specific component of the equation you are presently discretizing.
*  Note that the numbers of blocks and rows are implicitly determined by the sparsity pattern objects used.
*  Objects of this type are frequently used when a system of differential equations has solutions with variables that fall into different classes. For example, solutions of the Stokes or Navier-Stokes equations have  [2.x.5]  velocity components and one pressure component. In this case, it may make sense to consider the linear system of equations as a system of 2x2 blocks, and one can construct preconditioners or solvers based on this 2x2 block structure. This class can help you in these cases, as it allows to view the matrix alternatively as one big matrix, or as a number of individual blocks.
* 

*  [1.x.0]
*  Since this class simply forwards its calls to the subobjects (if necessary after adjusting indices denoting which subobject is meant), this class is completely independent of the actual type of the subobject. The functions that set up block matrices and destroy them, however, have to be implemented in derived classes. These functions also have to fill the data members provided by this base class, as they are only used passively in this class.
* 

*  Most of the functions take a vector or block vector argument. These functions can, in general, only successfully be compiled if the individual blocks of this matrix implement the respective functions operating on the vector type in question. For example, if you have a block sparse matrix over deal.II SparseMatrix objects, then you will likely not be able to form the matrix-vector multiplication with a block vector over  [2.x.6]  objects. If you attempt anyway, you will likely get a number of compiler errors.
* 

* 
*  [2.x.7]  Instantiations for this template are provided for <tt> [2.x.8]  and  [2.x.9]  others can be generated in application programs (see the section on  [2.x.10]  in the manual).
*   [2.x.11]   [2.x.12]  "Block (linear algebra)"

* 
* [0.x.38]*
   Typedef the type of the underlying matrix.  
* [0.x.39]*
   Type of matrix entries. These are analogous to alias in the standard   library containers.  
* [0.x.40]*
   Default constructor.  
* [0.x.41]*
   Destructor.  
* [0.x.42]*
   Copy the matrix given as argument into the current object.     Copying matrices is an expensive operation that we do not want to happen   by accident through compiler generated code for  [2.x.13] .   (This would happen, for example, if one accidentally declared a function   argument of the current type [1.x.1] rather than [1.x.2].) The functionality of copying matrices is implemented in   this member function instead. All copy operations of objects of this type   therefore require an explicit function call.     The source matrix may be a matrix of arbitrary type, as long as its data   type is convertible to the data type of this matrix.     The function returns a reference to <tt>this</tt>.  
* [0.x.43]*
   Access the block with the given coordinates.  
* [0.x.44]*
   Access the block with the given coordinates. Version for constant   objects.  
* [0.x.45]*
   Return the dimension of the codomain (or range) space. Note that the   matrix is of dimension  [2.x.14] .  
* [0.x.46]*
   Return the dimension of the domain space. Note that the matrix is of   dimension  [2.x.15] .  
* [0.x.47]*
   Return the number of blocks in a column. Returns zero if no sparsity   pattern is presently associated to this matrix.  
* [0.x.48]*
   Return the number of blocks in a row. Returns zero if no sparsity pattern   is presently associated to this matrix.  
* [0.x.49]*
   Set the element <tt>(i,j)</tt> to <tt>value</tt>. Throws an error if the   entry does not exist or if <tt>value</tt> is not a finite number. Still,   it is allowed to store zero values in non-existent fields.  
* [0.x.50]*
   Set all elements given in a FullMatrix into the sparse matrix locations   given by <tt>indices</tt>. In other words, this function writes the   elements in <tt>full_matrix</tt> into the calling matrix, using the   local-to-global indexing specified by <tt>indices</tt> for both the rows   and the columns of the matrix. This function assumes a quadratic sparse   matrix and a quadratic full_matrix, the usual situation in FE   calculations.     The optional parameter <tt>elide_zero_values</tt> can be used to specify   whether zero values should be set anyway or they should be filtered away   (and not change the previous content in the respective element if it   exists). The default value is <tt>false</tt>, i.e., even zero values are   treated.  
* [0.x.51]*
   Same function as before, but now including the possibility to use   rectangular full_matrices and different local-to-global indexing on rows   and columns, respectively.  
* [0.x.52]*
   Set several elements in the specified row of the matrix with column   indices as given by <tt>col_indices</tt> to the respective value.     The optional parameter <tt>elide_zero_values</tt> can be used to specify   whether zero values should be set anyway or they should be filtered away   (and not change the previous content in the respective element if it   exists). The default value is <tt>false</tt>, i.e., even zero values are   treated.  
* [0.x.53]*
   Set several elements to values given by <tt>values</tt> in a given row in   columns given by col_indices into the sparse matrix.     The optional parameter <tt>elide_zero_values</tt> can be used to specify   whether zero values should be inserted anyway or they should be filtered   away. The default value is <tt>false</tt>, i.e., even zero values are   inserted/replaced.  
* [0.x.54]*
   Add <tt>value</tt> to the element ([1.x.3]).  Throws an error if the   entry does not exist or if <tt>value</tt> is not a finite number. Still,   it is allowed to store zero values in non-existent fields.  
* [0.x.55]*
   Add all elements given in a FullMatrix<double> into sparse matrix   locations given by <tt>indices</tt>. In other words, this function adds   the elements in <tt>full_matrix</tt> to the respective entries in calling   matrix, using the local-to-global indexing specified by <tt>indices</tt>   for both the rows and the columns of the matrix. This function assumes a   quadratic sparse matrix and a quadratic full_matrix, the usual situation   in FE calculations.     The optional parameter <tt>elide_zero_values</tt> can be used to specify   whether zero values should be added anyway or these should be filtered   away and only non-zero data is added. The default value is <tt>true</tt>,   i.e., zero values won't be added into the matrix.  
* [0.x.56]*
   Same function as before, but now including the possibility to use   rectangular full_matrices and different local-to-global indexing on rows   and columns, respectively.  
* [0.x.57]*
   Set several elements in the specified row of the matrix with column   indices as given by <tt>col_indices</tt> to the respective value.     The optional parameter <tt>elide_zero_values</tt> can be used to specify   whether zero values should be added anyway or these should be filtered   away and only non-zero data is added. The default value is <tt>true</tt>,   i.e., zero values won't be added into the matrix.  
* [0.x.58]*
   Add an array of values given by <tt>values</tt> in the given global   matrix row at columns specified by col_indices in the sparse matrix.     The optional parameter <tt>elide_zero_values</tt> can be used to specify   whether zero values should be added anyway or these should be filtered   away and only non-zero data is added. The default value is <tt>true</tt>,   i.e., zero values won't be added into the matrix.  
* [0.x.59]*
   Add <tt>matrix</tt> scaled by <tt>factor</tt> to this matrix, i.e. the   matrix <tt>factor*matrix</tt> is added to <tt>this</tt>. If the sparsity   pattern of the calling matrix does not contain all the elements in the   sparsity pattern of the input matrix, this function will throw an   exception.     Depending on MatrixType, however, additional restrictions might arise.   Some sparse matrix formats require <tt>matrix</tt> to be based on the   same sparsity pattern as the calling matrix.  
* [0.x.60]*
   Return the value of the entry (i,j).  This may be an expensive operation   and you should always take care where to call this function.  In order to   avoid abuse, this function throws an exception if the wanted element does   not exist in the matrix.  
* [0.x.61]*
   This function is mostly like operator()() in that it returns the value of   the matrix entry <tt>(i,j)</tt>. The only difference is that if this   entry does not exist in the sparsity pattern, then instead of raising an   exception, zero is returned. While this may be convenient in some cases,   note that it is simple to write algorithms that are slow compared to an   optimal solution, since the sparsity of the matrix is not used.  
* [0.x.62]*
   Return the main diagonal element in the [1.x.4]th row. This function   throws an error if the matrix is not quadratic and also if the diagonal   blocks of the matrix are not quadratic.     This function is considerably faster than the operator()(), since for   quadratic matrices, the diagonal entry may be the first to be stored in   each row and access therefore does not involve searching for the right   column number.  
* [0.x.63]*
   Call the compress() function on all the subblocks of the matrix.       See    [2.x.16]  "Compressing distributed objects"   for more information.  
* [0.x.64]*
   Multiply the entire matrix by a fixed factor.  
* [0.x.65]*
   Divide the entire matrix by a fixed factor.  
* [0.x.66]*
   Adding Matrix-vector multiplication. Add  [2.x.17]  on  [2.x.18]  with  [2.x.19]  being   this matrix.  
* [0.x.67]*
   Adding Matrix-vector multiplication. Add [1.x.5] to   [1.x.6] with [1.x.7] being this matrix. This function does the same   as vmult_add() but takes the transposed matrix.  
* [0.x.68]*
   Return the norm of the vector [1.x.8] with respect to the norm induced   by this matrix, i.e. [1.x.9]. This is useful, e.g. in the   finite element context, where the [1.x.10]-norm of a function   equals the matrix norm with respect to the mass matrix of the vector   representing the nodal values of the finite element function. Note that   even though the function's name might suggest something different, for   historic reasons not the norm but its square is returned, as defined   above by the scalar product.     Obviously, the matrix needs to be square for this operation.  
* [0.x.69]*
   Return the frobenius norm of the matrix, i.e. the square root of the sum   of squares of all entries in the matrix.  
* [0.x.70]*
   Compute the matrix scalar product  [2.x.20] .  
* [0.x.71]*
   Compute the residual [1.x.11]. Write the residual into <tt>dst</tt>.  
* [0.x.72]*
   Print the matrix to the given stream, using the format <tt>(line,col)   value</tt>, i.e. one nonzero entry of the matrix per line. The optional   flag outputs the sparsity pattern in a different style according to the   underlying sparse matrix type.  
* [0.x.73]*
   Iterator starting at the first entry.  
* [0.x.74]*
   Final iterator.  
* [0.x.75]*
   Iterator starting at the first entry of row <tt>r</tt>.  
* [0.x.76]*
   Final iterator of row <tt>r</tt>.  
* [0.x.77]*
   Iterator starting at the first entry.  
* [0.x.78]*
   Final iterator.  
* [0.x.79]*
   Iterator starting at the first entry of row <tt>r</tt>.  
* [0.x.80]*
   Final iterator of row <tt>r</tt>.  
* [0.x.81]*
   Return a reference to the underlying BlockIndices data of the rows.  
* [0.x.82]*
   Return a reference to the underlying BlockIndices data of the columns.  
* [0.x.83]*
   Determine an estimate for the memory consumption (in bytes) of this   object. Note that only the memory reserved on the current processor is   returned in case this is called in an MPI-based program.  
* [0.x.84]*
    [2.x.21]  Exceptions    [2.x.22]   
* [0.x.85]*
   Exception  
* [0.x.86]*
   Exception  
* [0.x.87]*
   Release all memory and return to a state just like after having called   the default constructor. It also forgets the sparsity pattern it was   previously tied to.     This calls clear for all sub-matrices and then resets this object to have   no blocks at all.     This function is protected since it may be necessary to release   additional structures. A derived class can make it public again, if it is   sufficient.  
* [0.x.88]*
   Index arrays for rows and columns.  
* [0.x.89]*
   Array of sub-matrices.  
* [0.x.90]*
   This function collects the sizes of the sub-objects and stores them in   internal arrays, in order to be able to relay global indices into the   matrix to indices into the subobjects. Youmust* call this function each   time after you have changed the size of the sub-objects.     Derived classes should call this function whenever the size of the sub-   objects has changed and the  [2.x.23]  arrays need to be updated.     Note that this function is not public since not all derived classes need   to export its interface. For example, for the usual deal.II SparseMatrix   class, the sizes are implicitly determined whenever reinit() is called,   and individual blocks cannot be resized. For that class, this function   therefore does not have to be public. On the other hand, for the PETSc   classes, there is no associated sparsity pattern object that determines   the block sizes, and for these the function needs to be publicly   available. These classes therefore export this function.  
* [0.x.91]*
   Matrix-vector multiplication: let  [2.x.24]  with  [2.x.25]  being this   matrix.     Due to problems with deriving template arguments between the block and   non-block versions of the vmult/Tvmult functions, the actual functions   are implemented in derived classes, with implementations forwarding the   calls to the implementations provided here under a unique name for which   template arguments can be derived by the compiler.  
* [0.x.92]*
   Matrix-vector multiplication. Just like the previous function, but only   applicable if the matrix has only one block column.     Due to problems with deriving template arguments between the block and   non-block versions of the vmult/Tvmult functions, the actual functions   are implemented in derived classes, with implementations forwarding the   calls to the implementations provided here under a unique name for which   template arguments can be derived by the compiler.  
* [0.x.93]*
   Matrix-vector multiplication. Just like the previous function, but only   applicable if the matrix has only one block row.     Due to problems with deriving template arguments between the block and   non-block versions of the vmult/Tvmult functions, the actual functions   are implemented in derived classes, with implementations forwarding the   calls to the implementations provided here under a unique name for which   template arguments can be derived by the compiler.  
* [0.x.94]*
   Matrix-vector multiplication. Just like the previous function, but only   applicable if the matrix has only one block.     Due to problems with deriving template arguments between the block and   non-block versions of the vmult/Tvmult functions, the actual functions   are implemented in derived classes, with implementations forwarding the   calls to the implementations provided here under a unique name for which   template arguments can be derived by the compiler.  
* [0.x.95]*
   Matrix-vector multiplication: let  [2.x.26]  with  [2.x.27]  being this   matrix. This function does the same as vmult() but takes the transposed   matrix.     Due to problems with deriving template arguments between the block and   non-block versions of the vmult/Tvmult functions, the actual functions   are implemented in derived classes, with implementations forwarding the   calls to the implementations provided here under a unique name for which   template arguments can be derived by the compiler.  
* [0.x.96]*
   Matrix-vector multiplication. Just like the previous function, but only   applicable if the matrix has only one block row.     Due to problems with deriving template arguments between the block and   non-block versions of the vmult/Tvmult functions, the actual functions   are implemented in derived classes, with implementations forwarding the   calls to the implementations provided here under a unique name for which   template arguments can be derived by the compiler.  
* [0.x.97]*
   Matrix-vector multiplication. Just like the previous function, but only   applicable if the matrix has only one block column.     Due to problems with deriving template arguments between the block and   non-block versions of the vmult/Tvmult functions, the actual functions   are implemented in derived classes, with implementations forwarding the   calls to the implementations provided here under a unique name for which   template arguments can be derived by the compiler.  
* [0.x.98]*
   Matrix-vector multiplication. Just like the previous function, but only   applicable if the matrix has only one block.     Due to problems with deriving template arguments between the block and   non-block versions of the vmult/Tvmult functions, the actual functions   are implemented in derived classes, with implementations forwarding the   calls to the implementations provided here under a unique name for which   template arguments can be derived by the compiler.  
* [0.x.99]*
   Some matrix types, in particular PETSc, need to synchronize set and add   operations. This has to be done for all matrices in the BlockMatrix. This   routine prepares adding of elements by notifying all blocks. Called by   all internal routines before adding elements.  
* [0.x.100]*
   Notifies all blocks to let them prepare for setting elements, see   prepare_add_operation().  
* [0.x.101]*
   A structure containing some fields used by the set() and add() functions   that is used to pre-sort the input fields. Since one can reasonably   expect to call set() and add() from multiple threads at once as long as   the matrix indices that are touched are disjoint, these temporary data   fields need to be guarded by a mutex; the structure therefore contains   such a mutex as a member variable.  
* [0.x.102]*
     Temporary vector for counting the elements written into the individual     blocks when doing a collective add or set.    
* [0.x.103]*
     Temporary vector for column indices on each block when writing local to     global data on each sparse matrix.    
* [0.x.104]*
     Temporary vector for storing the local values (they need to be     reordered when writing local to global).    
* [0.x.105]*
     A mutex variable used to guard access to the member variables of this     structure;    
* [0.x.106]*
     Copy operator. This is needed because the default copy operator of this     class is deleted (since  [2.x.28]  is not copyable) and hence the     default copy operator of the enclosing class is also deleted.         The implementation here simply does nothing
* 
*  -  TemporaryData objects     are just scratch objects that are resized at the beginning of their     use, so there is no point actually copying anything.    
* [0.x.107]*
   A set of scratch arrays that can be used by the add() and set() functions   that take pointers to data to pre-sort indices before use. Access from   multiple threads is synchronized via the mutex variable that is part of   the structure.  
* [0.x.108]

include/deal.II-translator/lac/block_sparse_matrix_0.txt
[0.x.0]!  [2.x.0]  Matrix1 [2.x.1] 

* 
* [0.x.1]*
 Blocked sparse matrix based on the SparseMatrix class. This class implements the functions that are specific to the SparseMatrix base objects for a blocked sparse matrix, and leaves the actual work relaying most of the calls to the individual blocks to the functions implemented in the base class. See there also for a description of when this class is useful.
*   [2.x.2]   [2.x.3]  "Block (linear algebra)"

* 
* [0.x.2]*
   Typedef the base class for simpler access to its own alias.  
* [0.x.3]*
   Typedef the type of the underlying matrix.  
* [0.x.4]*
   Import the alias from the base class.  
* [0.x.5]*
    [2.x.4]  Constructors and initialization  
* [0.x.6]*
   Constructor; initializes the matrix to be empty, without any structure,   i.e.  the matrix is not usable at all. This constructor is therefore only   useful for matrices which are members of a class. All other matrices   should be created at a point in the data flow where all necessary   information is available.     You have to initialize the matrix before usage with   reinit(BlockSparsityPattern). The number of blocks per row and column are   then determined by that function.  
* [0.x.7]*
   Constructor. Takes the given matrix sparsity structure to represent the   sparsity pattern of this matrix. You can change the sparsity pattern   later on by calling the reinit() function.     This constructor initializes all sub-matrices with the sub-sparsity   pattern within the argument.     You have to make sure that the lifetime of the sparsity structure is at   least as long as that of this matrix or as long as reinit() is not called   with a new sparsity structure.  
* [0.x.8]*
   Destructor.  
* [0.x.9]*
   Pseudo copy operator only copying empty objects. The sizes of the block   matrices need to be the same.  
* [0.x.10]*
   This operator assigns a scalar to a matrix. Since this does usually not   make much sense (should we set all matrix entries to this value? Only the   nonzero entries of the sparsity pattern?), this operation is only allowed   if the actual value to be assigned is zero. This operator only exists to   allow for the obvious notation <tt>matrix=0</tt>, which sets all elements   of the matrix to zero, but keep the sparsity pattern previously used.  
* [0.x.11]*
   Release all memory and return to a state just like after having called   the default constructor. It also forgets the sparsity pattern it was   previously tied to.     This calls  [2.x.5]  on all sub-matrices and then resets this   object to have no blocks at all.  
* [0.x.12]*
   Reinitialize the sparse matrix with the given sparsity pattern. The   latter tells the matrix how many nonzero elements there need to be   reserved.     Basically, this function only calls  [2.x.6]  of the sub-   matrices with the block sparsity patterns of the parameter.     You have to make sure that the lifetime of the sparsity structure is at   least as long as that of this matrix or as long as reinit(const   SparsityPattern &) is not called with a new sparsity structure.     The elements of the matrix are set to zero by this function.  
* [0.x.13]*
    [2.x.7]  Information on the matrix  
* [0.x.14]*
   Return whether the object is empty. It is empty if either both dimensions   are zero or no BlockSparsityPattern is associated.  
* [0.x.15]*
   Return the number of entries in a specific row.  
* [0.x.16]*
   Return the number of nonzero elements of this matrix. Actually, it   returns the number of entries in the sparsity pattern; if any of the   entries should happen to be zero, it is counted anyway.  
* [0.x.17]*
   Return the number of actually nonzero elements. Just counts the number of   actually nonzero elements (with absolute value larger than threshold) of   all the blocks.  
* [0.x.18]*
   Return a (constant) reference to the underlying sparsity pattern of this   matrix.     Though the return value is declared <tt>const</tt>, you should be aware   that it may change if you call any nonconstant function of objects which   operate on it.  
* [0.x.19]*
   Determine an estimate for the memory consumption (in bytes) of this   object.  
* [0.x.20]*
    [2.x.8]  Multiplications  
* [0.x.21]*
   Matrix-vector multiplication: let  [2.x.9]  with  [2.x.10]  being this   matrix.  
* [0.x.22]*
   Matrix-vector multiplication. Just like the previous function, but only   applicable if the matrix has only one block column.  
* [0.x.23]*
   Matrix-vector multiplication. Just like the previous function, but only   applicable if the matrix has only one block row.  
* [0.x.24]*
   Matrix-vector multiplication. Just like the previous function, but only   applicable if the matrix has only one block.  
* [0.x.25]*
   Matrix-vector multiplication: let  [2.x.11]  with  [2.x.12]  being this   matrix. This function does the same as vmult() but takes the transposed   matrix.  
* [0.x.26]*
   Matrix-vector multiplication. Just like the previous function, but only   applicable if the matrix has only one block row.  
* [0.x.27]*
   Matrix-vector multiplication. Just like the previous function, but only   applicable if the matrix has only one block column.  
* [0.x.28]*
   Matrix-vector multiplication. Just like the previous function, but only   applicable if the matrix has only one block.  
* [0.x.29]*
    [2.x.13]  Preconditioning methods  
* [0.x.30]*
   Apply the Jacobi preconditioner, which multiplies every element of the   <tt>src</tt> vector by the inverse of the respective diagonal element and   multiplies the result with the relaxation parameter <tt>omega</tt>.     All diagonal blocks must be square matrices for this operation.  
* [0.x.31]*
   Apply the Jacobi preconditioner to a simple vector.     The matrix must be a single square block for this.  
* [0.x.32]*
    [2.x.14]  Input/Output  
* [0.x.33]*
   Print the matrix in the usual format, i.e. as a matrix and not as a list   of nonzero elements. For better readability, elements not in the matrix   are displayed as empty space, while matrix elements which are explicitly   set to zero are displayed as such.     The parameters allow for a flexible setting of the output format:   <tt>precision</tt> and <tt>scientific</tt> are used to determine the   number format, where <tt>scientific = false</tt> means fixed point   notation.  A zero entry for <tt>width</tt> makes the function compute a   width, but it may be changed to a positive value, if output is crude.     Additionally, a character for an empty value may be specified.     Finally, the whole matrix can be multiplied with a common denominator to   produce more readable output, even integers.      [2.x.15]  This function may produce [1.x.0] amounts of output if   applied to a large matrix!  
* [0.x.34]*
    [2.x.16]  Exceptions    [2.x.17]   
* [0.x.35]*
   Exception  
* [0.x.36]*
   Pointer to the block sparsity pattern used for this matrix. In order to   guarantee that it is not deleted while still in use, we subscribe to it   using the SmartPointer class.  
* [0.x.37]

include/deal.II-translator/lac/block_sparse_matrix_ez_0.txt
[0.x.0]!  [2.x.0]  Matrix1 [2.x.1] 

* 
* [0.x.1]*
 A block matrix consisting of blocks of type SparseMatrixEZ.
*  Like the other Block-objects, this matrix can be used like a SparseMatrixEZ, when it comes to access to entries. Then, there are functions for the multiplication with BlockVector and access to the individual blocks.
*   [2.x.2]   [2.x.3]  "Block (linear algebra)"

* 
* [0.x.2]*
   Declare type for container size.  
* [0.x.3]*
   Default constructor. The result is an empty object with zero dimensions.  
* [0.x.4]*
   Constructor setting up an object with given number of block rows and   columns. The blocks themselves still have zero dimension.  
* [0.x.5]*
   Copy constructor. This is needed for some container classes. It creates   an object of the same number of block rows and columns. Since it calls   the copy constructor of SparseMatrixEZ, the block s must be empty.  
* [0.x.6]*
   Copy operator. Like the copy constructor, this may be called for objects   with empty blocks only.  
* [0.x.7]*
   This operator assigns a scalar to a matrix. Since this does usually not   make much sense (should we set all matrix entries to this value? Only the   nonzero entries of the sparsity pattern?), this operation is only allowed   if the actual value to be assigned is zero. This operator only exists to   allow for the obvious notation <tt>matrix=0</tt>, which sets all elements   of the matrix to zero, but keep the sparsity pattern previously used.  
* [0.x.8]*
   Set matrix to zero dimensions and release memory.  
* [0.x.9]*
   Initialize to given block numbers.  After this operation, the matrix will   have the block dimensions provided. Each block will have zero dimensions   and must be initialized subsequently. After setting the sizes of the   blocks, collect_sizes() must be called to update internal data   structures.  
* [0.x.10]*
   This function collects the sizes of the sub-objects and stores them in   internal arrays, in order to be able to relay global indices into the   matrix to indices into the subobjects. Youmust* call this function each   time after you have changed the size of the sub-objects.  
* [0.x.11]*
   Access the block with the given coordinates.  
* [0.x.12]*
   Access the block with the given coordinates. Version for constant   objects.  
* [0.x.13]*
   Return the number of blocks in a column.  
* [0.x.14]*
   Return the number of blocks in a row.  
* [0.x.15]*
   Return whether the object is empty. It is empty if no memory is   allocated, which is the same as that both dimensions are zero. This   function is just the concatenation of the respective call to all sub-   matrices.  
* [0.x.16]*
   Return number of rows of this matrix, which equals the dimension of the   codomain (or range) space. It is the sum of the number of rows over the   sub-matrix blocks of this matrix. Recall that the matrix is of size m()   times n().  
* [0.x.17]*
   Return number of columns of this matrix, which equals the dimension of   the domain space. It is the sum of the number of columns over the sub-   matrix blocks of this matrix. Recall that the matrix is of size m() times   n().  
* [0.x.18]*
   Set the element <tt>(i,j)</tt> to  [2.x.4]   Throws an error if the entry   does not exist or if <tt>value</tt> is not a finite number. Still, it is   allowed to store zero values in non-existent fields.  
* [0.x.19]*
   Add  [2.x.5]  to the element <tt>(i,j)</tt>.  Throws an error if the entry   does not exist or if <tt>value</tt> is not a finite number. Still, it is   allowed to store zero values in non-existent fields.  
* [0.x.20]*
   Matrix-vector multiplication: let  [2.x.6]  with  [2.x.7]  being this   matrix.  
* [0.x.21]*
   Matrix-vector multiplication: let  [2.x.8]  with  [2.x.9]  being this   matrix. This function does the same as vmult() but takes the transposed   matrix.  
* [0.x.22]*
   Adding Matrix-vector multiplication. Add  [2.x.10]  on  [2.x.11]  with  [2.x.12]  being   this matrix.  
* [0.x.23]*
   Adding Matrix-vector multiplication. Add  [2.x.13]  to  [2.x.14]  with  [2.x.15]    being this matrix. This function does the same as vmult_add() but takes   the transposed matrix.  
* [0.x.24]*
   Print statistics. If  [2.x.16]  is  [2.x.17]  prints a histogram of all   existing row lengths and allocated row lengths. Otherwise, just the   relation of allocated and used entries is shown.  
* [0.x.25]*
   Object storing and managing the transformation of row indices to indices   of the sub-objects.  
* [0.x.26]*
   Object storing and managing the transformation of column indices to   indices of the sub-objects.  
* [0.x.27]*
   The actual matrices  
* [0.x.28]

include/deal.II-translator/lac/block_sparse_matrix_ez.templates_0.txt
[0.x.0]

include/deal.II-translator/lac/block_sparse_matrix.templates_0.txt
[0.x.0]

include/deal.II-translator/lac/block_sparsity_pattern_0.txt
[0.x.0]!  [2.x.0]  Sparsity [2.x.1] 

* 
* [0.x.1]*
 This is the base class for block versions of the sparsity pattern and dynamic sparsity pattern classes. It has not much functionality, but only administrates an array of sparsity pattern objects and delegates work to them. It has mostly the same interface as has the SparsityPattern, and DynamicSparsityPattern, and simply transforms calls to its member functions to calls to the respective member functions of the member sparsity patterns.
*  The largest difference between the SparsityPattern and DynamicSparsityPattern classes and this class is that mostly, the matrices have different properties and you will want to work on the blocks making up the matrix rather than the whole matrix. You can access the different blocks using the <tt>block(row,col)</tt> function.
*  Attention: this object is not automatically notified if the size of one of its subobjects' size is changed. After you initialize the sizes of the subobjects, you will therefore have to call the <tt>collect_sizes()</tt> function of this class! Note that, of course, all sub-matrices in a (block-)row have to have the same number of rows, and that all sub-matrices in a (block-)column have to have the same number of columns.
*  You will in general not want to use this class, but one of the derived classes.
*   [2.x.2]  Handle optimization of diagonal elements of the underlying SparsityPattern correctly.
*   [2.x.3]   [2.x.4]  "Block (linear algebra)"

* 
* [0.x.2]*
   Declare type for container size.  
* [0.x.3]*
   Define a value which is used to indicate that a certain value in the  [2.x.5]    colnums array is unused, i.e. does not represent a certain column number   index.     This value is only an alias to the respective value of the   SparsityPattern class.  
* [0.x.4]*
   Initialize the matrix empty, that is with no memory allocated. This is   useful if you want such objects as member variables in other classes. You   can make the structure usable by calling the reinit() function.  
* [0.x.5]*
   Initialize the matrix with the given number of block rows and columns.   The blocks themselves are still empty, and you have to call   collect_sizes() after you assign them sizes.  
* [0.x.6]*
   Copy constructor. This constructor is only allowed to be called if the   sparsity pattern to be copied is empty, i.e. there are no block allocated   at present. This is for the same reason as for the SparsityPattern, see   there for the details.  
* [0.x.7]*
   Destructor.  
* [0.x.8]*
   Resize the matrix, by setting the number of block rows and columns. This   deletes all blocks and replaces them with uninitialized ones, i.e. ones   for which also the sizes are not yet set. You have to do that by calling   the reinit() functions of the blocks themselves. Do not forget to call   collect_sizes() after that on this object.     The reason that you have to set sizes of the blocks yourself is that the   sizes may be varying, the maximum number of elements per row may be   varying, etc. It is simpler not to reproduce the interface of the   SparsityPattern class here but rather let the user call whatever function   they desire.  
* [0.x.9]*
   Copy operator. For this the same holds as for the copy constructor: it is   declared, defined and fine to be called, but the latter only for empty   objects.  
* [0.x.10]*
   This function collects the sizes of the sub-objects and stores them in   internal arrays, in order to be able to relay global indices into the   matrix to indices into the subobjects. Youmust* call this function each   time after you have changed the size of the sub-objects.  
* [0.x.11]*
   Access the block with the given coordinates.  
* [0.x.12]*
   Access the block with the given coordinates. Version for constant   objects.  
* [0.x.13]*
   Grant access to the object describing the distribution of row indices to   the individual blocks.  
* [0.x.14]*
   Grant access to the object describing the distribution of column indices   to the individual blocks.  
* [0.x.15]*
   This function compresses the sparsity structures that this object   represents. It simply calls  [2.x.6]  for all sub-objects.  
* [0.x.16]*
   Return the number of blocks in a column.  
* [0.x.17]*
   Return the number of blocks in a row.  
* [0.x.18]*
   Return whether the object is empty. It is empty if no memory is   allocated, which is the same as that both dimensions are zero. This   function is just the concatenation of the respective call to all sub-   matrices.  
* [0.x.19]*
   Return the maximum number of entries per row. It returns the maximal   number of entries per row accumulated over all blocks in a row, and the   maximum over all rows.  
* [0.x.20]*
   Add a nonzero entry to the matrix. This function may only be called for   non-compressed sparsity patterns.     If the entry already exists, nothing bad happens.     This function simply finds out to which block <tt>(i,j)</tt> belongs and   then relays to that block.  
* [0.x.21]*
   Add several nonzero entries to the specified matrix row.  This function   may only be called for non-compressed sparsity patterns.     If some of the entries already exist, nothing bad happens.     This function simply finds out to which blocks <tt>(row,col)</tt> for   <tt>col</tt> in the iterator range belong and then relays to those   blocks.  
* [0.x.22]*
   Return number of rows of this matrix, which equals the dimension of the   image space. It is the sum of rows of the (block-)rows of sub-matrices.  
* [0.x.23]*
   Return number of columns of this matrix, which equals the dimension of   the range space. It is the sum of columns of the (block-)columns of sub-   matrices.  
* [0.x.24]*
   Check if a value at a certain position may be non-zero.  
* [0.x.25]*
   Number of entries in a specific row, added up over all the blocks that   form this row.  
* [0.x.26]*
   Return the number of nonzero elements of this matrix. Actually, it   returns the number of entries in the sparsity pattern; if any of the   entries should happen to be zero, it is counted anyway.     This function may only be called if the matrix struct is compressed. It   does not make too much sense otherwise anyway.     In the present context, it is the sum of the values as returned by the   sub-objects.  
* [0.x.27]*
   Print the sparsity of the matrix. The output consists of one line per row   of the format <tt>[i,j1,j2,j3,...]</tt>. [1.x.0] is the row number and   [1.x.1] are the allocated columns in this row.  
* [0.x.28]*
   Print the sparsity of the matrix in a format that <tt>gnuplot</tt>   understands and which can be used to plot the sparsity pattern in a   graphical way. This is the same functionality implemented for usual   sparsity patterns, see    [2.x.7]   
* [0.x.29]*
   Print the sparsity of the matrix in <tt>svg</tt> format. This is the same   functionality implemented for usual sparsity patterns, see    [2.x.8]   
* [0.x.30]*
    [2.x.9]  Exceptions    [2.x.10]   
* [0.x.31]*
   Exception  
* [0.x.32]*
   Exception  
* [0.x.33]*
   Number of block rows.  
* [0.x.34]*
   Number of block columns.  
* [0.x.35]*
   Array of sparsity patterns.  
* [0.x.36]*
   Object storing and managing the transformation of row indices to indices   of the sub-objects.  
* [0.x.37]*
   Object storing and managing the transformation of column indices to   indices of the sub-objects.  
* [0.x.38]*
   Temporary vector for counting the elements written into the individual   blocks when doing a collective add or set.  
* [0.x.39]*
   Temporary vector for column indices on each block when writing local to   global data on each sparse matrix.  
* [0.x.40]*
 This class extends the base class to implement an array of sparsity patterns that can be used by block sparse matrix objects. It only adds a few additional member functions, but the main interface stems from the base class, see there for more information.
*  This class is an example of the "static" type of  [2.x.11] .

* 
* [0.x.41]*
   Initialize the matrix empty, that is with no memory allocated. This is   useful if you want such objects as member variables in other classes. You   can make the structure usable by calling the reinit() function.  
* [0.x.42]*
   Initialize the matrix with the given number of block rows and columns.   The blocks themselves are still empty, and you have to call   collect_sizes() after you assign them sizes.  
* [0.x.43]*
   Forwarding to  [2.x.12]   
* [0.x.44]*
   Initialize the pattern with two BlockIndices for the block structures of   matrix rows and columns as well as a row length vector.     The row length vector should be in the format produced by DoFTools.   Alternatively, there is a simplified version, where each of the inner   vectors has length one. Then, the corresponding entry is used as the   maximal row length.     For the diagonal blocks, the inner SparsityPattern is initialized with   optimized diagonals, while this is not done for the off-diagonal blocks.  
* [0.x.45]*
   Return whether the structure is compressed or not, i.e. whether all sub-   matrices are compressed.  
* [0.x.46]*
   Determine an estimate for the memory consumption (in bytes) of this   object.  
* [0.x.47]*
   Copy data from an object of type BlockDynamicSparsityPattern, i.e. resize   this object to the size of the given argument, and copy over the contents   of each of the subobjects. Previous content of this object is lost.  
* [0.x.48]*
 This class extends the base class to implement an array of compressed sparsity patterns that can be used to initialize objects of type BlockSparsityPattern. It does not add additional member functions, but rather acts as an  [2.x.13]  to introduce the name of this class, without requiring the user to specify the templated name of the base class. For information on the interface of this class refer to the base class. The individual blocks are based on the DynamicSparsityPattern class.
*  This class is an example of the "dynamic" type of  [2.x.14] .
*  [1.x.2]
*  Usage of this class is very similar to DynamicSparsityPattern, but since the use of block indices causes some additional complications, we give a short example.
*  After the DoFHandler <tt>dof</tt> and the AffineConstraints <tt>constraints</tt> have been set up with a system element, we must count the degrees of freedom in each matrix block:
* 

* 
* [1.x.3]
* 
*  Now, we are ready to set up the BlockDynamicSparsityPattern.
* 

* 
* [1.x.4]
* 
*  It is filled as if it were a normal pattern
* 

* 
* [1.x.5]
* 
*  In the end, it is copied to a normal BlockSparsityPattern for later use.
* 

* 
* [1.x.6]
* 

* 
* [0.x.49]*
   Initialize the matrix empty, that is with no memory allocated. This is   useful if you want such objects as member variables in other classes. You   can make the structure usable by calling the reinit() function.  
* [0.x.50]*
   Initialize the matrix with the given number of block rows and columns.   The blocks themselves are still empty, and you have to call   collect_sizes() after you assign them sizes.  
* [0.x.51]*
   Initialize the pattern with two BlockIndices for the block structures of   matrix rows and columns. This function is equivalent to calling the   previous constructor with the length of the two index vector and then   entering the index values.  
* [0.x.52]*
   Initialize the pattern with symmetric blocks. The number of IndexSets in   the vector determine the number of rows and columns of blocks. The size   of each block is determined by the size() of the respective IndexSet.   Each block only stores the rows given by the values in the IndexSet,   which is useful for distributed memory parallel computations and usually   corresponds to the locally owned DoFs.  
* [0.x.53]*
   Initialize the pattern with two BlockIndices for the block structures of   matrix rows and columns.  
* [0.x.54]*
   Resize the pattern to a tensor product of matrices with dimensions   defined by the arguments.     The matrix will have as many block rows and columns as there are entries   in the two arguments. The block at position ([1.x.7]) will have the   dimensions <tt>row_block_sizes[i]</tt> times <tt>col_block_sizes[j]</tt>.  
* [0.x.55]*
   Resize the pattern with symmetric blocks determined by the size() of each   IndexSet. See the constructor taking a vector of IndexSets for details.  
* [0.x.56]*
   Resize the matrix to a tensor product of matrices with dimensions defined   by the arguments. The two BlockIndices objects must be initialized and   the sparsity pattern will have the same block structure afterwards.  
* [0.x.57]*
   Access to column number field. Return the column number of the  [2.x.15]    th entry in row  [2.x.16]   
* [0.x.58]*
   Allow the use of the reinit functions of the base class as well.  
* [0.x.59]!  [2.x.17]  TrilinosWrappers   [2.x.18]   
* [0.x.60]*
   This class extends the base class to implement an array of Trilinos   sparsity patterns that can be used to initialize Trilinos block sparse   matrices that can be distributed among different processors. It is used in   the same way as the  [2.x.19]  except that it builds upon   the  [2.x.20]  instead of the    [2.x.21]      This class is has properties of the "dynamic" type of    [2.x.22]    (in the sense that it can extend the memory if too little elements were   allocated), but otherwise is more like the basic deal.II SparsityPattern   (in the sense that the method compress() needs to be called before the   pattern can be used).     This class is used in  [2.x.23] .  
* [0.x.61]*
     Initialize the matrix empty, that is with no memory allocated. This is     useful if you want such objects as member variables in other classes.     You can make the structure usable by calling the reinit() function.    
* [0.x.62]*
     Initialize the matrix with the given number of block rows and columns.     The blocks themselves are still empty, and you have to call     collect_sizes() after you assign them sizes.    
* [0.x.63]*
     Initialize the pattern with two BlockIndices for the block structures     of matrix rows and columns. This function is equivalent to calling the     previous constructor with the length of the two index vector and then     entering the index values.    
* [0.x.64]*
     Initialize the pattern with an array of index sets that specifies both     rows and columns of the matrix (so the final matrix will be a square     matrix), where the size() of the IndexSets specifies the size of the     blocks and the values in each IndexSet denotes the rows that are going     to be saved in each block.    
* [0.x.65]*
     Initialize the pattern with two arrays of index sets that specify rows     and columns of the matrix, where the size() of the IndexSets specifies     the size of the blocks and the values in each IndexSet denotes the rows     that are going to be saved in each block. The additional index set     writable_rows is used to set all rows that we allow to write locally.     This constructor is used to create matrices that allow several threads     to write simultaneously into the matrix (to different rows, of course),     see the method  [2.x.24]  method with     three index set arguments for more details.    
* [0.x.66]*
     Resize the matrix to a tensor product of matrices with dimensions     defined by the arguments.         The matrix will have as many block rows and columns as there are     entries in the two arguments. The block at position ([1.x.8]) will     have the dimensions <tt>row_block_sizes[i]</tt> times     <tt>col_block_sizes[j]</tt>.    
* [0.x.67]*
     Resize the matrix to a square tensor product of matrices. See the     constructor that takes a vector of IndexSets for details.    
* [0.x.68]*
     Resize the matrix to a rectangular block matrices. This method allows     rows and columns to be different, both in the outer block structure and     within the blocks.    
* [0.x.69]*
     Resize the matrix to a rectangular block matrices that furthermore     explicitly specify the writable rows in each of the blocks. This method     is used to create matrices that allow several threads to write     simultaneously into the matrix (to different rows, of course), see the     method  [2.x.25]  method with three     index set arguments for more details.    
* [0.x.70]*
     Allow the use of the reinit functions of the base class as well.    
* [0.x.71]

include/deal.II-translator/lac/block_vector_0.txt
[0.x.0]!  [2.x.0]  Vectors [2.x.1] 

* 
* [0.x.1]*
 An implementation of block vectors based on deal.II vectors. While the base class provides for most of the interface, this class handles the actual allocation of vectors and provides functions that are specific to the underlying vector type.
* 

* 
*  [2.x.2]  Instantiations for this template are provided for <tt> [2.x.3]  and  [2.x.4]  others can be generated in application programs (see the section on  [2.x.5]  in the manual).
*   [2.x.6]   [2.x.7]  "Block (linear algebra)"

* 
* [0.x.2]*
   Typedef the base class for simpler access to its own alias.  
* [0.x.3]*
   Typedef the type of the underlying vector.  
* [0.x.4]*
   Import the alias from the base class.  
* [0.x.5]*
   Constructor. There are three ways to use this constructor. First, without   any arguments, it generates an object with no blocks. Given one argument,   it initializes <tt>n_blocks</tt> blocks, but these blocks have size zero.   The third variant finally initializes all blocks to the same size   <tt>block_size</tt>.     Confer the other constructor further down if you intend to use blocks of   different sizes.  
* [0.x.6]*
   Copy Constructor. Dimension set to that of  [2.x.8]  all components are   copied from  [2.x.9]   
* [0.x.7]*
   Move constructor. Creates a new vector by stealing the internal data of   the given argument vector.  
* [0.x.8]*
   Copy constructor taking a BlockVector of another data type. This will   fail if there is no conversion path from <tt>OtherNumber</tt> to   <tt>Number</tt>. Note that you may lose accuracy when copying to a   BlockVector with data elements with less accuracy.     Older versions of gcc did not honor the  [2.x.10]  keyword on template   constructors. In such cases, it is easy to accidentally write code that   can be very inefficient, since the compiler starts performing hidden   conversions. To avoid this, this function is disabled if we have detected   a broken compiler during configuration.  
* [0.x.9]*
   A copy constructor taking a (parallel) Trilinos block vector and copying   it into the deal.II own format.  
* [0.x.10]*
   Constructor. Set the number of blocks to <tt>block_sizes.size()</tt> and   initialize each block with <tt>block_sizes[i]</tt> zero elements.  
* [0.x.11]*
   Constructor. Initialize vector to the structure found in the BlockIndices   argument.  
* [0.x.12]*
   Constructor. Set the number of blocks to <tt>block_sizes.size()</tt>.   Initialize the vector with the elements pointed to by the range of   iterators given as second and third argument. Apart from the first   argument, this constructor is in complete analogy to the respective   constructor of the  [2.x.11]  class, but the first argument is   needed in order to know how to subdivide the block vector into different   blocks.  
* [0.x.13]*
   Destructor. Clears memory  
* [0.x.14]*
   Call the compress() function on all the subblocks.     This functionality only needs to be called if using MPI based vectors and   exists in other objects for compatibility.     See    [2.x.12]  "Compressing distributed objects"   for more information.  
* [0.x.15]*
   Returns `false` as this is a serial block vector.     This functionality only needs to be called if using MPI based vectors and   exists in other objects for compatibility.  
* [0.x.16]*
   Copy operator: fill all components of the vector with the given scalar   value.  
* [0.x.17]*
   Copy operator for arguments of the same type. Resize the present vector   if necessary.  
* [0.x.18]*
   Move the given vector. This operator replaces the present vector with   the contents of the given argument vector.  
* [0.x.19]*
   Copy operator for template arguments of different types. Resize the   present vector if necessary.  
* [0.x.20]*
   Copy a regular vector into a block vector.  
* [0.x.21]*
   A copy constructor from a Trilinos block vector to a deal.II block   vector.  
* [0.x.22]*
   Reinitialize the BlockVector to contain <tt>n_blocks</tt> blocks of size   <tt>block_size</tt> each.     If the second argument is left at its default value, then the block   vector allocates the specified number of blocks but leaves them at zero   size. You then need to later reinitialize the individual blocks, and call   collect_sizes() to update the block system's knowledge of its individual   block's sizes.     If <tt>omit_zeroing_entries==false</tt>, the vector is filled with zeros.  
* [0.x.23]*
   Reinitialize the BlockVector such that it contains   <tt>block_sizes.size()</tt> blocks. Each block is reinitialized to   dimension <tt>block_sizes[i]</tt>.     If the number of blocks is the same as before this function was called,   all vectors remain the same and reinit() is called for each vector.     If <tt>omit_zeroing_entries==false</tt>, the vector is filled with zeros.     Note that you must call this (or the other reinit() functions) function,   rather than calling the reinit() functions of an individual block, to   allow the block vector to update its caches of vector sizes. If you call   reinit() on one of the blocks, then subsequent actions on this object may   yield unpredictable results since they may be routed to the wrong block.  
* [0.x.24]*
   Reinitialize the BlockVector to reflect the structure found in   BlockIndices.     If the number of blocks is the same as before this function was called,   all vectors remain the same and reinit() is called for each vector.     If <tt>omit_zeroing_entries==false</tt>, the vector is filled with zeros.  
* [0.x.25]*
   Change the dimension to that of the vector <tt>V</tt>. The same applies   as for the other reinit() function.     The elements of <tt>V</tt> are not copied, i.e.  this function is the   same as calling <tt>reinit (V.size(), omit_zeroing_entries)</tt>.     Note that you must call this (or the other reinit() functions) function,   rather than calling the reinit() functions of an individual block, to   allow the block vector to update its caches of vector sizes. If you call   reinit() of one of the blocks, then subsequent actions of this object may   yield unpredictable results since they may be routed to the wrong block.  
* [0.x.26]*
   Multiply each element of this vector by the corresponding element of   <tt>v</tt>.  
* [0.x.27]*
   Swap the contents of this vector and the other vector <tt>v</tt>. One   could do this operation with a temporary variable and copying over the   data elements, but this function is significantly more efficient since it   only swaps the pointers to the data of the two vectors and therefore does   not need to allocate temporary storage and move data around.     This function is analogous to the swap() function of all C++ standard   containers. Also, there is a global function swap(u,v) that simply calls   <tt>u.swap(v)</tt>, again in analogy to standard functions.  
* [0.x.28]*
   Print to a stream.  
* [0.x.29]*
   Write the vector en bloc to a stream. This is done in a binary mode, so   the output is neither readable by humans nor (probably) by other   computers using a different operating system or number format.  
* [0.x.30]*
   Read a vector en block from a file. This is done using the inverse   operations to the above function, so it is reasonably fast because the   bitstream is not interpreted.     The vector is resized if necessary.     A primitive form of error checking is performed which will recognize the   bluntest attempts to interpret some data as a vector stored bitwise to a   file, but not more.  
* [0.x.31]*
    [2.x.13]  Exceptions    [2.x.14]   
* [0.x.32]*
   Exception  
* [0.x.33]*
 Global function which overloads the default implementation of the C++ standard library which uses a temporary object. The function simply exchanges the data of the two vectors.
*   [2.x.15]  BlockVector

* 
* [0.x.34]*
     A helper class used internally in linear_operator.h. Specialization for     BlockVector<number>.    
* [0.x.35]*
 Declare  [2.x.16]  as serial vector.

* 
* [0.x.36]

include/deal.II-translator/lac/block_vector_base_0.txt
[0.x.0]!  [2.x.0]  Vectors [2.x.1] 

* 
* [0.x.1]*
 A class that can be used to determine whether a given type is a block vector type or not. For example,

* 
* [1.x.0]
*  has the value false, whereas

* 
* [1.x.1]
*  is true. This is sometimes useful in template contexts where we may want to do things differently depending on whether a template type denotes a regular or a block vector type.

* 
* [0.x.2]*
   Overload returning true if the class is derived from BlockVectorBase,   which is what block vectors do.  
* [0.x.3]*
   Catch all for all other potential vector types that are not block   matrices.  
* [0.x.4]*
   A statically computable value that indicates whether the template   argument to this class is a block vector (in fact whether the type is   derived from BlockVectorBase<T>).  
* [0.x.5]*
   Namespace in which iterators in block vectors are implemented.  
* [0.x.6]*
     General random-access iterator class for block vectors. Since we do not     want to have two classes for non-const iterator and const_iterator, we     take a second template argument which denotes whether the vector we     point into is a constant object or not. The first template argument is     always the number type of the block vector in use.         This class satisfies all requirements of random access iterators     defined in the C++ standard. Operations on these iterators are constant     in the number of elements in the block vector. However, they are     sometimes linear in the number of blocks in the vector, but since that     does rarely change dynamically within an application, this is a     constant and we again have that the iterator satisfies the requirements     of a random access iterator.    
* [0.x.7]*
       Declare the type for container size.      
* [0.x.8]*
       Type of the number this iterator points to. Depending on the value of       the second template parameter, this is either a constant or non-const       number.      
* [0.x.9]*
       Declare some alias which are standard for iterators and are used       by algorithms to enquire about the specifics of the iterators they       work on.      
* [0.x.10]*
       Typedef the type of the block vector (which differs in constness,       depending on the second template parameter).      
* [0.x.11]*
       Construct an iterator from a vector to which we point and the global       index of the element pointed to.             Depending on the value of the <tt>Constness</tt> template argument of       this class, the first argument of this constructor is either is a       const or non-const reference.      
* [0.x.12]*
       Copy constructor from an iterator of different constness.            
*  [2.x.2]  Constructing a non-const iterator from a const iterator does       not make sense. Attempting this will result in a compile-time error       (via  [2.x.3] ).      
* [0.x.13]*
       Copy constructor from an iterator with the same constness.      
* [0.x.14]*
       Constructor used internally in this class. The arguments match       exactly the values of the respective member variables.      
* [0.x.15]*
       Copy operator.      
* [0.x.16]*
       Dereferencing operator. If the template argument <tt>Constness</tt>       is <tt>true</tt>, then no writing to the result is possible, making       this a const_iterator.      
* [0.x.17]*
       Random access operator, grant access to arbitrary elements relative       to the one presently pointed to.      
* [0.x.18]*
       Prefix increment operator. This operator advances the iterator to the       next element and returns a reference to <tt>*this</tt>.      
* [0.x.19]*
       Postfix increment operator. This operator advances the iterator to       the next element and returns a copy of the old value of this       iterator.      
* [0.x.20]*
       Prefix decrement operator. This operator retracts the iterator to the       previous element and returns a reference to <tt>*this</tt>.      
* [0.x.21]*
       Postfix decrement operator. This operator retracts the iterator to       the previous element and returns a copy of the old value of this       iterator.      
* [0.x.22]*
       Compare for equality of iterators. This operator checks whether the       vectors pointed to are the same, and if not it throws an exception.      
* [0.x.23]*
       Compare for inequality of iterators. This operator checks whether the       vectors pointed to are the same, and if not it throws an exception.      
* [0.x.24]*
       Check whether this iterators points to an element previous to the one       pointed to by the given argument. This operator checks whether the       vectors pointed to are the same, and if not it throws an exception.      
* [0.x.25]*
       Comparison operator alike to the one above.      
* [0.x.26]*
       Comparison operator alike to the one above.      
* [0.x.27]*
       Comparison operator alike to the one above.      
* [0.x.28]*
       Return the distance between the two iterators, in elements.      
* [0.x.29]*
       Return an iterator which is the given number of elements in front of       the present one.      
* [0.x.30]*
       Return an iterator which is the given number of elements behind the       present one.      
* [0.x.31]*
       Move the iterator <tt>d</tt> elements forward at once, and return the       result.      
* [0.x.32]*
       Move the iterator <tt>d</tt> elements backward at once, and return       the result.      
* [0.x.33]*
        [2.x.4]  Exceptions        [2.x.5]       
* [0.x.34]*
       Exception thrown when one performs arithmetical comparisons on       iterators belonging to two different block vectors.      
* [0.x.35]*
       Pointer to the block vector object to which this iterator points.       Depending on the value of the <tt>Constness</tt> template argument of       this class, this is a <tt>const</tt> or non-<tt>const</tt> pointer.      
* [0.x.36]*
       Global index of the element to which we presently point.      
* [0.x.37]*
       Current block and index within this block of the element presently       pointed to.      
* [0.x.38]*
       Indices of the global element address at which we have to move on to       another block when moving forward and backward. These indices are       kept as a cache since this is much more efficient than always asking       the parent object.      
* [0.x.39]*
       Move forward one element.      
* [0.x.40]*
       Move backward one element.      
* [0.x.41]*
 A vector composed of several blocks each representing a vector of its own.
*  The BlockVector is a collection of vectors of a given type (e.g., deal.II Vector objects,  [2.x.6]  objects, etc.). Each of the vectors inside can have a different size.
*  The functionality of BlockVector includes everything a Vector can do, plus the access to a single Vector inside the BlockVector by <tt>block(i)</tt>. It also has a complete random access iterator, just as the other Vector classes or the standard C++ library template  [2.x.7]  Therefore, all algorithms working on iterators also work with objects of this class.
*  While this base class implements most of the functionality by dispatching calls to its member functions to the respective functions on each of the individual blocks, this class does not actually allocate some memory or change the size of vectors. For this, the constructors, assignment operators and reinit() functions of derived classes are responsible. This class only handles the common part that is independent of the actual vector type the block vector is built on.
* 

*  [1.x.2]
*  Apart from using this object as a whole, you can use each block separately as a vector, using the block() function.  There is a single caveat: if you have changed the size of one or several blocks, you must call the function collect_sizes() of the block vector to update its internal structures.
*   [2.x.8]  Warning: If you change the sizes of single blocks without calling collect_sizes(), results may be unpredictable. The debug version does not check consistency here for performance reasons!
*   [2.x.9]   [2.x.10]  "Block (linear algebra)"

* 
* [0.x.42]*
   Typedef the type of the underlying vector.  
* [0.x.43]   Declare standard types used in   all containers. These types   parallel those in the   <tt>C++</tt> standard   libraries    [2.x.11]    class. This includes iterator   types.  
* [0.x.44]*
   Declare a type that has holds real-valued numbers with the same precision   as the template argument to this class. If the template argument of this   class is a real data type, then real_type equals the template argument.   If the template argument is a  [2.x.12]  type then real_type equals the   type underlying the complex numbers.     This alias is used to represent the return type of norms.  
* [0.x.45]*
   Default constructor.  
* [0.x.46]*
   Copy constructor.  
* [0.x.47]*
   Move constructor. Each block of the argument vector is moved into the   current object if the underlying  [2.x.13]  is   move-constructible, otherwise they are copied.  
* [0.x.48]*
   Update internal structures after resizing vectors. Whenever you reinited   a block of a block vector, the internal data structures are corrupted.   Therefore, you should call this function after all blocks got their new   size.  
* [0.x.49]*
   Call the compress() function on all the subblocks of the matrix.     This functionality only needs to be called if using MPI based vectors and   exists in other objects for compatibility.     See    [2.x.14]  "Compressing distributed objects"   for more information.  
* [0.x.50]*
   Access to a single block.  
* [0.x.51]*
   Read-only access to a single block.  
* [0.x.52]*
   Return a reference on the object that describes the mapping between block   and global indices. The use of this function is highly deprecated and it   should vanish in one of the next versions  
* [0.x.53]*
   Number of blocks.  
* [0.x.54]*
   Return dimension of the vector. This is the sum of the dimensions of all   components.  
* [0.x.55]*
   Return local dimension of the vector. This is the sum of the local   dimensions (i.e., values stored on the current processor) of all   components.  
* [0.x.56]*
   Return an index set that describes which elements of this vector are   owned by the current processor. Note that this index set does not include   elements this vector may store locally as ghost elements but that are in   fact owned by another processor. As a consequence, the index sets   returned on different processors if this is a distributed vector will   form disjoint sets that add up to the complete index set. Obviously, if a   vector is created on only one processor, then the result would satisfy  
* [1.x.3]
*      For block vectors, this function returns the union of the locally owned   elements of the individual blocks, shifted by their respective index   offsets.  
* [0.x.57]*
   Return an iterator pointing to the first element.  
* [0.x.58]*
   Return an iterator pointing to the first element of a constant block   vector.  
* [0.x.59]*
   Return an iterator pointing to the element past the end.  
* [0.x.60]*
   Return an iterator pointing to the element past the end of a constant   block vector.  
* [0.x.61]*
   Access components, returns U(i).  
* [0.x.62]*
   Access components, returns U(i) as a writeable reference.  
* [0.x.63]*
   Access components, returns U(i).     Exactly the same as operator().  
* [0.x.64]*
   Access components, returns U(i) as a writeable reference.     Exactly the same as operator().  
* [0.x.65]*
   Instead of getting individual elements of a vector via operator(),   this function allows getting a whole set of elements at once. The   indices of the elements to be read are stated in the first argument, the   corresponding values are returned in the second.     If the current vector is called  [2.x.15]  then this function is the equivalent   to the code  
* [1.x.4]
*       [2.x.16]  The sizes of the  [2.x.17]  and  [2.x.18]  arrays must be identical.  
* [0.x.66]*
   Instead of getting individual elements of a vector via operator(),   this function allows getting a whole set of elements at once. In   contrast to the previous function, this function obtains the   indices of the elements by dereferencing all elements of the iterator   range provided by the first two arguments, and puts the vector   values into memory locations obtained by dereferencing a range   of iterators starting at the location pointed to by the third   argument.     If the current vector is called  [2.x.19]  then this function is the equivalent   to the code  
* [1.x.5]
*       [2.x.20]  It must be possible to write into as many memory locations     starting at  [2.x.21]  as there are iterators between      [2.x.22]  and  [2.x.23]   
* [0.x.67]*
   Copy operator: fill all components of the vector with the given scalar   value.  
* [0.x.68]*
   Copy operator for arguments of the same type.  
* [0.x.69]*
   Move assignment operator. Move each block of the given argument   vector into the current object if `VectorType` is   move-constructible, otherwise copy them.  
* [0.x.70]*
   Copy operator for template arguments of different types.  
* [0.x.71]*
   Copy operator from non-block vectors to block vectors.  
* [0.x.72]*
   Check for equality of two block vector types. This operation is only   allowed if the two vectors already have the same block structure.  
* [0.x.73]*
    [2.x.24] : scalar product.  
* [0.x.74]*
   Return the square of the  [2.x.25] -norm.  
* [0.x.75]*
   Return the mean value of the elements of this vector.  
* [0.x.76]*
   Return the  [2.x.26] -norm of the vector, i.e. the sum of the absolute values.  
* [0.x.77]*
   Return the  [2.x.27] -norm of the vector, i.e. the square root of the sum of   the squares of the elements.  
* [0.x.78]*
   Return the maximum absolute value of the elements of this vector, which   is the  [2.x.28] -norm of a vector.  
* [0.x.79]*
   Performs a combined operation of a vector addition and a subsequent inner   product, returning the value of the inner product. In other words, the   result of this function is the same as if the user called  
* [1.x.6]
*      The reason this function exists is that this operation involves less   memory transfer than calling the two functions separately on deal.II's   vector classes (Vector<Number> and    [2.x.29]  This method only needs to load   three vectors,  [2.x.30]   [2.x.31]   [2.x.32]  whereas calling separate methods   means to load the calling vector  [2.x.33]  twice. Since most vector   operations are memory transfer limited, this reduces the time by 25\% (or   50\% if  [2.x.34]  equals  [2.x.35]      For complex-valued vectors, the scalar product in the second step is   implemented as    [2.x.36] .  
* [0.x.80]*
   Return true if the given global index is in the local range of this   processor. Asks the corresponding block.  
* [0.x.81]*
   Return whether the vector contains only elements with value zero. This   function is mainly for internal consistency check and should seldom be   used when not in debug mode since it uses quite some time.  
* [0.x.82]*
   Return  [2.x.37]  if the vector has no negative entries, i.e. all entries   are zero or positive. This function is used, for example, to check   whether refinement indicators are really all positive (or zero).  
* [0.x.83]*
   Addition operator.  Fast equivalent to <tt>U.add(1, V)</tt>.  
* [0.x.84]*
   Subtraction operator.  Fast equivalent to <tt>U.add(-1, V)</tt>.  
* [0.x.85]*
   A collective add operation: This function adds a whole set of values   stored in  [2.x.38]  to the vector components specified by  [2.x.39]   
* [0.x.86]*
   This is a second collective add operation. As a difference, this function   takes a deal.II vector of values.  
* [0.x.87]*
   Take an address where <tt>n_elements</tt> are stored contiguously and add   them into the vector. Handles all cases which are not covered by the   other two <tt>add()</tt> functions above.  
* [0.x.88]*
    [2.x.40] .  Addition of <tt>s</tt> to all components. Note that   <tt>s</tt> is a scalar and not a vector.  
* [0.x.89]*
   U+=a*V. Simple addition of a scaled vector.  
* [0.x.90]*
   U+=a*V+b*W. Multiple addition of scaled vectors.  
* [0.x.91]*
   U=s*U+V. Scaling and simple vector addition.  
* [0.x.92]*
   U=s*U+a*V. Scaling and simple addition.  
* [0.x.93]*
   U=s*U+a*V+b*W. Scaling and multiple addition.  
* [0.x.94]*
   U=s*U+a*V+b*W+c*X. Scaling and multiple addition.  
* [0.x.95]*
   Scale each element of the vector by a constant value.  
* [0.x.96]*
   Scale each element of the vector by the inverse of the given value.  
* [0.x.97]*
   Multiply each element of this vector by the corresponding element of   <tt>v</tt>.  
* [0.x.98]*
   U=a*V. Assignment.  
* [0.x.99]*
   Update the ghost values by calling  [2.x.41]  for   each block.  
* [0.x.100]*
   Determine an estimate for the memory consumption (in bytes) of this   object.  
* [0.x.101]*
   Pointer to the array of components.  
* [0.x.102]*
   Object managing the transformation between global indices and indices   within the different blocks.  
* [0.x.103]

include/deal.II-translator/lac/block_vector.templates_0.txt
[0.x.0]

include/deal.II-translator/lac/chunk_sparse_matrix_0.txt
[0.x.0]!  [2.x.0]  Matrix1 [2.x.1] 

* 
* [0.x.1]*
 A namespace in which we declare iterators over the elements of sparse matrices.

* 
* [0.x.2]*
   General template for sparse matrix accessors. The first template argument   denotes the underlying numeric type, the second the constness of the   matrix.     The general template is not implemented, only the specializations for the   two possible values of the second template argument. Therefore, the   interface listed here only serves as a template provided since doxygen   does not link the specializations.  
* [0.x.3]*
     Value of this matrix entry.    
* [0.x.4]*
     Value of this matrix entry.    
* [0.x.5]*
     Return a reference to the matrix into which this accessor points. Note     that in the present case, this is a constant reference.    
* [0.x.6]*
   Accessor class for constant matrices, used in the const_iterators. This   class builds on the accessor classes used for sparsity patterns to loop   over all nonzero entries, and only adds the accessor functions to gain   access to the actual value stored at a certain location.  
* [0.x.7]*
     Typedef for the type (including constness) of the matrix to be used     here.    
* [0.x.8]*
     Constructor.    
* [0.x.9]*
     Constructor. Construct the end accessor for the given matrix.    
* [0.x.10]*
     Copy constructor to get from a non-const accessor to a const accessor.    
* [0.x.11]*
     Value of this matrix entry.    
* [0.x.12]*
     Return a reference to the matrix into which this accessor points. Note     that in the present case, this is a constant reference.    
* [0.x.13]*
     Pointer to the matrix we use.    
* [0.x.14]*
     Make the advance function of the base class available.    
* [0.x.15]*
   Accessor class for non-constant matrices, used in the iterators. This   class builds on the accessor classes used for sparsity patterns to loop   over all nonzero entries, and only adds the accessor functions to gain   access to the actual value stored at a certain location.  
* [0.x.16]*
     Reference class. This is what the accessor class returns when you call     the value() function. The reference acts just as if it were a reference     to the actual value of a matrix entry, i.e. you can read and write it,     you can add and multiply to it, etc, but since the matrix does not give     away the address of this matrix entry, we have to go through functions     to do all this.         The constructor takes a pointer to an accessor object that describes     which element of the matrix it points to. This creates an ambiguity     when one writes code like iterator->value()=0 (instead of     iterator->value()=0.0), since the right hand side is an integer that     can both be converted to a <tt>number</tt> (i.e., most commonly a     double) or to another object of type <tt>Reference</tt>. The compiler     then complains about not knowing which conversion to take.         For some reason, adding another overload operator=(int) doesn't seem to     cure the problem. We avoid it, however, by adding a second, dummy     argument to the Reference constructor, that is unused, but makes sure     there is no second matching conversion sequence using a one-argument     right hand side.    
* [0.x.17]*
       Constructor. For the second argument, see the general class       documentation.      
* [0.x.18]*
       Conversion operator to the data type of the matrix.      
* [0.x.19]*
       Set the element of the matrix we presently point to to  [2.x.2]       
* [0.x.20]*
       Add  [2.x.3]  to the element of the matrix we presently point to.      
* [0.x.21]*
       Subtract  [2.x.4]  from the element of the matrix we presently point to.      
* [0.x.22]*
       Multiply the element of the matrix we presently point to by  [2.x.5]       
* [0.x.23]*
       Divide the element of the matrix we presently point to by  [2.x.6]       
* [0.x.24]*
       Pointer to the accessor that denotes which element we presently point       to.      
* [0.x.25]*
     Typedef for the type (including constness) of the matrix to be used     here.    
* [0.x.26]*
     Constructor.    
* [0.x.27]*
     Constructor. Construct the end accessor for the given matrix.    
* [0.x.28]*
     Value of this matrix entry, returned as a read- and writable reference.    
* [0.x.29]*
     Return a reference to the matrix into which this accessor points. Note     that in the present case, this is a non-constant reference.    
* [0.x.30]*
     Pointer to the matrix we use.    
* [0.x.31]*
     Make the advance function of the base class available.    
* [0.x.32]*
   Iterator for constant and non-constant matrices.     The first template argument denotes the underlying numeric type, the   second the constness of the matrix.     Since there is a specialization of this class for   <tt>Constness=false</tt>, this class is for iterators to constant   matrices.  
* [0.x.33]*
     Typedef for the matrix type (including constness) we are to operate on.    
* [0.x.34]*
     An alias for the type you get when you dereference an iterator of the     current kind.    
* [0.x.35]*
     Constructor. Create an iterator into the matrix  [2.x.7]  for the given     row and the index within it.    
* [0.x.36]*
     Constructor. Create the end iterator for the given matrix.    
* [0.x.37]*
     Conversion constructor to get from a non-const iterator to a const     iterator.    
* [0.x.38]*
     Prefix increment.    
* [0.x.39]*
     Postfix increment.    
* [0.x.40]*
     Dereferencing operator.    
* [0.x.41]*
     Dereferencing operator.    
* [0.x.42]*
     Comparison. True, if both iterators point to the same matrix position.    
* [0.x.43]*
     Inverse of <tt>==</tt>.    
* [0.x.44]*
     Comparison operator. Result is true if either the first row number is     smaller or if the row numbers are equal and the first index is smaller.         This function is only valid if both iterators point into the same     matrix.    
* [0.x.45]*
     Comparison operator. Works in the same way as above operator, just the     other way round.    
* [0.x.46]*
     Return the distance between the current iterator and the argument. The     distance is given by how many times one has to apply operator++ to the     current iterator to get the argument (for a positive return value), or     operator-- (for a negative return value).    
* [0.x.47]*
     Return an iterator that is  [2.x.8]  ahead of the current one.    
* [0.x.48]*
     Store an object of the accessor class.    
* [0.x.49]*
 Sparse matrix. This class implements the function to store values in the locations of a sparse matrix denoted by a SparsityPattern. The separation of sparsity pattern and values is done since one can store data elements of different type in these locations without the SparsityPattern having to know this, and more importantly one can associate more than one matrix with the same sparsity pattern.
*  The use of this class is demonstrated in  [2.x.9] .
* 

* 
*  [2.x.10]  Instantiations for this template are provided for <tt> [2.x.11]  and  [2.x.12]  others can be generated in application programs (see the section on  [2.x.13]  in the manual).

* 
* [0.x.50]*
   Declare the type for container size.  
* [0.x.51]*
   Type of matrix entries. This alias is analogous to <tt>value_type</tt>   in the standard library containers.  
* [0.x.52]*
   Declare a type that has holds real-valued numbers with the same precision   as the template argument to this class. If the template argument of this   class is a real data type, then real_type equals the template argument.   If the template argument is a  [2.x.14]  type then real_type equals the   type underlying the complex numbers.     This alias is used to represent the return type of norms.  
* [0.x.53]*
   Typedef of an iterator class walking over all the nonzero entries of this   matrix. This iterator cannot change the values of the matrix.  
* [0.x.54]*
   Typedef of an iterator class walking over all the nonzero entries of this   matrix. This iterator  [2.x.15]  can change the values of the matrix, but of   course can't change the sparsity pattern as this is fixed once a sparse   matrix is attached to it.  
* [0.x.55]*
   A structure that describes some of the traits of this class in terms of   its run-time behavior. Some other classes (such as the block matrix   classes) that take one or other of the matrix classes as its template   parameters can tune their behavior based on the variables in this class.  
* [0.x.56]*
     It is safe to elide additions of zeros to individual elements of this     matrix.    
* [0.x.57]*
    [2.x.16]  Constructors and initialization.  
* [0.x.58]*
   Constructor; initializes the matrix to be empty, without any structure,   i.e.  the matrix is not usable at all. This constructor is therefore only   useful for matrices which are members of a class. All other matrices   should be created at a point in the data flow where all necessary   information is available.     You have to initialize the matrix before usage with reinit(const   ChunkSparsityPattern&).  
* [0.x.59]*
   Copy constructor. This constructor is only allowed to be called if the   matrix to be copied is empty. This is for the same reason as for the   ChunkSparsityPattern, see there for the details.     If you really want to copy a whole matrix, you can do so by using the   copy_from() function.  
* [0.x.60]*
   Constructor. Takes the given matrix sparsity structure to represent the   sparsity pattern of this matrix. You can change the sparsity pattern   later on by calling the reinit(const ChunkSparsityPattern&) function.     You have to make sure that the lifetime of the sparsity structure is at   least as long as that of this matrix or as long as reinit(const   ChunkSparsityPattern&) is not called with a new sparsity pattern.     The constructor is marked explicit so as to disallow that someone passes   a sparsity pattern in place of a sparse matrix to some function, where an   empty matrix would be generated then.  
* [0.x.61]*
   Copy constructor: initialize the matrix with the identity matrix. This   constructor will throw an exception if the sizes of the sparsity pattern   and the identity matrix do not coincide, or if the sparsity pattern does   not provide for nonzero entries on the entire diagonal.  
* [0.x.62]*
   Destructor. Free all memory, but do not release the memory of the   sparsity structure.  
* [0.x.63]*
   Copy operator. Since copying entire sparse matrices is a very expensive   operation, we disallow doing so except for the special case of empty   matrices of size zero. This doesn't seem particularly useful, but is   exactly what one needs if one wanted to have a    [2.x.17] : in that case,   one can create a vector (which needs the ability to copy objects) of   empty matrices that are then later filled with something useful.  
* [0.x.64]*
   Copy operator: initialize the matrix with the identity matrix. This   operator will throw an exception if the sizes of the sparsity pattern and   the identity matrix do not coincide, or if the sparsity pattern does not   provide for nonzero entries on the entire diagonal.  
* [0.x.65]*
   This operator assigns a scalar to a matrix. Since this does usually not   make much sense (should we set all matrix entries to this value?  Only   the nonzero entries of the sparsity pattern?), this operation is only   allowed if the actual value to be assigned is zero. This operator only   exists to allow for the obvious notation <tt>matrix=0</tt>, which sets   all elements of the matrix to zero, but keep the sparsity pattern   previously used.  
* [0.x.66]*
   Reinitialize the sparse matrix with the given sparsity pattern. The   latter tells the matrix how many nonzero elements there need to be   reserved.     Regarding memory allocation, the same applies as said above.     You have to make sure that the lifetime of the sparsity structure is at   least as long as that of this matrix or as long as reinit(const   ChunkSparsityPattern &) is not called with a new sparsity structure.     The elements of the matrix are set to zero by this function.  
* [0.x.67]*
   Release all memory and return to a state just like after having called   the default constructor. It also forgets the sparsity pattern it was   previously tied to.  
* [0.x.68]*
    [2.x.18]  Information on the matrix  
* [0.x.69]*
   Return whether the object is empty. It is empty if either both dimensions   are zero or no ChunkSparsityPattern is associated.  
* [0.x.70]*
   Return the dimension of the codomain (or range) space. Note that the   matrix is of dimension  [2.x.19] .  
* [0.x.71]*
   Return the dimension of the domain space. Note that the matrix is of   dimension  [2.x.20] .  
* [0.x.72]*
   Return the number of nonzero elements of this matrix. Actually, it   returns the number of entries in the sparsity pattern; if any of the   entries should happen to be zero, it is counted anyway.  
* [0.x.73]*
   Return the number of actually nonzero elements of this matrix.     Note, that this function does (in contrary to n_nonzero_elements()) not   count all entries of the sparsity pattern but only the ones that are   nonzero.  
* [0.x.74]*
   Return a (constant) reference to the underlying sparsity pattern of this   matrix.     Though the return value is declared <tt>const</tt>, you should be aware   that it may change if you call any nonconstant function of objects which   operate on it.  
* [0.x.75]*
   Determine an estimate for the memory consumption (in bytes) of this   object. See MemoryConsumption.  
* [0.x.76]*
    [2.x.21]  Modifying entries  
* [0.x.77]*
   Set the element ([1.x.0]) to <tt>value</tt>. Throws an error if the   entry does not exist or if <tt>value</tt> is not a finite number. Still,   it is allowed to store zero values in non-existent fields.  
* [0.x.78]*
   Add <tt>value</tt> to the element ([1.x.1]).  Throws an error if the   entry does not exist or if <tt>value</tt> is not a finite number. Still,   it is allowed to store zero values in non-existent fields.  
* [0.x.79]*
   Add an array of values given by <tt>values</tt> in the given global   matrix row at columns specified by col_indices in the sparse matrix.     The optional parameter <tt>elide_zero_values</tt> can be used to specify   whether zero values should be added anyway or these should be filtered   away and only non-zero data is added. The default value is <tt>true</tt>,   i.e., zero values won't be added into the matrix.  
* [0.x.80]*
   Multiply the entire matrix by a fixed factor.  
* [0.x.81]*
   Divide the entire matrix by a fixed factor.  
* [0.x.82]*
   Symmetrize the matrix by forming the mean value between the existing   matrix and its transpose,  [2.x.22] .     This operation assumes that the underlying sparsity pattern represents a   symmetric object. If this is not the case, then the result of this   operation will not be a symmetric matrix, since it only explicitly   symmetrizes by looping over the lower left triangular part for efficiency   reasons; if there are entries in the upper right triangle, then these   elements are missed in the symmetrization. Symmetrization of the sparsity   pattern can be obtain by  [2.x.23]   
* [0.x.83]*
   Copy the matrix given as argument into the current object.     Copying matrices is an expensive operation that we do not want to happen   by accident through compiler generated code for  [2.x.24] .   (This would happen, for example, if one accidentally declared a function   argument of the current type [1.x.2] rather than [1.x.3].) The functionality of copying matrices is implemented in   this member function instead. All copy operations of objects of this type   therefore require an explicit function call.     The source matrix may be a matrix of arbitrary type, as long as its data   type is convertible to the data type of this matrix.     The function returns a reference to <tt>*this</tt>.  
* [0.x.84]*
   This function is complete analogous to the    [2.x.25]  function in that it allows to   initialize a whole matrix in one step. See there for more information on   argument types and their meaning. You can also find a small example on   how to use this function there.     The only difference to the cited function is that the objects which the   inner iterator points to need to be of type  [2.x.26]  int,   value</tt>, where <tt>value</tt> needs to be convertible to the element   type of this class, as specified by the <tt>number</tt> template   argument.     Previous content of the matrix is overwritten. Note that the entries   specified by the input parameters need not necessarily cover all elements   of the matrix. Elements not covered remain untouched.  
* [0.x.85]*
   Copy the nonzero entries of a full matrix into this object. Previous   content is deleted. Note that the underlying sparsity pattern must be   appropriate to hold the nonzero entries of the full matrix.  
* [0.x.86]*
   Add <tt>matrix</tt> scaled by <tt>factor</tt> to this matrix, i.e. the   matrix <tt>factor*matrix</tt> is added to <tt>this</tt>. This function   throws an error if the sparsity patterns of the two involved matrices do   not point to the same object, since in this case the operation is   cheaper.     The source matrix may be a sparse matrix over an arbitrary underlying   scalar type, as long as its data type is convertible to the data type of   this matrix.  
* [0.x.87]*
    [2.x.27]  Entry Access  
* [0.x.88]*
   Return the value of the entry ([1.x.4]).  This may be an expensive   operation and you should always take care where to call this function. In   order to avoid abuse, this function throws an exception if the required   element does not exist in the matrix.     In case you want a function that returns zero instead (for entries that   are not in the sparsity pattern of the matrix), use the el() function.     If you are looping over all elements, consider using one of the iterator   classes instead, since they are tailored better to a sparse matrix   structure.  
* [0.x.89]*
   This function is mostly like operator()() in that it returns the value of   the matrix entry ([1.x.5]). The only difference is that if this entry   does not exist in the sparsity pattern, then instead of raising an   exception, zero is returned. While this may be convenient in some cases,   note that it is simple to write algorithms that are slow compared to an   optimal solution, since the sparsity of the matrix is not used.     If you are looping over all elements, consider using one of the iterator   classes instead, since they are tailored better to a sparse matrix   structure.  
* [0.x.90]*
   Return the main diagonal element in the [1.x.6]th row. This function   throws an error if the matrix is not quadratic.     This function is considerably faster than the operator()(), since for   quadratic matrices, the diagonal entry may be the first to be stored in   each row and access therefore does not involve searching for the right   column number.  
* [0.x.91]*
   Extracts a copy of the values and indices in the given matrix row.     The user is expected to pass the length of the arrays column_indices and   values, which gives a means for checking that we do not write to   unallocated memory. This method is motivated by a similar method in   Trilinos row matrices and gives faster access to entries in the matrix as   compared to iterators which are quite slow for this matrix type.  
* [0.x.92]*
    [2.x.28]  Matrix vector multiplications  
* [0.x.93]*
   Matrix-vector multiplication: let [1.x.7] with [1.x.8] being   this matrix.     Note that while this function can operate on all vectors that offer   iterator classes, it is only really effective for objects of type    [2.x.29] .   For all classes for which iterating over elements, or random member   access is expensive, this function is not efficient. In particular, if   you want to multiply with BlockVector objects, you should consider using   a BlockChunkSparseMatrix as well.     Source and destination must not be the same vector.  
* [0.x.94]*
   Matrix-vector multiplication: let [1.x.9] with   [1.x.10] being this matrix. This function does the same as vmult() but   takes the transposed matrix.     Note that while this function can operate on all vectors that offer   iterator classes, it is only really effective for objects of type    [2.x.30] .   For all classes for which iterating over elements, or random member   access is expensive, this function is not efficient. In particular, if   you want to multiply with BlockVector objects, you should consider using   a BlockChunkSparseMatrix as well.     Source and destination must not be the same vector.  
* [0.x.95]*
   Adding Matrix-vector multiplication. Add [1.x.11] on [1.x.12] with   [1.x.13] being this matrix.     Note that while this function can operate on all vectors that offer   iterator classes, it is only really effective for objects of type    [2.x.31] .   For all classes for which iterating over elements, or random member   access is expensive, this function is not efficient. In particular, if   you want to multiply with BlockVector objects, you should consider using   a BlockChunkSparseMatrix as well.     Source and destination must not be the same vector.  
* [0.x.96]*
   Adding Matrix-vector multiplication. Add [1.x.14] to   [1.x.15] with [1.x.16] being this matrix. This function does the same   as vmult_add() but takes the transposed matrix.     Note that while this function can operate on all vectors that offer   iterator classes, it is only really effective for objects of type    [2.x.32] .   For all classes for which iterating over elements, or random member   access is expensive, this function is not efficient. In particular, if   you want to multiply with BlockVector objects, you should consider using   a BlockChunkSparseMatrix as well.     Source and destination must not be the same vector.  
* [0.x.97]*
   Return the square of the norm of the vector  [2.x.33]  with respect to the norm   induced by this matrix, i.e.  [2.x.34] . This is useful, e.g. in   the finite element context, where the  [2.x.35]  norm of a function equals the   matrix norm with respect to the mass matrix of the vector representing   the nodal values of the finite element function.     Obviously, the matrix needs to be quadratic for this operation, and for   the result to actually be a norm it also needs to be either real   symmetric or complex hermitian.     The underlying template types of both this matrix and the given vector   should either both be real or complex-valued, but not mixed, for this   function to make sense.  
* [0.x.98]*
   Compute the matrix scalar product  [2.x.36] .  
* [0.x.99]*
   Compute the residual of an equation [1.x.17], where the residual is   defined to be [1.x.18]. Write the residual into <tt>dst</tt>. The   [1.x.19] norm of the residual vector is returned.     Source [1.x.20] and destination [1.x.21] must not be the same vector.  
* [0.x.100]*
    [2.x.37]  Matrix norms  
* [0.x.101]*
   Return the l1-norm of the matrix, that is  [2.x.38] , (max. sum of columns).  This is the natural   matrix norm that is compatible to the l1-norm for vectors, i.e.    [2.x.39] .  (cf. Haemmerlin-Hoffmann : Numerische   Mathematik)  
* [0.x.102]*
   Return the linfty-norm of the matrix, that is  [2.x.40] , (max. sum of rows).  This is the natural   matrix norm that is compatible to the linfty-norm of vectors, i.e.    [2.x.41] .  (cf. Haemmerlin-Hoffmann :   Numerische Mathematik)  
* [0.x.103]*
   Return the frobenius norm of the matrix, i.e. the square root of the sum   of squares of all entries in the matrix.  
* [0.x.104]*
    [2.x.42]  Preconditioning methods  
* [0.x.105]*
   Apply the Jacobi preconditioner, which multiplies every element of the   <tt>src</tt> vector by the inverse of the respective diagonal element and   multiplies the result with the relaxation factor <tt>omega</tt>.  
* [0.x.106]*
   Apply SSOR preconditioning to <tt>src</tt>.  
* [0.x.107]*
   Apply SOR preconditioning matrix to <tt>src</tt>.  
* [0.x.108]*
   Apply transpose SOR preconditioning matrix to <tt>src</tt>.  
* [0.x.109]*
   Perform SSOR preconditioning in-place.  Apply the preconditioner matrix   without copying to a second vector.  <tt>omega</tt> is the relaxation   parameter.  
* [0.x.110]*
   Perform an SOR preconditioning in-place.  <tt>omega</tt> is the   relaxation parameter.  
* [0.x.111]*
   Perform a transpose SOR preconditioning in-place.  <tt>omega</tt> is the   relaxation parameter.  
* [0.x.112]*
   Perform a permuted SOR preconditioning in-place.     The standard SOR method is applied in the order prescribed by   <tt>permutation</tt>, that is, first the row <tt>permutation[0]</tt>,   then <tt>permutation[1]</tt> and so on. For efficiency reasons, the   permutation as well as its inverse are required.     <tt>omega</tt> is the relaxation parameter.  
* [0.x.113]*
   Perform a transposed permuted SOR preconditioning in-place.     The transposed SOR method is applied in the order prescribed by   <tt>permutation</tt>, that is, first the row <tt>permutation[m()-1]</tt>,   then <tt>permutation[m()-2]</tt> and so on. For efficiency reasons, the   permutation as well as its inverse are required.     <tt>omega</tt> is the relaxation parameter.  
* [0.x.114]*
   Do one SOR step on <tt>v</tt>.  Performs a direct SOR step with right   hand side <tt>b</tt>.  
* [0.x.115]*
   Do one adjoint SOR step on <tt>v</tt>.  Performs a direct TSOR step with   right hand side <tt>b</tt>.  
* [0.x.116]*
   Do one SSOR step on <tt>v</tt>.  Performs a direct SSOR step with right   hand side <tt>b</tt> by performing TSOR after SOR.  
* [0.x.117]*
    [2.x.43]  Iterators  
* [0.x.118]*
   Iterator starting at first entry of the matrix. This is the version for   constant matrices.     Note that due to the layout in ChunkSparseMatrix, iterating over matrix   entries is considerably slower than for a sparse matrix, as the iterator   is travels row-by-row, whereas data is stored in chunks of several rows   and columns.  
* [0.x.119]*
   Final iterator. This is the version for constant matrices.     Note that due to the layout in ChunkSparseMatrix, iterating over matrix   entries is considerably slower than for a sparse matrix, as the iterator   is travels row-by-row, whereas data is stored in chunks of several rows   and columns.  
* [0.x.120]*
   Iterator starting at the first entry of the matrix. This is the version   for non-constant matrices.     Note that due to the layout in ChunkSparseMatrix, iterating over matrix   entries is considerably slower than for a sparse matrix, as the iterator   is travels row-by-row, whereas data is stored in chunks of several rows   and columns.  
* [0.x.121]*
   Final iterator. This is the version for non-constant matrices.     Note that due to the layout in ChunkSparseMatrix, iterating over matrix   entries is considerably slower than for a sparse matrix, as the iterator   is travels row-by-row, whereas data is stored in chunks of several rows   and columns.  
* [0.x.122]*
   Iterator starting at the first entry of row <tt>r</tt>. This is the   version for constant matrices.     Note that if the given row is empty, i.e. does not contain any nonzero   entries, then the iterator returned by this function equals   <tt>end(r)</tt>. Note also that the iterator may not be dereferenceable in   that case.     Note that due to the layout in ChunkSparseMatrix, iterating over matrix   entries is considerably slower than for a sparse matrix, as the iterator   is travels row-by-row, whereas data is stored in chunks of several rows   and columns.  
* [0.x.123]*
   Final iterator of row <tt>r</tt>. It points to the first element past the   end of line  [2.x.44]  or past the end of the entire sparsity pattern. This is   the version for constant matrices.     Note that the end iterator is not necessarily dereferenceable. This is in   particular the case if it is the end iterator for the last row of a   matrix.     Note that due to the layout in ChunkSparseMatrix, iterating over matrix   entries is considerably slower than for a sparse matrix, as the iterator   is travels row-by-row, whereas data is stored in chunks of several rows   and columns.  
* [0.x.124]*
   Iterator starting at the first entry of row <tt>r</tt>. This is the   version for non-constant matrices.     Note that if the given row is empty, i.e. does not contain any nonzero   entries, then the iterator returned by this function equals   <tt>end(r)</tt>. Note also that the iterator may not be dereferenceable in   that case.     Note that due to the layout in ChunkSparseMatrix, iterating over matrix   entries is considerably slower than for a sparse matrix, as the iterator   is travels row-by-row, whereas data is stored in chunks of several rows   and columns.  
* [0.x.125]*
   Final iterator of row <tt>r</tt>. It points to the first element past the   end of line  [2.x.45]  or past the end of the entire sparsity pattern. This is   the version for non-constant matrices.     Note that the end iterator is not necessarily dereferenceable. This is in   particular the case if it is the end iterator for the last row of a   matrix.     Note that due to the layout in ChunkSparseMatrix, iterating over matrix   entries is considerably slower than for a sparse matrix, as the iterator   is travels row-by-row, whereas data is stored in chunks of several rows   and columns.  
* [0.x.126]*
    [2.x.46]  Input/Output  
* [0.x.127]*
   Print the matrix to the given stream, using the format <tt>(line,col)   value</tt>, i.e. one nonzero entry of the matrix per line.  
* [0.x.128]*
   Print the matrix in the usual format, i.e. as a matrix and not as a list   of nonzero elements. For better readability, elements not in the matrix   are displayed as empty space, while matrix elements which are explicitly   set to zero are displayed as such.     The parameters allow for a flexible setting of the output format:   <tt>precision</tt> and <tt>scientific</tt> are used to determine the   number format, where <tt>scientific = false</tt> means fixed point   notation.  A zero entry for <tt>width</tt> makes the function compute a   width, but it may be changed to a positive value, if output is crude.     Additionally, a character for an empty value may be specified.     Finally, the whole matrix can be multiplied with a common denominator to   produce more readable output, even integers.      [2.x.47]  This function may produce [1.x.22] amounts of output if   applied to a large matrix!  
* [0.x.129]*
   Print the actual pattern of the matrix. For each entry with an absolute   value larger than threshold, a '*' is printed, a ':' for every value   smaller and a '.' for every entry not allocated.  
* [0.x.130]*
   Write the data of this object en bloc to a file. This is done in a binary   mode, so the output is neither readable by humans nor (probably) by other   computers using a different operating system or number format.     The purpose of this function is that you can swap out matrices and   sparsity pattern if you are short of memory, want to communicate between   different programs, or allow objects to be persistent across different   runs of the program.  
* [0.x.131]*
   Read data that has previously been written by block_write() from a file.   This is done using the inverse operations to the above function, so it is   reasonably fast because the bitstream is not interpreted except for a few   numbers up front.     The object is resized on this operation, and all previous contents are   lost. Note, however, that no checks are performed whether new data and   the underlying ChunkSparsityPattern object fit together. It is your   responsibility to make sure that the sparsity pattern and the data to be   read match.     A primitive form of error checking is performed which will recognize the   bluntest attempts to interpret some data as a matrix stored bitwise to a   file that wasn't actually created that way, but not more.  
* [0.x.132]*
    [2.x.48]  Exceptions    [2.x.49]   
* [0.x.133]*
   Exception  
* [0.x.134]*
   Exception  
* [0.x.135]*
   Exception  
* [0.x.136]*
   Exception  
* [0.x.137]*
   Pointer to the sparsity pattern used for this matrix. In order to   guarantee that it is not deleted while still in use, we subscribe to it   using the SmartPointer class.  
* [0.x.138]*
   Array of values for all the nonzero entries. The position of an   entry within the matrix, i.e., the row and column number for a   given value in this array can only be deduced using the sparsity   pattern. The same holds for the more common operation of finding   an entry by its coordinates.  
* [0.x.139]*
   Allocated size of #val. This can be larger than the actually used part if   the size of the matrix was reduced sometime in the past by associating a   sparsity pattern with a smaller size to this object, using the reinit()   function.  
* [0.x.140]*
   Return the location of entry  [2.x.50]  within the val array.  
* [0.x.141]

include/deal.II-translator/lac/chunk_sparse_matrix.templates_0.txt
[0.x.0]*
     Declare type for container size.    
* [0.x.1]*
     Add the result of multiplying a chunk of size chunk_size times     chunk_size by a source vector fragment of size chunk_size to the     destination vector fragment.    
* [0.x.2]*
     Like the previous function, but subtract. We need this for computing     the residual.    
* [0.x.3]*
     Add the result of multiplying the transpose of a chunk of size     chunk_size times chunk_size by a source vector fragment of size     chunk_size to the destination vector fragment.    
* [0.x.4]*
     Produce the result of the matrix scalar product  [2.x.0]  for an     individual chunk.    
* [0.x.5]*
     Perform a vmult_add using the ChunkSparseMatrix data structures, but     only using a subinterval of the matrix rows.         In the sequential case, this function is called on all rows, in the     parallel case it may be called on a subrange, at the discretion of the     task scheduler.    
* [0.x.6]

include/deal.II-translator/lac/chunk_sparsity_pattern_0.txt
[0.x.0]!  [2.x.0]  Sparsity [2.x.1] 

* 
* [0.x.1]*
 Iterators on sparsity patterns

* 
* [0.x.2]*
   Accessor class for iterators into sparsity patterns. This class is also   the base class for both const and non-const accessor classes into sparse   matrices.     Note that this class only allows read access to elements, providing their   row and column number. It does not allow modifying the sparsity pattern   itself.  
* [0.x.3]*
     Declare the type for container size.    
* [0.x.4]*
     Constructor.    
* [0.x.5]*
     Constructor. Construct the end accessor for the given sparsity pattern.    
* [0.x.6]*
     Row number of the element represented by this object. This function can     only be called for entries for which is_valid_entry() is true.    
* [0.x.7]*
     Return the global index from the reduced sparsity pattern.    
* [0.x.8]*
     Column number of the element represented by this object. This function     can only be called for entries for which is_valid_entry() is true.    
* [0.x.9]*
     Return whether the sparsity pattern entry pointed to by this iterator     is valid or not. Note that after compressing the sparsity pattern, all     entries are valid. However, before compression, the sparsity pattern     allocated some memory to be used while still adding new nonzero     entries; if you create iterators in this phase of the sparsity     pattern's lifetime, you will iterate over elements that are not valid.     If this is so, then this function will return false.    
* [0.x.10]*
     Comparison. True, if both iterators point to the same matrix position.    
* [0.x.11]*
     Comparison operator. Result is true if either the first row number is     smaller or if the row numbers are equal and the first index is smaller.         This function is only valid if both iterators point into the same     sparsity pattern.    
* [0.x.12]*
     The sparsity pattern we operate on accessed.    
* [0.x.13]*
     The accessor of the (reduced) sparsity pattern.    
* [0.x.14]*
     Current chunk row number.    
* [0.x.15]*
     Current chunk col number.    
* [0.x.16]*
     Move the accessor to the next nonzero entry in the matrix.    
* [0.x.17]*
   Iterator that walks over the elements of a sparsity pattern.  
* [0.x.18]*
     Declare the type for container size.    
* [0.x.19]*
     Constructor. Create an iterator into the sparsity pattern  [2.x.2]  for the     given row and the index within it.    
* [0.x.20]*
     Prefix increment.    
* [0.x.21]*
     Postfix increment.    
* [0.x.22]*
     Dereferencing operator.    
* [0.x.23]*
     Dereferencing operator.    
* [0.x.24]*
     Comparison. True, if both iterators point to the same matrix position.    
* [0.x.25]*
     Inverse of <tt>==</tt>.    
* [0.x.26]*
     Comparison operator. Result is true if either the first row number is     smaller or if the row numbers are equal and the first index is smaller.         This function is only valid if both iterators point into the same     matrix.    
* [0.x.27]*
     Store an object of the accessor class.    
* [0.x.28]*
 Structure representing the sparsity pattern of a sparse matrix. This class is an example of the "static" type of  [2.x.3] . It uses the compressed row storage (CSR) format to store data.
*  The use of this class is demonstrated in  [2.x.4] .

* 
* [0.x.29]*
   Declare the type for container size.  
* [0.x.30]*
   Typedef an iterator class that allows to walk over all nonzero elements   of a sparsity pattern.  
* [0.x.31]*
   Typedef an iterator class that allows to walk over all nonzero elements   of a sparsity pattern.     Since the iterator does not allow to modify the sparsity pattern, this   type is the same as that for  [2.x.5]   
* [0.x.32]*
   Define a value which is used to indicate that a certain value in the   colnums array is unused, i.e. does not represent a certain column number   index.     Indices with this invalid value are used to insert new entries to the   sparsity pattern using the add() member function, and are removed when   calling compress().     You should not assume that the variable declared here has a certain   value. The initialization is given here only to enable the compiler to   perform some optimizations, but the actual value of the variable may   change over time.  
* [0.x.33]*
   Initialize the matrix empty, that is with no memory allocated. This is   useful if you want such objects as member variables in other classes. You   can make the structure usable by calling the reinit() function.  
* [0.x.34]*
   Copy constructor. This constructor is only allowed to be called if the   matrix structure to be copied is empty. This is so in order to prevent   involuntary copies of objects for temporaries, which can use large   amounts of computing time. However, copy constructors are needed if one   wants to place a ChunkSparsityPattern in a container, e.g., to write such   statements like <tt>v.push_back (ChunkSparsityPattern());</tt>, with   <tt>v</tt> a vector of ChunkSparsityPattern objects.     Usually, it is sufficient to use the explicit keyword to disallow   unwanted temporaries, but this does not work for  [2.x.6]    Since copying a structure like this is not useful anyway because multiple   matrices can use the same sparsity structure, copies are only allowed for   empty objects, as described above.  
* [0.x.35]*
   Initialize a rectangular matrix.      [2.x.7]  m number of rows  [2.x.8]  n number of columns  [2.x.9]  max_per_row maximum   number of nonzero entries per row  
* [0.x.36]*
   Initialize a rectangular matrix.      [2.x.10]  m number of rows  [2.x.11]  n number of columns  [2.x.12]  row_lengths possible   number of nonzero entries for each row.  This vector must have one entry   for each row.  
* [0.x.37]*
   Initialize a quadratic matrix of dimension <tt>n</tt> with at most   <tt>max_per_row</tt> nonzero entries per row.     This constructor automatically enables optimized storage of diagonal   elements. To avoid this, use the constructor taking row and column   numbers separately.  
* [0.x.38]*
   Initialize a quadratic matrix.      [2.x.13]  m number of rows and columns  [2.x.14]  row_lengths possible number of   nonzero entries for each row.  This vector must have one entry for each   row.  
* [0.x.39]*
   Destructor.  
* [0.x.40]*
   Copy operator. For this the same holds as for the copy constructor: it is   declared, defined and fine to be called, but the latter only for empty   objects.  
* [0.x.41]*
   Reallocate memory and set up data structures for a new matrix with <tt>m   </tt>rows and <tt>n</tt> columns, with at most <tt>max_per_row</tt>   nonzero entries per row.     This function simply maps its operations to the other <tt>reinit</tt>   function.  
* [0.x.42]*
   Reallocate memory for a matrix of size <tt>m x n</tt>. The number of   entries for each row is taken from the array <tt>row_lengths</tt> which   has to give this number of each row <tt>i=1...m</tt>.     If <tt>m*n==0</tt> all memory is freed, resulting in a total   reinitialization of the object. If it is nonzero, new memory is only   allocated if the new size extends the old one. This is done to save time   and to avoid fragmentation of the heap.     If the number of rows equals the number of columns then diagonal elements   are stored first in each row to allow optimized access in relaxation   methods of SparseMatrix.  
* [0.x.43]*
   Same as above, but with an ArrayView argument instead.  
* [0.x.44]*
   This function compresses the sparsity structure that this object   represents.  It does so by eliminating unused entries and sorting the   remaining ones to allow faster access by usage of binary search   algorithms. A special sorting scheme is used for the diagonal entry of   quadratic matrices, which is always the first entry of each row.     The memory which is no more needed is released.     SparseMatrix objects require the ChunkSparsityPattern objects they are   initialized with to be compressed, to reduce memory requirements.  
* [0.x.45]*
   This function can be used as a replacement for reinit(), subsequent calls   to add() and a final call to close() if you know exactly in advance the   entries that will form the matrix sparsity pattern.     The first two parameters determine the size of the matrix. For the two   last ones, note that a sparse matrix can be described by a sequence of   rows, each of which is represented by a sequence of pairs of column   indices and values. In the present context, the begin() and end()   parameters designate iterators (of forward iterator type) into a   container, one representing one row. The distance between begin() and   end() should therefore be equal to n_rows(). These iterators may be   iterators of  [2.x.15]   [2.x.16]  pointers into a   C-style array, or any other iterator satisfying the requirements of a   forward iterator. The objects pointed to by these iterators (i.e. what we   get after applying <tt>operator*</tt> or <tt>operator-></tt> to one of   these iterators) must be a container itself that provides functions   <tt>begin</tt> and <tt>end</tt> designating a range of iterators that   describe the contents of one line. Dereferencing these inner iterators   must either yield a pair of an integer as column index and a   value of arbitrary type (such a type would be used if we wanted to   describe a sparse matrix with one such object), or simply an integer (if we   only wanted to describe a sparsity pattern). The function is able to   determine itself whether an integer or a pair is what we get after   dereferencing the inner iterators, through some template magic.     While the order of the outer iterators denotes the different rows of the   matrix, the order of the inner iterator denoting the columns does not   matter, as they are sorted internal to this function anyway.     Since that all sounds very complicated, consider the following example   code, which may be used to fill a sparsity pattern:  
* [1.x.0]
*      Note that this example works since the iterators dereferenced yield   containers with functions <tt>begin</tt> and <tt>end</tt> (namely    [2.x.17]  and the inner iterators dereferenced yield   integers as column indices. Note that we could have replaced   each of the two  [2.x.18]  occurrences by  [2.x.19]    and the inner one by  [2.x.20]  as well.     Another example would be as follows, where we initialize a whole matrix,   not only a sparsity pattern:  
* [1.x.1]
*      This example works because dereferencing iterators of the inner type   yields a pair of integers and a value, the first of which we   take as column index. As previously, the outer  [2.x.21]  could   be replaced by  [2.x.22]  and the inner    [2.x.23]  could be replaced by    [2.x.24]  ></tt>, or a list or set of   such pairs, as they all return iterators that point to such pairs.  
* [0.x.46]*
   Copy data from an object of type DynamicSparsityPattern. Previous content   of this object is lost, and the sparsity pattern is in compressed mode   afterwards.  
* [0.x.47]*
   Take a full matrix and use its nonzero entries to generate a sparse   matrix entry pattern for this object.     Previous content of this object is lost, and the sparsity pattern is in   compressed mode afterwards.  
* [0.x.48]*
   Set the sparsity pattern of the chunk sparsity pattern to be given by   <tt>chunk_size*chunksize</tt> blocks of the sparsity pattern for chunks   specified. Note that the final number of rows <tt>m</tt> of the sparsity   pattern will be approximately <tt>sparsity_pattern_for_chunks.n_rows()   chunk_size</tt> (modulo padding elements in the last chunk) and similarly   for the number of columns <tt>n</tt>.     This is a special initialization option in case you can tell the position   of the chunk already from the beginning without generating the sparsity   pattern using <tt>make_sparsity_pattern</tt> calls. This bypasses the   search for chunks but of course needs to be handled with care in order to   give a correct sparsity pattern.     Previous content of this object is lost, and the sparsity pattern is in   compressed mode afterwards.  
* [0.x.49]*
   Return whether the object is empty. It is empty if no memory is   allocated, which is the same as that both dimensions are zero.  
* [0.x.50]*
   Return the chunk size given as argument when constructing this object.  
* [0.x.51]*
   Return the maximum number of entries per row. Before compression, this   equals the number given to the constructor, while after compression, it   equals the maximum number of entries actually allocated by the user.  
* [0.x.52]*
   Add a nonzero entry to the matrix. This function may only be called for   non-compressed sparsity patterns.     If the entry already exists, nothing bad happens.  
* [0.x.53]*
   Make the sparsity pattern symmetric by adding the sparsity pattern of the   transpose object.     This function throws an exception if the sparsity pattern does not   represent a quadratic matrix.  
* [0.x.54]*
   Return number of rows of this matrix, which equals the dimension of the   image space.  
* [0.x.55]*
   Return number of columns of this matrix, which equals the dimension of   the range space.  
* [0.x.56]*
   Check if a value at a certain position may be non-zero.  
* [0.x.57]*
   Number of entries in a specific row.  
* [0.x.58]*
   Compute the bandwidth of the matrix represented by this structure. The   bandwidth is the maximum of  [2.x.25]  for which the index pair  [2.x.26]    represents a nonzero entry of the matrix. Consequently, the maximum   bandwidth a  [2.x.27]  matrix can have is  [2.x.28] .  
* [0.x.59]*
   Return the number of nonzero elements of this matrix. Actually, it   returns the number of entries in the sparsity pattern; if any of the   entries should happen to be zero, it is counted anyway.     This function may only be called if the matrix struct is compressed. It   does not make too much sense otherwise anyway.  
* [0.x.60]*
   Return whether the structure is compressed or not.  
* [0.x.61]*
   Return whether this object stores only those entries that have been added   explicitly, or if the sparsity pattern contains elements that have been   added through other means (implicitly) while building it. For the current   class, the result is true if and only if it is square because it then   unconditionally stores the diagonal entries whether they have been added   explicitly or not.     This function mainly serves the purpose of describing the current class   in cases where several kinds of sparsity patterns can be passed as   template arguments.  
* [0.x.62]*
   Iterator starting at the first entry of the matrix. The resulting   iterator can be used to walk over all nonzero entries of the sparsity   pattern.  
* [0.x.63]*
   Final iterator.  
* [0.x.64]*
   Iterator starting at the first entry of row <tt>r</tt>.     Note that if the given row is empty, i.e. does not contain any nonzero   entries, then the iterator returned by this function equals   <tt>end(r)</tt>. Note also that the iterator may not be dereferenceable in   that case.  
* [0.x.65]*
   Final iterator of row <tt>r</tt>. It points to the first element past the   end of line  [2.x.29]  or past the end of the entire sparsity pattern.     Note that the end iterator is not necessarily dereferenceable. This is in   particular the case if it is the end iterator for the last row of a   matrix.  
* [0.x.66]*
   Write the data of this object en bloc to a file. This is done in a binary   mode, so the output is neither readable by humans nor (probably) by other   computers using a different operating system of number format.     The purpose of this function is that you can swap out matrices and   sparsity pattern if you are short of memory, want to communicate between   different programs, or allow objects to be persistent across different   runs of the program.  
* [0.x.67]*
   Read data that has previously been written by block_write() from a file.   This is done using the inverse operations to the above function, so it is   reasonably fast because the bitstream is not interpreted except for a few   numbers up front.     The object is resized on this operation, and all previous contents are   lost.     A primitive form of error checking is performed which will recognize the   bluntest attempts to interpret some data as a vector stored bitwise to a   file, but not more.  
* [0.x.68]*
   Print the sparsity of the matrix. The output consists of one line per row   of the format <tt>[i,j1,j2,j3,...]</tt>. [1.x.2] is the row number and   [1.x.3] are the allocated columns in this row.  
* [0.x.69]*
   Print the sparsity of the matrix in a format that <tt>gnuplot</tt>   understands and which can be used to plot the sparsity pattern in a   graphical way. The format consists of pairs <tt>i j</tt> of nonzero   elements, each representing one entry of this matrix, one per line of the   output file. Indices are counted from zero on, as usual. Since sparsity   patterns are printed in the same way as matrices are displayed, we print   the negative of the column index, which means that the <tt>(0,0)</tt>   element is in the top left rather than in the bottom left corner.     Print the sparsity pattern in gnuplot by setting the data style to dots   or points and use the <tt>plot</tt> command.  
* [0.x.70]*
   Determine an estimate for the memory consumption (in bytes) of this   object. See MemoryConsumption.  
* [0.x.71]*
    [2.x.30]  Exceptions    [2.x.31]   
* [0.x.72]*
   Exception  
* [0.x.73]*
   Exception  
* [0.x.74]*
   Exception  
* [0.x.75]*
   The operation is only allowed after the SparsityPattern has been set up   and compress() was called.  
* [0.x.76]*
   This operation changes the structure of the SparsityPattern and is not   possible after compress() has been called.  
* [0.x.77]*
   Exception  
* [0.x.78]*
   Exception  
* [0.x.79]*
   Exception  
* [0.x.80]*
   Exception  
* [0.x.81]*
   Exception  
* [0.x.82]*
   Number of rows that this sparsity structure shall represent.  
* [0.x.83]*
   Number of columns that this sparsity structure shall represent.  
* [0.x.84]*
   The size of chunks.  
* [0.x.85]*
   The reduced sparsity pattern. We store only which chunks exist, with each   chunk a block in the matrix of size chunk_size by chunk_size.  
* [0.x.86]

include/deal.II-translator/lac/communication_pattern_base_0.txt
[0.x.0]*
   Alias for  [2.x.0]  This class was   originally defined in the LinearAlgebra namespace but is now used for more   general purposes.  
* [0.x.1]

include/deal.II-translator/lac/constrained_linear_operator_0.txt
[0.x.0]*
  [2.x.0]  Indirectly applying constraints to LinearOperator

* 
* [0.x.1]*
 This function takes an AffineConstraints object  [2.x.1]  and an operator exemplar  [2.x.2]  (this exemplar is usually a linear operator that describes the system matrix
* 
*  - it is only used to create domain and range vectors of appropriate sizes, its action <tt>vmult</tt> is never used). A LinearOperator object associated with the "homogeneous action" of the underlying AffineConstraints object is returned:
*  Applying the LinearOperator object on a vector  [2.x.3]  results in a vector  [2.x.4]  that stores the result of calling  [2.x.5]  on  [2.x.6] 
* 
*  - with one important difference: inhomogeneities are not applied, but always treated as 0 instead.
*  The LinearOperator object created by this function is primarily used internally in constrained_linear_operator() to build up a modified system of linear equations. How to solve a linear system of equations with this approach is explained in detail in the  [2.x.7]  module.
* 

* 

* 
*  [2.x.8]  Currently, this function may not work correctly for distributed data structures.
*   [2.x.9]  LinearOperator

* 
*  [2.x.10] 

* 
* [0.x.2]*
 Given a AffineConstraints  [2.x.11]  and an operator exemplar  [2.x.12]  exemplar, return a LinearOperator that is the projection to the subspace of constrained degrees of freedom, i.e. all entries of the result vector that correspond to unconstrained degrees of freedom are set to zero.
* 

*   [2.x.13]  LinearOperator

* 
*  [2.x.14] 

* 
* [0.x.3]*
 Given a AffineConstraints object  [2.x.15]  and a LinearOperator  [2.x.16]  this function creates a LinearOperator object consisting of the composition of three operations and a regularization:

* 
* [1.x.0]
*  with

* 
* [1.x.1]
*  and  [2.x.17]  is the projection to the subspace consisting of all vector entries associated with constrained degrees of freedom.
*  This LinearOperator object is used together with constrained_right_hand_side() to build up the following modified system of linear equations: [1.x.2] with a given (unconstrained) system matrix  [2.x.18] , right hand side  [2.x.19] , and linear constraints  [2.x.20]  with inhomogeneities  [2.x.21] .
*  A detailed explanation of this approach is given in the  [2.x.22]  module.
* 

* 

* 
*  [2.x.23]  Currently, this function may not work correctly for distributed data structures.
*   [2.x.24]  LinearOperator

* 
*  [2.x.25] 

* 
* [0.x.4]*
 Given a AffineConstraints object  [2.x.26]  a LinearOperator  [2.x.27]  linop and a right-hand side  [2.x.28]  this function creates a PackagedOperation that stores the following computation:

* 
* [1.x.3]
*  with

* 
* [1.x.4]
* 
*  This LinearOperator object is used together with constrained_right_hand_side() to build up the following modified system of linear equations: [1.x.5] with a given (unconstrained) system matrix  [2.x.29] , right hand side  [2.x.30] , and linear constraints  [2.x.31]  with inhomogeneities  [2.x.32] .
*  A detailed explanation of this approach is given in the  [2.x.33]  module.
* 

* 

* 
*  [2.x.34]  Currently, this function may not work correctly for distributed data structures.
*   [2.x.35]  LinearOperator

* 
*  [2.x.36] 

* 
* [0.x.5]

include/deal.II-translator/lac/constraint_matrix_0.txt
[0.x.0]

include/deal.II-translator/lac/cuda_atomic_0.txt
[0.x.0]*
     Provide atomicAdd for floats.          [2.x.0]  Use atomicAdd(address, val) directly.    
*  [2.x.1]     
* [0.x.1]*
     Provide atomicAdd for doubles.          [2.x.2]  Use atomicAdd(address, val) directly.    
*  [2.x.3]     
* [0.x.2]*
     Provide atomicMax for floats.        
*  [2.x.4]     
* [0.x.3]*
     Provide atomicMax for doubles.        
*  [2.x.5]     
* [0.x.4]

include/deal.II-translator/lac/cuda_kernels_0.txt
[0.x.0]*
     Namespace containing the CUDA kernels.    
* [0.x.1]*
       Multiply each entry of  [2.x.0]  of size  [2.x.1]  by  [2.x.2]             
*  [2.x.3]       
* [0.x.2]*
       Functor defining the addition of two Numbers.            
*  [2.x.4]       
* [0.x.3]*
       Functor defining the subtraction of two Numbers.            
*  [2.x.5]       
* [0.x.4]*
       Functor defining the maximum of two Numbers.            
*  [2.x.6]       
* [0.x.5]*
       Functor defining the maximum of two Numbers.            
*  [2.x.7]       
* [0.x.6]*
       Apply the functor  [2.x.8]  to each element of  [2.x.9]  and  [2.x.10]             
*  [2.x.11]       
* [0.x.7]*
       Apply the functor  [2.x.12]  to the elements of  [2.x.13]  that have       indices in  [2.x.14]  and  [2.x.15]  The size of  [2.x.16]  should be greater       than the size of  [2.x.17]   [2.x.18]  and  [2.x.19]  should have the same size  [2.x.20]        N.            
*  [2.x.21]       
* [0.x.8]*
       Structure implementing the functions used to add elements when       using a reduction.            
*  [2.x.22]       
* [0.x.9]*
       Structure implementing the functions used to compute the L1 norm       when using a reduction.            
*  [2.x.23]       
* [0.x.10]*
       Structure implementing the functions used to compute the L-infinity       norm when using a reduction.            
*  [2.x.24]       
* [0.x.11]*
       Perform a reduction on  [2.x.25]  using  [2.x.26]             
*  [2.x.27]       
* [0.x.12]*
       Structure implementing the functions used to compute the dot       product norm when using a double vector reduction.            
*  [2.x.28]       
* [0.x.13]*
       Perform a binary operation on each element of  [2.x.29]  and  [2.x.30]  followed       by reduction on the resulting array.            
*  [2.x.31]       
* [0.x.14]*
       Add  [2.x.32]  to each element of  [2.x.33]             
*  [2.x.34]       
* [0.x.15]*
       Addition of a multiple of a vector, i.e., <tt>val += a*V_val</tt>.            
*  [2.x.35]       
* [0.x.16]*
       Addition of multiple scaled vector, i.e., <tt>val += a*V_val +       b*W_val</tt>.            
*  [2.x.36]       
* [0.x.17]*
       Scaling and simple addition of a multiple of a vector, i.e. <tt>val       = = s*val + a*V_val</tt>            
*  [2.x.37]       
* [0.x.18]*
       Scaling and multiple additions of scaled vectors, i.e. <tt>val =       = s*val + a*V_val + b*W_val</tt>            
*  [2.x.38]       
* [0.x.19]*
       Scale each element of this vector by the corresponding element in       the argument.            
*  [2.x.39]       
* [0.x.20]*
       Assignment <tt>val = a*V_val</tt>.            
*  [2.x.40]       
* [0.x.21]*
       Assignment <tt>val = a*V_val + b*W_val</tt>.            
*  [2.x.41]       
* [0.x.22]*
       Perform a combined operation of a vector addition and a subsequent       inner product, returning the value of the inner product.            
*  [2.x.42]       
* [0.x.23]*
       Set each element of  [2.x.43]  to  [2.x.44]             
*  [2.x.45]       
* [0.x.24]*
       Set each element in  [2.x.46]  to  [2.x.47]  using  [2.x.48]  as permutation, i.e.,       <tt>val[indices[i]] = v[i]</tt>.            
*  [2.x.49]       
* [0.x.25]*
       Set each element in  [2.x.50]  to  [2.x.51]  using  [2.x.52]  as permutation, i.e.,       <tt>val[i] = v[indices[i]]</tt>.            
*  [2.x.53]       
* [0.x.26]*
       Add each element in  [2.x.54]  to  [2.x.55]  using  [2.x.56]  as permutation, i.e.,       <tt>val[indices[i]] += v[i]</tt>.            
*  [2.x.57]       
* [0.x.27]

include/deal.II-translator/lac/cuda_kernels.templates_0.txt
[0.x.0]

include/deal.II-translator/lac/cuda_precondition_0.txt
[0.x.0]*
   This class implements an incomplete Cholesky factorization (IC)   preconditioner for  [2.x.0]  symmetric  [2.x.1]  matrices.     The implementation closely follows the one documented in the cuSPARSE   documentation   (https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-lt-t-gt-csric02).    
*  [2.x.2]  Instantiations for this template are provided for <tt> [2.x.3]  and    [2.x.4]     
*  [2.x.5]   
* [0.x.1]*
     Declare the type for container size.    
* [0.x.2]*
     Standardized data struct to pipe additional flags to the     preconditioner.    
* [0.x.3]*
       Constructor. cuSPARSE allows to compute and use level information.       According to the documentation this might improve performance.       It is suggested to try both options.      
* [0.x.4]*
       Flag that determines if level information is used when creating and       applying the preconditioner. See the documentation for       cusparseSolvePolicy_t at       https://docs.nvidia.com/cuda/cusparse/index.html#cusparsesolvepolicy_t       for more information.      
* [0.x.5]*
     Constructor.    
* [0.x.6]*
     The copy constructor is deleted.    
* [0.x.7]*
     The copy assignment operator is deleted.    
* [0.x.8]*
     Destructor. Free all resources that were initialized in this class.    
* [0.x.9]*
     Initialize this object. In particular, the given matrix is copied to be     modified in-place. For the underlying sparsity pattern pointers are     stored. Specifically, this means     that the current object can only be used reliably as long as  [2.x.6]  is valid     and has not been changed since calling this function.         The  [2.x.7]  determines if level information are used.    
* [0.x.10]*
     Apply the preconditioner.    
* [0.x.11]*
     Apply the preconditioner. Since the preconditioner is symmetric, this     is the same as vmult().    
* [0.x.12]*
     Return the dimension of the codomain (or range) space. Note that the     matrix is square and has dimension  [2.x.8] .        
*  [2.x.9]  This function should only be called if the preconditioner has been     initialized.    
* [0.x.13]*
     Return the dimension of the codomain (or range) space. Note that the     matrix is square and has dimension  [2.x.10] .        
*  [2.x.11]  This function should only be called if the preconditioner has been     initialized.    
* [0.x.14]*
     cuSPARSE handle used to call cuSPARSE functions.    
* [0.x.15]*
     cuSPARSE description of the sparse matrix  [2.x.12] .    
* [0.x.16]*
     cuSPARSE description of the lower triangular matrix  [2.x.13] .    
* [0.x.17]*
     Solve and analysis structure for  [2.x.14] .    
* [0.x.18]*
     Solve and analysis structure for the lower triangular matrix  [2.x.15] .    
* [0.x.19]*
     Solve and analysis structure for the upper triangular matrix  [2.x.16] .    
* [0.x.20]*
     Pointer to the matrix this object was initialized with.    
* [0.x.21]*
     Pointer to the values (on the device) of the computed preconditioning     matrix.    
* [0.x.22]*
     Pointer to the row pointer (on the device) of the sparse matrix this     object was initialized with. Guarded by matrix_pointer.    
* [0.x.23]*
     Pointer to the column indices (on the device) of the sparse matrix this     object was initialized with. Guarded by matrix_pointer.    
* [0.x.24]*
     Pointer to the value (on the device) for a temporary (helper) vector     used in vmult().    
* [0.x.25]*
     Pointer to an internal buffer (on the device) that is used for     computing the decomposition.    
* [0.x.26]*
     Determine if level information should be generated for the lower     triangular matrix  [2.x.17] . This value can be modified through an     AdditionalData object.    
* [0.x.27]*
     Determine if level information should be generated for the upper     triangular matrix  [2.x.18] . This value can be modified through an     AdditionalData object.    
* [0.x.28]*
     Determine if level information should be generated for  [2.x.19] . This     value can be modified through an AdditionalData object.    
* [0.x.29]*
     The number of rows is the same as for the matrix this object has been     initialized with.    
* [0.x.30]*
     The number of non-zero elements is the same as for the matrix this     object has been initialized with.    
* [0.x.31]*
   This class implements an incomplete LU factorization preconditioner for    [2.x.20]  matrices.     The implementation closely follows the one documented in the cuSPARSE   documentation   (https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-lt-t-gt-csrilu02).    
*  [2.x.21]  Instantiations for this template are provided for <tt> [2.x.22]  and    [2.x.23]     
*  [2.x.24]   
* [0.x.32]*
     Declare the type for container size.    
* [0.x.33]*
     Standardized data struct to pipe additional flags to the     preconditioner.    
* [0.x.34]*
       Constructor. cuSPARSE allows to compute and use level information.        to the documentation this might improve performance.       It is suggested to try both options.      
* [0.x.35]*
       Flag that determines if level information is used when creating and       applying the preconditioner. See the documentation for       cusparseSolvePolicy_t at       https://docs.nvidia.com/cuda/cusparse/index.html#cusparsesolvepolicy_t       for more information.      
* [0.x.36]*
     Constructor.    
* [0.x.37]*
     The copy constructor is deleted.    
* [0.x.38]*
     The copy assignment operator is deleted.    
* [0.x.39]*
     Destructor. Free all resources that were initialized in this class.    
* [0.x.40]*
     Initialize this object. In particular, the given matrix is copied to be     modified in-place. For the underlying sparsity pattern pointers are     stored. Specifically, this means     that the current object can only be used reliably as long as  [2.x.25]  is valid     and has not been changed since calling this function.         The  [2.x.26]  determines if level information are used.    
* [0.x.41]*
     Apply the preconditioner.    
* [0.x.42]*
     Apply the transposed preconditioner. Not yet implemented.    
* [0.x.43]*
     Return the dimension of the codomain (or range) space. Note that the     matrix is square and has dimension  [2.x.27] .        
*  [2.x.28]  This function should only be called if the preconditioner has been     initialized.    
* [0.x.44]*
     Return the dimension of the codomain (or range) space. Note that the     matrix is square and has dimension  [2.x.29] .        
*  [2.x.30]  This function should only be called if the preconditioner has been     initialized.    
* [0.x.45]*
     cuSPARSE handle used to call cuSPARSE functions.    
* [0.x.46]*
     cuSPARSE description of the sparse matrix  [2.x.31] .    
* [0.x.47]*
     cuSPARSE description of the lower triangular matrix  [2.x.32] .    
* [0.x.48]*
     cuSPARSE description of the upper triangular matrix  [2.x.33] .    
* [0.x.49]*
     Solve and analysis structure for  [2.x.34] .    
* [0.x.50]*
     Solve and analysis structure for the lower triangular matrix  [2.x.35] .    
* [0.x.51]*
     Solve and analysis structure for the upper triangular matrix  [2.x.36] .    
* [0.x.52]*
     Pointer to the matrix this object was initialized with.    
* [0.x.53]*
     Pointer to the values (on the device) of the computed preconditioning     matrix.    
* [0.x.54]*
     Pointer to the row pointer (on the device) of the sparse matrix this     object was initialized with. Guarded by matrix_pointer.    
* [0.x.55]*
     Pointer to the column indices (on the device) of the sparse matrix this     object was initialized with. Guarded by matrix_pointer.    
* [0.x.56]*
     Pointer to the value (on the device) for a temporary (helper) vector     used in vmult().    
* [0.x.57]*
     Pointer to an internal buffer (on the device) that is used for     computing the decomposition.    
* [0.x.58]*
     Determine if level information should be generated for the lower     triangular matrix  [2.x.37] . This value can be modified through an     AdditionalData object.    
* [0.x.59]*
     Determine if level information should be generated for the upper     triangular matrix  [2.x.38] . This value can be modified through an     AdditionalData object.    
* [0.x.60]*
     Determine if level information should be generated for  [2.x.39] . This     value can be modified through an AdditionalData object.    
* [0.x.61]*
     The number of rows is the same as for the matrix this object has been     initialized with.    
* [0.x.62]*
     The number of non-zero elements is the same as for the matrix this     object has been initialized with.    
* [0.x.63]

include/deal.II-translator/lac/cuda_solver_direct_0.txt
[0.x.0]*
   Direct solvers. These solvers call cuSOLVER underneath.    
*  [2.x.0]  Instantiations for this template are provided for <tt> [2.x.1]    and <tt> [2.x.2]     
*  [2.x.3]   
* [0.x.1]*
     Struct for additional settings for SolverDirect.    
* [0.x.2]*
       Set the additional data field to the desired solver.      
* [0.x.3]*
       Set the solver type. Possibilities are:        [2.x.4]         [2.x.5]  "Cholesky" which performs a Cholesky decomposition on the device        [2.x.6]         [2.x.7]  "LU_dense" which converts the sparse matrix to a dense       matrix and uses LU factorization  [2.x.8]         [2.x.9]  "LU_host" which uses LU factorization on the host  [2.x.10]         [2.x.11]       
* [0.x.4]*
     Constructor. Takes the solver control object and creates the solver.    
* [0.x.5]*
     Destructor.    
* [0.x.6]*
     Solve the linear system <tt>Ax=b</tt>.    
* [0.x.7]*
     Access to object that controls convergence.    
* [0.x.8]*
     Handle    
* [0.x.9]*
     Reference to the object that controls convergence of the iterative     solver. In fact, for these CUDA wrappers, cuSOLVER and cuSPARSE do so     themselves, but we copy the data from this object before starting the     solution process, and copy the data back into it afterwards.    
* [0.x.10]*
     Store a copy of the flags for this particular solver.    
* [0.x.11]

include/deal.II-translator/lac/cuda_sparse_matrix_0.txt
[0.x.0]*
   This class is a wrapper around cuSPARSE csr sparse matrix. Unlike deal.II's   own SparseMatrix all elements within each row are stored in increasing   column index order.    
*  [2.x.0]  Instantiations for this template are provided for <tt> [2.x.1]  and    [2.x.2]     
*  [2.x.3]   
* [0.x.1]*
     Declare type for container size.    
* [0.x.2]*
     Type of the matrix entries.    
* [0.x.3]*
     Declare a type that holds real-valued numbers with the same precision     as the template argument to this class.    
* [0.x.4]*
      [2.x.4]  Constructors and initialization    
* [0.x.5]*
     Constructor. Initialize the matrix to be empty, without any structure,     i.e., the matrix is not usable at all. This constructor is therefore     only useful for matrices which are members of a class.         You have to initialize the matrix before usage with reinit.    
* [0.x.6]*
     Constructor. Takes a  [2.x.5]  and a sparse matrix on the     host. The sparse matrix on the host is copied on the device and the     elements are reordered according to the format supported by cuSPARSE.    
* [0.x.7]*
     Move constructor. Create a new SparseMatrix by stealing the internal     data.    
* [0.x.8]*
     Copy constructor is deleted.    
* [0.x.9]*
     Destructor. Free all memory.    
* [0.x.10]*
     Move assignment operator.    
* [0.x.11]*
     Copy assignment is deleted.    
* [0.x.12]*
     Reinitialize the sparse matrix. The sparse matrix on the host is copied     to the device and the elementes are reordered according to the format     supported by cuSPARSE.    
* [0.x.13]*
      [2.x.6]  Information on the matrix    
* [0.x.14]*
     Return the dimension of the codomain (or range) space. Note that the     matrix is of dimension  [2.x.7] .    
* [0.x.15]*
     Return the dimension of the domain space. Note that the matrix is of     dimension  [2.x.8] .    
* [0.x.16]*
     Return the number of nonzero elements of this matrix. Actually, it     returns the number of entries in the sparsity pattern; if any of the     entries should happen to be zero, it is counted anyway.    
* [0.x.17]*
     Print the matrix to the given stream, using the format <tt>(row,column)     value</tt>, i.e. one nonzero entry of the matrix per line. If     <tt>across</tt> is true, print all entries on a single line, using the     format row,column:value.         If the argument <tt>diagonal_first</tt> is true, diagonal elements of     quadratic matrices are printed first in their row. If it is false,     the elements in a row are written in ascending column order.    
* [0.x.18]*
     Print the matrix in the usual format, i.e. as a matrix and not as a list     of nonzero elements. For better readability, elements not in the matrix     are displayed as empty space, while matrix elements which are explicitly     set to zero are displayed as such.         The parameters allow for a flexible setting of the output format:     <tt>precision</tt> and <tt>scientific</tt> are used to determine the     number format, where <tt>scientific = false</tt> means fixed point     notation.  A zero entry for <tt>width</tt> makes the function compute a     width, but it may be changed to a positive value, if output is crude.         Additionally, a character for an empty value may be specified.         Finally, the whole matrix can be multiplied with a common denominator to     produce more readable output, even integers.          [2.x.9]  This function may produce [1.x.0] amounts of output if     applied to a large matrix!    
* [0.x.19]*
      [2.x.10]  Modifying entries    
* [0.x.20]*
     Multiply the entire matrix by a fixed factor.    
* [0.x.21]*
     Divide the entire matrix by a fixed factor.    
* [0.x.22]*
      [2.x.11]  Multiplications    
* [0.x.23]*
     Matrix-vector multiplication: let  [2.x.12]  with  [2.x.13]      being this matrix.    
* [0.x.24]*
     Matrix-vector multiplication: let  [2.x.14]  with      [2.x.15]  being this matrix. This function does the same as vmult() but     takes this transposed matrix.    
* [0.x.25]*
     Adding matrix-vector multiplication. Add  [2.x.16]  on  [2.x.17]      with  [2.x.18]  being this matrix.    
* [0.x.26]*
     Adding matrix-vector multiplication. Add  [2.x.19]  to      [2.x.20]  with  [2.x.21]  being this matrix. This function foes the same     as vmult_add() but takes the transposed matrix.    
* [0.x.27]*
     Return the square of the norm of the vector  [2.x.22]  with respect to the     norm induced by this matrix, i.e.,  [2.x.23] . This is useful,     e.g., in the finite context, where the  [2.x.24]  norm of a function equals     the matrix norm with respect to the mass matrix of the vector     representing the nodal values of the finite element function.         Obviously, the matrix needs to be quadratic for this operation.    
* [0.x.28]*
     Compute the matrix scalar product  [2.x.25] .    
* [0.x.29]*
     Compute the residual of an equation  [2.x.26] , where the residual is     defined to be  [2.x.27] . Write the residual into  [2.x.28] . The      [2.x.29]  norm of the residual vector is returned.         Source  [2.x.30]  and destination  [2.x.31]  must not be the same vector.    
* [0.x.30]*
      [2.x.32]  Matrix norms    
* [0.x.31]*
     Return the  [2.x.33] -norm of the matrix, that is  [2.x.34] , (max. sum of     columns). This is the natural matrix norm that is compatible to the      [2.x.35] -norm for vectors, i.e.,  [2.x.36] .    
* [0.x.32]*
     Return the  [2.x.37] -norm of the matrix, that is      [2.x.38] , (max. sum of rows). This is the natural norm that is     compatible to the  [2.x.39] -norm of vectors, i.e.,  [2.x.40] .    
* [0.x.33]*
     Return the frobenius norm of the matrix, i.e., the square root of the     sum of squares of all entries in the matrix.    
* [0.x.34]*
     [2.x.41]  Access to underlying CUDA data    
* [0.x.35]*
     Return a tuple containing the pointer to the values of matrix, the     pointer to the columns indices, the pointer to the rows pointer,     the cuSPARSE matrix description, and the cuSPARSE SP matrix description.    
* [0.x.36]*
     cuSPARSE handle used to call cuSPARSE functions.    
* [0.x.37]*
     Number of non-zero elements in the sparse matrix.    
* [0.x.38]*
     Number of rows of the sparse matrix.    
* [0.x.39]*
     Number of columns of the sparse matrix.    
* [0.x.40]*
     Pointer to the values (on the device) of the sparse matrix.    
* [0.x.41]*
     Pointer to the column indices (on the device) of the sparse matrix.    
* [0.x.42]*
     Pointer to the row pointer (on the device) of the sparse matrix.    
* [0.x.43]*
     cuSPARSE description of the matrix.    
* [0.x.44]*
     cuSPARSE description of the sparse matrix.    
* [0.x.45]

include/deal.II-translator/lac/cuda_vector_0.txt
[0.x.0]*
   A Namespace for the CUDA vectors.  
* [0.x.1]*
     This class implements a vector using CUDA for use on Nvidia GPUs. This     class is derived from the  [2.x.0]  class.        
*  [2.x.1]  Only float and double are supported.          [2.x.2]  CUDAWrappers    
*  [2.x.3]     
* [0.x.2]*
       Constructor. Create a vector of dimension zero.      
* [0.x.3]*
       Copy constructor.      
* [0.x.4]*
       Move constructor.      
* [0.x.5]*
       Constructor. Set dimension to  [2.x.4]  and initialize all elements with       zero.             The constructor is made explicit to avoid accident like this:       <tt>v=0;</tt>. Presumably, the user wants to set every elements of       the vector to zero, but instead, what happens is this call:       <tt>v=Vector [2.x.5]  i.e. the vector is replaced by one       of length zero.      
* [0.x.6]*
       Copy assignment operator.      
* [0.x.7]*
       Move assignment operator.      
* [0.x.8]*
       Swap the contents of this vector and the other vector  [2.x.6]  One could do       this operation with a temporary variable and copying over the data       elements, but this function is significantly more efficient since it       only swaps the pointers to the data of the two vectors and therefore       does not need to allocate temporary storage and move data around.             This function is analogous to the  [2.x.7]  function of all C++       standard containers. Also, there is a global function       <tt>swap(u,v)</tt> that simply calls <tt>u.swap(v)</tt>, again in       analogy to standard functions.             This function is virtual in order to allow for derived classes to       handle memory separately.      
* [0.x.9]*
       Reinit functionality. The flag <tt>omit_zeroing_entries</tt>       determines whether the vector should be filled with zero (false) or       left untouched (true).      
* [0.x.10]*
       Change the dimension to that of the vector V. The elements of V are not       copied.      
* [0.x.11]*
       Import all the element from the input vector  [2.x.8]         [2.x.9]   [2.x.10]  is used to decide if the       elements int  [2.x.11]  should be added to the current vector or replace       the current elements. The last parameter is not used. It is only used       for distributed vectors. This is the function that should be used to       copy a vector to the GPU.      
* [0.x.12]*
       Sets all elements of the vector to the scalar  [2.x.12]  This operation is       only allowed if  [2.x.13]  is equal to zero.      
* [0.x.13]*
       Multiply the entive vector by a fixed factor.      
* [0.x.14]*
       Divide the entire vector by a fixed factor.      
* [0.x.15]*
       Add the vector  [2.x.14]  to the present one.      
* [0.x.16]*
       Subtract the vector  [2.x.15]  from the present one.      
* [0.x.17]*
       Return the scalar product of two vectors.      
* [0.x.18]*
       Add  [2.x.16]  all components. Note that  [2.x.17]  is a scalar not a vector.      
* [0.x.19]*
       Simple addition of a multiple of a vector, i.e. <tt>*this += a*V</tt>.      
* [0.x.20]*
       Multiple additions of scaled vectors, i.e. <tt>*this += a*V+b*W</tt>.      
* [0.x.21]*
       Scaling and simple addition of a multiple of a vector, i.e. <tt>*this       = s*(*this)+a*V</tt>      
* [0.x.22]*
       Scale each element of this vector by the corresponding element in the       argument. This function is mostly meant to simulate multiplication       (and immediate re-assignment) by a diagonal scaling matrix.      
* [0.x.23]*
       Assignment <tt>*this = a*V</tt>.      
* [0.x.24]*
       Return whether the vector contains only elements with value zero.      
* [0.x.25]*
       Return the mean value of all the entries of this vector.      
* [0.x.26]*
       Return the l<sub>1</sub> norm of the vector (i.e., the sum of the       absolute values of all entries among all processors).      
* [0.x.27]*
       Return the l<sub>2</sub> norm of the vector (i.e., the square root of       the sum of the square of all entries among all processors).      
* [0.x.28]*
       Return the square of the  [2.x.18] -norm.      
* [0.x.29]*
       Return the maximum norm of the vector (i.e., the maximum absolute       value among all entries and among all processors).      
* [0.x.30]*
       Perform a combined operation of a vector addition and a subsequent       inner product, returning the value of the inner product. In other       words, the result of this function is the same as if the user called      
* [1.x.0]
*              The reason this function exists is that this operation involves less       memory transfer than calling the two functions separately. This       method only needs to load three vectors,  [2.x.19]   [2.x.20]   [2.x.21]  whereas       calling separate methods means to load the calling vector  [2.x.22]        twice. Since most vector operations are memory transfer limited, this       reduces the time by 25\% (or 50\% if  [2.x.23]  equals  [2.x.24]              For complex-valued vectors, the scalar product in the second step is       implemented as  [2.x.25] .      
* [0.x.31]*
       Return the pointer to the underlying array. Ownership still resides       with this class.      
* [0.x.32]*
       Return the size of the vector.      
* [0.x.33]*
       Return an index set that describe which elements of this vector are       owned by the current processor, i.e. [0, size).      
* [0.x.34]*
       Print the vector to the output stream  [2.x.26]       
* [0.x.35]*
       Return the memory consumption of this class in bytes.      
* [0.x.36]*
       Attempt to perform an operation between two incompatible vector types.            
*  [2.x.27]       
* [0.x.37]*
       Pointer to the array of elements of this vector.      
* [0.x.38]*
       Number of elements in the vector.      
* [0.x.39]*
 Global function  [2.x.28]  which overloads the default implementation of the C++ standard library which uses a temporary object. The function simply exchanges the data of the two vectors.
*   [2.x.29]  Vector

* 
* [0.x.40]

include/deal.II-translator/lac/diagonal_matrix_0.txt
[0.x.0]*
 This class represents a [1.x.0] diagonal matrix based on a vector of size [1.x.1]. The matrix-vector products are realized by  [2.x.0]   [2.x.1]  so the template vector class needs to provide a  [2.x.2]  method.
*  When using this class with  [2.x.3]  the underlying vector needs to provide write access to all entries referenced by cells in an assembly process. This means that this class also needs access to ghost entries that are owned by other processors than the calling one. In practice this requires initialization of the vector as follows

* 
* [1.x.2]
* 

* 
* [0.x.1]*
   Default constructor. The object needs still to be reinitialized to be   usable.  
* [0.x.2]*
   Constructor initializing this object as a diagonal matrix of size `n x n`   where `n` is the size of the vector, and with diagonal entries equal to the   elements of  [2.x.4]   
* [0.x.3]*
   Initialize with a given vector by copying the content of the vector    [2.x.5]   
* [0.x.4]*
   Compresses the data structures and allows the resulting matrix to be used   in all other operations like matrix-vector products. This is a collective   operation, i.e., it needs to be run on all processors when used in   parallel.  
* [0.x.5]*
   Return a reference to the underlying vector for manipulation of the   entries on the matrix diagonal.  
* [0.x.6]*
   Clear content of this object and reset to the state of default constructor.  
* [0.x.7]*
   Return a read-only reference to the underlying vector.  
* [0.x.8]*
   Number of rows of this matrix. This number corresponds to the size of the   underlying vector.  
* [0.x.9]*
   Number of columns of this matrix. This number corresponds to the size of   the underlying vector.  
* [0.x.10]*
   Read-only access to a value. This is restricted to the case where   [1.x.3] due to the matrix storage.     If the vector representing the diagonal is distributed with MPI, not all   of the indices [1.x.4] might actually be accessible. Refer to the method    [2.x.6]  for the entries that   actually are accessible.  
* [0.x.11]*
   Read-write access to a value. This is restricted to the case where   [1.x.5] due to the matrix storage.     If the vector representing the diagonal is distributed with MPI, not all   of the indices [1.x.6] might actually be accessible. Refer to the method    [2.x.7]  for the entries that   actually are accessible.  
* [0.x.12]*
   Add an array of values given by <tt>values</tt> in the given global   matrix row at columns specified by col_indices. Due to the storage of   this matrix, entries are only added to the diagonal of the matrix. All   other entries are ignored and no exception is thrown.     This function is for a consistent interface with the other matrix   classes in deal.II and can be used in    [2.x.8]  to get exactly the same   diagonal as when assembling into a sparse matrix.  
* [0.x.13]*
   Add value to the element (i,j).     Due to the storage of this matrix, entries are only added to the diagonal   of the matrix. All other entries are ignored and no exception is thrown.  
* [0.x.14]*
   Performs a matrix-vector multiplication with the given matrix.  
* [0.x.15]*
   Performs a transpose matrix-vector multiplication with the given   matrix. Since this represents a diagonal matrix, exactly the same as   vmult().  
* [0.x.16]*
   Adds the result of a matrix-vector multiplication into the destination   vector dst. Needs to create a temporary vector, which makes performance   slower than for  [2.x.9]   
* [0.x.17]*
   Adds the result of a transpose matrix-vector multiplication into the   destination vector dst. Needs to create a temporary vector, which makes   performance slower than for  [2.x.10]   
* [0.x.18]*
   Initialize vector  [2.x.11]  to have the same size and partition as    [2.x.12]  member of this class.     This is a part of the interface required   by linear_operator().  
* [0.x.19]*
   Return the memory consumption of this object.  
* [0.x.20]*
   The stored vector.  
* [0.x.21]

include/deal.II-translator/lac/dynamic_sparsity_pattern_0.txt
[0.x.0]!  [2.x.0]  Sparsity [2.x.1] 

* 
* [0.x.1]*
 Iterators on objects of type DynamicSparsityPattern.

* 
* [0.x.2]*
   Declare type for container size.  
* [0.x.3]*
   Accessor class for iterators into objects of type DynamicSparsityPattern.     Note that this class only allows read access to elements, providing their   row and column number (or alternatively the index within the complete   sparsity pattern). It does not allow modifying the sparsity pattern   itself.  
* [0.x.4]*
     Constructor.    
* [0.x.5]*
     Constructor. Construct the end accessor for the given sparsity pattern.    
* [0.x.6]*
     Default constructor creating a dummy accessor. This constructor is here     only to be able to store accessors in STL containers such as      [2.x.2]     
* [0.x.7]*
     Row number of the element represented by this object.    
* [0.x.8]*
     Index within the current row of the element represented by this object.    
* [0.x.9]*
     Column number of the element represented by this object.    
* [0.x.10]*
     Comparison. True, if both iterators point to the same matrix position.    
* [0.x.11]*
     Comparison operator. Result is true if either the first row number is     smaller or if the row numbers are equal and the first index is smaller.         This function is only valid if both iterators point into the same     sparsity pattern.    
* [0.x.12]*
     The sparsity pattern we operate on accessed.    
* [0.x.13]*
     The row we currently point into.    
* [0.x.14]*
     A pointer to the element within the current row that we currently point     to.    
* [0.x.15]*
     A pointer to the end of the current row. We store this to make     comparison against the end of line iterator cheaper as it otherwise     needs to do the IndexSet translation from row index to the index within     the 'lines' array of DynamicSparsityPattern.    
* [0.x.16]*
     Move the accessor to the next nonzero entry in the matrix.    
* [0.x.17]*
   An iterator class for walking over the elements of a sparsity pattern.     The typical use for these iterators is to iterate over the elements of a   sparsity pattern (or, since they also serve as the basis for iterating   over the elements of an associated matrix, over the elements of a sparse   matrix), or over the elements of individual rows. There is no guarantee   that the elements of a row are actually traversed in an order in which   column numbers monotonically increase. See the documentation of the   SparsityPattern class for more information.    
*  [2.x.3]  This class operates directly on the internal data structures of the   DynamicSparsityPattern class. As a consequence, some operations are cheap   and some are not. In particular, it is cheap to access the column index   of the sparsity pattern entry pointed to. On the other hand, it is   expensive to compute the distance between two iterators. As a   consequence, when you design algorithms that use these iterators, it is   common practice to not loop over [1.x.0] elements of a sparsity   pattern at once, but to have an outer loop over all rows and within this   loop iterate over the elements of this row. This way, you only ever need   to dereference the iterator to obtain the column indices whereas the   (expensive) lookup of the row index can be avoided by using the loop   index instead.  
* [0.x.18]*
     Constructor. Create an iterator into the sparsity pattern  [2.x.4]  for the     given global index (i.e., the index of the given element counting from     the zeroth row).    
* [0.x.19]*
     Constructor. Create an invalid (end) iterator into the sparsity pattern      [2.x.5]     
* [0.x.20]*
     Default constructor creating an invalid iterator. This constructor is     here only to be able to store iterators in STL containers such as      [2.x.6]     
* [0.x.21]*
     Prefix increment.    
* [0.x.22]*
     Postfix increment.    
* [0.x.23]*
     Dereferencing operator.    
* [0.x.24]*
     Dereferencing operator.    
* [0.x.25]*
     Comparison. True, if both iterators point to the same matrix position.    
* [0.x.26]*
     Inverse of <tt>==</tt>.    
* [0.x.27]*
     Comparison operator. Result is true if either the first row number is     smaller or if the row numbers are equal and the first index is smaller.         This function is only valid if both iterators point into the same     matrix.    
* [0.x.28]*
     Return the distance between the current iterator and the argument. The     distance is given by how many times one has to apply operator++ to the     current iterator to get the argument (for a positive return value), or     operator-- (for a negative return value).    
* [0.x.29]*
     Store an object of the accessor class.    
* [0.x.30]*
 This class acts as an intermediate form of the SparsityPattern class. From the interface it mostly represents a SparsityPattern object that is kept compressed at all times. However, since the final sparsity pattern is not known while constructing it, keeping the pattern compressed at all times can only be achieved at the expense of either increased memory or run time consumption upon use. The main purpose of this class is to avoid some memory bottlenecks, so we chose to implement it memory conservative. The chosen data format is too unsuited to be used for actual matrices, though. It is therefore necessary to first copy the data of this object over to an object of type SparsityPattern before using it in actual matrices.
*  Another viewpoint is that this class does not need up front allocation of a certain amount of memory, but grows as necessary.  An extensive description of sparsity patterns can be found in the documentation of the  [2.x.7]  module.
*  This class is an example of the "dynamic" type of  [2.x.8] . It is used in most tutorial programs in one way or another.
*  [1.x.1]
*  Since this class is intended as an intermediate replacement of the SparsityPattern class, it has mostly the same interface, with small changes where necessary. In particular, the add() function, and the functions inquiring properties of the sparsity pattern are the same.
* 

*  [1.x.2]
*  Usage of this class is explained in  [2.x.9]  (without constraints) and  [2.x.10]  (with AffineConstraints) and typically looks as follows:

* 
* [1.x.3]
* 

* 
* [0.x.31]*
   Declare the type for container size.  
* [0.x.32]*
   Typedef an for iterator class that allows to walk over all nonzero   elements of a sparsity pattern.     Since the iterator does not allow to modify the sparsity pattern, this   type is the same as that for  [2.x.11]   
* [0.x.33]*
   Typedef for an iterator class that allows to walk over all nonzero   elements of a sparsity pattern.  
* [0.x.34]*
   Initialize as an empty object. This is useful if you want such objects as   member variables in other classes. You can make the structure usable by   calling the reinit() function.  
* [0.x.35]*
   Copy constructor. This constructor is only allowed to be called if the   sparsity structure to be copied is empty. This is so in order to prevent   involuntary copies of objects for temporaries, which can use large   amounts of computing time.  However, copy constructors are needed if you   want to place a DynamicSparsityPattern in a container, e.g. to write such   statements like <tt>v.push_back (DynamicSparsityPattern());</tt>, with  [2.x.12]    v a vector of  [2.x.13]  objects.  
* [0.x.36]*
   Initialize a rectangular sparsity pattern with  [2.x.14]  rows and  [2.x.15]    columns. The  [2.x.16]  restricts the storage to elements in rows of this   set.  Adding elements outside of this set has no effect. The default   argument keeps all entries.  
* [0.x.37]*
   Create a square SparsityPattern using the given index set. The total size   is given by the size of  [2.x.17]  and only rows corresponding to   indices in  [2.x.18]  are stored on the current processor.  
* [0.x.38]*
   Initialize a square pattern of dimension  [2.x.19]   
* [0.x.39]*
   Copy operator. For this the same holds as for the copy constructor: it is   declared, defined and fine to be called, but the latter only for empty   objects.  
* [0.x.40]*
   Reallocate memory and set up data structures for a new sparsity pattern   with  [2.x.20]  rows and  [2.x.21]  columns. The  [2.x.22]  restricts the storage to   elements in rows of this set.  Adding elements outside of this set has no   effect. The default argument keeps all entries.  
* [0.x.41]*
   Since this object is kept compressed at all times anyway, this function   does nothing, but is declared to make the interface of this class as much   alike as that of the SparsityPattern class.  
* [0.x.42]*
   Return whether the object is empty. It is empty if no memory is   allocated, which is the same as that both dimensions are zero.  
* [0.x.43]*
   Return the maximum number of entries per row. Note that this number may   change as entries are added.  
* [0.x.44]*
   Add a nonzero entry. If the entry already exists, this call does nothing.  
* [0.x.45]*
   Add several nonzero entries to the specified row. Already existing   entries are ignored.  
* [0.x.46]*
   Check if a value at a certain position may be non-zero.  
* [0.x.47]*
   Return a view of this sparsity pattern.   That is, for all rows in  [2.x.23]  extract non-empty columns.   The resulting sparsity pattern will have number of rows equal   `rows.n_elements()`.  
* [0.x.48]*
   Make the sparsity pattern symmetric by adding the sparsity pattern of the   transpose object.     This function throws an exception if the sparsity pattern does not   represent a square matrix.  
* [0.x.49]*
   Construct and store in this object the sparsity pattern corresponding to   the product of  [2.x.24]  and  [2.x.25]  sparsity pattern.  
* [0.x.50]*
   Construct and store in this object the sparsity pattern corresponding to   the product of transposed  [2.x.26]  and non-transpose  [2.x.27]  sparsity pattern.  
* [0.x.51]*
   Print the sparsity pattern. The output consists of one line per row of   the format <tt>[i,j1,j2,j3,...]</tt>. [1.x.4] is the row number and   [1.x.5] are the allocated columns in this row.  
* [0.x.52]*
   Print the sparsity pattern in a format that  [2.x.28]  understands and   which can be used to plot the sparsity pattern in a graphical way. The   format consists of pairs <tt>i j</tt> of nonzero elements, each   representing one entry, one per line of the output file. Indices are   counted from zero on, as usual. Since sparsity patterns are printed in   the same way as matrices are displayed, we print the negative of the   column index, which means that the <tt>(0,0)</tt> element is in the top   left rather than in the bottom left corner.     Print the sparsity pattern in gnuplot by setting the data style to dots   or points and use the  [2.x.29]  command.  
* [0.x.53]*
   Return the number of rows, which equals the dimension of the image space.  
* [0.x.54]*
   Return the number of columns, which equals the dimension of the range   space.  
* [0.x.55]*
   Number of entries in a specific row. This function can only be called if   the given row is a member of the index set of rows that we want to store.  
* [0.x.56]*
   Clear all entries stored in a specific row.  
* [0.x.57]*
   Access to column number field.  Return the column number of the  [2.x.30]    indexth entry in  [2.x.31]   
* [0.x.58]*
   Return index of column  [2.x.32]  in row  [2.x.33]  If the column does not   exist in this sparsity pattern, the returned value will be    [2.x.34]   
* [0.x.59]*
    [2.x.35]  Iterators  
* [0.x.60]*
   Iterator starting at the first entry of the matrix. The resulting   iterator can be used to walk over all nonzero entries of the sparsity   pattern.     Note the discussion in the general documentation of this class about the   order in which elements are accessed.    
*  [2.x.36]  If the sparsity pattern has been initialized with an IndexSet that   denotes which rows to store, then iterators will simply skip over rows   that are not stored. In other words, they will look like empty rows, but   no exception will be generated when iterating over such rows.  
* [0.x.61]*
   Final iterator.  
* [0.x.62]*
   Iterator starting at the first entry of row <tt>r</tt>.     Note that if the given row is empty, i.e. does not contain any nonzero   entries, then the iterator returned by this function equals   <tt>end(r)</tt>. Note also that the iterator may not be dereferenceable in   that case.     Note also the discussion in the general documentation of this class about   the order in which elements are accessed.    
*  [2.x.37]  If the sparsity pattern has been initialized with an IndexSet that   denotes which rows to store, then iterators will simply skip over rows   that are not stored. In other words, they will look like empty rows, but   no exception will be generated when iterating over such rows.  
* [0.x.63]*
   Final iterator of row <tt>r</tt>. It points to the first element past the   end of line  [2.x.38]  or past the end of the entire sparsity pattern.     Note that the end iterator is not necessarily dereferenceable. This is in   particular the case if it is the end iterator for the last row of a   matrix.  
* [0.x.64]*
   Compute the bandwidth of the matrix represented by this structure. The   bandwidth is the maximum of  [2.x.39]  for which the index pair  [2.x.40]    represents a nonzero entry of the matrix.  
* [0.x.65]*
   Return the number of nonzero elements allocated through this sparsity   pattern.  
* [0.x.66]*
   Return the IndexSet that sets which rows are active on the current   processor. It corresponds to the IndexSet given to this class in the   constructor or in the reinit function.  
* [0.x.67]*
   Return the IndexSet that contains entries for all columns in which at least   one element exists in this sparsity pattern.    
*  [2.x.41]  In a parallel context, this only considers the locally stored rows.  
* [0.x.68]*
   Return the IndexSet that contains entries for all rows in which at least   one element exists in this sparsity pattern.    
*  [2.x.42]  In a parallel context, this only considers the locally stored rows.  
* [0.x.69]*
   return whether this object stores only those entries that have been added   explicitly, or if the sparsity pattern contains elements that have been   added through other means (implicitly) while building it. For the current   class, the result is always true.     This function mainly serves the purpose of describing the current class   in cases where several kinds of sparsity patterns can be passed as   template arguments.  
* [0.x.70]*
   Determine an estimate for the memory consumption (in bytes) of this   object.  
* [0.x.71]*
   A flag that stores whether any entries have been added so far.  
* [0.x.72]*
   Number of rows that this sparsity structure shall represent.  
* [0.x.73]*
   Number of columns that this sparsity structure shall represent.  
* [0.x.74]*
   A set that contains the valid rows.  
* [0.x.75]*
   Store some data for each row describing which entries of this row are   nonzero. Data is stored sorted in the  [2.x.43]   [2.x.44]   The vector   per row is dynamically growing upon insertion doubling its memory each   time.  
* [0.x.76]*
     Storage for the column indices of this row. This array is always kept     sorted.    
* [0.x.77]*
     Add the given column number to this line.    
* [0.x.78]*
     Add the columns specified by the iterator range to this line.    
* [0.x.79]*
     estimates memory consumption.    
* [0.x.80]*
   Actual data: store for each row the set of nonzero entries.  
* [0.x.81]

include/deal.II-translator/lac/eigen_0.txt
[0.x.0]*
 Power method (von Mises) for eigenvalue computations.
*  This method determines the largest eigenvalue of a matrix by applying increasing powers of this matrix to a vector. If there is an eigenvalue  [2.x.0]  with dominant absolute value, the iteration vectors will become aligned to its eigenspace and  [2.x.1] .
*  A shift parameter allows to shift the spectrum, so it is possible to compute the smallest eigenvalue, too.
*  Convergence of this method is known to be slow.

* 
* [0.x.1]*
   Declare type of container size.  
* [0.x.2]*
   Standardized data struct to pipe additional data to the solver.  
* [0.x.3]*
     Shift parameter. This parameter allows to shift the spectrum to compute     a different eigenvalue.    
* [0.x.4]*
     Constructor. Set the shift parameter.    
* [0.x.5]*
   Constructor.  
* [0.x.6]*
   Power method.  [2.x.2]  is the (not necessarily normalized, but nonzero) start   vector for the power method. After the iteration,  [2.x.3]  is the   approximated eigenvalue and  [2.x.4]  is the corresponding eigenvector,   normalized with respect to the l2-norm.  
* [0.x.7]*
   Shift parameter.  
* [0.x.8]*
 Inverse iteration (Wieland) for eigenvalue computations.
*  This class implements an adaptive version of the inverse iteration by Wieland.
*  There are two choices for the stopping criterion: by default, the norm of the residual  [2.x.5]  is computed. Since this might not converge to zero for non-symmetric matrices with non-trivial Jordan blocks, it can be replaced by checking the difference of successive eigenvalues. Use  [2.x.6]  for switching this option.
*  Usually, the initial guess entering this method is updated after each step, replacing it with the new approximation of the eigenvalue. Using a parameter  [2.x.7]  between 0 and 1, this update can be damped. With relaxation parameter 0, no update is performed. This damping allows for slower adaption of the shift value to make sure that the method converges to the eigenvalue closest to the initial guess. This can be aided by the parameter  [2.x.8]  which indicates the first iteration step in which the shift value should be adapted.

* 
* [0.x.9]*
   Declare type of container size.  
* [0.x.10]*
   Standardized data struct to pipe additional data to the solver.  
* [0.x.11]*
     Damping of the updated shift value.    
* [0.x.12]*
     Start step of adaptive shift parameter.    
* [0.x.13]*
     Flag for the stopping criterion.    
* [0.x.14]*
     Constructor.    
* [0.x.15]*
   Constructor.  
* [0.x.16]*
   Inverse method.  [2.x.9]  is the start guess for the eigenvalue and  [2.x.10]    is the (not necessarily normalized, but nonzero) start vector for the   power method. After the iteration,  [2.x.11]  is the approximated   eigenvalue and  [2.x.12]  is the corresponding eigenvector, normalized with   respect to the l2-norm.  
* [0.x.17]*
   Flags for execution.  
* [0.x.18]

include/deal.II-translator/lac/exceptions_0.txt
[0.x.0]*
    [2.x.0]  Exceptions  
* [0.x.1]*
   This function only works for quadratic matrices.  
* [0.x.2]*
   The operation cannot be finished since the matrix is singular.  
* [0.x.3]*
   Block indices of two block objects are different.  
* [0.x.4]*
   The operation requires a sparsity pattern.  
* [0.x.5]*
   Exception thrown when a PETSc function reports an error. If possible,   this exception uses the message provided by    [2.x.1]  to print a description of the error.    
*  [2.x.2]  For backwards compatibility this is defined whether or not deal.II   is compiled with PETSc.  
* [0.x.6]*
   An error of a Trilinos function was encountered. Check the Trilinos   documentation for details.  
* [0.x.7]

include/deal.II-translator/lac/full_matrix_0.txt
[0.x.0]!  [2.x.0]  Matrix1 [2.x.1] 

* 
* [0.x.1]*
 Implementation of a classical rectangular scheme of numbers. The data type of the entries is provided in the template argument <tt>number</tt>.  The interface is quite fat and in fact has grown every time a new feature was needed. So, a lot of functions are provided.
*  Internal calculations are usually done with the accuracy of the vector argument to functions. If there is no argument with a number type, the matrix number type is used.
* 

* 
*  [2.x.2]  Instantiations for this template are provided for <tt> [2.x.3]   [2.x.4]   [2.x.5]   [2.x.6]  Others can be generated in application programs, see  [2.x.7]  for details.

* 
* [0.x.2]*
   A type of used to index into this container.  
* [0.x.3]*
   Type of matrix entries. This alias is analogous to <tt>value_type</tt>   in the standard library containers.  
* [0.x.4]*
   Use the base class mutable iterator type.  
* [0.x.5]*
   Use the base class constant iterator type.  
* [0.x.6]*
   Use the base class iterator functions.  
* [0.x.7]*
   Use the base class iterator functions  
* [0.x.8]*
   Declare a type that has holds real-valued numbers with the same precision   as the template argument to this class. If the template argument of this   class is a real data type, then real_type equals the template argument.   If the template argument is a  [2.x.8]  type then real_type equals the   type underlying the complex numbers.     This alias is used to represent the return type of norms.  
* [0.x.9]*
    [2.x.9]  Constructors and initialization.  See also the base class Table.  
* [0.x.10]*
   Constructor. Initialize the matrix as a square matrix with dimension   <tt>n</tt>.     In order to avoid the implicit conversion of integers and other types to   a matrix, this constructor is declared <tt>explicit</tt>.     By default, no memory is allocated.  
* [0.x.11]*
   Constructor. Initialize the matrix as a rectangular matrix.  
* [0.x.12]*
   Constructor initializing from an array of numbers. The array is arranged   line by line. No range checking is performed.  
* [0.x.13]*
   Construct a full matrix that equals the identity matrix of the size of   the argument. Using this constructor, one can easily create an identity   matrix of size  [2.x.10]  by saying  
* [1.x.0]
*   
* [0.x.14]*
    [2.x.11]   
* [0.x.15]*
    [2.x.12]  Copying into and out of other matrices  
* [0.x.16]*
    [2.x.13]   
* [0.x.17]*
   Variable assignment operator.  
* [0.x.18]*
   This operator assigns a scalar to a matrix. To avoid confusion with the   semantics of this function, zero is the only value allowed for   <tt>d</tt>, allowing you to clear a matrix in an intuitive way.      [2.x.14]   
* [0.x.19]*
   Copy operator to create a full matrix that equals the identity matrix of   the size of the argument. This way, one can easily create an identity   matrix of size  [2.x.15]  by saying  
* [1.x.1]
*   
* [0.x.20]*
   Assignment operator for a LapackFullMatrix. The calling matrix must be of   the same size as the LAPACK matrix.  
* [0.x.21]*
   Assignment from different matrix classes. This assignment operator uses   iterators of the typename MatrixType. Therefore, sparse matrices are   possible sources.  
* [0.x.22]*
   Transposing assignment from different matrix classes. This assignment   operator uses iterators of the typename MatrixType. Therefore, sparse   matrices are possible sources.  
* [0.x.23]*
   Fill matrix with elements extracted from a tensor, taking rows included   between <tt>r_i</tt> and <tt>r_j</tt> and columns between <tt>c_i</tt>   and <tt>c_j</tt>. The resulting matrix is then inserted in the   destination matrix at position <tt>(dst_r, dst_c)</tt> Checks on the   indices are made.  
* [0.x.24]*
   Insert a submatrix (also rectangular) into a tensor, putting its upper   left element at the specified position <tt>(dst_r, dst_c)</tt> and the   other elements consequently. Default values are chosen so that no   parameter needs to be specified if the size of the tensor and that of the   matrix coincide.  
* [0.x.25]*
   Copy a subset of the rows and columns of another matrix into the current   object.      [2.x.16]  matrix The matrix from which a subset is to be taken from.    [2.x.17]  row_index_set The set of rows of  [2.x.18]  from which to extract.    [2.x.19]  column_index_set The set of columns of  [2.x.20]  from which to   extract.  [2.x.21]  The number of elements in  [2.x.22]  and  [2.x.23]    column_index_set shall be equal to the number of rows and columns in the   current object. In other words, the current object is not resized for   this operation.  
* [0.x.26]*
   Copy the elements of the current matrix object into a specified set of   rows and columns of another matrix. Thus, this is a scatter operation.      [2.x.24]  row_index_set The rows of  [2.x.25]  into which to write.    [2.x.26]  column_index_set The columns of  [2.x.27]  into which to write.    [2.x.28]  matrix The matrix within which certain elements are to be   replaced.  [2.x.29]  The number of elements in  [2.x.30]  and  [2.x.31]    column_index_set shall be equal to the number of rows and columns in the   current object. In other words, the current object is not resized for   this operation.  
* [0.x.27]*
   Fill rectangular block.     A rectangular block of the matrix <tt>src</tt> is copied into   <tt>this</tt>. The upper left corner of the block being copied is   <tt>(src_offset_i,src_offset_j)</tt>.  The upper left corner of the   copied block is <tt>(dst_offset_i,dst_offset_j)</tt>.  The size of the   rectangular block being copied is the maximum size possible, determined   either by the size of <tt>this</tt> or <tt>src</tt>.  
* [0.x.28]*
   Make function of base class available.  
* [0.x.29]*
   Fill with permutation of another matrix.     The matrix <tt>src</tt> is copied into the target. The two permutation   <tt>p_r</tt> and <tt>p_c</tt> operate in a way, such that <tt>result(i,j)   = src(p_r[i], p_c[j])</tt>.     The vectors may also be a selection from a larger set of integers, if the   matrix <tt>src</tt> is bigger. It is also possible to duplicate rows or   columns by this method.  
* [0.x.30]*
   Set a particular entry of the matrix to a value. Thus, calling    [2.x.32]  is entirely equivalent to the operation    [2.x.33] . This function exists for compatibility with   the various sparse matrix objects.      [2.x.34]  i The row index of the element to be set.    [2.x.35]  j The columns index of the element to be set.    [2.x.36]  value The value to be written into the element.  
* [0.x.31]*
    [2.x.37]   
* [0.x.32]*
    [2.x.38]  Non-modifying operators  
* [0.x.33]*
    [2.x.39]   
* [0.x.34]*
   Comparison operator. Be careful with this thing, it may eat up huge   amounts of computing time! It is most commonly used for internal   consistency checks of programs.  
* [0.x.35]*
   Number of rows of this matrix.  Note that the matrix is of dimension [1.x.2].  
* [0.x.36]*
   Number of columns of this matrix.  Note that the matrix is of dimension   [1.x.3].  
* [0.x.37]*
   Return whether the matrix contains only elements with value zero. This   function is mainly for internal consistency checks and should seldom be   used when not in debug mode since it uses quite some time.  
* [0.x.38]*
   Return the square of the norm of the vector <tt>v</tt> induced by this   matrix, i.e. [1.x.4]. This is useful, e.g. in the finite element   context, where the [1.x.5] norm of a function equals the   matrix norm with respect to the mass matrix of the vector representing   the nodal values of the finite element function.     Obviously, the matrix needs to be quadratic for this operation, and for   the result to actually be a norm it also needs to be either real   symmetric or complex hermitian.     The underlying template types of both this matrix and the given vector   should either both be real or complex-valued, but not mixed, for this   function to make sense.  
* [0.x.39]*
   Build the matrix scalar product <tt>u<sup>T</sup> M v</tt>. This function   is mostly useful when building the cellwise scalar product of two   functions in the finite element context.     The underlying template types of both this matrix and the given vector   should either both be real or complex-valued, but not mixed, for this   function to make sense.  
* [0.x.40]*
   Return the [1.x.6]-norm of the matrix, where  [2.x.40]  (maximum of the sums over columns).  
* [0.x.41]*
   Return the  [2.x.41] -norm of the matrix, where  [2.x.42]  (maximum of the sums over rows).  
* [0.x.42]*
   Compute the Frobenius norm of the matrix.  Return value is the root of   the square sum of all matrix entries.    
*  [2.x.43]  For the timid among us: this norm is not the norm compatible with   the [1.x.7]-norm of the vector space.  
* [0.x.43]*
   Compute the relative norm of the skew-symmetric part. The return value is   the Frobenius norm of the skew-symmetric part of the matrix divided by   that of the matrix.     Main purpose of this function is to check, if a matrix is symmetric   within a certain accuracy, or not.  
* [0.x.44]*
   Compute the determinant of a matrix.  This is only implemented for one,   two, and three dimensions, since for higher dimensions the numerical work   explodes.  Obviously, the matrix needs to be quadratic for this function.  
* [0.x.45]*
   Return the trace of the matrix, i.e. the sum of the diagonal values   (which happens to also equal the sum of the eigenvalues of a matrix).   Obviously, the matrix needs to be quadratic for this function.  
* [0.x.46]*
   Output of the matrix in user-defined format given by the specified   precision and width. This function saves width and precision of the   stream before setting these given values for output, and restores the   previous values after output.  
* [0.x.47]*
   Print the matrix and allow formatting of entries.     The parameters allow for a flexible setting of the output format:      [2.x.44]  <tt>precision</tt> denotes the number of trailing digits.      [2.x.45]  <tt>scientific</tt> is used to determine the number format, where   <tt>scientific</tt> = <tt>false</tt> means fixed point notation.      [2.x.46]  <tt>width</tt> denotes the with of each column. A zero entry for   <tt>width</tt> makes the function compute a width, but it may be changed   to a positive value, if output is crude.      [2.x.47]  <tt>zero_string</tt> specifies a string printed for zero entries.      [2.x.48]  <tt>denominator</tt> Multiply the whole matrix by this common   denominator to get nicer numbers.      [2.x.49]  <tt>threshold</tt>: all entries with absolute value smaller than   this are considered zero.  
* [0.x.48]*
   Determine an estimate for the memory consumption (in bytes) of this   object.  
* [0.x.49]*
   Mutable iterator starting at the first entry of row <tt>r</tt>.  
* [0.x.50]*
   One past the end mutable iterator of row <tt>r</tt>.  
* [0.x.51]*
   Constant iterator starting at the first entry of row <tt>r</tt>.  
* [0.x.52]*
   One past the end constant iterator of row <tt>r</tt>.  
* [0.x.53]*
   Scale the entire matrix by a fixed factor.  
* [0.x.54]*
   Scale the entire matrix by the inverse of the given factor.  
* [0.x.55]*
   Simple addition of a scaled matrix, i.e. <tt>*this += a*A</tt>.     The matrix <tt>A</tt> may be a full matrix over an arbitrary underlying   scalar type, as long as its data type is convertible to the data type of   this matrix.  
* [0.x.56]*
   Multiple addition of scaled matrices, i.e. <tt>*this += a*A + b*B</tt>.     The matrices <tt>A</tt> and <tt>B</tt> may be a full matrix over an   arbitrary underlying scalar type, as long as its data type is convertible   to the data type of this matrix.  
* [0.x.57]*
   Multiple addition of scaled matrices, i.e. <tt>*this += a*A + b*B +   c*C</tt>.     The matrices <tt>A</tt>, <tt>B</tt> and <tt>C</tt> may be a full matrix   over an arbitrary underlying scalar type, as long as its data type is   convertible to the data type of this matrix.  
* [0.x.58]*
   Add rectangular block.     A rectangular block of the matrix <tt>src</tt> is added to <tt>this</tt>.   The upper left corner of the block being copied is   <tt>(src_offset_i,src_offset_j)</tt>.  The upper left corner of the   copied block is <tt>(dst_offset_i,dst_offset_j)</tt>.  The size of the   rectangular block being copied is the maximum size possible, determined   either by the size of <tt>this</tt> or <tt>src</tt> and the given   offsets.  
* [0.x.59]*
   Weighted addition of the transpose of <tt>B</tt> to <tt>this</tt>.     [1.x.8]  
* [0.x.60]*
   Add transpose of a rectangular block.     A rectangular block of the matrix <tt>src</tt> is transposed and   addedadded to <tt>this</tt>. The upper left corner of the block being   copied is <tt>(src_offset_i,src_offset_j)</tt> in the coordinates of the   [1.x.9]-transposed matrix.  The upper left corner of the copied block   is <tt>(dst_offset_i,dst_offset_j)</tt>.  The size of the rectangular   block being copied is the maximum size possible, determined either by the   size of <tt>this</tt> or <tt>src</tt>.  
* [0.x.61]*
   Add a single element at the given position.  
* [0.x.62]*
   Add an array of values given by <tt>values</tt> in the given global   matrix row at columns specified by col_indices in the full matrix. This   function is present for compatibility with the various sparse matrices in   deal.II. In particular, the two boolean fields  [2.x.50]  and    [2.x.51]  do not impact the performance of this routine,   as opposed to the sparse matrix case and are indeed ignored in the   implementation.  
* [0.x.63]*
   [1.x.10].  Simple addition of rows of this  
* [0.x.64]*
   [1.x.11].  Multiple addition of   rows of this.  
* [0.x.65]*
   [1.x.12].  Simple addition of columns of this.  
* [0.x.66]*
   [1.x.13].  Multiple addition of   columns of this.  
* [0.x.67]*
   Swap [1.x.14].  Swap rows i and j of this  
* [0.x.68]*
   Swap [1.x.15].  Swap columns i and j of this  
* [0.x.69]*
   Add constant to diagonal elements of this, i.e. add a multiple of the   identity matrix.  
* [0.x.70]*
   Assignment <tt>*this = a*A</tt>.  
* [0.x.71]*
   Assignment <tt>*this = a*A + b*B</tt>.  
* [0.x.72]*
   Assignment <tt>*this = a*A + b*B + c*C</tt>.  
* [0.x.73]*
   Symmetrize the matrix by forming the mean value between the existing   matrix and its transpose, [1.x.16].     Obviously the matrix must be quadratic for this operation.  
* [0.x.74]*
   A=Inverse(A). A must be a square matrix.  Inversion of this matrix by   Gauss-Jordan algorithm with partial pivoting.  This process is well-   behaved for positive definite matrices, but be aware of round-off errors   in the indefinite case.     In case deal.II was configured with LAPACK, the functions Xgetrf and   Xgetri build an LU factorization and invert the matrix upon that   factorization, providing best performance up to matrices with a few   hundreds rows and columns.     The numerical effort to invert an <tt>n x n</tt> matrix is of the order   <tt>n**3</tt>.  
* [0.x.75]*
   Assign the inverse of the given matrix to <tt>*this</tt>. This function   is hardcoded for quadratic matrices of dimension one to four. However,   since the amount of code needed grows quickly, the method gauss_jordan()   is invoked implicitly if the dimension is larger.  
* [0.x.76]*
   Assign the Cholesky decomposition  [2.x.52]  of the given matrix  [2.x.53]  to   <tt>*this</tt>, where  [2.x.54]  is lower triangular matrix. The given matrix must   be symmetric positive definite.     ExcMatrixNotPositiveDefinite will be thrown in the case that the matrix   is not positive definite.  
* [0.x.77]*
   <tt>*this(i,j)</tt> =  [2.x.55]  where  [2.x.56]  are vectors of the same   length.  
* [0.x.78]*
   Assign the left_inverse of the given matrix to <tt>*this</tt>. The   calculation being performed is [1.x.17].  
* [0.x.79]*
   Assign the right_inverse of the given matrix to <tt>*this</tt>. The   calculation being performed is [1.x.18].  
* [0.x.80]*
   Matrix-matrix-multiplication.     The optional parameter <tt>adding</tt> determines, whether the result is   stored in <tt>C</tt> or added to <tt>C</tt>.     if (adding) [1.x.19]     if (!adding) [1.x.20]     Assumes that <tt>A</tt> and <tt>B</tt> have compatible sizes and that   <tt>C</tt> already has the right size.     This function uses the BLAS function Xgemm if the product of the three   matrix dimensions is larger than 300 and BLAS was detected during   configuration. Using BLAS usually results in considerable performance   gains.  
* [0.x.81]*
   Matrix-matrix-multiplication using transpose of <tt>this</tt>.     The optional parameter <tt>adding</tt> determines, whether the result is   stored in <tt>C</tt> or added to <tt>C</tt>.     if (adding) [1.x.21]     if (!adding) [1.x.22]     Assumes that <tt>A</tt> and <tt>B</tt> have compatible sizes and that   <tt>C</tt> already has the right size.     This function uses the BLAS function Xgemm if the product of the three   matrix dimensions is larger than 300 and BLAS was detected during   configuration. Using BLAS usually results in considerable performance   gains.  
* [0.x.82]*
   Matrix-matrix-multiplication using transpose of <tt>B</tt>.     The optional parameter <tt>adding</tt> determines, whether the result is   stored in <tt>C</tt> or added to <tt>C</tt>.     if (adding) [1.x.23]     if (!adding) [1.x.24]     Assumes that <tt>A</tt> and <tt>B</tt> have compatible sizes and that   <tt>C</tt> already has the right size.     This function uses the BLAS function Xgemm if the product of the three   matrix dimensions is larger than 300 and BLAS was detected during   configuration. Using BLAS usually results in considerable performance   gains.  
* [0.x.83]*
   Matrix-matrix-multiplication using transpose of <tt>this</tt> and   <tt>B</tt>.     The optional parameter <tt>adding</tt> determines, whether the result is   stored in <tt>C</tt> or added to <tt>C</tt>.     if (adding) [1.x.25]     if (!adding) [1.x.26]     Assumes that <tt>A</tt> and <tt>B</tt> have compatible sizes and that   <tt>C</tt> already has the right size.     This function uses the BLAS function Xgemm if the product of the three   matrix dimensions is larger than 300 and BLAS was detected during   configuration. Using BLAS usually results in considerable performance   gains.  
* [0.x.84]*
   Add to the current matrix the triple product [1.x.27]. Optionally,   use the transposes of the matrices [1.x.28] and [1.x.29]. The scaling   factor scales the whole product, which is helpful when adding a multiple   of the triple product to the matrix.     This product was written with the Schur complement [1.x.30] in mind.  Note that in this case the argument for   <tt>A</tt> must be the inverse of the matrix [1.x.31].  
* [0.x.85]*
   Matrix-vector-multiplication.     The optional parameter <tt>adding</tt> determines, whether the result is   stored in <tt>w</tt> or added to <tt>w</tt>.     if (adding) [1.x.32]     if (!adding) [1.x.33]     Source and destination must not be the same vector.  
* [0.x.86]*
   Adding Matrix-vector-multiplication.  [1.x.34]     Source and destination must not be the same vector.  
* [0.x.87]*
   Transpose matrix-vector-multiplication.     The optional parameter <tt>adding</tt> determines, whether the result is   stored in <tt>w</tt> or added to <tt>w</tt>.     if (adding) [1.x.35]     if (!adding) [1.x.36]       Source and destination must not be the same vector.  
* [0.x.88]*
   Adding transpose matrix-vector-multiplication.  [1.x.37]     Source and destination must not be the same vector.  
* [0.x.89]*
   Apply the Jacobi preconditioner, which multiplies every element of the   <tt>src</tt> vector by the inverse of the respective diagonal element and   multiplies the result with the damping factor <tt>omega</tt>.  
* [0.x.90]*
   [1.x.38]. Residual calculation, returns the   [1.x.39]-norm |[1.x.40]|.     Source [1.x.41] and destination [1.x.42] must not be the same vector.  
* [0.x.91]*
   Forward elimination of lower triangle.  Inverts the lower triangle of a   rectangular matrix for a given right hand side.     If the matrix has more columns than rows, this function only operates on   the left quadratic submatrix. If there are more rows, the upper quadratic   part of the matrix is considered.    
*  [2.x.57]  It is safe to use the same object for  [2.x.58]  and  [2.x.59]   
* [0.x.92]*
   Backward elimination of upper triangle.     See forward()    
*  [2.x.60]  It is safe to use the same object for  [2.x.61]  and  [2.x.62]   
* [0.x.93]*
    [2.x.63]  Exceptions    [2.x.64]   
* [0.x.94]*
   Exception  
* [0.x.95]*
   Exception  
* [0.x.96]*
   Exception  
* [0.x.97]*
   Exception  
* [0.x.98]*
   Exception  
* [0.x.99]

include/deal.II-translator/lac/full_matrix.templates_0.txt
[0.x.0]

include/deal.II-translator/lac/generic_linear_algebra_0.txt
[0.x.0]*
 A namespace in which the deal.II linear algebra classes are aliased to generic names. There are similar namespaces LinearAlgebraPETSc and LinearAlgebraTrilinos for alias to classes that interface with the PETSc and Trilinos libraries.

* 
* [0.x.1]*
   Typedef for the vector type used  
* [0.x.2]*
   Typedef for the block-vector type used  
* [0.x.3]*
   Typedef for sparse matrix type used  
* [0.x.4]*
   Typedef describing sparse matrices that consist of multiple blocks.  
* [0.x.5]*
   Typedef for the SSOR preconditioner used  
* [0.x.6]*
 A namespace in which the wrappers to the PETSc linear algebra classes are aliased to generic names. There are similar namespaces LinearAlgebraDealII and LinearAlgebraTrilinos for alias to deal.II's own classes and classes that interface with Trilinos.

* 
* [0.x.7]*
   Typedef for the CG solver type used.  
* [0.x.8]*
   Typedef for the GMRES solver type used.  
* [0.x.9]*
   A namespace with alias to generic names for parallel PETSc linear   algebra objects.  
* [0.x.10]*
     Typedef for the vector type used.    
* [0.x.11]*
     Typedef for the type used to describe vectors that consist of multiple     blocks.    
* [0.x.12]*
     Typedef for the sparse matrix type used.    
* [0.x.13]*
     Typedef for the type used to describe sparse matrices that consist of     multiple blocks.    
* [0.x.14]*
     Typedef for the compressed block sparsity pattern used.    
* [0.x.15]*
     Typedef for the AMG preconditioner type.    
* [0.x.16]*
     Typedef for the Incomplete Cholesky preconditioner.    
* [0.x.17]*
     Typedef for the Incomplete LU decomposition preconditioner.    
* [0.x.18]*
     Typedef for the Incomplete Jacobi decomposition preconditioner.    
* [0.x.19]*
     Typedef for the SSOR preconditioner.    
* [0.x.20]*
 A namespace in which the wrappers to the Trilinos linear algebra classes are aliased to generic names. There are similar namespaces LinearAlgebraDealII and LinearAlgebraPETSc for alias to deal.II's own classes and classes that interface with PETSc.

* 
* [0.x.21]*
   Typedef for the CG solver type used.  
* [0.x.22]*
   Typdef for the GMRES solver type used.  
* [0.x.23]*
   A namespace with alias to generic names for parallel Trilinos linear   algebra objects.  
* [0.x.24]*
     Typedef for the vector type used.    
* [0.x.25]*
     Typedef for the type used to describe vectors that consist of multiple     blocks.    
* [0.x.26]*
     Typedef for the sparse matrix type used.    
* [0.x.27]*
     Typedef for the type used to describe sparse matrices that consist of     multiple blocks.    
* [0.x.28]*
     Typedef for the type used for compressed block sparsity pattern.    
* [0.x.29]*
     Typedef for the AMG preconditioner type.    
* [0.x.30]*
     Typedef for the Incomplete Cholesky preconditioner.    
* [0.x.31]*
     Typedef for the Incomplete LU decomposition preconditioner.    
* [0.x.32]*
     Typedef for the Incomplete Jacobi decomposition preconditioner.    
* [0.x.33]*
     Typedef for the SSOR preconditioner    
* [0.x.34]

include/deal.II-translator/lac/ginkgo_solver_0.txt
[0.x.0]*
   This class forms the base class for all of Ginkgo's iterative solvers.   The various derived classes only take   the additional data that is specific to them and solve the given linear   system. The entire collection of solvers that Ginkgo implements is   available at [1.x.0].    
*  [2.x.0]   
* [0.x.1]*
     Constructor.         The  [2.x.1]  defines the paradigm where the solution is computed.     It is a string and the choices are "omp" , "reference" or "cuda".     The respective strings create the respective executors as given below.         Ginkgo currently supports three different executor types:         +    OmpExecutor specifies that the data should be stored and the     associated operations executed on an OpenMP-supporting device (e.g. host     CPU);     ```     auto omp =  [2.x.2]      ```     +    CudaExecutor specifies that the data should be stored and the          operations executed on the NVIDIA GPU accelerator;     ```      [2.x.3]  > 0 ) {        auto cuda =  [2.x.4]      }     ```     +    ReferenceExecutor executes a non-optimized reference implementation,          which can be used to debug the library.     ```     auto ref =  [2.x.5]      ```         The following code snippet demonstrates the using of the OpenMP executor     to create a solver which would use the OpenMP paradigm to the solve the     system on the CPU.         ```     auto omp =  [2.x.6]      using cg =  [2.x.7]      auto solver_gen =          [2.x.8]               .with_criteria(                   [2.x.9]                    [2.x.10]                       .with_reduction_factor(1e-6)                      .on(omp))              .on(omp);     auto solver = solver_gen->generate(system_matrix);         solver->apply(lend(rhs), lend(solution));     ```             The  [2.x.11]  object is the same as for other     deal.II iterative solvers.    
* [0.x.2]*
     Destructor.    
* [0.x.3]*
     Initialize the matrix and copy over its data to Ginkgo's data structures.    
* [0.x.4]*
     Solve the linear system <tt>Ax=b</tt>. Dependent on the information     provided by derived classes one of Ginkgo's linear solvers is     chosen.    
* [0.x.5]*
     Solve the linear system <tt>Ax=b</tt>. Dependent on the information     provided by derived classes one of Ginkgo's linear solvers is     chosen.    
* [0.x.6]*
     Access to the object that controls convergence.    
* [0.x.7]*
     Reference to the object that controls convergence of the iterative     solvers.    
* [0.x.8]*
     The Ginkgo generated solver factory object.    
* [0.x.9]*
     The residual criterion object that controls the reduction of the residual     based on the tolerance set in the solver_control member.    
* [0.x.10]*
     The Ginkgo convergence logger used to check for convergence and other     solver data if needed.    
* [0.x.11]*
     The Ginkgo combined factory object is used to create a combined stopping     criterion to be passed to the solver.    
* [0.x.12]*
     The execution paradigm in Ginkgo. The choices are between      [2.x.12]   [2.x.13]  and  [2.x.14]      and more details can be found in Ginkgo's documentation.    
* [0.x.13]*
     Initialize the Ginkgo logger object with event masks. Refer to     [1.x.1]    
* [0.x.14]*
     Ginkgo matrix data structure. First template parameter is for storing the     array of the non-zeros of the matrix. The second is for the row pointers     and the column indices.          [2.x.15]  Templatize based on Matrix type.    
* [0.x.15]*
     The execution paradigm as a string to be set by the user. The choices     are between `omp`, `cuda` and `reference` and more details can be found     in Ginkgo's documentation.    
* [0.x.16]*
   An implementation of the solver interface using the Ginkgo CG solver.    
*  [2.x.16]   
* [0.x.17]*
     A standardized data struct to pipe additional data to the solver.    
* [0.x.18]*
     Constructor.          [2.x.17]  solver_control The solver control object is then used to     set the parameters and setup the CG solver from the CG factory which     solves the linear system.          [2.x.18]  exec_type The execution paradigm for the CG solver.          [2.x.19]  data The additional data required by the solver.    
* [0.x.19]*
     Constructor.          [2.x.20]  solver_control The solver control object is then used to     set the parameters and setup the CG solver from the CG factory which     solves the linear system.          [2.x.21]  exec_type The execution paradigm for the CG solver.          [2.x.22]  preconditioner The preconditioner for the solver.          [2.x.23]  data The additional data required by the solver.    
* [0.x.20]*
     Store a copy of the settings for this particular solver.    
* [0.x.21]*
   An implementation of the solver interface using the Ginkgo Bicgstab solver.    
*  [2.x.24]   
* [0.x.22]*
     A standardized data struct to pipe additional data to the solver.    
* [0.x.23]*
     Constructor.          [2.x.25]  solver_control The solver control object is then used to     set the parameters and setup the Bicgstab solver from the Bicgstab     factory which solves the linear system.          [2.x.26]  exec_type The execution paradigm for the Bicgstab solver.          [2.x.27]  data The additional data required by the solver.    
* [0.x.24]*
     Constructor.          [2.x.28]  solver_control The solver control object is then used to     set the parameters and setup the Bicgstab solver from the Bicgstab     factory which solves the linear system.          [2.x.29]  exec_type The execution paradigm for the Bicgstab solver.          [2.x.30]  preconditioner The preconditioner for the solver.          [2.x.31]  data The additional data required by the solver.    
* [0.x.25]*
     Store a copy of the settings for this particular solver.    
* [0.x.26]*
   An implementation of the solver interface using the Ginkgo CGS solver.     CGS or the conjugate gradient square method is an iterative type Krylov   subspace method which is suitable for general systems.    
*  [2.x.32]   
* [0.x.27]*
     A standardized data struct to pipe additional data to the solver.    
* [0.x.28]*
     Constructor.          [2.x.33]  solver_control The solver control object is then used to     set the parameters and setup the CGS solver from the CGS factory which     solves the linear system.          [2.x.34]  exec_type The execution paradigm for the CGS solver.          [2.x.35]  data The additional data required by the solver.    
* [0.x.29]*
     Constructor.          [2.x.36]  solver_control The solver control object is then used to     set the parameters and setup the CGS solver from the CGS factory which     solves the linear system.          [2.x.37]  exec_type The execution paradigm for the CGS solver.          [2.x.38]  preconditioner The preconditioner for the solver.          [2.x.39]  data The additional data required by the solver.    
* [0.x.30]*
     Store a copy of the settings for this particular solver.    
* [0.x.31]*
   An implementation of the solver interface using the Ginkgo FCG solver.     FCG or the flexible conjugate gradient method is an iterative type Krylov   subspace method which is suitable for symmetric positive definite methods.     Though this method performs very well for symmetric positive definite   matrices, it is in general not suitable for general matrices.     In contrast to the standard CG based on the Polack-Ribiere formula, the   flexible CG uses the Fletcher-Reeves formula for creating the orthonormal   vectors spanning the Krylov subspace. This increases the computational cost   of every Krylov solver iteration but allows for non-constant   preconditioners.    
*  [2.x.40]   
* [0.x.32]*
     A standardized data struct to pipe additional data to the solver.    
* [0.x.33]*
     Constructor.          [2.x.41]  solver_control The solver control object is then used to     set the parameters and setup the FCG solver from the FCG factory which     solves the linear system.          [2.x.42]  exec_type The execution paradigm for the FCG solver.          [2.x.43]  data The additional data required by the solver.    
* [0.x.34]*
     Constructor.          [2.x.44]  solver_control The solver control object is then used to     set the parameters and setup the FCG solver from the FCG factory which     solves the linear system.          [2.x.45]  exec_type The execution paradigm for the FCG solver.          [2.x.46]  preconditioner The preconditioner for the solver.          [2.x.47]  data The additional data required by the solver.    
* [0.x.35]*
     Store a copy of the settings for this particular solver.    
* [0.x.36]*
   An implementation of the solver interface using the Ginkgo GMRES solver.    
*  [2.x.48]   
* [0.x.37]*
     A standardized data struct to pipe additional data to the solver.    
* [0.x.38]*
       Constructor. By default, set the number of temporary vectors to 30,       i.e. do a restart every 30 iterations.      
* [0.x.39]*
       Maximum number of tmp vectors.      
* [0.x.40]*
     Constructor.          [2.x.49]  solver_control The solver control object is then used to     set the parameters and setup the GMRES solver from the GMRES factory     which solves the linear system.          [2.x.50]  exec_type The execution paradigm for the GMRES solver.          [2.x.51]  data The additional data required by the solver.    
* [0.x.41]*
     Constructor.          [2.x.52]  solver_control The solver control object is then used to     set the parameters and setup the GMRES solver from the GMRES factory     which solves the linear system.          [2.x.53]  exec_type The execution paradigm for the GMRES solver.          [2.x.54]  preconditioner The preconditioner for the solver.          [2.x.55]  data The additional data required by the solver.    
* [0.x.42]*
     Store a copy of the settings for this particular solver.    
* [0.x.43]*
   An implementation of the solver interface using the Ginkgo IR solver.     Iterative refinement (IR) is an iterative method that uses another coarse   method to approximate the error of the current solution via the current   residual.    
*  [2.x.56]   
* [0.x.44]*
     A standardized data struct to pipe additional data to the solver.    
* [0.x.45]*
     Constructor.          [2.x.57]  solver_control The solver control object is then used to     set the parameters and setup the IR solver from the IR factory which     solves the linear system.          [2.x.58]  exec_type The execution paradigm for the IR solver.          [2.x.59]  data The additional data required by the solver.    
* [0.x.46]*
     Constructor.          [2.x.60]  solver_control The solver control object is then used to     set the parameters and setup the IR solver from the IR factory which     solves the linear system.          [2.x.61]  exec_type The execution paradigm for the IR solver.          [2.x.62]  inner_solver The Inner solver for the IR solver.          [2.x.63]  data The additional data required by the solver.    
* [0.x.47]*
     Store a copy of the settings for this particular solver.    
* [0.x.48]

include/deal.II-translator/lac/householder_0.txt
[0.x.0]!  [2.x.0]  Matrix2 [2.x.1] 

* 
* [0.x.1]*
 QR-decomposition of a full matrix.
*  This class computes the QR-decomposition of given matrix by the Householder algorithm. Then, the function least_squares() can be used to compute the vector  [2.x.2]  minimizing  [2.x.3]  for a given vector  [2.x.4] . The QR decomposition of  [2.x.5]  is useful for this purpose because the minimizer is given by the equation  [2.x.6]  which is easy to compute because  [2.x.7]  is an orthogonal matrix, and consequently  [2.x.8] . Thus,  [2.x.9] . Furthermore,  [2.x.10]  is triangular, so applying  [2.x.11]  to a vector only involves a backward or forward solve.
* 

*  [1.x.0]
*  The class does not in fact store the  [2.x.12]  and  [2.x.13]  factors explicitly as matrices. It does store  [2.x.14] , but the  [2.x.15]  factor is stored as the product of Householder reflections of the form  [2.x.16]  where the vectors  [2.x.17]  are so that they can be stored in the lower-triangular part of an underlying matrix object, whereas  [2.x.18]  is stored in the upper triangular part.
*  The  [2.x.19]  vectors and the  [2.x.20]  matrix now are in conflict because they both want to use the diagonal entry of the matrix, but we can only store one in these positions, of course. Consequently, the entries  [2.x.21]  are stored separately in the `diagonal` member variable.
* 

* 
*  [2.x.22]  Instantiations for this template are provided for <tt> [2.x.23]  and  [2.x.24]  others can be generated in application programs (see the section on  [2.x.25]  in the manual).

* 
* [0.x.2]*
   Declare type of container size type.  
* [0.x.3]*
   Create an empty object.  
* [0.x.4]*
   Create an object holding the QR-decomposition of the matrix  [2.x.26] .  
* [0.x.5]*
   Compute the QR-decomposition of the given matrix  [2.x.27] .     This overwrites any previously computed QR decomposition.  
* [0.x.6]*
   Solve the least-squares problem for the right hand side <tt>src</tt>. The   returned scalar value is the Euclidean norm of the approximation error.      [2.x.28]   [2.x.29]  dst contains the solution of the least squares problem on return.      [2.x.30]   [2.x.31]  src contains the right hand side [1.x.1] of the least squares   problem. It will be changed during the algorithm and is unusable on   return.  
* [0.x.7]*
   This function does the same as the previous one, but for BlockVectors.  
* [0.x.8]*
   A wrapper to least_squares(), implementing the standard MatrixType   interface.  
* [0.x.9]*
   A wrapper to least_squares() that implements multiplication with   the transpose matrix.  
* [0.x.10]*
   Storage for the diagonal elements of the orthogonal   transformation. See the class documentation for more information.  
* [0.x.11]*
   Storage that is internally used for the Householder transformation.  
* [0.x.12]

include/deal.II-translator/lac/identity_matrix_0.txt
[0.x.0]!  [2.x.0]  Matrix1 [2.x.1] 

* 
* [0.x.1]*
 Implementation of a simple class representing the identity matrix of a given size, i.e. a matrix with entries  [2.x.2] . While it has the most important ingredients of a matrix, in particular that one can ask for its size and perform matrix-vector products with it, a matrix of this type is really only useful in two contexts: preconditioning and initializing other matrices.
*  [1.x.0]
*  The main usefulness of this class lies in its ability to initialize other matrix, like this:

* 
* [1.x.1]
* 
*  This creates a  [2.x.3]  matrix with ones on the diagonal and zeros everywhere else. Most matrix types, in particular FullMatrix and SparseMatrix, have conversion constructors and assignment operators for IdentityMatrix, and can therefore be filled rather easily with identity matrices.
* 

*  [1.x.2]
*  No preconditioning at all is equivalent to preconditioning with preconditioning with the identity matrix. deal.II has a specialized class for this purpose, PreconditionIdentity, than can be used in a context as shown in the documentation of that class. The present class can be used in much the same way, although without any additional benefit:

* 
* [1.x.3]
* 

* 
* [0.x.2]*
   Declare type for container size.  
* [0.x.3]*
   Default constructor. Creates a zero-sized matrix that should be resized   later on using the reinit() function.  
* [0.x.4]*
   Constructor. Creates a identity matrix of size #n.  
* [0.x.5]*
   Resize the matrix to be of size #n by #n.  
* [0.x.6]*
   Number of rows of this matrix. For the present matrix, the number of rows   and columns are equal, of course.  
* [0.x.7]*
   Number of columns of this matrix. For the present matrix, the number of   rows and columns are equal, of course.  
* [0.x.8]*
   Matrix-vector multiplication. For the present case, this of course   amounts to simply copying the input vector to the output vector.  
* [0.x.9]*
   Matrix-vector multiplication with addition to the output vector. For the   present case, this of course amounts to simply adding the input vector to   the output vector.  
* [0.x.10]*
   Matrix-vector multiplication with the transpose matrix. For the present   case, this of course amounts to simply copying the input vector to the   output vector.  
* [0.x.11]*
   Matrix-vector multiplication with the transpose matrix, with addition to   the output vector. For the present case, this of course amounts to simply   adding the input vector to the output vector.  
* [0.x.12]*
   Number of rows and columns of this matrix.  
* [0.x.13]

include/deal.II-translator/lac/lapack_full_matrix_0.txt
[0.x.0]*
 A variant of FullMatrix using LAPACK functions wherever possible. In order to do this, the matrix is stored in transposed order. The element access functions hide this fact by reverting the transposition.
* 

* 
*  [2.x.0]  In order to perform LAPACK functions, the class contains a lot of auxiliary data in the private section. The names of these data vectors are usually the names chosen for the arguments in the LAPACK documentation.
* 

* 
*  [2.x.1] 

* 
* [0.x.1]*
   Declare type for container size.  
* [0.x.2]*
   Constructor. Initialize the matrix as a square matrix with dimension    [2.x.2]      In order to avoid the implicit conversion of integers and other types to   a matrix, this constructor is declared <tt>explicit</tt>.     By default, no memory is allocated.  
* [0.x.3]*
   Constructor. Initialize the matrix as a rectangular matrix  [2.x.3] .  
* [0.x.4]*
   Copy constructor. This constructor does a deep copy of the matrix.   Therefore, it poses a possible efficiency problem, if for example,   function arguments are passed by value rather than by reference.   Unfortunately, we can't mark this copy constructor <tt>explicit</tt>,   since that prevents the use of this class in containers, such as    [2.x.4] . The responsibility to check performance of   programs must therefore remain with the user of this class.  
* [0.x.5]*
   Assignment operator.  
* [0.x.6]*
   Assignment operator from a regular FullMatrix.    
*  [2.x.5]  Since LAPACK expects matrices in transposed order, this   transposition is included here.  
* [0.x.7]*
   Assignment operator from a regular SparseMatrix.    
*  [2.x.6]  Since LAPACK expects matrices in transposed order, this   transposition is included here.  
* [0.x.8]*
   This operator assigns a scalar to a matrix. To avoid confusion with   constructors, zero (when cast to the  [2.x.7]  type) is the only   value allowed for  [2.x.8]   
* [0.x.9]*
   This operator multiplies all entries by a fixed  [2.x.9]   
* [0.x.10]*
   This operator divides all entries by a fixed  [2.x.10]   
* [0.x.11]*
   Set a particular entry of the matrix to a  [2.x.11]    Thus, calling  [2.x.12]  is entirely equivalent to the   operation  [2.x.13] . This function exists for   compatibility with the various sparse matrix objects.      [2.x.14]  i The row index of the element to be set.    [2.x.15]  j The column index of the element to be set.    [2.x.16]  value The value to be written into the element.  
* [0.x.12]*
   Simple addition of a scaled matrix, i.e.  [2.x.17] .  
* [0.x.13]*
   Perform a rank-1 update of a symmetric matrix    [2.x.18] .     This function also works for Cholesky factorization.   In that case, updating ( [2.x.19] ) is   performed via Givens rotations, whereas downdating ( [2.x.20] ) via hyperbolic   rotations. Note that the latter case might lead to a negative definite   matrix in which case the error will be thrown (because Cholesky   factorizations are only valid for symmetric and positive definite   matrices).  
* [0.x.14]*
   Apply [1.x.0]    [2.x.21]  (a triplet of cosine, sine and radius, see    [2.x.22]  for the definition of the   rotation matrix  [2.x.23] )   to this matrix in the plane spanned by the  [2.x.24]  and  [2.x.25]  unit vectors.   If  [2.x.26]  is  [2.x.27] , the rotation is applied from left    [2.x.28]    and only rows  [2.x.29]  and  [2.x.30]  are affected.   Otherwise, transpose of the rotation matrix is applied from right    [2.x.31]    and only columns  [2.x.32]  and  [2.x.33]  are affected.  
* [0.x.15]*
   Assignment from different matrix classes, performing the usual conversion   to the transposed format expected by LAPACK. This assignment operator   uses iterators of the typename MatrixType. Therefore, sparse matrices are   possible sources.  
* [0.x.16]*
   Regenerate the current matrix by one that has the same properties as if   it were created by the constructor of this class with the same argument   list as this present function.  
* [0.x.17]*
   Same as above but will preserve the values of matrix upon resizing.   The original values   of the matrix are kept on increasing the size   [1.x.1]   Whereas if the new size is smaller, the matrix will contain the upper left   block of the original one [1.x.2]  
* [0.x.18]*
   Remove row  [2.x.34]  and column  [2.x.35]  from the matrix.   [1.x.3]  
* [0.x.19]*
   Regenerate the current matrix by one that has the same properties as if   it were created by the constructor of this class with the same argument   list as this present function.  
* [0.x.20]*
   Assign  [2.x.36]  to this matrix.  
* [0.x.21]*
   Return the dimension of the codomain (or range) space.    
*  [2.x.37]  The matrix is of dimension  [2.x.38] .  
* [0.x.22]*
   Return the dimension of the domain space.    
*  [2.x.39]  The matrix is of dimension  [2.x.40] .  
* [0.x.23]*
   Fill rectangular block.     A rectangular block of the matrix <tt>src</tt> is copied into   <tt>this</tt>. The upper left corner of the block being copied is   <tt>(src_offset_i,src_offset_j)</tt>.  The upper left corner of the   copied block is <tt>(dst_offset_i,dst_offset_j)</tt>.  The size of the   rectangular block being copied is the maximum size possible, determined   either by the size of <tt>this</tt> or <tt>src</tt>.     The final two arguments allow to enter a multiple of the source or its   transpose.  
* [0.x.24]*
   Matrix-vector-multiplication.     Depending on previous transformations recorded in #state, the result of   this function is one of    [2.x.41]     [2.x.42]  If #state is  [2.x.43]  or  [2.x.44]    this will be a regular matrix vector product using LAPACK gemv().    [2.x.45]  If #state is  [2.x.46]  or  [2.x.47]  this   function first multiplies with the right transformation matrix, then with   the diagonal matrix of singular values or their reciprocal values, and   finally with the left transformation matrix.    [2.x.48]      The optional parameter  [2.x.49]  determines, whether the result is   stored in the vector    [2.x.50]    or added to it    [2.x.51] .    
*  [2.x.52]  Source and destination must not be the same vector.    
*  [2.x.53]  The template with  [2.x.54]  only exists for compile-time   compatibility with FullMatrix. Only the case  [2.x.55]  =  [2.x.56]  is   implemented due to limitations in the underlying LAPACK interface. All   other variants throw an error upon invocation.  
* [0.x.25]*
   Specialization of above function for compatible  [2.x.57]   
* [0.x.26]*
   Adding Matrix-vector-multiplication  [2.x.58] .     See the documentation of vmult() for details on the implementation.  
* [0.x.27]*
   Specialization of above function for compatible  [2.x.59]   
* [0.x.28]*
   Transpose matrix-vector-multiplication.     The optional parameter  [2.x.60]  determines, whether the result is   stored in the vector    [2.x.61]    or added to it    [2.x.62] .     See the documentation of vmult() for details on the implementation.  
* [0.x.29]*
   Specialization of above function for compatible  [2.x.63]   
* [0.x.30]*
   Adding transpose matrix-vector-multiplication  [2.x.64] .     See the documentation of vmult() for details on the implementation.  
* [0.x.31]*
   Specialization of above function for compatible  [2.x.65]   
* [0.x.32]*
   Matrix-matrix-multiplication.     The optional parameter  [2.x.66]  determines, whether the result is   stored in the matrix    [2.x.67]    or added to it    [2.x.68] .    
*  [2.x.69]  It is assumed that  [2.x.70]  and  [2.x.71]  have compatible sizes and that    [2.x.72]  already has the right size.      [2.x.73]  function uses the BLAS function Xgemm.  
* [0.x.33]*
   Same as before, but stores the result in a FullMatrix, not in a   LAPACKFullMatrix.  
* [0.x.34]*
   Matrix-matrix-multiplication using transpose of <tt>this</tt>.     The optional parameter  [2.x.74]  determines, whether the result is   stored in the matrix    [2.x.75]    or added to it    [2.x.76] .    
*  [2.x.77]  It is assumed that  [2.x.78]  and  [2.x.79]  have compatible sizes and that    [2.x.80]  already has the right size.    
*  [2.x.81]  This function uses the BLAS function Xgemm.  
* [0.x.35]*
   Same as before, but stores the result in a FullMatrix, not in a   LAPACKFullMatrix.  
* [0.x.36]*
   Matrix-matrix-multiplication using transpose of <tt>this</tt> and a   diagonal vector  [2.x.82]      If the  [2.x.83]  then the result is stored in the matrix    [2.x.84]    otherwise it is added  [2.x.85] .    
*  [2.x.86]  It is assumed that  [2.x.87]   [2.x.88]  and  [2.x.89]  have compatible sizes and that    [2.x.90]  already has the right size.    
*  [2.x.91]  This function is not provided by LAPACK. The function first forms  [2.x.92]  product and   then uses Xgemm function.  
* [0.x.37]*
   Matrix-matrix-multiplication using transpose of  [2.x.93]      The optional parameter  [2.x.94]  determines, whether the result is   stored in the matrix    [2.x.95]    or added to it    [2.x.96] .    
*  [2.x.97]  It is assumed that  [2.x.98]  and  [2.x.99]  have compatible sizes and that    [2.x.100]  already has the right size.    
*  [2.x.101]  This function uses the BLAS function Xgemm.  
* [0.x.38]*
   Same as before, but stores the result in a FullMatrix, not in a   LAPACKFullMatrix.  
* [0.x.39]*
   Matrix-matrix-multiplication using transpose of <tt>this</tt> and    [2.x.102]      The optional parameter  [2.x.103]  determines, whether the result is   stored in the matrix    [2.x.104]    or added to it    [2.x.105] .    
*  [2.x.106]  It is assumed that  [2.x.107]  and  [2.x.108]  have compatible sizes and that    [2.x.109]  already has the right size.    
*  [2.x.110]  This function uses the BLAS function Xgemm.  
* [0.x.40]*
   Same as before, but stores the result in a FullMatrix, not in a   LAPACKFullMatrix.  
* [0.x.41]*
   Performs out-place transposition.   Matrix  [2.x.111]  should be appropriately sized.    
*  [2.x.112]  for complex number types, conjugate transpose will be performed.    
*  [2.x.113]  If deal.II is configured with Intel-MKL, `mkl_?omatcopy` will be used,   otherwise transposition is done element by element.  
* [0.x.42]*
   Scale rows of this matrix by  [2.x.114]  . This is equivalent to premultiplication   with a diagonal matrix  [2.x.115] .  
* [0.x.43]*
   Compute the LU factorization of the matrix using LAPACK function Xgetrf.  
* [0.x.44]*
   Compute the Cholesky factorization of the matrix using LAPACK function   Xpotrf.    
*  [2.x.116]  The factorization is stored in the lower-triangular part of the matrix.  
* [0.x.45]*
   Estimate the reciprocal of the condition number  [2.x.117]  in  [2.x.118]    norm ( [2.x.119] ) of a symmetric   positive definite matrix using Cholesky factorization. This function can   only be called if the matrix is already factorized.    
*  [2.x.120]  The condition number  [2.x.121]  can be used to estimate the numerical   error related to the matrix inversion or the solution of the   system of linear algebraic equations as    [2.x.122] .   Alternatively one can get the number of accurate digits    [2.x.123] .    
*  [2.x.124]  The function computes reciprocal of the condition number to   avoid possible overflow if the matrix is nearly singular.      [2.x.125]  l1_norm Is the  [2.x.126]  norm of the matrix before calling Cholesky   factorization. It can be obtained by calling l1_norm().  
* [0.x.46]*
   Estimate the reciprocal of the condition number  [2.x.127]  in  [2.x.128]    norm for triangular matrices. The matrix has to have the    [2.x.129]  set to either    [2.x.130]  or    [2.x.131]  see set_property().  
* [0.x.47]*
   Compute the determinant of a matrix. As it requires the LU factorization of   the matrix, this function can only be called after   compute_lu_factorization() has been called.  
* [0.x.48]*
   Compute  [2.x.132]  norm.  
* [0.x.49]*
   Compute  [2.x.133]  norm.  
* [0.x.50]*
   Compute Frobenius norm  
* [0.x.51]*
   Compute trace of the matrix, i.e. the sum of the diagonal values.   Obviously, the matrix needs to be quadratic for this function.  
* [0.x.52]*
   Invert the matrix by first computing an LU/Cholesky factorization with the   LAPACK function Xgetrf/Xpotrf and then building the actual inverse using   Xgetri/Xpotri.  
* [0.x.53]*
   Solve the linear system with right hand side  [2.x.134]  and put the solution   back to  [2.x.135]  The matrix should be either triangular or LU/Cholesky   factorization should be previously computed.     The flag transposed indicates whether the solution of the transposed   system is to be performed.  
* [0.x.54]*
   Same as above but for multiple right hand sides (as many as there   are columns in the matrix  [2.x.136]   
* [0.x.55]*
   Compute eigenvalues of the matrix. After this routine has been called,   eigenvalues can be retrieved using the eigenvalue() function. The matrix   itself will be  [2.x.137]  after this operation.     The optional arguments allow to compute left and right eigenvectors as   well.     Note that the function does not return the computed eigenvalues right   away since that involves copying data around between the output arrays of   the LAPACK functions and any return array. This is often unnecessary   since one may not be interested in all eigenvalues at once, but for   example only the extreme ones. In that case, it is cheaper to just have   this function compute the eigenvalues and have a separate function that   returns whatever eigenvalue is requested.    
*  [2.x.138]  Calls the LAPACK function Xgeev.  
* [0.x.56]*
   Compute eigenvalues and eigenvectors of a real symmetric matrix. Only   eigenvalues in the interval  [2.x.139]  are   computed with the absolute tolerance  [2.x.140] . An approximate   eigenvalue is accepted as converged when it is determined to lie in an   interval  [2.x.141]  of width less than or equal to  [2.x.142] , where  [2.x.143]  is the machine precision.  If    [2.x.144]  is less than or equal to zero, then    [2.x.145]  will be used in its place, where  [2.x.146]  is   the 1-norm of the tridiagonal matrix obtained by reducing  [2.x.147]  to   tridiagonal form. Eigenvalues will be computed most accurately when    [2.x.148]  is set to twice the underflow threshold, not zero.   After this routine has been called, all eigenvalues in  [2.x.149]  will be stored in eigenvalues and the corresponding   eigenvectors will be stored in the columns of eigenvectors, whose dimension   is set accordingly.    
*  [2.x.150]  Calls the LAPACK function Xsyevx.  
* [0.x.57]*
   Compute generalized eigenvalues and eigenvectors of a real generalized   symmetric eigenproblem of the form
* 

* 
* 

* 
* 

* 
* 
*  - itype = 1:  [2.x.151] 
* 

* 
* 

* 
* 

* 
* 
*  - itype = 2:  [2.x.152] 
* 

* 
* 

* 
* 

* 
* 
*  - itype = 3:  [2.x.153]      where  [2.x.154]  is this matrix.  [2.x.155]    and  [2.x.156]  are assumed to be symmetric, and  [2.x.157]  has to be   positive definite. Only eigenvalues in the interval  [2.x.158]  are computed with the absolute tolerance    [2.x.159] .  An approximate eigenvalue is accepted as converged   when it is determined to lie in an interval  [2.x.160]  of width less than or   equal to  [2.x.161] , where  [2.x.162]  is   the machine precision. If  [2.x.163]  is less than or equal to   zero, then  [2.x.164]  will be used in its place, where    [2.x.165]  is the 1-norm of the tridiagonal matrix obtained by   reducing  [2.x.166]  to tridiagonal form. Eigenvalues will be computed most   accurately when  [2.x.167]  is set to twice the underflow   threshold, not zero. After this routine has been called, all eigenvalues in    [2.x.168]  will be stored in eigenvalues and   the corresponding eigenvectors will be stored in eigenvectors, whose   dimension is set accordingly.    
*  [2.x.169]  Calls the LAPACK function Xsygvx.  
* [0.x.58]*
   Same as the other compute_generalized_eigenvalues_symmetric function   except that all eigenvalues are computed and the tolerance is set   automatically.  Note that this function does not return the computed   eigenvalues right away since that involves copying data around between   the output arrays of the LAPACK functions and any return array. This is   often unnecessary since one may not be interested in all eigenvalues at   once, but for example only the extreme ones. In that case, it is cheaper   to just have this function compute the eigenvalues and have a separate   function that returns whatever eigenvalue is requested. Eigenvalues can   be retrieved using the eigenvalue() function.  The number of computed   eigenvectors is equal to eigenvectors.size()    
*  [2.x.170]  Calls the LAPACK function Xsygv.  
* [0.x.59]*
   Compute the singular value decomposition of the matrix using LAPACK   function Xgesdd.     Requires that the #state is  [2.x.171]  fills the data members   #wr, #svd_u, and #svd_vt, and leaves the object in the #state    [2.x.172]      The singular value decomposition factorizes the provided matrix (A) into   three parts: U, sigma, and the transpose of V (V^T), such that A = U sigma   V^T. Sigma is a MxN matrix which contains the singular values of A on   the diagonal while all the other elements are zero. U is a MxM orthogonal   matrix containing the left singular vectors corresponding to the singular   values of A. V is a NxN orthonal matrix containing the right singular   vectors corresponding the singular values of A.     Note that the variable #svd_vt contains the tranpose of V and can be   accessed by get_svd_vt(), while U is accessed with get_svd_u().  
* [0.x.60]*
   Compute the inverse of the matrix by singular value decomposition.     Requires that #state is either  [2.x.173]  or    [2.x.174]  In the first case, this function calls compute_svd().   After this function, the object will have the #state    [2.x.175]      For a singular value decomposition, the inverse is simply computed by   replacing all singular values by their reciprocal values. If the matrix   does not have maximal rank, singular values 0 are not touched, thus   computing the minimal norm right inverse of the matrix.     The parameter  [2.x.176]  determines, when a singular value should   be considered zero. It is the ratio of the smallest to the largest   nonzero singular value  [2.x.177] . Thus, the inverses of all   singular values less than   [2.x.178]  will   be set to zero.  
* [0.x.61]*
   Same as above but provide the size of the kernel instead of a threshold,   i.e. the  [2.x.179]  smallest eigenvalues.  
* [0.x.62]*
   Retrieve eigenvalue after compute_eigenvalues() was called.  
* [0.x.63]*
   Retrieve singular values after compute_svd() or compute_inverse_svd() was   called.  
* [0.x.64]*
   Retrieve the matrix #svd_u after compute_svd() or compute_inverse_svd() was   called.  
* [0.x.65]*
   Retrieve the matrix #svd_vt after compute_svd() or compute_inverse_svd()   was called.  
* [0.x.66]*
   Print the matrix and allow formatting of entries.     The parameters allow for a flexible setting of the output format:      [2.x.180]  out This specifies the stream to write to.      [2.x.181]  precision denotes the number of trailing digits.      [2.x.182]  scientific is used to determine the number format, where    [2.x.183]  means fixed point notation.      [2.x.184]  width denotes the with of each column. A zero entry for    [2.x.185]  makes the function compute a width, but it may be changed   to a positive value, if output is crude.      [2.x.186]  zero_string specifies a string printed for zero entries.      [2.x.187]  denominator Multiply the whole matrix by this common   denominator to get nicer numbers.      [2.x.188]  threshold all entries with absolute value smaller than   this are considered zero.    
*  [2.x.189]  The entries stored resemble a matrix only if the state is either    [2.x.190]  or  [2.x.191]  Otherwise, calling this   function is not allowed.  
* [0.x.67]*
   Internal function to compute various norms.  
* [0.x.68]*
   Since LAPACK operations notoriously change the meaning of the matrix   entries, we record the current state after the last operation here.  
* [0.x.69]*
   Additional property of the matrix which may help to select more   efficient LAPACK functions.  
* [0.x.70]*
   The working array used for some LAPACK functions.  
* [0.x.71]*
   Integer working array used for some LAPACK functions.  
* [0.x.72]*
   The vector storing the permutations applied for pivoting in the LU-   factorization.     Also used as the scratch array IWORK for LAPACK functions needing it.  
* [0.x.73]*
   Workspace for calculating the inverse matrix from an LU factorization.  
* [0.x.74]*
   Real parts of eigenvalues or the singular values. Filled by   compute_eigenvalues() or compute_svd().  
* [0.x.75]*
   Imaginary parts of eigenvalues, or, in the complex scalar case, the   eigenvalues themselves. Filled by compute_eigenvalues.  
* [0.x.76]*
   Space where left eigenvectors can be stored.  
* [0.x.77]*
   Space where right eigenvectors can be stored.  
* [0.x.78]*
   The matrix  [2.x.192]  in the singular value decomposition    [2.x.193] .  
* [0.x.79]*
   The matrix  [2.x.194]   in the singular value decomposition    [2.x.195] .  
* [0.x.80]*
   Thread mutex.  
* [0.x.81]*
 A preconditioner based on the LU-factorization of LAPACKFullMatrix.
* 

* 
*  [2.x.196] 

* 
* [0.x.82]

include/deal.II-translator/lac/lapack_support_0.txt
[0.x.0]*
   Integer type in BLAS.  
* [0.x.1]*
   Integer type in BLAS.  
* [0.x.2]*
 A namespace containing constants, exceptions, enumerations, and other utilities used by the deal.II LAPACK bindings.

* 
* [0.x.3]*
   Most of the LAPACK functions one can apply to a matrix (e.g., by calling   the member functions of this class) change its content in some ways. For   example, they may invert the matrix, or may replace it by a matrix whose   columns represent the eigenvectors of the original content of the matrix.   The elements of this enumeration are therefore used to track what is   currently being stored by this object.  
* [0.x.4]*
   %Function printing the name of a State.  
* [0.x.5]*
   A matrix can have certain features allowing for optimization, but hard to   test. These are listed here.  
* [0.x.6]*
   %Function printing the name of a Property.  
* [0.x.7]*
   Character constant.  
* [0.x.8]*
   Character constant.  
* [0.x.9]*
   Character constant.  
* [0.x.10]*
   Character constant.  
* [0.x.11]*
   Character constant.  
* [0.x.12]*
   Character constant.  
* [0.x.13]*
   Character constant.  
* [0.x.14]*
   Integer constant.  
* [0.x.15]*
   Integer constant.  
* [0.x.16]*
   A LAPACK function returned an error code.  
* [0.x.17]*
   Exception thrown when a matrix is not in a suitable state for an   operation. For instance, a LAPACK routine may have left the matrix in an   unusable state, then vmult does not make sense anymore.  
* [0.x.18]*
   Exception thrown when a matrix does not have suitable properties for an   operation.  
* [0.x.19]*
   This exception is thrown if a certain LAPACK function is not available   because no LAPACK installation was detected during configuration.  
* [0.x.20]

include/deal.II-translator/lac/lapack_templates_0.txt
[0.x.0]

include/deal.II-translator/lac/la_parallel_block_vector_0.txt
[0.x.0]!  [2.x.0]  Vectors     [2.x.1]     
* [0.x.1]*
     An implementation of block vectors based on distributed deal.II     vectors. While the base class provides for most of the interface, this     class handles the actual allocation of vectors and provides functions     that are specific to the underlying vector type.        
*  [2.x.2]  Instantiations for this template are provided for <tt> [2.x.3]      and  [2.x.4]  others can be generated in application programs     (see the section on      [2.x.5]      in the manual).          [2.x.6]       [2.x.7]  "Block (linear algebra)"    
* [0.x.2]*
       The chunks size to split communication in update_ghost_values()       and compress() calls.             Most common MPI implementations will get slow when too many       messages/requests are outstanding. Even when messages are small,       say 1 kB only, we should collect enough data with  [2.x.8]        to cover typical infiniband latencies which are around a few       microseconds. Sending 20 kB at a throughput of 5 GB/s takes 4       microseconds, so we should arrive at the bandwidth dominated regime       then which is good enough.      
* [0.x.3]*
       Typedef the base class for simpler access to its own alias.      
* [0.x.4]*
       Typedef the type of the underlying vector.      
* [0.x.5]*
       Import the alias from the base class.      
* [0.x.6]*
        [2.x.9]  1: Basic operations      
* [0.x.7]*
       Constructor. There are three ways to use this constructor. First,       without any arguments, it generates an object with no blocks. Given       one argument, it initializes <tt>num_blocks</tt> blocks, but these       blocks have size zero. The third variant finally initializes all       blocks to the same size <tt>block_size</tt>.             Confer the other constructor further down if you intend to use blocks       of different sizes.      
* [0.x.8]*
       Copy-Constructor. Dimension set to that of V, all components are       copied from V      
* [0.x.9]*
       Copy constructor taking a BlockVector of another data type. This will       fail if there is no conversion path from <tt>OtherNumber</tt> to       <tt>Number</tt>. Note that you may lose accuracy when copying to a       BlockVector with data elements with less accuracy.             Older versions of gcc did not honor the  [2.x.10]  keyword on       template constructors. In such cases, it is easy to accidentally       write code that can be very inefficient, since the compiler starts       performing hidden conversions. To avoid this, this function is       disabled if we have detected a broken compiler during configuration.      
* [0.x.10]*
       Constructor. Set the number of blocks to <tt>block_sizes.size()</tt>       and initialize each block with <tt>block_sizes[i]</tt> zero elements.      
* [0.x.11]*
       Construct a block vector with an IndexSet for the local range and       ghost entries for each block.      
* [0.x.12]*
       Same as above but the ghost indices are assumed to be empty.      
* [0.x.13]*
       Destructor.            
*  [2.x.11]  We need to explicitly provide a destructor, otherwise the         linker may think it is unused and discards it, although required         in a different section. The Intel compiler is prone to this         behavior.      
* [0.x.14]*
       Copy operator: fill all components of the vector with the given       scalar value.      
* [0.x.15]*
       Copy operator for arguments of the same type. Resize the present       vector if necessary.      
* [0.x.16]*
       Copy operator for template arguments of different types. Resize the       present vector if necessary.      
* [0.x.17]*
       Copy a regular vector into a block vector.      
* [0.x.18]*
       Copy the content of a PETSc vector into the calling vector. This       function assumes that the vectors layouts have already been       initialized to match.             This operator is only available if deal.II was configured with PETSc.      
* [0.x.19]*
       Copy the content of a Trilinos vector into the calling vector. This       function assumes that the vectors layouts have already been       initialized to match.             This operator is only available if deal.II was configured with       Trilinos.      
* [0.x.20]*
       Reinitialize the BlockVector to contain <tt>num_blocks</tt> blocks of       size <tt>block_size</tt> each.             If the second argument is left at its default value, then the block       vector allocates the specified number of blocks but leaves them at       zero size. You then need to later reinitialize the individual blocks,       and call collect_sizes() to update the block system's knowledge of       its individual block's sizes.             If <tt>omit_zeroing_entries==false</tt>, the vector is filled with       zeros.      
* [0.x.21]*
       Reinitialize the BlockVector such that it contains       <tt>block_sizes.size()</tt> blocks. Each block is reinitialized to       dimension <tt>block_sizes[i]</tt>.             If the number of blocks is the same as before this function was       called, all vectors remain the same and reinit() is called for each       vector.             If <tt>omit_zeroing_entries==false</tt>, the vector is filled with       zeros.             Note that you must call this (or the other reinit() functions)       function, rather than calling the reinit() functions of an individual       block, to allow the block vector to update its caches of vector       sizes. If you call reinit() on one of the blocks, then subsequent       actions on this object may yield unpredictable results since they may       be routed to the wrong block.      
* [0.x.22]*
       Change the dimension to that of the vector <tt>V</tt>. The same       applies as for the other reinit() function.             The elements of <tt>V</tt> are not copied, i.e.  this function is the       same as calling <tt>reinit (V.size(), omit_zeroing_entries)</tt>.             Note that you must call this (or the other reinit() functions)       function, rather than calling the reinit() functions of an individual       block, to allow the block vector to update its caches of vector       sizes. If you call reinit() of one of the blocks, then subsequent       actions of this object may yield unpredictable results since they may       be routed to the wrong block.      
* [0.x.23]*
       This function copies the data that has accumulated in the data buffer       for ghost indices to the owning processor. For the meaning of the       argument  [2.x.12]  see the entry on        [2.x.13]  "Compressing distributed vectors and matrices"       in the glossary.             There are two variants for this function. If called with argument  [2.x.14]         [2.x.15]  adds all the data accumulated in ghost elements       to the respective elements on the owning processor and clears the       ghost array afterwards. If called with argument  [2.x.16]         [2.x.17]  a set operation is performed. Since setting       elements in a vector with ghost elements is ambiguous (as one can set       both the element on the ghost site as well as the owning site), this       operation makes the assumption that all data is set correctly on the       owning processor. Upon call of  [2.x.18]  all       ghost entries are therefore simply zeroed out (using       zero_ghost_values()). In debug mode, a check is performed that makes       sure that the data set is actually consistent between processors,       i.e., whenever a non-zero ghost element is found, it is compared to       the value on the owning processor and an exception is thrown if these       elements do not agree.      
* [0.x.24]*
       Fills the data field for ghost indices with the values stored in the       respective positions of the owning processor. This function is needed       before reading from ghosts. The function is  [2.x.19]  even though       ghost data is changed. This is needed to allow functions with a  [2.x.20]        const vector to perform the data exchange without creating       temporaries.      
* [0.x.25]*
       This method zeros the entries on ghost dofs, but does not touch       locally owned DoFs.             After calling this method, read access to ghost elements of the       vector is forbidden and an exception is thrown. Only write access to       ghost elements is allowed in this state.              [2.x.21]  Use zero_out_ghost_values() instead.      
* [0.x.26]*
       This method zeros the entries on ghost dofs, but does not touch       locally owned DoFs.             After calling this method, read access to ghost elements of the       vector is forbidden and an exception is thrown. Only write access to       ghost elements is allowed in this state.      
* [0.x.27]*
       Return if this Vector contains ghost elements.      
* [0.x.28]*
       This is a collective add operation that adds a whole set of values       stored in  [2.x.22]  to the vector components specified by  [2.x.23]       
* [0.x.29]*
       Scaling and simple vector addition, i.e.  <tt>*this =       s*(*this)+V</tt>.      
* [0.x.30]*
       Return whether the vector contains only elements with value zero.       This function is mainly for internal consistency checks and should       seldom be used when not in debug mode since it uses quite some time.      
* [0.x.31]*
       Compute the mean value of all the entries in the vector.      
* [0.x.32]*
        [2.x.24] -norm of the vector. The pth root of the sum of the pth powers       of the absolute values of the elements.      
* [0.x.33]*
       Swap the contents of this vector and the other vector <tt>v</tt>. One       could do this operation with a temporary variable and copying over       the data elements, but this function is significantly more efficient       since it only swaps the pointers to the data of the two vectors and       therefore does not need to allocate temporary storage and move data       around.             Limitation: right now this function only works if both vectors have       the same number of blocks. If needed, the numbers of blocks should be       exchanged, too.             This function is analogous to the swap() function of all C++       standard containers. Also, there is a global function swap(u,v) that       simply calls <tt>u.swap(v)</tt>, again in analogy to standard       functions.      
* [0.x.34]*
        [2.x.25]  2: Implementation of VectorSpaceVector      
* [0.x.35]*
       Change the dimension to that of the vector V. The elements of V are not       copied.      
* [0.x.36]*
       Multiply the entire vector by a fixed factor.      
* [0.x.37]*
       Divide the entire vector by a fixed factor.      
* [0.x.38]*
       Add the vector  [2.x.26]  to the present one.      
* [0.x.39]*
       Subtract the vector  [2.x.27]  from the present one.      
* [0.x.40]*
       Import all the elements present in the vector's IndexSet from the input       vector  [2.x.28]   [2.x.29]   [2.x.30]  is used to decide if       the elements in  [2.x.31]  should be added to the current vector or replace the       current elements. The last parameter can be used if the same       communication pattern is used multiple times. This can be used to       improve performance.      
* [0.x.41]*
       Return the scalar product of two vectors.      
* [0.x.42]*
       Calculate the scalar product between each block of this vector and  [2.x.32]        and store the result in a full matrix  [2.x.33]  This function       computes the result by forming  [2.x.34]  where  [2.x.35]        and  [2.x.36]  indicate the  [2.x.37] th block (not element!) of  [2.x.38]  and the        [2.x.39] th block of  [2.x.40] , respectively. If  [2.x.41]  is        [2.x.42] , it is assumed that inner product results in a       square symmetric matrix and almost half of the scalar products can be       avoided.             Obviously, this function can only be used if all blocks of both vectors       are of the same size.            
*  [2.x.43]  Internally, a single global reduction will be called to       accumulate scalar product between locally owned degrees of freedom.      
* [0.x.43]*
       Calculate the scalar product between each block of this vector and  [2.x.44]        using a metric tensor  [2.x.45]  This function       computes the result of  [2.x.46]  where  [2.x.47]        and  [2.x.48]  indicate the  [2.x.49] th block (not element) of  [2.x.50]  and the        [2.x.51] th block of  [2.x.52] , respectively. If  [2.x.53]  is        [2.x.54] , it is assumed that  [2.x.55]  and  [2.x.56]  are       symmetric matrices and almost half of the scalar products can be       avoided.             Obviously, this function can only be used if all blocks of both vectors       are of the same size.            
*  [2.x.57]  Internally, a single global reduction will be called to       accumulate the scalar product between locally owned degrees of freedom.      
* [0.x.44]*
       Set each block of this vector as follows:        [2.x.58]  where  [2.x.59]        and  [2.x.60]  indicate the  [2.x.61] th block (not element) of  [2.x.62]  and the        [2.x.63] th block of  [2.x.64] , respectively.             Obviously, this function can only be used if all blocks of both vectors       are of the same size.      
* [0.x.45]*
       Add  [2.x.65]  to all components. Note that  [2.x.66]  is a scalar not a vector.      
* [0.x.46]*
       Simple addition of a multiple of a vector, i.e. <tt>*this += a*V</tt>.      
* [0.x.47]*
       Multiple addition of scaled vectors, i.e. <tt>*this += a*V+b*W</tt>.      
* [0.x.48]*
       A collective add operation: This function adds a whole set of values       stored in  [2.x.67]  to the vector components specified by  [2.x.68]       
* [0.x.49]*
       Scaling and simple addition of a multiple of a vector, i.e. <tt>*this =       s*(*this)+a*V</tt>.      
* [0.x.50]*
       Scale each element of this vector by the corresponding element in the       argument. This function is mostly meant to simulate multiplication (and       immediate re-assignment) by a diagonal scaling matrix.      
* [0.x.51]*
       Assignment <tt>*this = a*V</tt>.      
* [0.x.52]*
       Return the l<sub>1</sub> norm of the vector (i.e., the sum of the       absolute values of all entries among all processors).      
* [0.x.53]*
       Return the  [2.x.69]  norm of the vector (i.e., the square root of       the sum of the square of all entries among all processors).      
* [0.x.54]*
       Return the square of the  [2.x.70]  norm of the vector.      
* [0.x.55]*
       Return the maximum norm of the vector (i.e., the maximum absolute value       among all entries and among all processors).      
* [0.x.56]*
       Perform a combined operation of a vector addition and a subsequent       inner product, returning the value of the inner product. In other       words, the result of this function is the same as if the user called      
* [1.x.0]
*              The reason this function exists is that this operation involves less       memory transfer than calling the two functions separately. This method       only needs to load three vectors,  [2.x.71]   [2.x.72]   [2.x.73]  whereas calling       separate methods means to load the calling vector  [2.x.74]  twice. Since       most vector operations are memory transfer limited, this reduces the       time by 25\% (or 50\% if  [2.x.75]  equals  [2.x.76]              For complex-valued vectors, the scalar product in the second step is       implemented as        [2.x.77] .      
* [0.x.57]*
       Return the global size of the vector, equal to the sum of the number of       locally owned indices among all processors.      
* [0.x.58]*
       Return an index set that describes which elements of this vector are       owned by the current processor. As a consequence, the index sets       returned on different processors if this is a distributed vector will       form disjoint sets that add up to the complete index set. Obviously, if       a vector is created on only one processor, then the result would       satisfy      
* [1.x.1]
*       
* [0.x.59]*
       Print the vector to the output stream  [2.x.78]       
* [0.x.60]*
       Return the memory consumption of this class in bytes.      
* [0.x.61]*
        [2.x.79]  Exceptions        [2.x.80]       
* [0.x.62]*
       Attempt to perform an operation between two incompatible vector types.            
*  [2.x.81]       
* [0.x.63]*
       Exception      
* [0.x.64]*
 Global function which overloads the default implementation of the C++ standard library which uses a temporary object. The function simply exchanges the data of the two vectors.
*   [2.x.82]  BlockVector

* 
* [0.x.65]*
 Declare  [2.x.83]  as distributed vector.

* 
* [0.x.66]

include/deal.II-translator/lac/la_parallel_block_vector.templates_0.txt
[0.x.0]

include/deal.II-translator/lac/la_parallel_vector_0.txt
[0.x.0]*
   A namespace for parallel implementations of vectors.  
* [0.x.1]!  [2.x.0]  Vectors     [2.x.1]     
* [0.x.2]*
     Implementation of a parallel vector class. The design of this class is     similar to the standard  [2.x.2]  class in deal.II, with the     exception that storage is distributed with MPI.         The vector is designed for the following scheme of parallel     partitioning:      [2.x.3]       [2.x.4]  The indices held by individual processes (locally owned part) in     the MPI parallelization form a contiguous range      [2.x.5] .      [2.x.6]  Ghost indices residing on arbitrary positions of other processors     are allowed. It is in general more efficient if ghost indices are     clustered, since they are stored as a set of intervals. The     communication pattern of the ghost indices is determined when calling     the function <code>reinit (locally_owned, ghost_indices,     communicator)</code>, and retained until the partitioning is changed.     This allows for efficient parallel communication of indices. In     particular, it stores the communication pattern, rather than having to     compute it again for every communication. For more information on ghost     vectors, see also the      [2.x.7]  "glossary entry on vectors with ghost elements".      [2.x.8]  Besides the usual global access operator() it is also possible to     access vector entries in the local index space with the function  [2.x.9]      local_element(). Locally owned indices are placed first, [0,     locally_owned_size()), and then all ghost indices follow after them     contiguously, [locally_owned_size(),     locally_owned_size()+n_ghost_entries()).      [2.x.10]          Functions related to parallel functionality:      [2.x.11]       [2.x.12]  The function  [2.x.13]  goes through the data     associated with ghost indices and communicates it to the owner process,     which can then add it to the correct position. This can be used e.g.     after having run an assembly routine involving ghosts that fill this     vector. Note that the  [2.x.14]  mode of  [2.x.15]  does not set the     elements included in ghost entries but simply discards them, assuming     that the owning processor has set them to the desired value already     (See also the      [2.x.16]  "glossary entry on compress").      [2.x.17]  The  [2.x.18]  function imports the data     from the owning processor to the ghost indices in order to provide read     access to the data associated with ghosts.      [2.x.19]  It is possible to split the above functions into two phases, where     the first initiates the communication and the second one finishes it.     These functions can be used to overlap communication with computations     in other parts of the code.      [2.x.20]  Of course, reduction operations (like norms) make use of     collective all-to-all MPI communications.      [2.x.21]          This vector can take two different states with respect to ghost     elements:      [2.x.22]       [2.x.23]  After creation and whenever zero_out_ghost_values() is called (or      [2.x.24] ), the vector does only allow writing into     ghost elements but not reading from ghost elements.      [2.x.25]  After a call to update_ghost_values(), the vector does not allow     writing into ghost elements but only reading from them. This is to     avoid undesired ghost data artifacts when calling compress() after     modifying some vector entries. The current status of the ghost entries     (read mode or write mode) can be queried by the method     has_ghost_elements(), which returns  [2.x.26]  exactly when     ghost elements have been updated and  [2.x.27]  otherwise,     irrespective of the actual number of ghost entries in the vector layout     (for that information, use n_ghost_entries() instead).      [2.x.28]          This vector uses the facilities of the class  [2.x.29]  for     implementing the operations on the local range of the vector. In     particular, it also inherits thread parallelism that splits most     vector-vector operations into smaller chunks if the program uses     multiple threads. This may or may not be desired when working also with     MPI.         [1.x.0]         This vector class is based on two different number types for indexing.     The so-called global index type encodes the overall size of the vector.     Its type is  [2.x.30]  The largest possible value is      [2.x.31]  or approximately 4 billion in case 64 bit integers     are disabled at configuration of deal.II (default case) or      [2.x.32]  if 64 bit     integers are enabled (see the glossary entry on      [2.x.33]      for further information).         The second relevant index type is the local index used within one MPI     rank. As opposed to the global index, the implementation assumes 32-bit     unsigned integers unconditionally. In other words, to actually use a     vector with more than four billion entries, you need to use MPI with     more than one rank (which in general is a safe assumption since four     billion entries consume at least 16 GB of memory for floats or 32 GB of     memory for doubles) and enable 64-bit indices. If more than 4 billion     local elements are present, the implementation tries to detect that,     which triggers an exception and aborts the code. Note, however, that     the detection of overflow is tricky and the detection mechanism might     fail in some circumstances. Therefore, it is strongly recommended to     not rely on this class to automatically detect the unsupported case.         [1.x.1]         This vector class supports two different memory spaces: Host and CUDA. By     default, the memory space is Host and all the data are allocated on the     CPU. When the memory space is CUDA, all the data is allocated on the GPU.     The operations on the vector are performed on the chosen memory space.     From the host, there are two methods to access the elements of the Vector     when using the CUDA memory space:      [2.x.34]       [2.x.35]  use get_values():    
* [1.x.2]
*       [2.x.36]  use import():    
* [1.x.3]
*       [2.x.37]      The import method is a lot safer and will perform an MPI communication if     necessary. Since an MPI communication may be performed, import needs to     be called on all the processors.        
*  [2.x.38]  By default, all the ranks will try to access the device 0. This is     fine is if you have one rank per node and one gpu per node. If you     have multiple GPUs on one node, we need each process to access a     different GPU. If each node has the same number of GPUs, this can be done     as follows:     <code> int n_devices = 0; cudaGetDeviceCount(&n_devices); int     device_id = my_rank % n_devices;     cudaSetDevice(device_id);     </code>         [1.x.4]         In Host mode, this class allows to use MPI-3 shared-memory features     by providing a separate MPI communicator that consists of processes on     the same shared-memory domain. By calling     `vector.shared_vector_data();`,     users have read-only access to both locally-owned and ghost values of     processes combined in the shared-memory communicator ( [2.x.39]  in     reinit()).         For this to work, you have to call the constructor or one of the reinit()     functions of this class with a non-default value for the `comm_sm`     argument, where the argument corresponds to a communicator consisting of     all processes on the same shared-memory domain. This kind of communicator     can be created using the following code snippet:    
* [1.x.5]
*           [2.x.40]  CUDAWrappers    
* [0.x.3]*
        [2.x.41]  1: Basic Object-handling      
* [0.x.4]*
       Empty constructor.      
* [0.x.5]*
       Copy constructor. Uses the parallel partitioning of  [2.x.42]        It should be noted that this constructor automatically sets ghost       values to zero. Call  [2.x.43]  directly following       construction if a ghosted vector is required.      
* [0.x.6]*
       Construct a parallel vector of the given global size without any       actual parallel distribution.      
* [0.x.7]*
       Construct a parallel vector. The local range is specified by  [2.x.44]        locally_owned_set (note that this must be a contiguous interval,       multiple intervals are not possible). The IndexSet  [2.x.45]        specifies ghost indices, i.e., indices which one might need to read       data from or accumulate data from. It is allowed that the set of       ghost indices also contains the local range, but it does not need to.             This function involves global communication, so it should only be       called once for a given layout. Use the constructor with       Vector<Number> argument to create additional vectors with the same       parallel layout.              [2.x.46]         [2.x.47]  "vectors with ghost elements"      
* [0.x.8]*
       Same constructor as above but without any ghost indices.      
* [0.x.9]*
       Create the vector based on the parallel partitioning described in  [2.x.48]        partitioner. The input argument is a shared pointer, which stores the       partitioner data only once and share it between several vectors with       the same layout.      
* [0.x.10]*
       Destructor.      
* [0.x.11]*
       Set the global size of the vector to  [2.x.49]  without any actual       parallel distribution.      
* [0.x.12]*
       Uses the parallel layout of the input vector  [2.x.50]  and       allocates memory for this vector. Recommended initialization function       when several vectors with the same layout should be created.             If the flag  [2.x.51]  is set to false, the memory will       be initialized with zero, otherwise the memory will be untouched (and       the user must make sure to fill it with reasonable data before using       it).      
* [0.x.13]*
       Initialize the vector. The local range is specified by  [2.x.52]        locally_owned_set (note that this must be a contiguous interval,       multiple intervals are not possible). The IndexSet  [2.x.53]        specifies ghost indices, i.e., indices which one might need to read       data from or accumulate data from. It is allowed that the set of       ghost indices also contains the local range, but it does not need to.             This function involves global communication, so it should only be       called once for a given layout. Use the  [2.x.54]  function with       Vector<Number> argument to create additional vectors with the same       parallel layout.              [2.x.55]         [2.x.56]  "vectors with ghost elements"      
* [0.x.14]*
       Same as above, but without ghost entries.      
* [0.x.15]*
       Initialize the vector given to the parallel partitioning described in        [2.x.57]  The input argument is a shared pointer, which stores       the partitioner data only once and share it between several vectors       with the same layout.             The optional argument  [2.x.58]  which consists of processes on       the same shared-memory domain, allows users have read-only access to       both locally-owned and ghost values of processes combined in the       shared-memory communicator. See the general documentation of this class       for more information about this argument.      
* [0.x.16]*
       Initialize vector with  [2.x.59]  locally-owned and  [2.x.60]        ghost degrees of freedoms.             The optional argument  [2.x.61]  which consists of processes on       the same shared-memory domain, allows users have read-only access to       both locally-owned and ghost values of processes combined in the       shared-memory communicator. See the general documentation of this class       for more information about this argument.            
*  [2.x.62]  In the created underlying partitioner, the local index range is         translated to global indices in an ascending and one-to-one fashion,         i.e., the indices of process  [2.x.63]  sit exactly between the indices of         the processes  [2.x.64]  and  [2.x.65] , respectively. Setting the          [2.x.66]  variable to an appropriate value provides memory space         for the ghost data in a vector's memory allocation as and allows         access to it via local_element(). However, the associated global         indices must be handled externally in this case.      
* [0.x.17]*
       Swap the contents of this vector and the other vector  [2.x.67]  One could       do this operation with a temporary variable and copying over the data       elements, but this function is significantly more efficient since it       only swaps the pointers to the data of the two vectors and therefore       does not need to allocate temporary storage and move data around.             This function is analogous to the  [2.x.68]  function of all C++       standard containers. Also, there is a global function       <tt>swap(u,v)</tt> that simply calls <tt>u.swap(v)</tt>, again in       analogy to standard functions.      
* [0.x.18]*
       Assigns the vector to the parallel partitioning of the input vector        [2.x.69]  and copies all the data.             If one of the input vector or the calling vector (to the left of the       assignment operator) had ghost elements set before this operation,       the calling vector will have ghost values set. Otherwise, it will be       in write mode. If the input vector does not have any ghost elements       at all, the vector will also update its ghost values in analogy to       the respective setting the Trilinos and PETSc vectors.      
* [0.x.19]*
       Assigns the vector to the parallel partitioning of the input vector        [2.x.70]  and copies all the data.             If one of the input vector or the calling vector (to the left of the       assignment operator) had ghost elements set before this operation,       the calling vector will have ghost values set. Otherwise, it will be       in write mode. If the input vector does not have any ghost elements       at all, the vector will also update its ghost values in analogy to       the respective setting the Trilinos and PETSc vectors.      
* [0.x.20]*
        [2.x.71]  2: Parallel data exchange      
* [0.x.21]*
       This function copies the data that has accumulated in the data buffer       for ghost indices to the owning processor. For the meaning of the       argument  [2.x.72]  see the entry on        [2.x.73]  "Compressing distributed vectors and matrices"       in the glossary.             There are four variants for this function. If called with argument  [2.x.74]         [2.x.75]  adds all the data accumulated in ghost elements       to the respective elements on the owning processor and clears the       ghost array afterwards. If called with argument  [2.x.76]         [2.x.77]  a set operation is performed. Since setting       elements in a vector with ghost elements is ambiguous (as one can set       both the element on the ghost site as well as the owning site), this       operation makes the assumption that all data is set correctly on the       owning processor. Upon call of  [2.x.78]  all       ghost entries are thus simply zeroed out (using zero_ghost_values()).       In debug mode, a check is performed for whether the data set is       actually consistent between processors, i.e., whenever a non-zero       ghost element is found, it is compared to the value on the owning       processor and an exception is thrown if these elements do not agree.       If called with  [2.x.79]  or  [2.x.80]  the       minimum or maximum on all elements across the processors is set.      
*  [2.x.81]  This vector class has a fixed set of ghost entries attached to       the local representation. As a consequence, all ghost entries are       assumed to be valid and will be exchanged unconditionally according       to the given VectorOperation. Make sure to initialize all ghost       entries with the neutral element of the given VectorOperation or       touch all ghost entries. The neutral element is zero for        [2.x.82]  and  [2.x.83]  `+inf` for        [2.x.84]  and `-inf` for  [2.x.85]  If all       values are initialized with values below zero and compress is called       with  [2.x.86]  two times subsequently, the maximal value       after the second calculation will be zero.      
* [0.x.22]*
       Fills the data field for ghost indices with the values stored in the       respective positions of the owning processor. This function is needed       before reading from ghosts. The function is  [2.x.87]  even though       ghost data is changed. This is needed to allow functions with a  [2.x.88]        const vector to perform the data exchange without creating       temporaries.             After calling this method, write access to ghost elements of the       vector is forbidden and an exception is thrown. Only read access to       ghost elements is allowed in this state. Note that all subsequent       operations on this vector, like global vector addition, etc., will       also update the ghost values by a call to this method after the       operation. However, global reduction operations like norms or the       inner product will always ignore ghost elements in order to avoid       counting the ghost data more than once. To allow writing to ghost       elements again, call zero_out_ghost_values().              [2.x.89]         [2.x.90]  "vectors with ghost elements"      
* [0.x.23]*
       Initiates communication for the  [2.x.91]  function with non-       blocking communication. This function does not wait for the transfer       to finish, in order to allow for other computations during the time       it takes until all data arrives.             Before the data is actually exchanged, the function must be followed       by a call to  [2.x.92]              In case this function is called for more than one vector before  [2.x.93]        compress_finish() is invoked, it is mandatory to specify a unique       communication channel to each such call, in order to avoid several       messages with the same ID that will corrupt this operation. Any       communication channel less than 100 is a valid value (in particular,       the range  [2.x.94]  is reserved for        [2.x.95]       
* [0.x.24]*
       For all requests that have been initiated in compress_start, wait for       the communication to finish. Once it is finished, add or set the data       (depending on the flag operation) to the respective positions in the       owning processor, and clear the contents in the ghost data fields.       The meaning of this argument is the same as in compress().             This function should be called exactly once per vector after calling       compress_start, otherwise the result is undefined. In particular, it       is not well-defined to call compress_start on the same vector again       before compress_finished has been called. However, there is no       warning to prevent this situation.             Must follow a call to the  [2.x.96]  function.             When the MemorySpace is CUDA and MPI is not CUDA-aware, data changed on       the device after the call to compress_start will be lost.      
* [0.x.25]*
       Initiates communication for the  [2.x.97]  function       with non-blocking communication. This function does not wait for the       transfer to finish, in order to allow for other computations during       the time it takes until all data arrives.             Before the data is actually exchanged, the function must be followed       by a call to  [2.x.98]              In case this function is called for more than one vector before  [2.x.99]        update_ghost_values_finish() is invoked, it is mandatory to specify a       unique communication channel to each such call, in order to avoid       several messages with the same ID that will corrupt this operation.       Any communication channel less than 100 is a valid value (in       particular, the range  [2.x.100]  is reserved for        [2.x.101]       
* [0.x.26]*
       For all requests that have been started in update_ghost_values_start,       wait for the communication to finish.             Must follow a call to the  [2.x.102]  function       before reading data from ghost indices.      
* [0.x.27]*
       This method zeros the entries on ghost dofs, but does not touch       locally owned DoFs.             After calling this method, read access to ghost elements of the       vector is forbidden and an exception is thrown. Only write access to       ghost elements is allowed in this state.              [2.x.103]  Use zero_out_ghost_values() instead.      
* [0.x.28]*
       This method zeros the entries on ghost dofs, but does not touch       locally owned DoFs.             After calling this method, read access to ghost elements of the       vector is forbidden and an exception is thrown. Only write access to       ghost elements is allowed in this state.      
* [0.x.29]*
       Return whether the vector currently is in a state where ghost values       can be read or not. This is the same functionality as other parallel       vectors have. If this method returns false, this only means that       read-access to ghost elements is prohibited whereas write access is       still possible (to those entries specified as ghosts during       initialization), not that there are no ghost elements at all.              [2.x.104]         [2.x.105]  "vectors with ghost elements"      
* [0.x.30]*
       This method copies the data in the locally owned range from another       distributed vector  [2.x.106]  into the calling vector. As opposed to       operator= that also includes ghost entries, this operation ignores       the ghost range. The only prerequisite is that the local range on the       calling vector and the given vector  [2.x.107]  are the same on all       processors. It is explicitly allowed that the two vectors have       different ghost elements that might or might not be related to each       other.             Since no data exchange is performed, make sure that neither  [2.x.108]        nor the calling vector have pending communications in order to obtain       correct results.      
* [0.x.31]*
       Import all the elements present in the distributed vector  [2.x.109]         [2.x.110]   [2.x.111]  is used to decide if the elements       in  [2.x.112]  should be added to the current vector or replace the current       elements. The main purpose of this function is to get data from one       memory space, e.g. CUDA, to the other, e.g. the Host.            
*  [2.x.113]  The partitioners of the two distributed vectors need to be the       same as no MPI communication is performed.      
* [0.x.32]*
        [2.x.114]  3: Implementation of VectorSpaceVector      
* [0.x.33]*
       Change the dimension to that of the vector V. The elements of V are not       copied.      
* [0.x.34]*
       Multiply the entire vector by a fixed factor.      
* [0.x.35]*
       Divide the entire vector by a fixed factor.      
* [0.x.36]*
       Add the vector  [2.x.115]  to the present one.      
* [0.x.37]*
       Subtract the vector  [2.x.116]  from the present one.      
* [0.x.38]*
       Import all the elements present in the vector's IndexSet from the input       vector  [2.x.117]   [2.x.118]   [2.x.119]  is used to decide if       the elements in  [2.x.120]  should be added to the current vector or replace the       current elements. The last parameter can be used if the same       communication pattern is used multiple times. This can be used to       improve performance.            
*  [2.x.121]  If the MemorySpace is CUDA, the data in the ReadWriteVector will       be moved to the device.      
* [0.x.39]*
       Return the scalar product of two vectors.      
* [0.x.40]*
       Add  [2.x.122]  to all components. Note that  [2.x.123]  is a scalar not a vector.      
* [0.x.41]*
       Simple addition of a multiple of a vector, i.e. <tt>*this += a*V</tt>.      
* [0.x.42]*
       Multiple addition of scaled vectors, i.e. <tt>*this += a*V+b*W</tt>.      
* [0.x.43]*
       A collective add operation: This function adds a whole set of values       stored in  [2.x.124]  to the vector components specified by  [2.x.125]       
* [0.x.44]*
       Scaling and simple addition of a multiple of a vector, i.e. <tt>*this =       s*(*this)+a*V</tt>.      
* [0.x.45]*
       Scale each element of this vector by the corresponding element in the       argument. This function is mostly meant to simulate multiplication (and       immediate re-assignment) by a diagonal scaling matrix.      
* [0.x.46]*
       Assignment <tt>*this = a*V</tt>.      
* [0.x.47]*
       Return the l<sub>1</sub> norm of the vector (i.e., the sum of the       absolute values of all entries among all processors).      
* [0.x.48]*
       Return the  [2.x.126]  norm of the vector (i.e., the square root of       the sum of the square of all entries among all processors).      
* [0.x.49]*
       Return the square of the  [2.x.127]  norm of the vector.      
* [0.x.50]*
       Return the maximum norm of the vector (i.e., the maximum absolute value       among all entries and among all processors).      
* [0.x.51]*
       Perform a combined operation of a vector addition and a subsequent       inner product, returning the value of the inner product. In other       words, the result of this function is the same as if the user called      
* [1.x.6]
*              The reason this function exists is that this operation involves less       memory transfer than calling the two functions separately. This method       only needs to load three vectors,  [2.x.128]   [2.x.129]   [2.x.130]  whereas calling       separate methods means to load the calling vector  [2.x.131]  twice. Since       most vector operations are memory transfer limited, this reduces the       time by 25\% (or 50\% if  [2.x.132]  equals  [2.x.133]              For complex-valued vectors, the scalar product in the second step is       implemented as        [2.x.134] .      
* [0.x.52]*
       Return the global size of the vector, equal to the sum of the number of       locally owned indices among all processors.      
* [0.x.53]*
       Return an index set that describes which elements of this vector are       owned by the current processor. As a consequence, the index sets       returned on different processors if this is a distributed vector will       form disjoint sets that add up to the complete index set. Obviously, if       a vector is created on only one processor, then the result would       satisfy      
* [1.x.7]
*       
* [0.x.54]*
       Print the vector to the output stream  [2.x.135]       
* [0.x.55]*
       Return the memory consumption of this class in bytes.      
* [0.x.56]*
        [2.x.136]  4: Other vector operations not included in VectorSpaceVector      
* [0.x.57]*
       Sets all elements of the vector to the scalar  [2.x.137]  If the scalar is       zero, also ghost elements are set to zero, otherwise they remain       unchanged.      
* [0.x.58]*
       This is a collective add operation that adds a whole set of values       stored in  [2.x.138]  to the vector components specified by  [2.x.139]       
* [0.x.59]*
       Take an address where n_elements are stored contiguously and add them       into the vector.      
* [0.x.60]*
       Scaling and simple vector addition, i.e.  <tt>*this =       s*(*this)+V</tt>.      
* [0.x.61]*
        [2.x.140]  5: Entry access and local data representation      
* [0.x.62]*
       Return the local size of the vector, i.e., the number of indices       owned locally.              [2.x.141]  Use locally_owned_size() instead.      
* [0.x.63]*
       Return the local size of the vector, i.e., the number of indices       owned locally.      
* [0.x.64]*
       Return true if the given global index is in the local range of this       processor.      
* [0.x.65]*
       Make the  [2.x.142]  class a bit like the <tt>vector<></tt> class of       the C++ standard library by returning iterators to the start and end       of the [1.x.8] elements of this vector.             It holds that end()
* 
*  - begin() == locally_owned_size().            
*  [2.x.143]  For the CUDA memory space, the iterator points to memory on the       device.      
* [0.x.66]*
       Return constant iterator to the start of the locally owned elements       of the vector.            
*  [2.x.144]  For the CUDA memory space, the iterator points to memory on the       device.      
* [0.x.67]*
       Return an iterator pointing to the element past the end of the array       of locally owned entries.            
*  [2.x.145]  For the CUDA memory space, the iterator points to memory on the       device.      
* [0.x.68]*
       Return a constant iterator pointing to the element past the end of       the array of the locally owned entries.            
*  [2.x.146]  For the CUDA memory space, the iterator points to memory on the       device.      
* [0.x.69]*
       Read access to the data in the position corresponding to  [2.x.147]        global_index. The index must be either in the local range of the       vector or be specified as a ghost index at construction.             Performance: <tt>O(1)</tt> for locally owned elements that represent       a contiguous range and <tt>O(log(n<sub>ranges</sub>))</tt> for ghost       elements (quite fast, but slower than local_element()).      
* [0.x.70]*
       Read and write access to the data in the position corresponding to  [2.x.148]        global_index. The index must be either in the local range of the       vector or be specified as a ghost index at construction.             Performance: <tt>O(1)</tt> for locally owned elements that represent       a contiguous range and <tt>O(log(n<sub>ranges</sub>))</tt> for ghost       elements (quite fast, but slower than local_element()).      
* [0.x.71]*
       Read access to the data in the position corresponding to  [2.x.149]        global_index. The index must be either in the local range of the       vector or be specified as a ghost index at construction.             This function does the same thing as operator().      
* [0.x.72]*
       Read and write access to the data in the position corresponding to  [2.x.150]        global_index. The index must be either in the local range of the       vector or be specified as a ghost index at construction.             This function does the same thing as operator().      
* [0.x.73]*
       Read access to the data field specified by  [2.x.151]  Locally       owned indices can be accessed with indices        [2.x.152] , and ghost indices with indices        [2.x.153] .             Performance: Direct array access (fast).      
* [0.x.74]*
       Read and write access to the data field specified by  [2.x.154]        Locally owned indices can be accessed with indices        [2.x.155] , and ghost indices with indices        [2.x.156] .             Performance: Direct array access (fast).      
* [0.x.75]*
       Return the pointer to the underlying raw array.            
*  [2.x.157]  For the CUDA memory space, the pointer points to memory on the       device.      
* [0.x.76]*
       Instead of getting individual elements of a vector via operator(),       this function allows getting a whole set of elements at once. The       indices of the elements to be read are stated in the first argument,       the corresponding values are returned in the second.             If the current vector is called  [2.x.158]  then this function is the equivalent       to the code      
* [1.x.9]
*               [2.x.159]  The sizes of the  [2.x.160]  and  [2.x.161]  arrays must be identical.            
*  [2.x.162]  This function is not implemented for CUDA memory space.      
* [0.x.77]*
       Instead of getting individual elements of a vector via operator(),       this function allows getting a whole set of elements at once. In       contrast to the previous function, this function obtains the       indices of the elements by dereferencing all elements of the iterator       range provided by the first two arguments, and puts the vector       values into memory locations obtained by dereferencing a range       of iterators starting at the location pointed to by the third       argument.             If the current vector is called  [2.x.163]  then this function is the equivalent       to the code      
* [1.x.10]
*               [2.x.164]  It must be possible to write into as many memory locations         starting at  [2.x.165]  as there are iterators between          [2.x.166]  and  [2.x.167]       
* [0.x.78]*
       Return whether the vector contains only elements with value zero.       This is a collective operation. This function is expensive, because       potentially all elements have to be checked.      
* [0.x.79]*
       Compute the mean value of all the entries in the vector.      
* [0.x.80]*
        [2.x.168] -norm of the vector. The pth root of the sum of the pth powers       of the absolute values of the elements.      
* [0.x.81]*
        [2.x.169]  6: Mixed stuff      
* [0.x.82]*
       Return a reference to the MPI communicator object in use with this       vector.      
* [0.x.83]*
       Return the MPI partitioner that describes the parallel layout of the       vector. This object can be used to initialize another vector with the       respective reinit() call, for additional queries regarding the       parallel communication, or the compatibility of partitioners.      
* [0.x.84]*
       Check whether the given partitioner is compatible with the       partitioner used for this vector. Two partitioners are compatible if       they have the same local size and the same ghost indices. They do not       necessarily need to be the same data field of the shared pointer.       This is a local operation only, i.e., if only some processors decide       that the partitioning is not compatible, only these processors will       return  [2.x.170]  whereas the other processors will return  [2.x.171]       
* [0.x.85]*
       Check whether the given partitioner is compatible with the       partitioner used for this vector. Two partitioners are compatible if       they have the same local size and the same ghost indices. They do not       necessarily need to be the same data field. As opposed to       partitioners_are_compatible(), this method checks for compatibility       among all processors and the method only returns  [2.x.172]  if the       partitioner is the same on all processors.             This method performs global communication, so make sure to use it       only in a context where all processors call it the same number of       times.      
* [0.x.86]*
       Change the ghost state of this vector to  [2.x.173]       
* [0.x.87]*
       Get pointers to the beginning of the values of the other       processes of the same shared-memory domain.      
* [0.x.88]*
       Attempt to perform an operation between two incompatible vector types.            
*  [2.x.174]       
* [0.x.89]*
       Attempt to perform an operation not implemented on the device.            
*  [2.x.175]       
* [0.x.90]*
       Exception      
* [0.x.91]*
       Exception      
* [0.x.92]*
       Simple addition of a multiple of a vector, i.e. <tt>*this += a*V</tt>       without MPI communication.      
* [0.x.93]*
       Scaling and simple addition of a multiple of a vector, i.e. <tt>*this =       s*(*this)+a*V</tt> without MPI communication.      
* [0.x.94]*
       Local part of the inner product of two vectors.      
* [0.x.95]*
       Local part of norm_sqr().      
* [0.x.96]*
       Local part of mean_value().      
* [0.x.97]*
       Local part of l1_norm().      
* [0.x.98]*
       Local part of lp_norm().      
* [0.x.99]*
       Local part of linfty_norm().      
* [0.x.100]*
       Local part of the addition followed by an inner product of two       vectors. The same applies for complex-valued vectors as for       the add_and_dot() function.      
* [0.x.101]*
       Shared pointer to store the parallel partitioning information. This       information can be shared between several vectors that have the same       partitioning.      
* [0.x.102]*
       The size that is currently allocated in the val array.      
* [0.x.103]*
       Underlying data structure storing the local elements of this vector.      
* [0.x.104]*
       For parallel loops with TBB, this member variable stores the affinity       information of loops.      
* [0.x.105]*
       Temporary storage that holds the data that is sent to this processor       in compress() or sent from this processor in update_ghost_values().      
* [0.x.106]*
       Stores whether the vector currently allows for reading ghost elements       or not. Note that this is to ensure consistent ghost data and does       not indicate whether the vector actually can store ghost elements. In       particular, when assembling a vector we do not allow reading       elements, only writing them.      
* [0.x.107]*
       A vector that collects all requests from compress() operations.       This class uses persistent MPI communicators, i.e., the communication       channels are stored during successive calls to a given function. This       reduces the overhead involved with setting up the MPI machinery, but       it does not remove the need for a receive operation to be posted       before the data can actually be sent.      
* [0.x.108]*
       A vector that collects all requests from update_ghost_values()       operations. This class uses persistent MPI communicators.      
* [0.x.109]*
       A lock that makes sure that the compress() and update_ghost_values()       functions give reasonable results also when used       with several threads.      
* [0.x.110]*
       Communicator to be used for the shared-memory domain. See the general       documentation of this class for more information about the purpose of       `comm_sm`.      
* [0.x.111]*
       A helper function that clears the compress_requests and       update_ghost_values_requests field. Used in reinit() functions.      
* [0.x.112]*
       A helper function that is used to resize the val array.      
* [0.x.113]*
 Global function  [2.x.176]  which overloads the default implementation of the C++ standard library which uses a temporary object. The function simply exchanges the data of the two vectors.
*   [2.x.177]  Vector

* 
* [0.x.114]*
 Declare  [2.x.178]  as distributed vector.

* 
* [0.x.115]*
     A helper class used internally in linear_operator.h. Specialization for      [2.x.179]     
* [0.x.116]

include/deal.II-translator/lac/la_parallel_vector.templates_0.txt
[0.x.0]

include/deal.II-translator/lac/la_vector_0.txt
[0.x.0]*
 A namespace for vector classes.
*  This namespace contains various classes that provide wrappers to vector classes from different external libraries like Trilinos (EPetra) or PETSc and native implementations like  [2.x.0] 
*  The different vector classes are derived from VectorSpaceVector to provide a joint interface for vector space operations, are derived from ReadWriteVector (or ReadWriteVector itself), or both. The separation of vector space operations (like norms or vector additions) through VectorSpaceVector and element access through ReadWriteVector are by design and improve performance.

* 
* [0.x.1]!  [2.x.1]  Vectors   [2.x.2]   
* [0.x.2]*
   Numerical vector of data. This class derives from both    [2.x.3]  and    [2.x.4]  As opposed to the array of   the C++ standard library, this class implements an element of a vector   space suitable for numerical computations.  
* [0.x.3]*
     Constructor. Create a vector of dimension zero.    
* [0.x.4]*
     Copy constructor. Sets the dimension to that of the given vector and     copies all elements.    
* [0.x.5]*
     Constructor. Set dimension to  [2.x.5]  and initialize all elements with     zero.         The constructor is made explicit to avoid accident like this:     <tt>v=0;</tt>. Presumably, the user wants to set every element of the     vector to zero, but instead, what happens is this call:     <tt>v=Vector [2.x.6]  i.e. the vector is replaced by one of     length zero.    
* [0.x.6]*
     Initialize the vector with a given range of values pointed to by the     iterators. This function exists in analogy to the  [2.x.7]  class.    
* [0.x.7]*
     Set the global size of the vector to  [2.x.8]  The stored elements have     their index in [0,size).         If the flag  [2.x.9]  is set to false, the memory will be     initialized with zero, otherwise the memory will be untouched (and the     user must make sure to fill it with reasonable data before using it).    
* [0.x.8]*
     Uses the same IndexSet as the one of the input vector  [2.x.10]  and     allocates memory for this vector.         If the flag  [2.x.11]  is set to false, the memory will be     initialized with zero, otherwise the memory will be untouched (and the     user must make sure to fill it with reasonable data before using it).    
* [0.x.9]*
     Initializes the vector. The indices are specified by  [2.x.12]      locally_stored_indices.         If the flag  [2.x.13]  is set to false, the memory will be     initialized with zero, otherwise the memory will be untouched (and the     user must make sure to fill it with reasonable data before using it).     locally_stored_indices.    
* [0.x.10]*
     Change the dimension to that of the vector V. The elements of V are not     copied.    
* [0.x.11]*
     Returns `false` as this is a serial vector.         This functionality only needs to be called if using MPI based vectors and     exists in other objects for compatibility.    
* [0.x.12]*
     Copies the data of the input vector  [2.x.14]     
* [0.x.13]*
     Copies the data of the input vector  [2.x.15]     
* [0.x.14]*
     Sets all elements of the vector to the scalar  [2.x.16]  This operation is     only allowed if  [2.x.17]  is equal to zero.    
* [0.x.15]*
     Multiply the entire vector by a fixed factor.    
* [0.x.16]*
     Divide the entire vector by a fixed factor.    
* [0.x.17]*
     Add the vector  [2.x.18]  to the present one.    
* [0.x.18]*
     Subtract the vector  [2.x.19]  from the present one.    
* [0.x.19]*
     Return the scalar product of two vectors.    
* [0.x.20]*
     This function is not implemented and will throw an exception.    
* [0.x.21]*
     Add  [2.x.20]  to all components. Note that  [2.x.21]  is a scalar not a vector.    
* [0.x.22]*
     Simple addition of a multiple of a vector, i.e. <tt>*this += a*V</tt>.    
* [0.x.23]*
     Multiple addition of a multiple of a vector, i.e. <tt>*this +=     a*V+b*W</tt>.    
* [0.x.24]*
     Scaling and simple addition of a multiple of a vector, i.e. <tt>*this =     s*(*this)+a*V</tt>.    
* [0.x.25]*
     Scale each element of this vector by the corresponding element in the     argument. This function is mostly meant to simulate multiplication (and     immediate re-assignment) by a diagonal scaling matrix.    
* [0.x.26]*
     Assignment <tt>*this = a*V</tt>.    
* [0.x.27]*
     Return whether the vector contains only elements with value zero.    
* [0.x.28]*
     Return the mean value of all the entries of this vector.    
* [0.x.29]*
     Return the l<sub>1</sub> norm of the vector (i.e., the sum of the     absolute values of all entries).    
* [0.x.30]*
     Return the l<sub>2</sub> norm of the vector (i.e., the square root of     the sum of the square of all entries among all processors).    
* [0.x.31]*
     Return the maximum norm of the vector (i.e., the maximum absolute value     among all entries and among all processors).    
* [0.x.32]*
     Perform a combined operation of a vector addition and a subsequent     inner product, returning the value of the inner product. In other     words, the result of this function is the same as if the user called    
* [1.x.0]
*          The reason this function exists is that this operation involves less     memory transfer than calling the two functions separately. This method     only needs to load three vectors,  [2.x.22]   [2.x.23]   [2.x.24]  whereas calling     separate methods means to load the calling vector  [2.x.25]  twice. Since     most vector operations are memory transfer limited, this reduces the time     by 25\% (or 50\% if  [2.x.26]  equals  [2.x.27]          For complex-valued vectors, the scalar product in the second step is     implemented as      [2.x.28] .    
* [0.x.33]*
     Return the global size of the vector, equal to the sum of the number of     locally owned indices among all processors.    
* [0.x.34]*
     Return an index set that describes which elements of this vector are     owned by the current processor. As a consequence, the index sets     returned on different processors if this is a distributed vector will     form disjoint sets that add up to the complete index set. Obviously, if     a vector is created on only one processor, then the result would     satisfy    
* [1.x.1]
*     
* [0.x.35]*
     Print the vector to the output stream  [2.x.29]     
* [0.x.36]*
     Print the vector to the output stream  [2.x.30]  in a format that can be     read by  [2.x.31]  Note that the IndexSet is not printed but only     the values stored in the Vector. To load the vector in python just do     <code>     vector = numpy.loadtxt('my_vector.txt')     </code>    
* [0.x.37]*
     Write the vector en bloc to a file. This is done in a binary mode, so     the output is neither readable by humans nor (probably) by other     computers using a different operating system or number format.    
* [0.x.38]*
     Read a vector en block from a file. This is done using the inverse     operations to the above function, so it is reasonably fast because the     bitstream is not interpreted.         The vector is resized if necessary.         A primitive form of error checking is performed which will recognize     the bluntest attempts to interpret some data as a vector stored bitwise     to a file, but not more.    
* [0.x.39]*
     Return the memory consumption of this class in bytes.    
* [0.x.40]*
     Write and read the data of this object from a stream for the purpose     of serialization using the [BOOST serialization     library](https://www.boost.org/doc/libs/1_74_0/libs/serialization/doc/index.html).    
* [0.x.41]*
     Attempt to perform an operation between two incompatible vector types.        
*  [2.x.32]     
* [0.x.42]*
 Declare  [2.x.33]  as serial vector.

* 
* [0.x.43]

include/deal.II-translator/lac/la_vector.templates_0.txt
[0.x.0]

include/deal.II-translator/lac/linear_operator_0.txt
[0.x.0]*
 A class to store the abstract concept of a linear operator.
*  The class essentially consists of  [2.x.0]  objects that store the knowledge of how to apply the linear operator by implementing the abstract  [2.x.1]  interface:

* 
* [1.x.0]
* 
*  But, in contrast to a usual matrix object, the domain and range of the linear operator are also bound to the LinearOperator class on the type level. Because of this,  [2.x.2]  has two additional function objects

* 
* [1.x.1]
*  that store the knowledge how to initialize (resize + internal data structures) an arbitrary vector of the  [2.x.3]  and  [2.x.4]  space.
*  The primary purpose of this class is to provide syntactic sugar for complex matrix-vector operations and free the user from having to create, set up and handle intermediate storage locations by hand.
*  As an example consider the operation  [2.x.5] , where  [2.x.6] ,  [2.x.7]  and  [2.x.8]  denote (possible different) matrices. In order to construct a LinearOperator  [2.x.9]  that stores the knowledge of this operation, one can write:
* 

* 
* [1.x.2]
* 
* 

* 
*  [2.x.10]  This class makes heavy use of  [2.x.11]  objects and lambda functions. This flexibility comes with a run-time penalty. Only use this object to encapsulate matrix object of medium to large size (as a rule of thumb, sparse matrices with a size  [2.x.12] , or larger).
* 

* 
*  [2.x.13]  In order to use Trilinos or PETSc sparse matrices and preconditioners in conjunction with the LinearOperator class, it is necessary to extend the functionality of the LinearOperator class by means of an additional Payload.
*  For example: LinearOperator instances representing matrix inverses usually require calling some linear solver. These solvers may not have interfaces to the LinearOperator (which, for example, may represent a composite operation). The  [2.x.14]  therefore provides an interface extension to the LinearOperator so that it can be passed to the solver and used by the solver as if it were a Trilinos operator. This implies that all of the necessary functionality of the specific Trilinos operator has been overloaded within the Payload class. This includes operator-vector multiplication and inverse operator-vector multiplication, where the operator can be either a  [2.x.15]  or a  [2.x.16]  and the vector is a native Trilinos vector.
*  Another case where payloads provide a crucial supplement to the LinearOperator class are when composite operations are constructed (via operator overloading). In this instance, it is again necessary to provide an interface that produces the result of this composite operation that is compatible with Trilinos operator used by Trilinos solvers.
* 

* 
*  [2.x.17]  Many use cases of LinearOperator lead to intermediate expressions requiring a PackagedOperation. In order to include all necessary header files in one go consider using

* 
* [1.x.3]
* 
*  In order to use the full LinearOperator and PackagedOperation
* 

* 
*  [2.x.18]  To ensure that the correct payload is provided, wrapper functions for linear operators have been provided within the respective TrilinosWrappers (and, in the future, PETScWrappers) namespaces.
* 

* 
*  [2.x.19]  The  [2.x.20]  tutorial program has a detailed usage example of the LinearOperator class.
* 

* 

* 
*  [2.x.21] 

* 
* [0.x.1]*
   Create an empty LinearOperator object.   When a payload is passed to this constructor, the resulting operator is   constructed with a functional payload.   In either case, this constructor yields an object that can not actually   be used for any linear operator operations, and will throw an exception   upon invocation.  
* [0.x.2]*
   Default copy constructor.  
* [0.x.3]*
   Templated copy constructor that creates a LinearOperator object from an   object  [2.x.22]  for which the conversion function    [2.x.23]  is defined.  
* [0.x.4]*
   Default copy assignment operator.  
* [0.x.5]*
   Templated copy assignment operator for an object  [2.x.24]  for which the   conversion function  [2.x.25]  is defined.  
* [0.x.6]*
   Application of the LinearOperator object to a vector u of the  [2.x.26]    space giving a vector v of the  [2.x.27]  space.  
* [0.x.7]*
   Application of the LinearOperator object to a vector u of the  [2.x.28]    space. The result is added to the vector v.  
* [0.x.8]*
   Application of the transpose LinearOperator object to a vector u of the    [2.x.29]  space giving a vector v of the  [2.x.30]  space.  
* [0.x.9]*
   Application of the transpose LinearOperator object to a vector  [2.x.31]  of   the  [2.x.32]  space.The result is added to the vector  [2.x.33]   
* [0.x.10]*
   Initializes a vector v of the Range space to be directly usable as the   destination parameter in an application of vmult. Similar to the reinit   functions of the vector classes, the boolean determines whether a fast   initialization is done, i.e., if it is set to false the content of the   vector is set to 0.  
* [0.x.11]*
   Initializes a vector of the Domain space to be directly usable as the   source parameter in an application of vmult. Similar to the reinit   functions of the vector classes, the boolean determines whether a fast   initialization is done, i.e., if it is set to false the content of the   vector is set to 0.  
* [0.x.12]*
    [2.x.34]  In-place vector space operations  
* [0.x.13]*
   Addition with a LinearOperator  [2.x.35]  with the same  [2.x.36]  and    [2.x.37]   
* [0.x.14]*
   Subtraction with a LinearOperator  [2.x.38]  with the same  [2.x.39]    and  [2.x.40]   
* [0.x.15]*
   Composition of the LinearOperator with an endomorphism  [2.x.41]  of   the  [2.x.42]  space.  
* [0.x.16]*
   Scalar multiplication of the LinearOperator with  [2.x.43]  from the   right.  
* [0.x.17]*
   This bool is used to determine whether a linear operator is a null   operator. In this case the class is able to optimize some operations like   multiplication or addition.  
* [0.x.18]*
  [2.x.44]  Vector space operations

* 
* [0.x.19]*
  [2.x.45]  LinearOperator
*  Addition of two linear operators  [2.x.46]  and  [2.x.47]  given by  [2.x.48] 
* 

* 
*  [2.x.49] 

* 
* [0.x.20]*
  [2.x.50]  LinearOperator
*  Subtraction of two linear operators  [2.x.51]  and  [2.x.52]  given by  [2.x.53] 
* 

* 
*  [2.x.54] 

* 
* [0.x.21]*
  [2.x.55]  LinearOperator
*  Scalar multiplication of a ScalarOperator object  [2.x.56]  with  [2.x.57]  from the left.
*  The  [2.x.58]  and  [2.x.59]  types must implement the following  [2.x.60]  member functions accepting the appropriate scalar Number type for rescaling:
* 

* 
* [1.x.4]
* 
* 

* 
*  [2.x.61] 

* 
* [0.x.22]*
  [2.x.62]  LinearOperator
*  Scalar multiplication of a ScalarOperator object from the right.
*  The  [2.x.63]  and  [2.x.64]  types must implement the following  [2.x.65]  member functions for rescaling:
* 

* 
* [1.x.5]
* 
* 

* 
*  [2.x.66] 

* 
* [0.x.23]*
  [2.x.67]  Composition and manipulation of a LinearOperator

* 
* [0.x.24]*
  [2.x.68]  LinearOperator
*  Composition of two linear operators  [2.x.69]  and  [2.x.70]  given by  [2.x.71] 
* 

* 
*  [2.x.72] 

* 
* [0.x.25]*
  [2.x.73]  LinearOperator
*  Return the transpose linear operations of  [2.x.74] 
* 

* 

* 
*  [2.x.75] 

* 
* [0.x.26]*
  [2.x.76]  LinearOperator
*  Return an object representing the inverse of the LinearOperator  [2.x.77] 
*  The function takes references  [2.x.78]  and  [2.x.79]  to an iterative solver and a preconditioner that are used in the  [2.x.80]  implementations of the LinearOperator object.
*  The LinearOperator object that is created stores a reference to  [2.x.81]  and  [2.x.82]  Thus, both objects must remain a valid reference for the whole lifetime of the LinearOperator object. Internal data structures of the  [2.x.83]  object will be modified upon invocation of  [2.x.84] .
* 

* 

* 
*  [2.x.85] 

* 
* [0.x.27]*
  [2.x.86]  LinearOperator
*  Variant of above function that takes a LinearOperator  [2.x.87]  as preconditioner argument.
* 

* 
*  [2.x.88] 

* 
* [0.x.28]*
  [2.x.89]  LinearOperator
*  Variant of above function without a preconditioner argument. In this case the identity_operator() of the  [2.x.90]  argument is used as a preconditioner. This is equivalent to using PreconditionIdentity.
* 

* 
*  [2.x.91] 

* 
* [0.x.29]*
  [2.x.92]  LinearOperator
*  Special overload of above function that takes a PreconditionIdentity argument.
* 

* 
*  [2.x.93] 

* 
* [0.x.30]*
  [2.x.94]  Creation of a LinearOperator

* 
* [0.x.31]*
  [2.x.95]  LinearOperator
*  Return a LinearOperator that is the identity of the vector space  [2.x.96] 
*  The function takes an  [2.x.97]  object  [2.x.98]  as an argument to initialize the  [2.x.99]  and  [2.x.100]  objects of the LinearOperator object.
* 

* 
*  [2.x.101] 

* 
* [0.x.32]*
  [2.x.102]  LinearOperator
*  Return a LinearOperator that is the identity of the vector space  [2.x.103] 
*  The function takes a LinearOperator  [2.x.104]  and uses its range initializer to create an identity operator. In contrast to the function above, this function also ensures that the underlying Payload matches that of the input  [2.x.105] 
* 

* 
*  [2.x.106] 

* 
* [0.x.33]*
  [2.x.107]  LinearOperator
*  Return a nulled variant of the LinearOperator  [2.x.108]  i.e. with optimized  [2.x.109]   [2.x.110]  etc. functions and with  [2.x.111]  set to true.
* 

* 
*  [2.x.112] 

* 
* [0.x.34]*
  [2.x.113]  LinearOperator
*  Return a LinearOperator that acts as a mean value filter. The vmult() functions of this matrix subtract the mean values of the vector.
*  The function takes an  [2.x.114]  object  [2.x.115]  as an argument to initialize the  [2.x.116]  and  [2.x.117]  objects of the LinearOperator object.
* 

* 
*  [2.x.118] 

* 
* [0.x.35]*
  [2.x.119]  LinearOperator
*  Return a LinearOperator that acts as a mean value filter. The vmult() functions of this matrix subtract the mean values of the vector.
*  The function takes a LinearOperator  [2.x.120]  and uses its range initializer to create an mean value filter operator. The function also ensures that the underlying Payload matches that of the input  [2.x.121] 
* 

* 
*  [2.x.122] 

* 
* [0.x.36]*
     A helper class that is responsible for the initialization of a vector     to be directly usable as the destination parameter, or source parameter     in an application of vmult of a matrix.         The generic version of this class just calls      [2.x.123]  with the result of      [2.x.124] , respectively.     This class is specialized for more complicated data structures, such as      [2.x.125]  etc.    
* [0.x.37]*
       Initializes a vector v of the Range space to be directly usable as       the destination parameter in an application of vmult. Similar to the       reinit functions of the vector classes, the boolean determines       whether a fast initialization is done, i.e., if it is set to false the       content of the vector is set to 0.             The generic version of this class just calls        [2.x.126]  with the result of        [2.x.127] .      
* [0.x.38]*
       Initializes a vector of the Domain space to be directly usable as the       source parameter in an application of vmult. Similar to the reinit       functions of the vector classes, the boolean determines whether a       fast initialization is done, i.e., if it is set to false the content       of the vector is set to 0.             The generic version of this class just calls        [2.x.128]  with the result of        [2.x.129] .      
* [0.x.39]*
     A dummy class for LinearOperators that do not require any extensions to     facilitate the operations of the matrix.         This is the Payload class typically associated with deal.II's native     SparseMatrix. To use Trilinos and PETSc sparse matrix classes it is     necessary to initialize a LinearOperator with their associated Payload.            
*  [2.x.130]     
* [0.x.40]*
       Default constructor             Since this class does not do anything in particular and needs no       special configuration, we have only one generic constructor that can       be called under any conditions.      
* [0.x.41]*
       Return a payload configured for identity operations      
* [0.x.42]*
       Return a payload configured for null operations      
* [0.x.43]*
       Return a payload configured for transpose operations      
* [0.x.44]*
       Return a payload configured for inverse operations      
* [0.x.45]*
     Operator that returns a payload configured to support the addition of     two LinearOperators    
* [0.x.46]*
     Operator that returns a payload configured to support the     multiplication of two LinearOperators    
* [0.x.47]*
  [2.x.131]  LinearOperator
*  A function that encapsulates generic  [2.x.132]  objects that act on a compatible Vector type into a LinearOperator. The LinearOperator object that is created stores a reference to the matrix object. Thus,  [2.x.133]  must remain a valid reference for the whole lifetime of the LinearOperator object.
*  All changes made on  [2.x.134]  after the creation of the LinearOperator object are reflected by the operator object. For example, it is a valid procedure to first create a LinearOperator and resize, reassemble the matrix later.
*  The Matrix class in question must provide the following minimal interface:
* 

* 
* [1.x.6]
* 
*  The following (optional) interface is used if available:
* 

* 
* [1.x.7]
* 
*  If the Matrix does not provide  [2.x.135]  and  [2.x.136] , they are implemented in terms of  [2.x.137]  (requiring intermediate storage).
* 

* 

* 
*  [2.x.138] 

* 
* [0.x.48]*
  [2.x.139]  LinearOperator
*  Variant of above function that takes an operator object  [2.x.140]  operator_exemplar as an additional reference. This object is used to populate the reinit_domain_vector and reinit_range_vector function objects. The reference  [2.x.141]  is used to construct vmult, Tvmult, etc.
*  This variant can, for example, be used to encapsulate preconditioners (that typically do not expose any information about the underlying matrix).
* 

* 

* 
*  [2.x.142] 

* 
* [0.x.49]*
  [2.x.143]  LinearOperator
*  Variant of above function that takes a LinearOperator  [2.x.144]  operator_exemplar as an additional reference. The reinit_domain_vector and reinit_range_vector function are copied from the  [2.x.145]  object.
*  The reference  [2.x.146]  is used to construct vmult, Tvmult, etc.
*  This variant can, for example, be used to encapsulate preconditioners (that typically do not expose any information about the underlying matrix).
* 

* 

* 
*  [2.x.147] 

* 
* [0.x.50]

include/deal.II-translator/lac/linear_operator_tools_0.txt
[0.x.0]

include/deal.II-translator/lac/matrix_block_0.txt
[0.x.0]*
 A wrapper around a matrix object, storing the coordinates in a block matrix as well.
*  This class is an alternative to BlockMatrixBase, if you only want to generate a single block of the system, not the whole system. Using the add() functions of this class, it is possible to use the standard assembling functions used for block matrices, but only enter in one of the blocks and still avoiding the index computations involved.  The reason for this class is, that we may need a different number of matrices for different blocks in a block system. For example, a preconditioner for the Oseen system can be built as a block system, where the pressure block is of the form [1.x.0]<sup>-1</sup>[1.x.1]<sup>-1</sup> with [1.x.2] the pressure mass matrix, [1.x.3] the pressure Laplacian and [1.x.4] the advection diffusion operator applied to the pressure space. Since only a single matrix is needed for the other blocks, using BlockSparseMatrix or similar would be a waste of memory.
*  While the add() functions make a MatrixBlock appear like a block matrix for assembling, the functions vmult(), Tvmult(), vmult_add(), and Tvmult_add() make it behave like a MatrixType, when it comes to applying it to a vector. This behavior allows us to store MatrixBlock objects in vectors, for instance in MGLevelObject without extracting the #matrix first.
*  MatrixBlock comes handy when using BlockMatrixArray. Once the MatrixBlock has been properly initialized and filled, it can be used in the simplest case as:

* 
* [1.x.5]
* 
*  Here, we have not gained very much, except that we do not need to set up empty blocks in the block system.
* 

* 
*  [2.x.0]  This class expects, that the row and column BlockIndices objects for the system are equal. If they are not, some functions will throw ExcNotImplemented.
*   [2.x.1]  Example for the product preconditioner of the pressure Schur complement.
* 

* 
*  [2.x.2] 

* 
*  [2.x.3] 
*   [2.x.4]   [2.x.5]  "Block (linear algebra)"

* 
* [0.x.1]*
   Declare type for container size.  
* [0.x.2]*
   Declare a type for matrix entries.  
* [0.x.3]*
   Constructor rendering an uninitialized object.  
* [0.x.4]*
   Copy constructor.  
* [0.x.5]*
   Assignment operator.  
* [0.x.6]*
   Constructor setting block coordinates, but not initializing the matrix.  
* [0.x.7]*
   Reinitialize the matrix for a new BlockSparsityPattern. This adjusts the   #matrix as well as the #row_indices and #column_indices.    
*  [2.x.6]  The row and column block structure of the sparsity pattern must be   equal.  
* [0.x.8]*
   Add <tt>value</tt> to the element ([1.x.6]). Throws an error if the   entry does not exist or if it is in a different block.  
* [0.x.9]*
   Add all elements in a FullMatrix into sparse matrix locations given by   <tt>indices</tt>. This function assumes a quadratic sparse matrix and a   quadratic full_matrix.  The global locations are translated into   locations in this block and ExcBlockIndexMismatch is thrown, if the   global index does not point into the block referred to by #row and   #column.      [2.x.7]  <tt>elide_zero_values</tt> is currently ignored.     The optional parameter <tt>elide_zero_values</tt> can be used to specify   whether zero values should be added anyway or these should be filtered   away and only non-zero data is added. The default value is <tt>true</tt>,   i.e., zero values won't be added into the matrix.  
* [0.x.10]*
   Add all elements in a FullMatrix into global locations given by   <tt>row_indices</tt> and <tt>col_indices</tt>, respectively. The global   locations are translated into locations in this block and   ExcBlockIndexMismatch is thrown, if the global index does not point into   the block referred to by #row and #column.      [2.x.8]  <tt>elide_zero_values</tt> is currently ignored.     The optional parameter <tt>elide_zero_values</tt> can be used to specify   whether zero values should be added anyway or these should be filtered   away and only non-zero data is added. The default value is <tt>true</tt>,   i.e., zero values won't be added into the matrix.  
* [0.x.11]*
   Set several elements in the specified row of the matrix with column   indices as given by <tt>col_indices</tt> to the respective value. This is   the function doing the actual work for the ones adding full matrices. The   global locations <tt>row_index</tt> and <tt>col_indices</tt> are   translated into locations in this block and ExcBlockIndexMismatch is   thrown, if the global index does not point into the block referred to by   #row and #column.      [2.x.9]  <tt>elide_zero_values</tt> is currently ignored.     The optional parameter <tt>elide_zero_values</tt> can be used to specify   whether zero values should be added anyway or these should be filtered   away and only non-zero data is added. The default value is <tt>true</tt>,   i.e., zero values won't be added into the matrix.  
* [0.x.12]*
   Add an array of values given by <tt>values</tt> in the given global   matrix row at columns specified by col_indices in the sparse matrix.     The optional parameter <tt>elide_zero_values</tt> can be used to specify   whether zero values should be added anyway or these should be filtered   away and only non-zero data is added. The default value is <tt>true</tt>,   i.e., zero values won't be added into the matrix.  
* [0.x.13]*
   Matrix-vector-multiplication, forwarding to the same function in   MatrixType. No index computations are done, thus, the vectors need to   have sizes matching #matrix.  
* [0.x.14]*
   Matrix-vector-multiplication, forwarding to the same function in   MatrixType. No index computations are done, thus, the vectors need to   have sizes matching #matrix.  
* [0.x.15]*
   Matrix-vector-multiplication, forwarding to the same function in   MatrixType. No index computations are done, thus, the vectors need to   have sizes matching #matrix.  
* [0.x.16]*
   Matrix-vector-multiplication, forwarding to the same function in   MatrixType. No index computations are done, thus, the vectors need to   have sizes matching #matrix.  
* [0.x.17]*
   The memory used by this object.  
* [0.x.18]*
   The block number computed from an index by using BlockIndices does not   match the block coordinates stored in this object.  
* [0.x.19]*
   Row coordinate.  This is the position of the data member matrix on the   global matrix.  
* [0.x.20]*
   Column coordinate.  This is the position of the data member matrix on the   global matrix.  
* [0.x.21]*
   The matrix itself  
* [0.x.22]*
   The row BlockIndices of the whole system. Using row(), this allows us to   find the index of the first row degree of freedom for this block.  
* [0.x.23]*
   The column BlockIndices of the whole system. Using column(), this allows   us to find the index of the first column degree of freedom for this   block.  
* [0.x.24]*
 A vector of MatrixBlock, which is implemented using shared pointers, in order to allow for copying and rearranging. Each matrix block can be identified by name.
*   [2.x.10]  MatrixBlock

* 
*  [2.x.11] 

* 
* [0.x.25]*
   Declare type for container size.  
* [0.x.26]*
   The type of object stored.  
* [0.x.27]*
   The pointer type used for storing the objects. We use a shard pointer,   such that they get deleted automatically when not used anymore.  
* [0.x.28]*
   Add a new matrix block at the position <tt>(row,column)</tt> in the block   system.  
* [0.x.29]*
   For matrices using a SparsityPattern, this function reinitializes each   matrix in the vector with the correct pattern from the block system.  
* [0.x.30]*
   Clear the object.     Since often only clearing of the individual matrices is desired, but not   removing the blocks themselves, there is an optional argument. If the   argument is missing or  [2.x.12]  all matrices will be empty, but the size   of this object and the block positions will not change. If  [2.x.13]    really_clean is  [2.x.14]  then the object will contain no blocks at the   end.  
* [0.x.31]*
   The memory used by this object.  
* [0.x.32]*
   Access a constant reference to the block at position [1.x.7].  
* [0.x.33]*
   Access a reference to the block at position [1.x.8].  
* [0.x.34]*
   Access the matrix at position [1.x.9] for read and write access.  
* [0.x.35]*
   import functions from private base class  
* [0.x.36]*
 A vector of MGLevelObject<MatrixBlock>, which is implemented using shared pointers, in order to allow for copying and rearranging. Each matrix block can be identified by name.
*   [2.x.15]  MatrixBlock

* 
*  [2.x.16] 

* 
* [0.x.37]*
   Declare type for container size.  
* [0.x.38]*
   The type of object stored.  
* [0.x.39]*
   Constructor, determining which matrices should be stored.     If <tt>edge_matrices</tt> is true, then objects for edge matrices for   discretizations with degrees of freedom on faces are allocated.     If <tt>edge_flux_matrices</tt> is true, then objects for DG fluxes on the   refinement edge are allocated.  
* [0.x.40]*
   The number of blocks.  
* [0.x.41]*
   Add a new matrix block at the position <tt>(row,column)</tt> in the block   system. The third argument allows to give the matrix a name for later   identification.  
* [0.x.42]*
   For matrices using a SparsityPattern, this function reinitializes each   matrix in the vector with the correct pattern from the block system.     This function reinitializes the level matrices.  
* [0.x.43]*
   For matrices using a SparsityPattern, this function reinitializes each   matrix in the vector with the correct pattern from the block system.     This function reinitializes the matrices for degrees of freedom on the   refinement edge.  
* [0.x.44]*
   For matrices using a SparsityPattern, this function reinitializes each   matrix in the vector with the correct pattern from the block system.     This function reinitializes the flux matrices over the refinement edge.  
* [0.x.45]*
   Clear the object.     Since often only clearing of the individual matrices is desired, but not   removing the blocks themselves, there is an optional argument. If the   argument is missing or  [2.x.17]  all matrices will be empty, but the size   of this object and the block positions will not change. If  [2.x.18]    really_clean is  [2.x.19]  then the object will contain no blocks at the   end.  
* [0.x.46]*
   Access a constant reference to the matrix block at position [1.x.10].  
* [0.x.47]*
   Access a reference to the matrix block at position [1.x.11].  
* [0.x.48]*
   Access a constant reference to the edge matrix block at position   [1.x.12].  
* [0.x.49]*
   Access a reference to the edge matrix block at position [1.x.13].  
* [0.x.50]*
   Access a constant reference to the edge matrix block at position   [1.x.14].  
* [0.x.51]*
   Access a reference to the edge matrix block at position [1.x.15].  
* [0.x.52]*
   Access a constant reference to the  edge flux matrix block at position   [1.x.16].  
* [0.x.53]*
   Access a reference to the  edge flux matrix block at position [1.x.17].  
* [0.x.54]*
   Access a constant reference to the  edge flux matrix block at position   [1.x.18].  
* [0.x.55]*
   Access a reference to the edge flux matrix block at position [1.x.19].  
* [0.x.56]*
   The memory used by this object.  
* [0.x.57]

include/deal.II-translator/lac/matrix_iterator_0.txt
[0.x.0]*
 Iterator for constant and non-constant matrices.
*  This iterator is abstracted from the actual matrix type and can be used for any matrix having the required ACCESSOR type.

* 
* [0.x.1]*
   Declare type for container size.  
* [0.x.2]*
   Typedef for the matrix type (including constness) we are to operate on.  
* [0.x.3]*
   Constructor. Create an iterator into the matrix <tt>matrix</tt> for the   given <tt>row</tt> and the <tt>index</tt> within it.  
* [0.x.4]*
   Copy from another matrix iterator. Mostly implemented to allow   initialization of a constant iterator from a non constant, this function   only requires that a conversion from the other iterator's accessor to   this accessor object is possible.  
* [0.x.5]*
   Prefix increment.  
* [0.x.6]*
   Postfix increment.  
* [0.x.7]*
   Dereferencing operator.  
* [0.x.8]*
   Dereferencing operator.  
* [0.x.9]*
   Comparison. True, if both accessors are equal.  
* [0.x.10]*
   Inverse of <tt>==</tt>.  
* [0.x.11]*
   Comparison operator. Result is true if either the first row number is   smaller or if the row numbers are equal and the first index is smaller.     This function is only valid if both iterators point into the same matrix.  
* [0.x.12]*
   Comparison operator. Works in the same way as above operator, just the   other way round.  
* [0.x.13]*
   Store an object of the accessor class.  
* [0.x.14]

include/deal.II-translator/lac/matrix_out_0.txt
[0.x.0]*
 Output a matrix in graphical form using the generic format independent output routines of the base class. The matrix is converted into a list of patches on a 2d domain where the height is given by the elements of the matrix. The functions of the base class can then write this "mountain representation" of the matrix in a variety of graphical output formats. The coordinates of the matrix output are that the columns run with increasing x-axis, as usual, starting from zero, while the rows run into the negative y-axis, also starting from zero. Note that due to some internal restrictions, this class can only output one matrix at a time, i.e. it can not take advantage of the multiple dataset capabilities of the base class.
*  A typical usage of this class would be as follows:

* 
* [1.x.0]
*  Of course, you can as well choose a different graphical output format. Also, this class supports any matrix, not only of type FullMatrix, as long as it satisfies a number of requirements, stated with the member functions of this class.
*  The generation of patches through the build_patches() function can be modified by giving it an object holding certain flags. See the documentation of the members of the Options class for a description of these flags.
* 

* 

* 
*  [2.x.0] 

* 
* [0.x.1]*
   Declare type for container size.  
* [0.x.2]*
   Class holding various variables which are used to modify the output of   the MatrixOut class.  
* [0.x.3]*
     If  [2.x.1]  only show the absolute values of the matrix entries, rather     than their true values including the sign. Default value is  [2.x.2]     
* [0.x.4]*
     If larger than one, do not show each element of the matrix, but rather     an average over a number of entries. The number of output patches is     accordingly smaller. This flag determines how large each shown block     shall be (in rows/columns). For example, if it is two, then always four     entries are collated into one.         Default value is one.    
* [0.x.5]*
     If true, plot discontinuous patches, one for each entry.    
* [0.x.6]*
     Default constructor. Set all elements of this structure to their     default values.    
* [0.x.7]*
   Destructor. Declared in order to make it virtual.  
* [0.x.8]*
   Generate a list of patches from the given matrix and use the given string   as the name of the data set upon writing to a file. Once patches have   been built, you can use the functions of the base class to write the data   into a files, using one of the supported output formats.     You may give a structure holding various options. See the description of   the fields of this structure for more information.     Note that this function requires that we can extract elements of the   matrix, which is done using the get_element() function declared in an   internal namespace. By adding specializations, you can extend this class   to other matrix classes which are not presently supported. Furthermore,   we need to be able to extract the size of the matrix, for which we assume   that the matrix type offers member functions <tt>m()</tt> and   <tt>n()</tt>, which return the number of rows and columns, respectively.  
* [0.x.9]*
   Abbreviate the somewhat lengthy name for the  [2.x.3]    class.  
* [0.x.10]*
   This is a list of patches that is created each time build_patches() is   called. These patches are used in the output routines of the base   classes.  
* [0.x.11]*
   Name of the matrix to be written.  
* [0.x.12]*
   %Function by which the base class's functions get to know what patches   they shall write to a file.  
* [0.x.13]*
   Virtual function through which the names of data sets are obtained by the   output functions of the base class.  
* [0.x.14]*
   Get the value of the matrix at gridpoint <tt>(i,j)</tt>. Depending on the   given flags, this can mean different things, for example if only absolute   values shall be shown then the absolute value of the matrix entry is   taken. If the block size is larger than one, then an average of several   matrix entries is taken.  
* [0.x.15]*
     Return the element with given indices of a sparse matrix.    
* [0.x.16]*
     Return the element with given indices of a block sparse matrix.    
* [0.x.17]*
     Return the element with given indices of a Trilinos sparse matrix.    
* [0.x.18]*
     Return the element with given indices of a Trilinos block sparse     matrix.    
* [0.x.19]*
     Return the element with given indices from any matrix type for which     no specialization of this function was declared above. This will call     <tt>operator()</tt> on the matrix.    
* [0.x.20]

include/deal.II-translator/lac/packaged_operation_0.txt
[0.x.0]*
 A class to store a computation.
*  The PackagedOperation class allows lazy evaluation of expressions involving vectors and linear operators. This is done by storing the computational expression and only performing the computation when either the object is implicitly converted to a vector object, or  [2.x.0]  (or  [2.x.1] ) is invoked by hand. This avoids unnecessary temporary storage of intermediate results.
*  The class essentially consists of  [2.x.2]  objects that store the knowledge of how to generate the result of a computation and store it in a vector:

* 
* [1.x.0]
* 
*  Similar to the LinearOperator class it also has knowledge about how to initialize a vector of the  [2.x.3]  space:

* 
* [1.x.1]
* 
*  As an example consider the addition of multiple vectors

* 
* [1.x.2]
*  or the computation of a residual  [2.x.4] :

* 
* [1.x.3]
*  The expression  [2.x.5]  is of type  [2.x.6] . It stores references to  [2.x.7]  and defers the actual computation until  [2.x.8]  are explicitly invoked,

* 
* [1.x.4]
*  or until the  [2.x.9]  object is implicitly converted:

* 
* [1.x.5]
* 
* 

* 
*  [2.x.10]  The  [2.x.11]  tutorial program has a detailed usage example of the LinearOperator class.
* 

* 

* 
*  [2.x.12] 

* 
* [0.x.1]*
   Create an empty PackagedOperation object. All  [2.x.13]    member objects are initialized with default variants that throw an   exception upon invocation.  
* [0.x.2]*
   Default copy constructor.  
* [0.x.3]*
   Constructor that creates a PackagedOperation object from a reference   vector  [2.x.14]  The PackagedOperation returns  [2.x.15]      The PackagedOperation object that is created stores a reference to  [2.x.16]    Thus, the vector must remain a valid reference for the whole lifetime of   the PackagedOperation object. All changes made on  [2.x.17]  after the creation   of the PackagedOperation object are reflected by the operator object.  
* [0.x.4]*
   Default copy assignment operator.  
* [0.x.5]*
   Copy assignment operator that creates a PackagedOperation object from a   reference vector  [2.x.18]  The PackagedOperation returns  [2.x.19]      The PackagedOperation object that is created stores a reference to  [2.x.20]    Thus, the vector must remain a valid reference for the whole lifetime of   the PackagedOperation object. All changes made on  [2.x.21]  after the creation   of the PackagedOperation object are reflected by the operator object.  
* [0.x.6]*
   Convert a PackagedOperation to its result.     This conversion operator creates a vector of the Range space and calls    [2.x.22]  on it.  
* [0.x.7]*
    [2.x.23]  In-place vector space operations  
* [0.x.8]*
   Addition with a PackagedOperation  [2.x.24]  with the same  [2.x.25]   
* [0.x.9]*
   Subtraction with a PackagedOperation  [2.x.26]  with the same  [2.x.27]    Range.  
* [0.x.10]*
   Add a constant  [2.x.28]  (of the  [2.x.29]  space) to the result of a   PackagedOperation.  
* [0.x.11]*
   Subtract a constant  [2.x.30]  (of the  [2.x.31]  space) from the result of   a PackagedOperation.  
* [0.x.12]*
   Scalar multiplication of the PackagedOperation with a  [2.x.32]   
* [0.x.13]*
   Store the result of the PackagedOperation in a vector v of the  [2.x.33]    space.  
* [0.x.14]*
   Add the result of the PackagedOperation to a vector v of the  [2.x.34]    space.  
* [0.x.15]*
   Initializes a vector v of the Range space to be directly usable as the   destination parameter in an application of apply, or apply_add. Similar   to the reinit functions of the vector classes, the boolean determines   whether a fast initialization is done, i.e., if it is set to false the   content of the vector is set to 0.  
* [0.x.16]*
  [2.x.35]  Vector space operations

* 
* [0.x.17]*
  [2.x.36]  PackagedOperation
*  Addition of two PackagedOperation objects  [2.x.37]  and  [2.x.38]  given by vector space addition of the corresponding results.
* 

* 
*  [2.x.39] 

* 
* [0.x.18]*
  [2.x.40]  PackagedOperation
*  Subtraction of two PackagedOperation objects  [2.x.41]  and  [2.x.42]  second_comp given by vector space addition of the corresponding results.
* 

* 
*  [2.x.43] 

* 
* [0.x.19]*
  [2.x.44]  PackagedOperation
*  Scalar multiplication of a PackagedOperation objects  [2.x.45]  with a scalar  [2.x.46]  given by a scaling PackagedOperation result with  [2.x.47] 
* 

* 
*  [2.x.48] 

* 
* [0.x.20]*
  [2.x.49]  PackagedOperation
*  Scalar multiplication of a PackagedOperation objects  [2.x.50]  with a scalar  [2.x.51]  given by a scaling PackagedOperation result with  [2.x.52] 
* 

* 
*  [2.x.53] 

* 
* [0.x.21]*
  [2.x.54]  PackagedOperation
*  Add a constant  [2.x.55]  (of the  [2.x.56]  space) to the result of a PackagedOperation.
* 

* 
*  [2.x.57] 

* 
* [0.x.22]*
  [2.x.58]  PackagedOperation
*  Add a constant  [2.x.59]  (of the  [2.x.60]  space) to the result of a PackagedOperation.
* 

* 
*  [2.x.61] 

* 
* [0.x.23]*
  [2.x.62]  PackagedOperation
*  Subtract a constant  [2.x.63]  (of the  [2.x.64]  space) from the result of a PackagedOperation.
* 

* 
*  [2.x.65] 

* 
* [0.x.24]*
  [2.x.66]  PackagedOperation
*  Subtract a computational result from a constant  [2.x.67]  (of the  [2.x.68]  space). The result is a PackagedOperation object that applies this computation.
* 

* 
*  [2.x.69] 

* 
* [0.x.25]*
  [2.x.70]  Creation of a PackagedOperation object

* 
* [0.x.26]*
  [2.x.71]  PackagedOperation
*  Create a PackagedOperation object that stores the addition of two vectors.
*  The PackagedOperation object that is created stores a reference to  [2.x.72]  and  [2.x.73]  Thus, the vectors must remain valid references for the whole lifetime of the PackagedOperation object. All changes made on  [2.x.74]  or  [2.x.75]  after the creation of the PackagedOperation object are reflected by the operator object.
* 

* 
*  [2.x.76] 

* 
* [0.x.27]*
  [2.x.77]  PackagedOperation
*  Create a PackagedOperation object that stores the subtraction of two vectors.
*  The PackagedOperation object that is created stores a reference to  [2.x.78]  and  [2.x.79]  Thus, the vectors must remain valid references for the whole lifetime of the PackagedOperation object. All changes made on  [2.x.80]  or  [2.x.81]  after the creation of the PackagedOperation object are reflected by the operator object.
* 

* 
*  [2.x.82] 

* 
* [0.x.28]*
  [2.x.83]  PackagedOperation
*  Create a PackagedOperation object that stores the scaling of a vector with a  [2.x.84] 
*  The PackagedOperation object that is created stores a reference to  [2.x.85]  Thus, the vectors must remain valid references for the whole lifetime of the PackagedOperation object. All changes made on  [2.x.86]  or  [2.x.87]  after the creation of the PackagedOperation object are reflected by the operator object.
* 

* 
*  [2.x.88] 

* 
* [0.x.29]*
  [2.x.89]  PackagedOperation
*  Create a PackagedOperation object that stores the scaling of a vector with a  [2.x.90] 
*  The PackagedOperation object that is created stores a reference to  [2.x.91]  Thus, the vectors must remain valid references for the whole lifetime of the PackagedOperation object. All changes made on  [2.x.92]  or  [2.x.93]  after the creation of the PackagedOperation object are reflected by the operator object.
* 

* 
*  [2.x.94] 

* 
* [0.x.30]*
  [2.x.95]  PackagedOperation
*  Create a PackagedOperation object from a LinearOperator and a reference to a vector  [2.x.96]  of the Domain space. The object stores the PackagedOperation  [2.x.97]  (in matrix notation).  [2.x.98]  ( [2.x.99]  ( [2.x.100] ).
*  The PackagedOperation object that is created stores a reference to  [2.x.101]  Thus, the vector must remain a valid reference for the whole lifetime of the PackagedOperation object. All changes made on  [2.x.102]  after the creation of the PackagedOperation object are reflected by the operator object.
* 

* 
*  [2.x.103] 

* 
* [0.x.31]*
  [2.x.104]  PackagedOperation
*  Create a PackagedOperation object from a LinearOperator and a reference to a vector  [2.x.105]  of the Range space. The object stores the PackagedOperation  [2.x.106]  (in matrix notation).  [2.x.107]  ( [2.x.108]  ( [2.x.109] ).
*  The PackagedOperation object that is created stores a reference to  [2.x.110]  Thus, the vector must remain a valid reference for the whole lifetime of the PackagedOperation object. All changes made on  [2.x.111]  after the creation of the PackagedOperation object are reflected by the operator object.
* 

* 
*  [2.x.112] 

* 
* [0.x.32]*
  [2.x.113]  PackagedOperation
*  Composition of a PackagedOperation object with a LinearOperator. The object stores the computation  [2.x.114]  (in matrix notation).
* 

* 
*  [2.x.115] 

* 
* [0.x.33]*
  [2.x.116]  PackagedOperation
*  Composition of a PackagedOperation object with a LinearOperator. The object stores the computation  [2.x.117]  (in matrix notation).
* 

* 
*  [2.x.118] 

* 
* [0.x.34]

include/deal.II-translator/lac/parallel_block_vector_0.txt
[0.x.0]!  [2.x.0]  Vectors     [2.x.1]     
* [0.x.1]*
     An implementation of block vectors based on distributed deal.II     vectors. While the base class provides for most of the interface, this     class handles the actual allocation of vectors and provides functions     that are specific to the underlying vector type.        
*  [2.x.2]  Instantiations for this template are provided for <tt> [2.x.3]      and  [2.x.4]  others can be generated in application programs     (see the section on      [2.x.5]      in the manual).          [2.x.6]       [2.x.7]  "Block (linear algebra)"          [2.x.8]  Use  [2.x.9]  instead.    
* [0.x.2]

include/deal.II-translator/lac/parpack_solver_0.txt
[0.x.0]*
 Interface for using PARPACK. PARPACK is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems. Here we interface to the routines  [2.x.0] ,  [2.x.1]  of PARPACK.  The package is designed to compute a few eigenvalues and corresponding eigenvectors of a general n by n matrix A. It is most appropriate for large sparse matrices A.
*  In this class we make use of the method applied to the generalized eigenspectrum problem  [2.x.2] , for  [2.x.3] ; where  [2.x.4]  is a system matrix,  [2.x.5]  is a mass matrix, and  [2.x.6]  are a set of eigenvalues and eigenvectors respectively.
*  The ArpackSolver can be used in application codes in the following way:

* 
* [1.x.0]
*  for the generalized eigenvalue problem  [2.x.7] , where the variable  [2.x.8]  tells PARPACK the number of eigenvector/eigenvalue pairs to solve for. Here,  [2.x.9]  is a vector that will contain the eigenvalues computed,  [2.x.10]  a vector of objects of type  [2.x.11]  that will contain the eigenvectors computed.
*  Currently, only three modes of (P)Arpack are implemented. In mode 3 (default),  [2.x.12]  is an inverse operation for the matrix <code>A
* 
*  - sigma B</code>, where  [2.x.13]  is a shift value, set to zero by default. Whereas in mode 2,  [2.x.14]  is an inverse of  [2.x.15] . Finally, mode 1 corresponds to standard eigenvalue problem without spectral transformation  [2.x.16] . The mode can be specified via AdditionalData object. Note that for shift-and-invert (mode=3), the sought eigenpairs are those after the spectral transformation is applied.
*  The  [2.x.17]  can be specified by using a LinearOperator:

* 
* [1.x.1]
* 
*  The class is intended to be used with MPI and can work on arbitrary vector and matrix distributed classes.  Both symmetric and non-symmetric  [2.x.18]  are supported.
*  For further information on how the PARPACK routines  [2.x.19] ,  [2.x.20]  work and also how to set the parameters appropriately please take a look into the PARPACK manual.

* 
* [0.x.1]*
   Declare the type for container size.  
* [0.x.2]*
   An enum that lists the possible choices for which eigenvalues to compute   in the solve() function. Note, that this corresponds to the problem after   shift-and-invert (the only currently supported spectral transformation)   is applied.     A particular choice is limited based on symmetric or non-symmetric matrix    [2.x.21]  considered.  
* [0.x.3]*
     The algebraically largest eigenvalues.    
* [0.x.4]*
     The algebraically smallest eigenvalues.    
* [0.x.5]*
     The eigenvalue with the largest magnitudes.    
* [0.x.6]*
     The eigenvalue with the smallest magnitudes.    
* [0.x.7]*
     The eigenvalues with the largest real parts.    
* [0.x.8]*
     The eigenvalues with the smallest real parts.    
* [0.x.9]*
     The eigenvalues with the largest imaginary parts.    
* [0.x.10]*
     The eigenvalues with the smallest imaginary parts.    
* [0.x.11]*
     Compute half of the eigenvalues from the high end of the spectrum and     the other half from the low end. If the number of requested     eigenvectors is odd, then the extra eigenvector comes from the high end     of the spectrum.    
* [0.x.12]*
   Standardized data struct to pipe additional data to the solver, should it   be needed.  
* [0.x.13]*
   Access to the object that controls convergence.  
* [0.x.14]*
   Constructor.  
* [0.x.15]*
   Initialize internal variables.  
* [0.x.16]*
   Initialize internal variables when working with BlockVectors.    [2.x.22]  is used to set the dimension of the problem,   whereas  [2.x.23]  is used for calling the reinit of the deal.II   blockvectors used.  
* [0.x.17]*
   Initialize internal variables from the input  [2.x.24]   
* [0.x.18]*
   Set initial vector for building Krylov space.  
* [0.x.19]*
   Set shift  [2.x.25]  for shift-and-invert spectral transformation.     If this function is not called, the shift is assumed to be zero.    
*  [2.x.26]  only relevant for  [2.x.27]  (see the general documentation of this   class for a definition of what the different modes are).  
* [0.x.20]*
   Solve the generalized eigensprectrum problem  [2.x.28]  by calling   the  [2.x.29]  functions of   PARPACK.     In  [2.x.30] ,  [2.x.31]  should correspond to  [2.x.32] ,   whereas in  [2.x.33]  it should represent  [2.x.34] . For    [2.x.35]  both  [2.x.36]  and  [2.x.37]  are ignored.  
* [0.x.21]*
   Same as above but takes eigenvectors as pointers.  
* [0.x.22]*
   Return the memory consumption of this class in bytes.  
* [0.x.23]*
   Reference to the object that controls convergence of the iterative   solver.  
* [0.x.24]*
   Store a copy of the flags for this particular solver.  
* [0.x.25]*
   C++ MPI communicator.  
* [0.x.26]*
   Fortran MPI communicator.  
* [0.x.27]*
   Length of the work array workl.  
* [0.x.28]*
   Double precision  work array of length lworkl  
* [0.x.29]*
   Double precision  work array of length 3*N  
* [0.x.30]*
   Number of local degrees of freedom.  
* [0.x.31]*
   Number of Arnoldi basis vectors specified in additional_data  
* [0.x.32]*
   The leading dimension of the array v  
* [0.x.33]*
   Double precision vector of size ldv by NCV.  Will contains the final set   of Arnoldi basis vectors.  
* [0.x.34]*
   An auxiliary flag which is set to true when initial vector is provided.  
* [0.x.35]*
   The initial residual vector, possibly from a previous run.  On output, it   contains the final residual vector.  
* [0.x.36]*
   The leading dimension of the array Z equal to nloc.  
* [0.x.37]*
   A vector of minimum size of nloc by NEV+1.  Z contains the B-orthonormal   Ritz vectors of the eigensystem A*z = lambda*B*z corresponding to the   Ritz value approximations.  
* [0.x.38]*
   The size of the workev array.  
* [0.x.39]*
   Double precision  work array of dimension 3* NCV.  
* [0.x.40]*
   A vector of dimension NCV.  
* [0.x.41]*
   Temporary vectors used between Arpack and deal.II  
* [0.x.42]*
   Indices of local degrees of freedom.  
* [0.x.43]*
   Real part of the shift  
* [0.x.44]*
   Imaginary part of the shift  
* [0.x.45]*
   Initialize internal variables which depend on    [2.x.38]      This function is called inside the reinit() functions  
* [0.x.46]*
   PArpackExcInfoPdnaupds.  
* [0.x.47]

include/deal.II-translator/lac/petsc_block_sparse_matrix_0.txt
[0.x.0]!  [2.x.0]  PETScWrappers     [2.x.1]     
* [0.x.1]*
     Blocked sparse matrix based on the  [2.x.2]      class. This class implements the functions that are specific to the     PETSc SparseMatrix base objects for a blocked sparse matrix, and leaves     the actual work relaying most of the calls to the individual blocks to     the functions implemented in the base class. See there also for a     description of when this class is useful.         In contrast to the deal.II-type SparseMatrix class, the PETSc matrices     do not have external objects for the sparsity patterns. Thus, one does     not determine the size of the individual blocks of a block matrix of     this type by attaching a block sparsity pattern, but by calling     reinit() to set the number of blocks and then by setting the size of     each block separately. In order to fix the data structures of the block     matrix, it is then necessary to let it know that we have changed the     sizes of the underlying matrices. For this, one has to call the     collect_sizes() function, for much the same reason as is documented     with the BlockSparsityPattern class.        
*  [2.x.3]       [2.x.4]  "Block (linear algebra)"    
* [0.x.2]*
       Typedef the base class for simpler access to its own alias.      
* [0.x.3]*
       Typedef the type of the underlying matrix.      
* [0.x.4]*
       Import the alias from the base class.      
* [0.x.5]*
       Constructor; initializes the matrix to be empty, without any       structure, i.e.  the matrix is not usable at all. This constructor is       therefore only useful for matrices which are members of a class. All       other matrices should be created at a point in the data flow where       all necessary information is available.             You have to initialize the matrix before usage with       reinit(BlockSparsityPattern). The number of blocks per row and column       are then determined by that function.      
* [0.x.6]*
       Destructor.      
* [0.x.7]*
       Pseudo copy operator only copying empty objects. The sizes of the       block matrices need to be the same.      
* [0.x.8]*
       This operator assigns a scalar to a matrix. Since this does usually       not make much sense (should we set all matrix entries to this value?       Only the nonzero entries of the sparsity pattern?), this operation is       only allowed if the actual value to be assigned is zero. This       operator only exists to allow for the obvious notation       <tt>matrix=0</tt>, which sets all elements of the matrix to zero, but       keep the sparsity pattern previously used.      
* [0.x.9]*
       Resize the matrix, by setting the number of block rows and columns.       This deletes all blocks and replaces them with uninitialized ones,       i.e.  ones for which also the sizes are not yet set. You have to do       that by calling the  [2.x.5]  functions of the blocks themselves. Do       not forget to call collect_sizes() after that on this object.             The reason that you have to set sizes of the blocks yourself is that       the sizes may be varying, the maximum number of elements per row may       be varying, etc. It is simpler not to reproduce the interface of the       SparsityPattern class here but rather let the user call whatever       function they desire.      
* [0.x.10]*
       Efficiently reinit the block matrix for a parallel computation. Only       the BlockSparsityPattern of the Simple type can efficiently store       large sparsity patterns in parallel, so this is the only supported       argument. The IndexSets describe the locally owned range of DoFs for       each block. Note that the IndexSets needs to be ascending and 1:1.       For a symmetric structure hand in the same vector for the first two       arguments.      
* [0.x.11]*
       Same as above but for a symmetric structure only.      
* [0.x.12]*
       Matrix-vector multiplication: let  [2.x.6]  with  [2.x.7]  being this       matrix.      
* [0.x.13]*
       Matrix-vector multiplication. Just like the previous function, but       only applicable if the matrix has only one block column.      
* [0.x.14]*
       Matrix-vector multiplication. Just like the previous function, but       only applicable if the matrix has only one block row.      
* [0.x.15]*
       Matrix-vector multiplication. Just like the previous function, but       only applicable if the matrix has only one block.      
* [0.x.16]*
       Matrix-vector multiplication: let  [2.x.8]  with  [2.x.9]  being this       matrix. This function does the same as vmult() but takes the       transposed matrix.      
* [0.x.17]*
       Matrix-vector multiplication. Just like the previous function, but       only applicable if the matrix has only one block row.      
* [0.x.18]*
       Matrix-vector multiplication. Just like the previous function, but       only applicable if the matrix has only one block column.      
* [0.x.19]*
       Matrix-vector multiplication. Just like the previous function, but       only applicable if the matrix has only one block.      
* [0.x.20]*
       This function collects the sizes of the sub-objects and stores them       in internal arrays, in order to be able to relay global indices into       the matrix to indices into the subobjects. Youmust* call this       function each time after you have changed the size of the sub-       objects.      
* [0.x.21]*
       Return the partitioning of the domain space of this matrix, i.e., the       partitioning of the vectors this matrix has to be multiplied with.      
* [0.x.22]*
       Return the partitioning of the range space of this matrix, i.e., the       partitioning of the vectors that are result from matrix-vector       products.      
* [0.x.23]*
       Return a reference to the MPI communicator object in use with this       matrix.      
* [0.x.24]*
       Make the clear() function in the base class visible, though it is       protected.      
* [0.x.25]

include/deal.II-translator/lac/petsc_block_vector_0.txt
[0.x.0]!  [2.x.0]  PETScWrappers     [2.x.1]     
* [0.x.1]*
     An implementation of block vectors based on the parallel vector class     implemented in PETScWrappers. While the base class provides for most of     the interface, this class handles the actual allocation of vectors and     provides functions that are specific to the underlying vector type.         The model of distribution of data is such that each of the blocks is     distributed across all MPI processes named in the MPI communicator.     I.e. we don't just distribute the whole vector, but each component. In     the constructors and reinit() functions, one therefore not only has to     specify the sizes of the individual blocks, but also the number of     elements of each of these blocks to be stored on the local process.        
*  [2.x.2]       [2.x.3]  "Block (linear algebra)"    
* [0.x.2]*
       Typedef the base class for simpler access to its own alias.      
* [0.x.3]*
       Typedef the type of the underlying vector.      
* [0.x.4]*
       Import the alias from the base class.      
* [0.x.5]*
       Default constructor. Generate an empty vector without any blocks.      
* [0.x.6]*
       Constructor. Generate a block vector with  [2.x.4]  blocks, each of       which is a parallel vector across  [2.x.5]  with  [2.x.6]        elements of which  [2.x.7]  elements are stored on the       present process.      
* [0.x.7]*
       Copy constructor. Set all the properties of the parallel vector to       those of the given argument and copy the elements.      
* [0.x.8]*
       Constructor. Set the number of blocks to <tt>block_sizes.size()</tt>       and initialize each block with <tt>block_sizes[i]</tt> zero elements.       The individual blocks are distributed across the given communicator,       and each store <tt>local_elements[i]</tt> elements on the present       process.      
* [0.x.9]*
       Create a BlockVector with parallel_partitioning.size() blocks, each       initialized with the given IndexSet.      
* [0.x.10]*
       Same as above, but include ghost elements      
* [0.x.11]*
       Destructor. Clears memory      
* [0.x.12]*
       Copy operator: fill all components of the vector that are locally       stored with the given scalar value.      
* [0.x.13]*
       Copy operator for arguments of the same type.      
* [0.x.14]*
       Reinitialize the BlockVector to contain  [2.x.8]  of size  [2.x.9]        block_size, each of which stores  [2.x.10]  elements       locally. The  [2.x.11]  argument denotes which MPI channel each       of these blocks shall communicate.             If <tt>omit_zeroing_entries==false</tt>, the vector is filled with       zeros.      
* [0.x.15]*
       Reinitialize the BlockVector such that it contains       <tt>block_sizes.size()</tt> blocks. Each block is reinitialized to       dimension <tt>block_sizes[i]</tt>. Each of them stores       <tt>locally_owned_sizes[i]</tt> elements on the present process.             If the number of blocks is the same as before this function was       called, all vectors remain the same and reinit() is called for each       vector.             If <tt>omit_zeroing_entries==false</tt>, the vector is filled with       zeros.             Note that you must call this (or the other reinit() functions)       function, rather than calling the reinit() functions of an individual       block, to allow the block vector to update its caches of vector       sizes. If you call reinit() of one of the blocks, then subsequent       actions on this object may yield unpredictable results since they may       be routed to the wrong block.      
* [0.x.16]*
       Change the dimension to that of the vector <tt>V</tt>. The same       applies as for the other reinit() function.             The elements of <tt>V</tt> are not copied, i.e.  this function is the       same as calling <tt>reinit (V.size(), omit_zeroing_entries)</tt>.             Note that you must call this (or the other reinit() functions)       function, rather than calling the reinit() functions of an individual       block, to allow the block vector to update its caches of vector       sizes. If you call reinit() on one of the blocks, then subsequent       actions on this object may yield unpredictable results since they may       be routed to the wrong block.      
* [0.x.17]*
       Reinitialize the BlockVector using IndexSets. See the constructor       with the same arguments for details.      
* [0.x.18]*
       Same as above but include ghost entries.      
* [0.x.19]*
       Change the number of blocks to <tt>num_blocks</tt>. The individual       blocks will get initialized with zero size, so it is assumed that the       user resizes the individual blocks by herself in an appropriate way,       and calls <tt>collect_sizes</tt> afterwards.      
* [0.x.20]*
       Return if this vector is a ghosted vector (and thus read-only).      
* [0.x.21]*
       Return a reference to the MPI communicator object in use with this       vector.      
* [0.x.22]*
       Swap the contents of this vector and the other vector <tt>v</tt>. One       could do this operation with a temporary variable and copying over       the data elements, but this function is significantly more efficient       since it only swaps the pointers to the data of the two vectors and       therefore does not need to allocate temporary storage and move data       around.             Limitation: right now this function only works if both vectors have       the same number of blocks. If needed, the numbers of blocks should be       exchanged, too.             This function is analogous to the swap() function of all C++       standard containers. Also, there is a global function swap(u,v) that       simply calls <tt>u.swap(v)</tt>, again in analogy to standard       functions.      
* [0.x.23]*
       Print to a stream.      
* [0.x.24]*
       Exception      
* [0.x.25]*
       Exception      
* [0.x.26]*
     Global function which overloads the default implementation of the C++     standard library which uses a temporary object. The function simply     exchanges the data of the two vectors.          [2.x.12]   [2.x.13]     
* [0.x.27]*
     A helper class used internally in linear_operator.h. Specialization for      [2.x.14]     
* [0.x.28]*
 Declare  [2.x.15]  as distributed vector.

* 
* [0.x.29]

include/deal.II-translator/lac/petsc_compatibility_0.txt
[0.x.0] Rather than using ifdefs everywhere, try to wrap older versions of PETSc functions in one place.

* 
* [0.x.1]*
   Set an option in the global PETSc database. This function just wraps   PetscOptionsSetValue with a version check (the signature of this function   changed in PETSc 3.7.0).  
* [0.x.2]*
   Destroy a PETSc matrix. This function wraps MatDestroy with a version   check (the signature of this function changed in PETSc 3.2.0).      [2.x.0]  Since the primary intent of this function is to enable RAII   semantics in the PETSc wrappers, this function will not throw an   exception if an error occurs, but instead just returns the error code   given by MatDestroy.  
* [0.x.3]*
   Destroy a Krylov Subspace (KSP) PETSc solver. This function wraps   KSPDestroy with a version check (the signature of this function changed   in PETSc 3.2.0).      [2.x.1]  Since the primary intent of this function is to enable RAII   semantics in the PETSc wrappers, this function will not throw an   exception if an error occurs, but instead just returns the error code   given by MatDestroy.  
* [0.x.4]*
   Set a PETSc matrix option. This function wraps MatSetOption with a   version check.      [2.x.2]  The argument option_value is ignored in versions of PETSc   before 3.0.0 since the corresponding function did not take this argument.  
* [0.x.5]*
   Tell PETSc that we are not planning on adding new entries to the   matrix. Generate errors in debug mode.  
* [0.x.6]*
   Tell PETSc to keep the SparsityPattern entries even if we delete a   row with clear_rows() which calls MatZeroRows(). Otherwise one can   not write into that row afterwards.  
* [0.x.7]

include/deal.II-translator/lac/petsc_full_matrix_0.txt
[0.x.0]!  [2.x.0]  PETScWrappers   [2.x.1]   
* [0.x.1]*
   Implementation of a sequential dense matrix class based on PETSc. All the   functionality is actually in the base class, except for the calls to   generate a sequential dense matrix. This is possible since PETSc only   works on an abstract matrix type and internally distributes to functions   that do the actual work depending on the actual matrix type (much like   using virtual functions). Only the functions creating a matrix of   specific type differ, and are implemented in this particular class.    
*  [2.x.2]   
* [0.x.2]*
     Declare type for container size.    
* [0.x.3]*
     Default constructor. Create an empty matrix.    
* [0.x.4]*
     Create a full matrix of dimensions  [2.x.3]  times  [2.x.4]     
* [0.x.5]*
     Throw away the present matrix and generate one that has the same     properties as if it were created by the constructor of this class with     the same argument list as the present function.    
* [0.x.6]*
     Return a reference to the MPI communicator object in use with this     matrix. Since this is a sequential matrix, it returns the MPI_COMM_SELF     communicator.    
* [0.x.7]*
     Do the actual work for the respective reinit() function and the     matching constructor, i.e. create a matrix. Getting rid of the previous     matrix is left to the caller.    
* [0.x.8]

include/deal.II-translator/lac/petsc_matrix_base_0.txt
[0.x.0]*
     This class acts as an iterator walking over the elements of PETSc     matrices. Since PETSc offers a uniform interface for all types of     matrices, this iterator can be used to access both sparse and full     matrices.         Note that PETSc does not give any guarantees as to the order of     elements within each row. Note also that accessing the elements of a     full matrix surprisingly only shows the nonzero elements of the matrix,     not all elements.        
*  [2.x.0]     
* [0.x.1]*
       Accessor class for iterators      
* [0.x.2]*
         Declare type for container size.        
* [0.x.3]*
         Constructor. Since we use accessors only for read access, a const         matrix pointer is sufficient.        
* [0.x.4]*
         Row number of the element represented by this object.        
* [0.x.5]*
         Index in row of the element represented by this object.        
* [0.x.6]*
         Column number of the element represented by this object.        
* [0.x.7]*
         Value of this matrix entry.        
* [0.x.8]*
         Exception        
* [0.x.9]*
         Exception        
* [0.x.10]*
         The matrix accessed.        
* [0.x.11]*
         Current row number.        
* [0.x.12]*
         Current index in row.        
* [0.x.13]*
         Cache where we store the column indices of the present row. This is         necessary, since PETSc makes access to the elements of its matrices         rather hard, and it is much more efficient to copy all column         entries of a row once when we enter it than repeatedly asking PETSc         for individual ones. This also makes some sense since it is likely         that we will access them sequentially anyway.                 In order to make copying of iterators/accessor of acceptable         performance, we keep a shared pointer to these entries so that more         than one accessor can access this data if necessary.        
* [0.x.14]*
         Similar cache for the values of this row.        
* [0.x.15]*
         Discard the old row caches (they may still be used by other         accessors) and generate new ones for the row pointed to presently         by this accessor.        
* [0.x.16]*
       Declare type for container size.      
* [0.x.17]*
       Constructor. Create an iterator into the matrix  [2.x.1]  for the       given row and the index within it.      
* [0.x.18]*
       Prefix increment.      
* [0.x.19]*
       Postfix increment.      
* [0.x.20]*
       Dereferencing operator.      
* [0.x.21]*
       Dereferencing operator.      
* [0.x.22]*
       Comparison. True, if both iterators point to the same matrix       position.      
* [0.x.23]*
       Inverse of <tt>==</tt>.      
* [0.x.24]*
       Comparison operator. Result is true if either the first row number is       smaller or if the row numbers are equal and the first index is       smaller.      
* [0.x.25]*
       Exception      
* [0.x.26]*
       Store an object of the accessor class.      
* [0.x.27]*
   Base class for all matrix classes that are implemented on top of the   PETSc matrix types. Since in PETSc all matrix types (i.e. sequential and   parallel, sparse, blocked, etc.)  are built by filling the contents of an   abstract object that is only referenced through a pointer of a type that   is independent of the actual matrix type, we can implement almost all   functionality of matrices in this base class. Derived classes will then   only have to provide the functionality to create one or the other kind of   matrix.     The interface of this class is modeled after the existing SparseMatrix   class in deal.II. It has almost the same member functions, and is often   exchangeable. However, since PETSc only supports a single scalar type   (either double, float, or a complex data type), it is not templated, and   only works with whatever your PETSc installation has defined the data   type PetscScalar to.     Note that PETSc only guarantees that operations do what you expect if the   functions  [2.x.2]  and  [2.x.3]  have been called   after matrix assembly. Therefore, you need to call    [2.x.4]  before you actually use the matrix. This also   calls  [2.x.5]  that compresses the storage format for sparse   matrices by discarding unused elements. PETSc allows to continue with   assembling the matrix after calls to these functions, but since there are   no more free entries available after that any more, it is better to only   call  [2.x.6]  once at the end of the assembly stage and   before the matrix is actively used.    
*  [2.x.7]   
*  [2.x.8]   
* [0.x.28]*
     Declare an alias for the iterator class.    
* [0.x.29]*
     Declare type for container size.    
* [0.x.30]*
     Declare an alias in analogy to all the other container classes.    
* [0.x.31]*
     Default constructor.    
* [0.x.32]*
     Copy constructor. It is deleted as copying this base class     without knowing the concrete kind of matrix stored may both     miss important details and be expensive if the matrix is large.    
* [0.x.33]*
     Copy operator. It is deleted as copying this base class     without knowing the concrete kind of matrix stored may both     miss important details and be expensive if the matrix is large.    
* [0.x.34]*
     Destructor. Made virtual so that one can use pointers to this class.    
* [0.x.35]*
     This operator assigns a scalar to a matrix. Since this does usually not     make much sense (should we set all matrix entries to this value? Only     the nonzero entries of the sparsity pattern?), this operation is only     allowed if the actual value to be assigned is zero. This operator only     exists to allow for the obvious notation <tt>matrix=0</tt>, which sets     all elements of the matrix to zero, but keeps the sparsity pattern     previously used.    
* [0.x.36]*
     Release all memory and return to a state just like after having called     the default constructor.    
* [0.x.37]*
     Set the element ([1.x.0]) to  [2.x.9]          If the present object (from a derived class of this one) happens to be     a sparse matrix, then this function adds a new entry to the matrix if     it didn't exist before, very much in contrast to the SparseMatrix class     which throws an error if the entry does not exist. If <tt>value</tt> is     not a finite number an exception is thrown.    
* [0.x.38]*
     Set all elements given in a FullMatrix<double> into the sparse matrix     locations given by <tt>indices</tt>. In other words, this function     writes the elements in <tt>full_matrix</tt> into the calling matrix,     using the local-to-global indexing specified by <tt>indices</tt> for     both the rows and the columns of the matrix. This function assumes a     quadratic sparse matrix and a quadratic full_matrix, the usual     situation in FE calculations.         If the present object (from a derived class of this one) happens to be     a sparse matrix, then this function adds some new entries to the matrix     if they didn't exist before, very much in contrast to the SparseMatrix     class which throws an error if the entry does not exist.         The optional parameter <tt>elide_zero_values</tt> can be used to     specify whether zero values should be inserted anyway or they should be     filtered away. The default value is <tt>false</tt>, i.e., even zero     values are inserted/replaced.    
* [0.x.39]*
     Same function as before, but now including the possibility to use     rectangular full_matrices and different local-to-global indexing on     rows and columns, respectively.    
* [0.x.40]*
     Set several elements in the specified row of the matrix with column     indices as given by <tt>col_indices</tt> to the respective value.         If the present object (from a derived class of this one) happens to be     a sparse matrix, then this function adds some new entries to the matrix     if they didn't exist before, very much in contrast to the SparseMatrix     class which throws an error if the entry does not exist.         The optional parameter <tt>elide_zero_values</tt> can be used to     specify whether zero values should be inserted anyway or they should be     filtered away. The default value is <tt>false</tt>, i.e., even zero     values are inserted/replaced.    
* [0.x.41]*
     Set several elements to values given by <tt>values</tt> in a given row     in columns given by col_indices into the sparse matrix.         If the present object (from a derived class of this one) happens to be     a sparse matrix, then this function adds some new entries to the matrix     if they didn't exist before, very much in contrast to the SparseMatrix     class which throws an error if the entry does not exist.         The optional parameter <tt>elide_zero_values</tt> can be used to     specify whether zero values should be inserted anyway or they should be     filtered away. The default value is <tt>false</tt>, i.e., even zero     values are inserted/replaced.    
* [0.x.42]*
     Add  [2.x.10]  to the element ([1.x.1]).         If the present object (from a derived class of this one) happens to be     a sparse matrix, then this function adds a new entry to the matrix if     it didn't exist before, very much in contrast to the SparseMatrix class     which throws an error if the entry does not exist. If <tt>value</tt> is     not a finite number an exception is thrown.    
* [0.x.43]*
     Add all elements given in a FullMatrix<double> into sparse matrix     locations given by <tt>indices</tt>. In other words, this function adds     the elements in <tt>full_matrix</tt> to the respective entries in     calling matrix, using the local-to-global indexing specified by     <tt>indices</tt> for both the rows and the columns of the matrix. This     function assumes a quadratic sparse matrix and a quadratic full_matrix,     the usual situation in FE calculations.         If the present object (from a derived class of this one) happens to be     a sparse matrix, then this function adds some new entries to the matrix     if they didn't exist before, very much in contrast to the SparseMatrix     class which throws an error if the entry does not exist.         The optional parameter <tt>elide_zero_values</tt> can be used to     specify whether zero values should be added anyway or these should be     filtered away and only non-zero data is added. The default value is     <tt>true</tt>, i.e., zero values won't be added into the matrix.    
* [0.x.44]*
     Same function as before, but now including the possibility to use     rectangular full_matrices and different local-to-global indexing on     rows and columns, respectively.    
* [0.x.45]*
     Set several elements in the specified row of the matrix with column     indices as given by <tt>col_indices</tt> to the respective value.         If the present object (from a derived class of this one) happens to be     a sparse matrix, then this function adds some new entries to the matrix     if they didn't exist before, very much in contrast to the SparseMatrix     class which throws an error if the entry does not exist.         The optional parameter <tt>elide_zero_values</tt> can be used to     specify whether zero values should be added anyway or these should be     filtered away and only non-zero data is added. The default value is     <tt>true</tt>, i.e., zero values won't be added into the matrix.    
* [0.x.46]*
     Add an array of values given by <tt>values</tt> in the given global     matrix row at columns specified by col_indices in the sparse matrix.         If the present object (from a derived class of this one) happens to be     a sparse matrix, then this function adds some new entries to the matrix     if they didn't exist before, very much in contrast to the SparseMatrix     class which throws an error if the entry does not exist.         The optional parameter <tt>elide_zero_values</tt> can be used to     specify whether zero values should be added anyway or these should be     filtered away and only non-zero data is added. The default value is     <tt>true</tt>, i.e., zero values won't be added into the matrix.    
* [0.x.47]*
     Remove all elements from this <tt>row</tt> by setting them to zero. The     function does not modify the number of allocated nonzero entries, it     only sets some entries to zero. It may drop them from the sparsity     pattern, though (but retains the allocated memory in case new entries     are again added later).         This operation is used in eliminating constraints (e.g. due to hanging     nodes) and makes sure that we can write this modification to the matrix     without having to read entries (such as the locations of non-zero     elements) from it
* 
*  -  without this operation, removing constraints on     parallel matrices is a rather complicated procedure.         The second parameter can be used to set the diagonal entry of this row     to a value different from zero. The default is to set it to zero.    
* [0.x.48]*
     Same as clear_row(), except that it works on a number of rows at once.         The second parameter can be used to set the diagonal entries of all     cleared rows to something different from zero. Note that all of these     diagonal entries get the same value
* 
*  -  if you want different values for     the diagonal entries, you have to set them by hand.    
* [0.x.49]*
     PETSc matrices store their own sparsity patterns. So, in analogy to our     own SparsityPattern class, this function compresses the sparsity     pattern and allows the resulting matrix to be used in all other     operations where before only assembly functions were allowed. This     function must therefore be called once you have assembled the matrix.         See      [2.x.11]  "Compressing distributed objects"     for more information.    
* [0.x.50]*
     Return the value of the entry ([1.x.2]).  This may be an expensive     operation and you should always take care where to call this function.     In contrast to the respective function in the  [2.x.12]  class, we     don't throw an exception if the respective entry doesn't exist in the     sparsity pattern of this class, since PETSc does not transmit this     information.         This function is therefore exactly equivalent to the <tt>el()</tt>     function.    
* [0.x.51]*
     Return the value of the matrix entry ([1.x.3]). If this entry does     not exist in the sparsity pattern, then zero is returned. While this     may be convenient in some cases, note that it is simple to write     algorithms that are slow compared to an optimal solution, since the     sparsity of the matrix is not used.    
* [0.x.52]*
     Return the main diagonal element in the [1.x.4]th row. This function     throws an error if the matrix is not quadratic.         Since we do not have direct access to the underlying data structure,     this function is no faster than the elementwise access using the el()     function. However, we provide this function for compatibility with the     SparseMatrix class.    
* [0.x.53]*
     Return the number of rows in this matrix.    
* [0.x.54]*
     Return the number of columns in this matrix.    
* [0.x.55]*
     Return the local dimension of the matrix, i.e. the number of rows     stored on the present MPI process. For sequential matrices, this number     is the same as m(), but for parallel matrices it may be smaller.         To figure out which elements exactly are stored locally, use     local_range().    
* [0.x.56]*
     Return a pair of indices indicating which rows of this matrix are     stored locally. The first number is the index of the first row stored,     the second the index of the one past the last one that is stored     locally. If this is a sequential matrix, then the result will be the     pair (0,m()), otherwise it will be a pair (i,i+n), where     <tt>n=local_size()</tt>.    
* [0.x.57]*
     Return whether  [2.x.13]  is in the local range or not, see also     local_range().    
* [0.x.58]*
     Return a reference to the MPI communicator object in use with this     matrix. This function has to be implemented in derived classes.    
* [0.x.59]*
     Return the number of nonzero elements of this matrix. Actually, it     returns the number of entries in the sparsity pattern; if any of the     entries should happen to be zero, it is counted anyway.    
* [0.x.60]*
     Number of entries in a specific row.    
* [0.x.61]*
     Return the l1-norm of the matrix, that is  [2.x.14] , (max. sum of columns). This is the natural     matrix norm that is compatible to the l1-norm for vectors, i.e.      [2.x.15] . (cf. Haemmerlin-Hoffmann: Numerische     Mathematik)    
* [0.x.62]*
     Return the linfty-norm of the matrix, that is  [2.x.16] , (max. sum of rows). This is the natural     matrix norm that is compatible to the linfty-norm of vectors, i.e.      [2.x.17] . (cf. Haemmerlin-Hoffmann:     Numerische Mathematik)    
* [0.x.63]*
     Return the frobenius norm of the matrix, i.e. the square root of the     sum of squares of all entries in the matrix.    
* [0.x.64]*
     Return the square of the norm of the vector  [2.x.18]  with respect to the     norm induced by this matrix, i.e.  [2.x.19] . This is useful,     e.g. in the finite element context, where the  [2.x.20]  norm of a function     equals the matrix norm with respect to the mass matrix of the vector     representing the nodal values of the finite element function.         Obviously, the matrix needs to be quadratic for this operation.         The implementation of this function is not as efficient as the one in     the  [2.x.21]  class used in deal.II (i.e. the original one, not the     PETSc wrapper class) since PETSc doesn't support this operation and     needs a temporary vector.         Note that if the current object represents a parallel distributed     matrix (of type  [2.x.22]  then the given     vector has to be a distributed vector as well. Conversely, if the     matrix is not distributed, then neither may the vector be.    
* [0.x.65]*
     Compute the matrix scalar product  [2.x.23] .         The implementation of this function is not as efficient as the one in     the  [2.x.24]  class used in deal.II (i.e. the original one, not the     PETSc wrapper class) since PETSc doesn't support this operation and     needs a temporary vector.         Note that if the current object represents a parallel distributed     matrix (of type  [2.x.25]  then both vectors     have to be distributed vectors as well. Conversely, if the matrix is     not distributed, then neither of the vectors may be.    
* [0.x.66]*
     Return the trace of the matrix, i.e. the sum of all diagonal entries in     the matrix.    
* [0.x.67]*
     Multiply the entire matrix by a fixed factor.    
* [0.x.68]*
     Divide the entire matrix by a fixed factor.    
* [0.x.69]*
     Add the matrix  [2.x.26]  scaled by the factor  [2.x.27]  to the current     matrix.    
* [0.x.70]*
     Matrix-vector multiplication: let [1.x.5] with [1.x.6]     being this matrix.         Source and destination must not be the same vector.         Note that if the current object represents a parallel distributed     matrix (of type  [2.x.28]  then both vectors     have to be distributed vectors as well. Conversely, if the matrix is     not distributed, then neither of the vectors may be.    
* [0.x.71]*
     Matrix-vector multiplication: let [1.x.7] with     [1.x.8] being this matrix. This function does the same as vmult() but     takes the transposed matrix.         Source and destination must not be the same vector.         Note that if the current object represents a parallel distributed     matrix (of type  [2.x.29]  then both vectors     have to be distributed vectors as well. Conversely, if the matrix is     not distributed, then neither of the vectors may be.    
* [0.x.72]*
     Adding Matrix-vector multiplication. Add [1.x.9] on [1.x.10]     with [1.x.11] being this matrix.         Source and destination must not be the same vector.         Note that if the current object represents a parallel distributed     matrix (of type  [2.x.30]  then both vectors     have to be distributed vectors as well. Conversely, if the matrix is     not distributed, then neither of the vectors may be.    
* [0.x.73]*
     Adding Matrix-vector multiplication. Add [1.x.12] to     [1.x.13] with [1.x.14] being this matrix. This function does the same     as vmult_add() but takes the transposed matrix.         Source and destination must not be the same vector.         Note that if the current object represents a parallel distributed     matrix (of type  [2.x.31]  then both vectors     have to be distributed vectors as well. Conversely, if the matrix is     not distributed, then neither of the vectors may be.    
* [0.x.74]*
     Compute the residual of an equation [1.x.15], where the residual is     defined to be [1.x.16]. Write the residual into  [2.x.32]  The     [1.x.17] norm of the residual vector is returned.         Source [1.x.18] and destination [1.x.19] must not be the same vector.         Note that if the current object represents a parallel distributed     matrix (of type  [2.x.33]  then all vectors     have to be distributed vectors as well. Conversely, if the matrix is     not distributed, then neither of the vectors may be.    
* [0.x.75]*
     Iterator starting at the first entry. This can only be called on a     processor owning the entire matrix. In all other cases refer to the     version of begin() taking a row number as an argument.    
* [0.x.76]*
     Final iterator. This can only be called on a processor owning the entire     matrix. In all other cases refer to the version of end() taking a row     number as an argument.    
* [0.x.77]*
     Iterator starting at the first entry of row  [2.x.34]          Note that if the given row is empty, i.e. does not contain any nonzero     entries, then the iterator returned by this function equals     <tt>end(r)</tt>. Note also that the iterator may not be dereferenceable     in that case.    
* [0.x.78]*
     Final iterator of row <tt>r</tt>. It points to the first element past     the end of line  [2.x.35]  or past the end of the entire sparsity pattern.         Note that the end iterator is not necessarily dereferenceable. This is     in particular the case if it is the end iterator for the last row of a     matrix.    
* [0.x.79]*
     Conversion operator to gain access to the underlying PETSc type. If you     do this, you cut this class off some information it may need, so this     conversion operator should only be used if you know what you do. In     particular, it should only be used for read-only operations into the     matrix.    
* [0.x.80]*
     Return a reference to the underlying PETSc type. It can be used to     modify the underlying data, so use it only when you know what you     are doing.    
* [0.x.81]*
     Make an in-place transpose of a matrix.    
* [0.x.82]*
     Test whether a matrix is symmetric.  Default tolerance is      [2.x.36] -bit machine precision.    
* [0.x.83]*
     Test whether a matrix is Hermitian, i.e. it is the complex conjugate of     its transpose. Default tolerance is  [2.x.37] -bit machine     precision.    
* [0.x.84]*
     Print the PETSc matrix object values using PETSc internal matrix viewer     function <tt>MatView</tt>. The default format prints the non- zero     matrix elements. For other valid view formats, consult     http://www.mcs.anl.gov/petsc/petsc-current/docs/manualpages/Mat/MatView.html    
* [0.x.85]*
     Print the elements of a matrix to the given output stream.          [2.x.38]  out The output stream to which to write.      [2.x.39]  alternative_output This argument is ignored. It exists for     compatibility with similar functions in other matrix classes.    
* [0.x.86]*
     Return the number bytes consumed by this matrix on this CPU.    
* [0.x.87]*
     Exception    
* [0.x.88]*
     Exception.    
* [0.x.89]*
     A generic matrix object in PETSc. The actual type, a sparse matrix, is     set in the constructor.    
* [0.x.90]*
     Store whether the last action was a write or add operation.    
* [0.x.91]*
     Ensure that the add/set mode that is required for actions following     this call is compatible with the current mode. Should be called from     all internal functions accessing matrix elements.    
* [0.x.92]*
     Internal function that checks that there are no pending insert/add     operations. Throws an exception otherwise. Useful before calling any     PETSc internal functions modifying the matrix.    
* [0.x.93]*
     For some matrix storage formats, in particular for the PETSc     distributed blockmatrices, set and add operations on individual     elements can not be freely mixed. Rather, one has to synchronize     operations when one wants to switch from setting elements to adding to     elements. BlockMatrixBase automatically synchronizes the access by     calling this helper function for each block. This function ensures that     the matrix is in a state that allows adding elements; if it previously     already was in this state, the function does nothing.    
* [0.x.94]*
     Same as prepare_add() but prepare the matrix for setting elements if     the representation of elements in this class requires such an     operation.    
* [0.x.95]*
     Base function to perform the matrix-matrix multiplication  [2.x.40] ,     or, if a vector  [2.x.41]  whose size is compatible with B is given,      [2.x.42] , where  [2.x.43]  defines a     diagonal matrix with the vector entries.         This function assumes that the calling matrix  [2.x.44]  and  [2.x.45]      have compatible sizes. The size of  [2.x.46]  will be set within this     function.         The content as well as the sparsity pattern of the matrix  [2.x.47]  will be     reset by this function, so make sure that the sparsity pattern is not     used somewhere else in your program. This is an expensive operation, so     think twice before you use this function.    
* [0.x.96]*
     Base function to perform the matrix-matrix multiplication with     the transpose of <tt>this</tt>, i.e.,  [2.x.48] , or,     if an optional vector  [2.x.49]  whose size is compatible with  [2.x.50]  is given,      [2.x.51] , where  [2.x.52]  defines a     diagonal matrix with the vector entries.         This function assumes that the calling matrix  [2.x.53]  and  [2.x.54]      have compatible sizes. The size of  [2.x.55]  will be set within this     function.         The content as well as the sparsity pattern of the matrix  [2.x.56]  will be     changed by this function, so make sure that the sparsity pattern is not     used somewhere else in your program. This is an expensive operation, so     think twice before you use this function.    
* [0.x.97]*
     An internal array of integer values that is used to store the column     indices when adding/inserting local data into the (large) sparse     matrix.         This variable does not store any "state" of the matrix     object. Rather, it is only used as a temporary buffer by some     of the member functions of this class. As with all  [2.x.57]      member variables, the use of this variable is not thread-safe     unless guarded by a mutex. However, since PETSc matrix     operations are not thread-safe anyway, there is no need to     attempt to make things thread-safe, and so there is no mutex     associated with this variable.    
* [0.x.98]*
     An internal array of double values that is used to store the column     indices when adding/inserting local data into the (large) sparse     matrix.         The same comment as for the  [2.x.58]  variable above     applies.    
* [0.x.99]

include/deal.II-translator/lac/petsc_matrix_free_0.txt
[0.x.0]*
   Implementation of a parallel matrix class based on PETSc   <tt>MatShell</tt> matrix-type. This base class implements only the   interface to the PETSc matrix object, while all the functionality is   contained in the matrix-vector multiplication which must be reimplemented   in derived classes.     This interface is an addition to the  [2.x.0]  class to realize   user-defined matrix-classes together with PETSc solvers and   functionalities. See also the documentation of  [2.x.1]  class   and  [2.x.2]  and  [2.x.3] .     Similar to other matrix classes in namespaces PETScWrappers and    [2.x.4]  the MatrixFree class provides the usual matrix-vector   multiplication <tt>vmult(VectorBase &dst, const VectorBase &src)</tt>   which is pure virtual and must be reimplemented in derived classes.   Besides the usual interface, this class has a matrix-vector   multiplication <tt>vmult(Vec &dst, const Vec &src)</tt> taking PETSc Vec   objects, which will be called by <tt>matrix_free_mult(Mat A, Vec src, Vec   dst)</tt> registered as matrix-vector multiplication of this PETSc matrix   object. The default implementation of the vmult function in the base   class wraps the given PETSc vectors with the  [2.x.5]    class and then calls the usual vmult function with the usual interface.    
*  [2.x.6]   
*  [2.x.7]   
* [0.x.1]*
     Default constructor. Create an empty matrix object.    
* [0.x.2]*
     Create a matrix object of dimensions  [2.x.8]  times  [2.x.9]  with communication     happening over the provided  [2.x.10]          For the meaning of the  [2.x.11]  and  [2.x.12]  parameters,     see the  [2.x.13]  class documentation.         As other PETSc matrices, also the matrix-free object needs to have     a size and to perform matrix vector multiplications efficiently in     parallel also  [2.x.14]  and  [2.x.15]  But in contrast to      [2.x.16]  classes a PETSc matrix-free object does not need     any estimation of non_zero entries and has no option     <tt>is_symmetric</tt>.    
* [0.x.3]*
     Create a matrix object of dimensions  [2.x.17]  times  [2.x.18]  with communication     happening over the provided  [2.x.19]          As other PETSc matrices, also the matrix-free object needs to have     a size and to perform matrix vector multiplications efficiently in     parallel also  [2.x.20]  and  [2.x.21]  But in contrast to      [2.x.22]  classes a PETSc matrix-free object does not need     any estimation of non_zero entries and has no option     <tt>is_symmetric</tt>.    
* [0.x.4]*
     Constructor for the serial case: Same function as     <tt>MatrixFree()</tt>, see above, with <tt>communicator =     MPI_COMM_WORLD</tt>.    
* [0.x.5]*
     Constructor for the serial case: Same function as     <tt>MatrixFree()</tt>, see above, with <tt>communicator =     MPI_COMM_WORLD</tt>.    
* [0.x.6]*
     Throw away the present matrix and generate one that has the same     properties as if it were created by the constructor of this class with     the same argument list as the present function.    
* [0.x.7]*
     Throw away the present matrix and generate one that has the same     properties as if it were created by the constructor of this class with     the same argument list as the present function.    
* [0.x.8]*
     Call the  [2.x.23]  function above with <tt>communicator =     MPI_COMM_WORLD</tt>.    
* [0.x.9]*
     Call the  [2.x.24]  function above with <tt>communicator =     MPI_COMM_WORLD</tt>.    
* [0.x.10]*
     Release all memory and return to a state just like after having called     the default constructor.    
* [0.x.11]*
     Return a reference to the MPI communicator object in use with this     matrix.    
* [0.x.12]*
     Matrix-vector multiplication: let [1.x.0] with [1.x.1]     being this matrix.         Source and destination must not be the same vector.         Note that if the current object represents a parallel distributed     matrix (of type  [2.x.25]  then both vectors     have to be distributed vectors as well. Conversely, if the matrix is     not distributed, then neither of the vectors may be.    
* [0.x.13]*
     Matrix-vector multiplication: let [1.x.2] with     [1.x.3] being this matrix. This function does the same as  [2.x.26]      but takes the transposed matrix.         Source and destination must not be the same vector.         Note that if the current object represents a parallel distributed     matrix then both vectors have to be distributed vectors as well.     Conversely, if the matrix is not distributed, then neither of the     vectors may be.    
* [0.x.14]*
     Adding Matrix-vector multiplication. Add [1.x.4] on [1.x.5]     with [1.x.6] being this matrix.         Source and destination must not be the same vector.         Note that if the current object represents a parallel distributed     matrix then both vectors have to be distributed vectors as well.     Conversely, if the matrix is not distributed, then neither of the     vectors may be.    
* [0.x.15]*
     Adding Matrix-vector multiplication. Add [1.x.7] to     [1.x.8] with [1.x.9] being this matrix. This function does the same     as  [2.x.27]  but takes the transposed matrix.         Source and destination must not be the same vector.         Note that if the current object represents a parallel distributed     matrix then both vectors have to be distributed vectors as well.     Conversely, if the matrix is not distributed, then neither of the     vectors may be.    
* [0.x.16]*
     The matrix-vector multiplication called by  [2.x.28]  This     function can be reimplemented in derived classes for efficiency. The     default implementation copies the given vectors into      [2.x.29]  and calls <tt>vmult(VectorBase &dst, const     VectorBase &src)</tt> which is purely virtual and must be reimplemented     in derived classes.    
* [0.x.17]*
     Copy of the communicator object to be used for this parallel matrix-     free object.    
* [0.x.18]*
     Callback-function registered as the matrix-vector multiplication of     this matrix-free object called by PETSc routines. This function must be     static and takes a PETSc matrix  [2.x.30]  and vectors  [2.x.31]  and  [2.x.32]      where [1.x.10]         Source and destination must not be the same vector.         This function calls <tt>vmult(Vec &dst, const Vec &src)</tt> which     should be reimplemented in derived classes.    
* [0.x.19]*
     Do the actual work for the respective  [2.x.33]  function and the     matching constructor, i.e. create a matrix object. Getting rid of the     previous matrix is left to the caller.    
* [0.x.20]

include/deal.II-translator/lac/petsc_parallel_block_sparse_matrix_0.txt
[0.x.0]

include/deal.II-translator/lac/petsc_parallel_block_vector_0.txt
[0.x.0]

include/deal.II-translator/lac/petsc_parallel_sparse_matrix_0.txt
[0.x.0]

include/deal.II-translator/lac/petsc_parallel_vector_0.txt
[0.x.0]

include/deal.II-translator/lac/petsc_precondition_0.txt
[0.x.0]*
   Base class for preconditioner classes using the PETSc functionality. The   classes in this hierarchy don't do a whole lot, except for providing a   function that sets the preconditioner and certain parameters on the   preconditioning context of the solver. These classes are basically here   only to allow a similar interface as already used for the deal.II solver   and preconditioner classes.     Note that derived classes only provide interfaces to the relevant   functionality of PETSc. PETSc does not implement all preconditioners for   all matrix types. In particular, some preconditioners are not going to   work for parallel jobs, such as for example the ILU preconditioner.    
*  [2.x.0]   
* [0.x.1]*
     Constructor.    
* [0.x.2]*
     Destructor.    
* [0.x.3]*
     Destroys the preconditioner, leaving an object like just after having     called the constructor.    
* [0.x.4]*
     Apply the preconditioner once to the given src vector.    
* [0.x.5]*
     Apply the transpose preconditioner once to the given src vector.    
* [0.x.6]*
     Give access to the underlying PETSc object.    
* [0.x.7]*
     the PETSc preconditioner object    
* [0.x.8]*
     A pointer to the matrix that acts as a preconditioner.    
* [0.x.9]*
     Internal function to create the PETSc preconditioner object. Fails if     called twice.    
* [0.x.10]*
     Conversion operator to get a representation of the matrix that     represents this preconditioner. We use this inside the actual solver,     where we need to pass this matrix to the PETSc solvers.    
* [0.x.11]*
   A class that implements the interface to use the PETSc Jacobi   preconditioner.     See the comment in the base class    [2.x.1]    for when this preconditioner may or may not work.    
*  [2.x.2]   
* [0.x.12]*
     Standardized data struct to pipe additional flags to the     preconditioner.    
* [0.x.13]*
     Empty Constructor. You need to call initialize() before using this     object.    
* [0.x.14]*
     Constructor. Take the matrix which is used to form the preconditioner,     and additional flags if there are any.    
* [0.x.15]*
     Same as above but without setting a matrix to form the preconditioner.     Intended to be used with SLEPc objects.    
* [0.x.16]*
     Initialize the preconditioner object and calculate all data that is     necessary for applying it in a solver. This function is automatically     called when calling the constructor with the same arguments and is only     used if you create the preconditioner without arguments.    
* [0.x.17]*
     Store a copy of the flags for this particular preconditioner.    
* [0.x.18]*
     Initialize the preconditioner object without knowing a particular     matrix. This function sets up appropriate parameters to the underlying     PETSc object after it has been created.    
* [0.x.19]*
   A class that implements the interface to use the PETSc Block Jacobi   preconditioner. PETSc defines the term "block Jacobi" as a preconditioner   in which it looks at a number of diagonal blocks of the matrix and then   defines a preconditioner in which the preconditioner matrix has the same   block structure as only these diagonal blocks, and each diagonal block   of the preconditioner is an approximation of the inverse of the   corresponding block of the original matrix.   The blocking structure of the matrix is determined by the   association of degrees of freedom to the individual processors in an   MPI-parallel job. If you use this preconditioner on a sequential job (or an   MPI job with only one process) then the entire matrix is the only block.     By default, PETSc uses an ILU(0) decomposition of each diagonal block of   the matrix for preconditioning. This can be changed, as is explained in   the relevant section of the PETSc manual, but is not implemented here.     See the comment in the base class    [2.x.3]    for when this preconditioner may or may not work.    
*  [2.x.4]   
* [0.x.20]*
     Standardized data struct to pipe additional flags to the     preconditioner.    
* [0.x.21]*
     Empty Constructor. You need to call initialize() before using this     object.    
* [0.x.22]*
     Constructor. Take the matrix which is used to form the preconditioner,     and additional flags if there are any.    
* [0.x.23]*
     Same as above but without setting a matrix to form the preconditioner.     Intended to be used with SLEPc objects.    
* [0.x.24]*
     Initialize the preconditioner object and calculate all data that is     necessary for applying it in a solver. This function is automatically     called when calling the constructor with the same arguments and is only     used if you create the preconditioner without arguments.    
* [0.x.25]*
     Store a copy of the flags for this particular preconditioner.    
* [0.x.26]*
     Initialize the preconditioner object without knowing a particular     matrix. This function sets up appropriate parameters to the underlying     PETSc object after it has been created.    
* [0.x.27]*
   A class that implements the interface to use the PETSc SOR   preconditioner.    
*  [2.x.5]  Only works in serial with a  [2.x.6]     
*  [2.x.7]   
* [0.x.28]*
     Standardized data struct to pipe additional flags to the     preconditioner.    
* [0.x.29]*
       Constructor. By default, set the damping parameter to one.      
* [0.x.30]*
       Relaxation parameter.      
* [0.x.31]*
     Empty Constructor. You need to call initialize() before using this     object.    
* [0.x.32]*
     Constructor. Take the matrix which is used to form the preconditioner,     and additional flags if there are any.    
* [0.x.33]*
     Initialize the preconditioner object and calculate all data that is     necessary for applying it in a solver. This function is automatically     called when calling the constructor with the same arguments and is only     used if you create the preconditioner without arguments.    
* [0.x.34]*
     Store a copy of the flags for this particular preconditioner.    
* [0.x.35]*
   A class that implements the interface to use the PETSc SSOR   preconditioner.    
*  [2.x.8]  Only works in serial with a  [2.x.9]     
*  [2.x.10]   
* [0.x.36]*
     Standardized data struct to pipe additional flags to the     preconditioner.    
* [0.x.37]*
       Constructor. By default, set the damping parameter to one.      
* [0.x.38]*
       Relaxation parameter.      
* [0.x.39]*
     Empty Constructor. You need to call initialize() before using this     object.    
* [0.x.40]*
     Constructor. Take the matrix which is used to form the preconditioner,     and additional flags if there are any.    
* [0.x.41]*
     Initialize the preconditioner object and calculate all data that is     necessary for applying it in a solver. This function is automatically     called when calling the constructor with the same arguments and is only     used if you create the preconditioner without arguments.    
* [0.x.42]*
     Store a copy of the flags for this particular preconditioner.    
* [0.x.43]*
   A class that implements the interface to use the PETSc Eisenstat   preconditioner, which implements SSOR on the diagonal block owned by   each processor.     See the comment in the base class    [2.x.11]    for when this preconditioner may or may not work.    
*  [2.x.12]   
* [0.x.44]*
     Standardized data struct to pipe additional flags to the     preconditioner.    
* [0.x.45]*
       Constructor. By default, set the damping parameter to one.      
* [0.x.46]*
       Relaxation parameter.      
* [0.x.47]*
     Empty Constructor. You need to call initialize() before using this     object.    
* [0.x.48]*
     Constructor. Take the matrix which is used to form the preconditioner,     and additional flags if there are any.    
* [0.x.49]*
     Initialize the preconditioner object and calculate all data that is     necessary for applying it in a solver. This function is automatically     called when calling the constructor with the same arguments and is only     used if you create the preconditioner without arguments.    
* [0.x.50]*
     Store a copy of the flags for this particular preconditioner.    
* [0.x.51]*
   A class that implements the interface to use the PETSc Incomplete   Cholesky preconditioner.    
*  [2.x.13]  Only works in serial with a  [2.x.14]     
*  [2.x.15]   
* [0.x.52]*
     Standardized data struct to pipe additional flags to the     preconditioner.    
* [0.x.53]*
       Constructor. By default, set the fill-in parameter to zero.      
* [0.x.54]*
       Fill-in parameter.      
* [0.x.55]*
     Empty Constructor. You need to call initialize() before using this     object.    
* [0.x.56]*
     Constructor. Take the matrix which is used to form the preconditioner,     and additional flags if there are any.    
* [0.x.57]*
     Initialize the preconditioner object and calculate all data that is     necessary for applying it in a solver. This function is automatically     called when calling the constructor with the same arguments and is only     used if you create the preconditioner without arguments.    
* [0.x.58]*
     Store a copy of the flags for this particular preconditioner.    
* [0.x.59]*
   A class that implements the interface to use the PETSc ILU   preconditioner.    
*  [2.x.16]  Only works in serial with a  [2.x.17]     
*  [2.x.18]   
* [0.x.60]*
     Standardized data struct to pipe additional flags to the     preconditioner.    
* [0.x.61]*
       Constructor. By default, set the fill-in parameter to zero.      
* [0.x.62]*
       Fill-in parameter.      
* [0.x.63]*
     Empty Constructor. You need to call initialize() before using this     object.    
* [0.x.64]*
     Constructor. Take the matrix which is used to form the preconditioner,     and additional flags if there are any.    
* [0.x.65]*
     Initialize the preconditioner object and calculate all data that is     necessary for applying it in a solver. This function is automatically     called when calling the constructor with the same arguments and is only     used if you create the preconditioner without arguments.    
* [0.x.66]*
     Store a copy of the flags for this particular preconditioner.    
* [0.x.67]*
   A class that implements the interface to use the PETSc LU preconditioner   ( [2.x.19]  Unlike classes like PreconditionILU, this class usually   (depending on the settings) performs an exact factorization of the   matrix, so it is not necessary to wrap it in an iterative solver. This   class is typically used with SolverPreOnly to get a direct   solver. Alternatively, you can use  [2.x.20]  directly.    
*  [2.x.21]  This is not a parallel preconditioner so it only works in serial   computations with a single processor.    
*  [2.x.22]   
* [0.x.68]*
     Standardized data struct to pipe additional flags to the     preconditioner.    
* [0.x.69]*
       Constructor. (Default values taken from function PCCreate_LU of the       PETSc lib.)      
* [0.x.70]*
       Determines, when Pivoting is done during LU decomposition. 0.0       indicates no pivoting, and 1.0 complete pivoting. Confer PETSc manual       for more details.      
* [0.x.71]*
       Size at which smaller pivots are declared to be zero. Confer PETSc       manual for more details.      
* [0.x.72]*
       This quantity is added to the diagonal of the matrix during       factorization.      
* [0.x.73]*
     Empty Constructor. You need to call initialize() before using this     object.    
* [0.x.74]*
     Constructor. Take the matrix which is used to form the preconditioner,     and additional flags if there are any.    
* [0.x.75]*
     Initialize the preconditioner object and calculate all data that is     necessary for applying it in a solver. This function is automatically     called when calling the constructor with the same arguments and is only     used if you create the preconditioner without arguments.    
* [0.x.76]*
     Store a copy of the flags for this particular preconditioner.    
* [0.x.77]*
   A class that implements the interface to use the BoomerAMG algebraic   multigrid preconditioner from the HYPRE suite. Note that PETSc has to be   configured with HYPRE (e.g. with \--download-hypre=1).     The preconditioner does support parallel distributed computations. See    [2.x.23]  for an example.    
*  [2.x.24]   
* [0.x.78]*
     Standardized data struct to pipe additional flags to the     preconditioner.    
* [0.x.79]*
       Defines the available relaxation types for BoomerAMG.      
* [0.x.80]*
       Constructor. Note that BoomerAMG offers a lot more options to set       than what is exposed here.      
* [0.x.81]*
       Set this flag to true if you have a symmetric system matrix and you       want to use a solver which assumes a symmetric preconditioner like       CG.      
* [0.x.82]*
       Threshold of when nodes are considered strongly connected. See       HYPRE_BoomerAMGSetStrongThreshold(). Recommended values are 0.25 for       2d and 0.5 for 3d problems, but it is problem dependent.      
* [0.x.83]*
       If set to a value smaller than 1.0 then diagonally dominant parts of       the matrix are treated as having no strongly connected nodes. If the       row sum weighted by the diagonal entry is bigger than the given       value, it is considered diagonally dominant. This feature is turned       of by setting the value to 1.0. This is the default as some matrices       can result in having only diagonally dominant entries and thus no       multigrid levels are constructed. The default in BoomerAMG for this       is 0.9. When you try this, check for a reasonable number of levels       created.      
* [0.x.84]*
       Number of levels of aggressive coarsening. Increasing this value       reduces the construction time and memory requirements but may       decrease effectiveness.      
* [0.x.85]*
       Setting this flag to true produces debug output from HYPRE, when the       preconditioner is constructed.      
* [0.x.86]*
       Choose relaxation type up.      
* [0.x.87]*
       Choose relaxation type down.      
* [0.x.88]*
       Choose relaxation type coarse.      
* [0.x.89]*
       Choose number of sweeps on coarse grid.      
* [0.x.90]*
       Choose BommerAMG tolerance.      
* [0.x.91]*
       Choose BommerAMG maximum number of cycles.      
* [0.x.92]*
       Defines whether a w-cycle should be used instead of the standard       setting of a v-cycle.      
* [0.x.93]*
     Empty Constructor. You need to call initialize() before using this     object.    
* [0.x.94]*
     Constructor. Take the matrix which is used to form the preconditioner,     and additional flags if there are any.    
* [0.x.95]*
     Same as above but without setting a matrix to form the preconditioner.     Intended to be used with SLEPc objects.    
* [0.x.96]*
     Initialize the preconditioner object and calculate all data that is     necessary for applying it in a solver. This function is automatically     called when calling the constructor with the same arguments and is only     used if you create the preconditioner without arguments.    
* [0.x.97]*
     Store a copy of the flags for this particular preconditioner.    
* [0.x.98]*
     Initialize the preconditioner object without knowing a particular     matrix. This function sets up appropriate parameters to the underlying     PETSc object after it has been created.    
* [0.x.99]*
   A class that implements the interface to use the ParaSails sparse   approximate inverse preconditioner from the HYPRE suite. Note that PETSc   has to be configured with HYPRE (e.g. with \--download-hypre=1).     ParaSails uses least-squares minimization to compute a sparse approximate   inverse. The sparsity pattern used is the pattern of a power of a   sparsified matrix. ParaSails also uses a post-filtering technique to   reduce the cost of applying the preconditioner.     ParaSails solves symmetric positive definite (SPD) problems using a   factorized SPD preconditioner and can also solve general (nonsymmetric   and/or indefinite) problems with a nonfactorized preconditioner. The   problem type has to be set in  [2.x.25]      The preconditioner does support parallel distributed computations.    
*  [2.x.26]   
* [0.x.100]*
     Standardized data struct to pipe additional flags to the     preconditioner.    
* [0.x.101]*
       Constructor.      
* [0.x.102]*
       This parameter specifies the type of problem to solve:        [2.x.27]         [2.x.28]   [2.x.29]  nonsymmetric and/or indefinite problem, and nonsymmetric       preconditioner        [2.x.30]   [2.x.31]  SPD problem, and SPD (factored) preconditioner        [2.x.32]   [2.x.33]  nonsymmetric, definite problem, and SPD (factored)       preconditioner        [2.x.34]        Default is <tt>symmetric = 1</tt>.      
* [0.x.103]*
       The sparsity pattern used for the approximate inverse is the pattern       of a power <tt>B^m</tt> where <tt>B</tt> has been sparsified from the       given matrix <tt>A</tt>, <tt>n_level</tt> is equal to <tt>m+1</tt>.       Default value is <tt>n_levels = 1</tt>.      
* [0.x.104]*
       Sparsification is performed by dropping nonzeros which are smaller       than <tt>thresh</tt> in magnitude. Lower values of <tt>thresh</tt>       lead to more accurate, but also more expensive preconditioners.       Default value is <tt>thresh = 0.1</tt>. Setting <tt>thresh < 0</tt> a       threshold is selected automatically, such that <tt>-thresh</tt>       represents the fraction of nonzero elements that are dropped. For       example, if <tt>thresh =
* 
*  - .9</tt>, then <tt>B</tt> will contain       about ten percent of the nonzeros of the given matrix <tt>A</tt>.      
* [0.x.105]*
       Filtering is a post-processing procedure, <tt>filter</tt> represents       a fraction of nonzero elements that are dropped after creating the       approximate inverse sparsity pattern. Default value is <tt>filter =       0.05</tt>. Setting <tt>filter < 0</tt> a value is selected       automatically, such that <tt>-filter</tt> represents the fraction of       nonzero elements that are dropped. For example, if <tt>thresh =
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - .9</tt>, then about 90 percent of the entries in the computed       approximate inverse are dropped.      
* [0.x.106]*
       Setting this flag to true produces output from HYPRE, when the       preconditioner is constructed.      
* [0.x.107]*
     Empty Constructor. You need to call initialize() before using this     object.    
* [0.x.108]*
     Constructor. Take the matrix which is used to form the preconditioner,     and additional flags if there are any.    
* [0.x.109]*
     Initialize the preconditioner object and calculate all data that is     necessary for applying it in a solver. This function is automatically     called when calling the constructor with the same arguments and is only     used if you create the preconditioner without arguments.    
* [0.x.110]*
     Store a copy of the flags for this particular preconditioner.    
* [0.x.111]*
   A class that implements a non-preconditioned method.    
*  [2.x.35]   
* [0.x.112]*
     Standardized data struct to pipe additional flags to the     preconditioner.    
* [0.x.113]*
     Empty Constructor. You need to call initialize() before using this     object.    
* [0.x.114]*
     Constructor. Take the matrix which is used to form the preconditioner,     and additional flags if there are any. The matrix is completely ignored     in computations.    
* [0.x.115]*
     Initialize the preconditioner object and calculate all data that is     necessary for applying it in a solver. This function is automatically     called when calling the constructor with the same arguments and is only     used if you create the preconditioner without arguments. The matrix is     completely ignored in computations.    
* [0.x.116]*
     Store a copy of the flags for this particular preconditioner.    
* [0.x.117]*
   Alias for backwards-compatibility.    [2.x.36]  Use  [2.x.37]  instead.  
* [0.x.118]

include/deal.II-translator/lac/petsc_solver_0.txt
[0.x.0]*
   Base class for solver classes using the PETSc solvers. Since solvers in   PETSc are selected based on flags passed to a generic solver object,   basically all the actual solver calls happen in this class, and derived   classes simply set the right flags to select one solver or another, or to   set certain parameters for individual solvers.     Optionally, the user can create a solver derived from the SolverBase   class and can set the default arguments necessary to solve the linear   system of equations with SolverControl. These default options can be   overridden by specifying command line arguments of the form  [2.x.0] 
* 
*  - sp_*.   For example,  [2.x.1] 
* 
*  - sp_monitor_true_residual prints out true residual norm   (unpreconditioned) at each iteration and  [2.x.2] 
* 
*  - sp_view provides   information about the linear solver and the preconditioner used in the   current context. The type of the solver can also be changed during   runtime by specifying  [2.x.3] 
* 
*  - sp_type {richardson, cg, gmres, fgmres, ..} to   dynamically test the optimal solver along with a suitable preconditioner   set using  [2.x.4] 
* 
*  - c_type {jacobi, bjacobi, ilu, lu, ..}. There are several   other command line options available to modify the behavior of the PETSc   linear solver and can be obtained from the [1.x.0].    
*  [2.x.5]  Repeated calls to solve() on a solver object with a Preconditioner   must be used with care. The preconditioner is initialized in the first   call to solve() and subsequent calls reuse the solver and preconditioner   object. This is done for performance reasons. The solver and   preconditioner can be reset by calling reset().     One of the gotchas of PETSc is that
* 
*  -  in particular in MPI mode
* 
*  -  it   often does not produce very helpful error messages. In order to save   other users some time in searching a hard to track down error, here is   one situation and the error message one gets there: when you don't   specify an MPI communicator to your solver's constructor. In this case,   you will get an error of the following form from each of your parallel   processes:  
* [1.x.1]
*      This error, on which one can spend a very long time figuring out what   exactly goes wrong, results from not specifying an MPI communicator. Note   that the communicator  [2.x.6]  must match that of the matrix and all vectors   in the linear system which we want to solve. Aggravating the situation is   the fact that the default argument to the solver classes,  [2.x.7]    PETSC_COMM_SELF, is the appropriate argument for the sequential case   (which is why it is the default argument), so this error only shows up in   parallel mode.    
*  [2.x.8]   
* [0.x.1]*
     Constructor. Takes the solver control object and the MPI communicator     over which parallel computations are to happen.         Note that the communicator used here must match the communicator used     in the system matrix, solution, and right hand side object of the solve     to be done with this solver. Otherwise, PETSc will generate hard to     track down errors, see the documentation of the SolverBase class.    
* [0.x.2]*
     Destructor.    
* [0.x.3]*
     Solve the linear system <tt>Ax=b</tt>. Depending on the information     provided by derived classes and the object passed as a preconditioner,     one of the linear solvers and preconditioners of PETSc is chosen.     Repeated calls to solve() do not reconstruct the preconditioner for     performance reasons. See class Documentation.    
* [0.x.4]*
     Resets the contained preconditioner and solver object. See class     description for more details.    
* [0.x.5]*
     Sets a prefix name for the solver object. Useful when customizing the     PETSc KSP object with command-line options.    
* [0.x.6]*
     Access to object that controls convergence.    
* [0.x.7]*
     initialize the solver with the preconditioner. This function is     intended for use with SLEPc spectral transformation class.    
* [0.x.8]*
     Reference to the object that controls convergence of the iterative     solver. In fact, for these PETSc wrappers, PETSc does so itself, but we     copy the data from this object before starting the solution process,     and copy the data back into it afterwards.    
* [0.x.9]*
     Copy of the MPI communicator object to be used for the solver.    
* [0.x.10]*
     %Function that takes a Krylov Subspace Solver context object, and sets     the type of solver that is requested by the derived class.    
* [0.x.11]*
     Solver prefix name to qualify options specific to the PETSc KSP object     in the current context. Note: A hyphen (-) must NOT be given at the     beginning of the prefix name. The first character of all runtime     options is AUTOMATICALLY the hyphen.    
* [0.x.12]*
     A function that is used in PETSc as a callback to check on convergence.     It takes the information provided from PETSc and checks it against     deal.II's own SolverControl objects to see if convergence has been     reached.    
* [0.x.13]*
     A structure that contains the PETSc solver and preconditioner objects.     This object is preserved between subsequent calls to the solver if the     same preconditioner is used as in the previous solver step. This may     save some computation time, if setting up a preconditioner is     expensive, such as in the case of an ILU for example.         The actual declaration of this class is complicated by the fact that     PETSc changed its solver interface completely and incompatibly between     versions 2.1.6 and 2.2.0 :-(         Objects of this type are explicitly created, but are destroyed when the     surrounding solver object goes out of scope, or when we assign a new     value to the pointer to this object. The respectiveDestroy functions     are therefore written into the destructor of this object, even though     the object does not have a constructor.    
* [0.x.14]*
       Destructor      
* [0.x.15]*
       Object for Krylov subspace solvers.      
* [0.x.16]*
     Pointer to an object that stores the solver context. This is recreated     in the main solver routine if necessary.    
* [0.x.17]*
   An implementation of the solver interface using the PETSc Richardson   solver.    
*  [2.x.9]   
* [0.x.18]*
     Standardized data struct to pipe additional data to the solver.    
* [0.x.19]*
       Constructor. By default, set the damping parameter to one.      
* [0.x.20]*
       Relaxation parameter.      
* [0.x.21]*
     Constructor. In contrast to deal.II's own solvers, there is no need to     give a vector memory object. However, PETSc solvers want to have an MPI     communicator context over which computations are parallelized. By     default,  [2.x.10]  is used here, but you can change this. Note     that for single processor (non-MPI) versions, this parameter does not     have any effect.         The last argument takes a structure with additional, solver dependent     flags for tuning.         Note that the communicator used here must match the communicator used     in the system matrix, solution, and right hand side object of the solve     to be done with this solver. Otherwise, PETSc will generate hard to     track down errors, see the documentation of the SolverBase class.    
* [0.x.22]*
     Store a copy of the flags for this particular solver.    
* [0.x.23]*
     %Function that takes a Krylov Subspace Solver context object, and sets     the type of solver that is appropriate for this class.    
* [0.x.24]*
   An implementation of the solver interface using the PETSc Chebyshev (or,   prior version 3.3, Chebychev) solver.    
*  [2.x.11]   
* [0.x.25]*
     Standardized data struct to pipe additional data to the solver.    
* [0.x.26]*
     Constructor. In contrast to deal.II's own solvers, there is no need to     give a vector memory object. However, PETSc solvers want to have an MPI     communicator context over which computations are parallelized. By     default,  [2.x.12]  is used here, but you can change this. Note     that for single processor (non-MPI) versions, this parameter does not     have any effect.         The last argument takes a structure with additional, solver dependent     flags for tuning.         Note that the communicator used here must match the communicator used     in the system matrix, solution, and right hand side object of the solve     to be done with this solver. Otherwise, PETSc will generate hard to     track down errors, see the documentation of the SolverBase class.    
* [0.x.27]*
     Store a copy of the flags for this particular solver.    
* [0.x.28]*
     %Function that takes a Krylov Subspace Solver context object, and sets     the type of solver that is appropriate for this class.    
* [0.x.29]*
   An implementation of the solver interface using the PETSc CG solver.    
*  [2.x.13]   
* [0.x.30]*
     Standardized data struct to pipe additional data to the solver.    
* [0.x.31]*
     Constructor. In contrast to deal.II's own solvers, there is no need to     give a vector memory object. However, PETSc solvers want to have an MPI     communicator context over which computations are parallelized. By     default,  [2.x.14]  is used here, but you can change this. Note     that for single processor (non-MPI) versions, this parameter does not     have any effect.         The last argument takes a structure with additional, solver dependent     flags for tuning.         Note that the communicator used here must match the communicator used     in the system matrix, solution, and right hand side object of the solve     to be done with this solver. Otherwise, PETSc will generate hard to     track down errors, see the documentation of the SolverBase class.    
* [0.x.32]*
     Store a copy of the flags for this particular solver.    
* [0.x.33]*
     %Function that takes a Krylov Subspace Solver context object, and sets     the type of solver that is appropriate for this class.    
* [0.x.34]*
   An implementation of the solver interface using the PETSc BiCG solver.    
*  [2.x.15]   
* [0.x.35]*
     Standardized data struct to pipe additional data to the solver.    
* [0.x.36]*
     Constructor. In contrast to deal.II's own solvers, there is no need to     give a vector memory object. However, PETSc solvers want to have an MPI     communicator context over which computations are parallelized. By     default,  [2.x.16]  is used here, but you can change this. Note     that for single processor (non-MPI) versions, this parameter does not     have any effect.         The last argument takes a structure with additional, solver dependent     flags for tuning.         Note that the communicator used here must match the communicator used     in the system matrix, solution, and right hand side object of the solve     to be done with this solver. Otherwise, PETSc will generate hard to     track down errors, see the documentation of the SolverBase class.    
* [0.x.37]*
     Store a copy of the flags for this particular solver.    
* [0.x.38]*
     %Function that takes a Krylov Subspace Solver context object, and sets     the type of solver that is appropriate for this class.    
* [0.x.39]*
   An implementation of the solver interface using the PETSc GMRES solver.    
*  [2.x.17]   
* [0.x.40]*
     Standardized data struct to pipe additional data to the solver.    
* [0.x.41]*
       Constructor. By default, set the number of temporary vectors to 30,       i.e. do a restart every 30 iterations.      
* [0.x.42]*
       Maximum number of tmp vectors.      
* [0.x.43]*
       Flag for right preconditioning.      
* [0.x.44]*
     Constructor. In contrast to deal.II's own solvers, there is no need to     give a vector memory object. However, PETSc solvers want to have an MPI     communicator context over which computations are parallelized. By     default,  [2.x.18]  is used here, but you can change this. Note     that for single processor (non-MPI) versions, this parameter does not     have any effect.         The last argument takes a structure with additional, solver dependent     flags for tuning.         Note that the communicator used here must match the communicator used     in the system matrix, solution, and right hand side object of the solve     to be done with this solver. Otherwise, PETSc will generate hard to     track down errors, see the documentation of the SolverBase class.    
* [0.x.45]*
     Store a copy of the flags for this particular solver.    
* [0.x.46]*
     %Function that takes a Krylov Subspace Solver context object, and sets     the type of solver that is appropriate for this class.    
* [0.x.47]*
   An implementation of the solver interface using the PETSc BiCGStab   solver.    
*  [2.x.19]   
* [0.x.48]*
     Standardized data struct to pipe additional data to the solver.    
* [0.x.49]*
     Constructor. In contrast to deal.II's own solvers, there is no need to     give a vector memory object. However, PETSc solvers want to have an MPI     communicator context over which computations are parallelized. By     default,  [2.x.20]  is used here, but you can change this. Note     that for single processor (non-MPI) versions, this parameter does not     have any effect.         The last argument takes a structure with additional, solver dependent     flags for tuning.         Note that the communicator used here must match the communicator used     in the system matrix, solution, and right hand side object of the solve     to be done with this solver. Otherwise, PETSc will generate hard to     track down errors, see the documentation of the SolverBase class.    
* [0.x.50]*
     Store a copy of the flags for this particular solver.    
* [0.x.51]*
     %Function that takes a Krylov Subspace Solver context object, and sets     the type of solver that is appropriate for this class.    
* [0.x.52]*
   An implementation of the solver interface using the PETSc CG Squared   solver.    
*  [2.x.21]   
* [0.x.53]*
     Standardized data struct to pipe additional data to the solver.    
* [0.x.54]*
     Constructor. In contrast to deal.II's own solvers, there is no need to     give a vector memory object. However, PETSc solvers want to have an MPI     communicator context over which computations are parallelized. By     default,  [2.x.22]  is used here, but you can change this. Note     that for single processor (non-MPI) versions, this parameter does not     have any effect.         The last argument takes a structure with additional, solver dependent     flags for tuning.         Note that the communicator used here must match the communicator used     in the system matrix, solution, and right hand side object of the solve     to be done with this solver. Otherwise, PETSc will generate hard to     track down errors, see the documentation of the SolverBase class.    
* [0.x.55]*
     Store a copy of the flags for this particular solver.    
* [0.x.56]*
     %Function that takes a Krylov Subspace Solver context object, and sets     the type of solver that is appropriate for this class.    
* [0.x.57]*
   An implementation of the solver interface using the PETSc TFQMR solver.    
*  [2.x.23]   
* [0.x.58]*
     Standardized data struct to pipe additional data to the solver.    
* [0.x.59]*
     Constructor. In contrast to deal.II's own solvers, there is no need to     give a vector memory object. However, PETSc solvers want to have an MPI     communicator context over which computations are parallelized. By     default,  [2.x.24]  is used here, but you can change this. Note     that for single processor (non-MPI) versions, this parameter does not     have any effect.         The last argument takes a structure with additional, solver dependent     flags for tuning.         Note that the communicator used here must match the communicator used     in the system matrix, solution, and right hand side object of the solve     to be done with this solver. Otherwise, PETSc will generate hard to     track down errors, see the documentation of the SolverBase class.    
* [0.x.60]*
     Store a copy of the flags for this particular solver.    
* [0.x.61]*
     %Function that takes a Krylov Subspace Solver context object, and sets     the type of solver that is appropriate for this class.    
* [0.x.62]*
   An implementation of the solver interface using the PETSc TFQMR-2 solver   (called TCQMR in PETSc). Note that this solver had a serious bug in   versions up to and including PETSc 2.1.6, in that it did not check   convergence and always returned an error code. Thus, this class will   abort with an error indicating failure to converge with PETSc 2.1.6 and   prior. This should be fixed in later versions of PETSc, though.    
*  [2.x.25]   
* [0.x.63]*
     Standardized data struct to pipe additional data to the solver.    
* [0.x.64]*
     Constructor. In contrast to deal.II's own solvers, there is no need to     give a vector memory object. However, PETSc solvers want to have an MPI     communicator context over which computations are parallelized. By     default,  [2.x.26]  is used here, but you can change this. Note     that for single processor (non-MPI) versions, this parameter does not     have any effect.         The last argument takes a structure with additional, solver dependent     flags for tuning.         Note that the communicator used here must match the communicator used     in the system matrix, solution, and right hand side object of the solve     to be done with this solver. Otherwise, PETSc will generate hard to     track down errors, see the documentation of the SolverBase class.    
* [0.x.65]*
     Store a copy of the flags for this particular solver.    
* [0.x.66]*
     %Function that takes a Krylov Subspace Solver context object, and sets     the type of solver that is appropriate for this class.    
* [0.x.67]*
   An implementation of the solver interface using the PETSc CR solver.    
*  [2.x.27]   
* [0.x.68]*
     Standardized data struct to pipe additional data to the solver.    
* [0.x.69]*
     Constructor. In contrast to deal.II's own solvers, there is no need to     give a vector memory object. However, PETSc solvers want to have an MPI     communicator context over which computations are parallelized. By     default,  [2.x.28]  is used here, but you can change this. Note     that for single processor (non-MPI) versions, this parameter does not     have any effect.         The last argument takes a structure with additional, solver dependent     flags for tuning.         Note that the communicator used here must match the communicator used     in the system matrix, solution, and right hand side object of the solve     to be done with this solver. Otherwise, PETSc will generate hard to     track down errors, see the documentation of the SolverBase class.    
* [0.x.70]*
     Store a copy of the flags for this particular solver.    
* [0.x.71]*
     %Function that takes a Krylov Subspace Solver context object, and sets     the type of solver that is appropriate for this class.    
* [0.x.72]*
   An implementation of the solver interface using the PETSc Least Squares   solver.    
*  [2.x.29]   
* [0.x.73]*
     Standardized data struct to pipe additional data to the solver.    
* [0.x.74]*
     Constructor. In contrast to deal.II's own solvers, there is no need to     give a vector memory object. However, PETSc solvers want to have an MPI     communicator context over which computations are parallelized. By     default,  [2.x.30]  is used here, but you can change this. Note     that for single processor (non-MPI) versions, this parameter does not     have any effect.         The last argument takes a structure with additional, solver dependent     flags for tuning.         Note that the communicator used here must match the communicator used     in the system matrix, solution, and right hand side object of the solve     to be done with this solver. Otherwise, PETSc will generate hard to     track down errors, see the documentation of the SolverBase class.    
* [0.x.75]*
     Store a copy of the flags for this particular solver.    
* [0.x.76]*
     %Function that takes a Krylov Subspace Solver context object, and sets     the type of solver that is appropriate for this class.    
* [0.x.77]*
   An implementation of the solver interface using the PETSc PREONLY solver.   Actually this is NOT a real solution algorithm. solve() only applies the   preconditioner once and returns immediately. Its only purpose is to   provide a solver object, when the preconditioner should be used as a real   solver. It is very useful in conjunction with the complete LU   decomposition preconditioner <tt> PreconditionLU </tt>, which in   conjunction with this solver class becomes a direct solver.    
*  [2.x.31]   
* [0.x.78]*
     Standardized data struct to pipe additional data to the solver.    
* [0.x.79]*
     Constructor. In contrast to deal.II's own solvers, there is no need to     give a vector memory object. However, PETSc solvers want to have an MPI     communicator context over which computations are parallelized. By     default,  [2.x.32]  is used here, but you can change this. Note     that for single processor (non-MPI) versions, this parameter does not     have any effect.         The last argument takes a structure with additional, solver dependent     flags for tuning.         Note that the communicator used here must match the communicator used     in the system matrix, solution, and right hand side object of the solve     to be done with this solver. Otherwise, PETSc will generate hard to     track down errors, see the documentation of the SolverBase class.    
* [0.x.80]*
     Store a copy of the flags for this particular solver.    
* [0.x.81]*
     %Function that takes a Krylov Subspace Solver context object, and sets     the type of solver that is appropriate for this class.    
* [0.x.82]*
   An implementation of the solver interface using the sparse direct MUMPS   solver through PETSc. This class has the usual interface of all other   solver classes but it is of course different in that it doesn't implement   an iterative solver. As a consequence, things like the SolverControl   object have no particular meaning here.     MUMPS allows to make use of symmetry in this matrix. In this class this   is made possible by the set_symmetric_mode() function. If your matrix is   symmetric, you can use this class as follows:  
* [1.x.2]
*     
*  [2.x.33]  The class internally calls KSPSetFromOptions thus you are able to   use all the PETSc parameters for MATSOLVERMUMPS package. See   http://www.mcs.anl.gov/petsc/petsc-current/docs/manualpages/Mat/MATSOLVERMUMPS.html    
*  [2.x.34]   
* [0.x.83]*
     Standardized data structure to pipe additional data to the solver.    
* [0.x.84]*
     Constructor    
* [0.x.85]*
     The method to solve the linear system.    
* [0.x.86]*
     The method allows to take advantage if the system matrix is symmetric     by using LDL^T decomposition instead of more expensive LU. The argument     indicates whether the matrix is symmetric or not.    
* [0.x.87]*
     Store a copy of flags for this particular solver.    
* [0.x.88]*
     A function that is used in PETSc as a callback to check convergence. It     takes the information provided from PETSc and checks it against     deal.II's own SolverControl objects to see if convergence has been     reached.    
* [0.x.89]*
     A structure that contains the PETSc solver and preconditioner objects.     Since the solve member function in the base is not used here, the     private SolverData struct located in the base could not be used either.    
* [0.x.90]*
       Destructor      
* [0.x.91]*
     Flag specifies whether matrix being factorized is symmetric or not. It     influences the type of the used preconditioner (PCLU or PCCHOLESKY)    
* [0.x.92]

include/deal.II-translator/lac/petsc_sparse_matrix_0.txt
[0.x.0]*
   Implementation of a sequential sparse matrix class based on PETSc. All   the functionality is actually in the base class, except for the calls to   generate a sequential sparse matrix. This is possible since PETSc only   works on an abstract matrix type and internally distributes to functions   that do the actual work depending on the actual matrix type (much like   using virtual functions). Only the functions creating a matrix of   specific type differ, and are implemented in this particular class.    
*  [2.x.0]   
*  [2.x.1]   
* [0.x.1]*
     A structure that describes some of the traits of this class in terms of     its run-time behavior. Some other classes (such as the block matrix     classes) that take one or other of the matrix classes as its template     parameters can tune their behavior based on the variables in this     class.    
* [0.x.2]*
       It is safe to elide additions of zeros to individual elements of this       matrix.      
* [0.x.3]*
     Default constructor. Create an empty matrix.    
* [0.x.4]*
     Create a sparse matrix of dimensions  [2.x.2]  times  [2.x.3]  with an initial     guess of  [2.x.4]  nonzero elements per row. PETSc is able     to cope with the situation that more than this number of elements is     later allocated for a row, but this involves copying data, and is thus     expensive.         The  [2.x.5]  flag determines whether we should tell PETSc that     the matrix is going to be symmetric (as indicated by the call     <tt>MatSetOption(mat, MAT_SYMMETRIC)</tt>. Note that the PETSc     documentation states that one cannot form an ILU decomposition of a     matrix for which this flag has been set to  [2.x.6]  only an ICC. The     default value of this flag is  [2.x.7]     
* [0.x.5]*
     Initialize a rectangular matrix with  [2.x.8]  rows and  [2.x.9]  columns.  The     maximal number of nonzero entries for each row separately is given by     the  [2.x.10]  array.         Just as for the other constructors: PETSc is able to cope with the     situation that more than this number of elements is later allocated for     a row, but this involves copying data, and is thus expensive.         The  [2.x.11]  flag determines whether we should tell PETSc that     the matrix is going to be symmetric (as indicated by the call     <tt>MatSetOption(mat, MAT_SYMMETRIC)</tt>. Note that the PETSc     documentation states that one cannot form an ILU decomposition of a     matrix for which this flag has been set to  [2.x.12]  only an ICC. The     default value of this flag is  [2.x.13]     
* [0.x.6]*
     Initialize a sparse matrix using the given sparsity pattern.         Note that PETSc can be very slow if you do not provide it with a good     estimate of the lengths of rows. Using the present function is a very     efficient way to do this, as it uses the exact number of nonzero     entries for each row of the matrix by using the given sparsity pattern     argument. If the  [2.x.14]  flag is  [2.x.15]  this     function in addition not only sets the correct row sizes up front, but     also pre-allocated the correct nonzero entries in the matrix.         PETsc allows to later add additional nonzero entries to a matrix, by     simply writing to these elements. However, this will then lead to     additional memory allocations which are very inefficient and will     greatly slow down your program. It is therefore significantly more     efficient to get memory allocation right from the start.    
* [0.x.7]*
     This operator assigns a scalar to a matrix. Since this does usually not     make much sense (should we set all matrix entries to this value? Only     the nonzero entries of the sparsity pattern?), this operation is only     allowed if the actual value to be assigned is zero. This operator only     exists to allow for the obvious notation <tt>matrix=0</tt>, which sets     all elements of the matrix to zero, but keep the sparsity pattern     previously used.    
* [0.x.8]*
     The copy constructor is deleted.    
* [0.x.9]*
     The copy assignment operator is deleted.    
* [0.x.10]*
     Throw away the present matrix and generate one that has the same     properties as if it were created by the constructor of this class with     the same argument list as the present function.    
* [0.x.11]*
     Throw away the present matrix and generate one that has the same     properties as if it were created by the constructor of this class with     the same argument list as the present function.    
* [0.x.12]*
     Initialize a sparse matrix using the given sparsity pattern.         Note that PETSc can be very slow if you do not provide it with a good     estimate of the lengths of rows. Using the present function is a very     efficient way to do this, as it uses the exact number of nonzero     entries for each row of the matrix by using the given sparsity pattern     argument. If the  [2.x.16]  flag is  [2.x.17]  this     function in addition not only sets the correct row sizes up front, but     also pre-allocated the correct nonzero entries in the matrix.         PETsc allows to later add additional nonzero entries to a matrix, by     simply writing to these elements. However, this will then lead to     additional memory allocations which are very inefficient and will     greatly slow down your program. It is therefore significantly more     efficient to get memory allocation right from the start.         Despite the fact that it would seem to be an obvious win, setting the      [2.x.18]  flag to  [2.x.19]  doesn't seem to accelerate     program. Rather on the contrary, it seems to be able to slow down     entire programs somewhat. This is surprising, since we can use     efficient function calls into PETSc that allow to create multiple     entries at once; nevertheless, given the fact that it is inefficient,     the respective flag has a default value equal to  [2.x.20]     
* [0.x.13]*
     Return a reference to the MPI communicator object in use with this     matrix. Since this is a sequential matrix, it returns the MPI_COMM_SELF     communicator.    
* [0.x.14]*
     Return the number of rows of this matrix.    
* [0.x.15]*
     Return the number of columns of this matrix.    
* [0.x.16]*
     Perform the matrix-matrix multiplication  [2.x.21] , or,      [2.x.22]  given a compatible vector  [2.x.23] .         This function calls  [2.x.24]  to do the actual work.    
* [0.x.17]*
     Perform the matrix-matrix multiplication with the transpose of     <tt>this</tt>, i.e.,  [2.x.25] , or,      [2.x.26]  given a compatible vector  [2.x.27] .         This function calls  [2.x.28]  to do the actual work.    
* [0.x.18]*
     Do the actual work for the respective reinit() function and the     matching constructor, i.e. create a matrix. Getting rid of the previous     matrix is left to the caller.    
* [0.x.19]*
     Same as previous function.    
* [0.x.20]*
     Same as previous function.    
* [0.x.21]*
     Implementation of a parallel sparse matrix class based on PETSc, with     rows of the matrix distributed across an MPI network. All the     functionality is actually in the base class, except for the calls to     generate a parallel sparse matrix. This is possible since PETSc only     works on an abstract matrix type and internally distributes to     functions that do the actual work depending on the actual matrix type     (much like using virtual functions). Only the functions creating a     matrix of specific type differ, and are implemented in this particular     class.         There are a number of comments on the communication model as well as     access to individual elements in the documentation to the parallel     vector class. These comments apply here as well.             [1.x.0]         PETSc partitions parallel matrices so that each MPI process "owns" a     certain number of rows (i.e. only this process stores the respective     entries in these rows). The number of rows each process owns has to be     passed to the constructors and reinit() functions via the argument  [2.x.29]      local_rows. The individual values passed as  [2.x.30]  on all the     MPI processes of course have to add up to the global number of rows of     the matrix.         In addition to this, PETSc also partitions the rectangular chunk of the     matrix it owns (i.e. the  [2.x.31]  times n() elements in the     matrix), so that matrix vector multiplications can be performed     efficiently. This column-partitioning therefore has to match the     partitioning of the vectors with which the matrix is multiplied, just     as the row-partitioning has to match the partitioning of destination     vectors. This partitioning is passed to the constructors and reinit()     functions through the  [2.x.32]  variable, which again has to add     up to the global number of columns in the matrix. The name  [2.x.33]      local_columns may be named inappropriately since it does not reflect     that only these columns are stored locally, but it reflects the fact     that these are the columns for which the elements of incoming vectors     are stored locally.         To make things even more complicated, PETSc needs a very good estimate     of the number of elements to be stored in each row to be efficient.     Otherwise it spends most of the time with allocating small chunks of     memory, a process that can slow down programs to a crawl if it happens     to often. As if a good estimate of the number of entries per row isn't     even, it even needs to split this as follows: for each row it owns, it     needs an estimate for the number of elements in this row that fall into     the columns that are set apart for this process (see above), and the     number of elements that are in the rest of the columns.         Since in general this information is not readily available, most of the     initializing functions of this class assume that all of the number of     elements you give as an argument to  [2.x.34]  or by  [2.x.35]      row_lengths fall into the columns "owned" by this process, and none     into the other ones. This is a fair guess for most of the rows, since     in a good domain partitioning, nodes only interact with nodes that are     within the same subdomain. It does not hold for nodes on the interfaces     of subdomain, however, and for the rows corresponding to these nodes,     PETSc will have to allocate additional memory, a costly process.         The only way to avoid this is to tell PETSc where the actual entries of     the matrix will be. For this, there are constructors and reinit()     functions of this class that take a DynamicSparsityPattern object     containing all this information. While in the general case it is     sufficient if the constructors and reinit() functions know the number     of local rows and columns, the functions getting a sparsity pattern     also need to know the number of local rows ( [2.x.36]      and columns ( [2.x.37]  for all other processes, in     order to compute which parts of the matrix are which. Thus, it is not     sufficient to just count the number of degrees of freedom that belong     to a particular process, but you have to have the numbers for all     processes available at all processes.        
*  [2.x.38]     
*  [2.x.39]     
* [0.x.22]*
       Declare type for container size.      
* [0.x.23]*
       A structure that describes some of the traits of this class in terms       of its run-time behavior. Some other classes (such as the block       matrix classes) that take one or other of the matrix classes as its       template parameters can tune their behavior based on the variables in       this class.      
* [0.x.24]*
         It is not safe to elide additions of zeros to individual elements         of this matrix. The reason is that additions to the matrix may         trigger collective operations synchronizing buffers on multiple         processes. If an addition is elided on one process, this may lead         to other processes hanging in an infinite waiting loop.        
* [0.x.25]*
       Default constructor. Create an empty matrix.      
* [0.x.26]*
       Destructor to free the PETSc object.      
* [0.x.27]*
       Initialize using the given sparsity pattern with communication       happening over the provided  [2.x.40]              For the meaning of the  [2.x.41]  and  [2.x.42]        local_columns_per_process parameters, see the class documentation.             Note that PETSc can be very slow if you do not provide it with a good       estimate of the lengths of rows. Using the present function is a very       efficient way to do this, as it uses the exact number of nonzero       entries for each row of the matrix by using the given sparsity       pattern argument. If the  [2.x.43]  flag is  [2.x.44]        this function in addition not only sets the correct row sizes up       front, but also pre-allocated the correct nonzero entries in the       matrix.             PETsc allows to later add additional nonzero entries to a matrix, by       simply writing to these elements. However, this will then lead to       additional memory allocations which are very inefficient and will       greatly slow down your program. It is therefore significantly more       efficient to get memory allocation right from the start.      
* [0.x.28]*
       This operator assigns a scalar to a matrix. Since this does usually       not make much sense (should we set all matrix entries to this value?       Only the nonzero entries of the sparsity pattern?), this operation is       only allowed if the actual value to be assigned is zero. This       operator only exists to allow for the obvious notation       <tt>matrix=0</tt>, which sets all elements of the matrix to zero, but       keep the sparsity pattern previously used.      
* [0.x.29]*
       Make a copy of the PETSc matrix  [2.x.45]  It is assumed that both       matrices have the same SparsityPattern.      
* [0.x.30]*
       Initialize using the given sparsity pattern with communication       happening over the provided  [2.x.46]              Note that PETSc can be very slow if you do not provide it with a good       estimate of the lengths of rows. Using the present function is a very       efficient way to do this, as it uses the exact number of nonzero       entries for each row of the matrix by using the given sparsity       pattern argument. If the  [2.x.47]  flag is  [2.x.48]        this function in addition not only sets the correct row sizes up       front, but also pre-allocated the correct nonzero entries in the       matrix.             PETsc allows to later add additional nonzero entries to a matrix, by       simply writing to these elements. However, this will then lead to       additional memory allocations which are very inefficient and will       greatly slow down your program. It is therefore significantly more       efficient to get memory allocation right from the start.      
* [0.x.31]*
       Create a matrix where the size() of the IndexSets determine the       global number of rows and columns and the entries of the IndexSet       give the rows and columns for the calling processor. Note that only       ascending, 1:1 IndexSets are supported.      
* [0.x.32]*
       Initialize this matrix to have the same structure as  [2.x.49]  This       will not copy the values of the other matrix, but you can use       copy_from() for this.      
* [0.x.33]*
       Return a reference to the MPI communicator object in use with this       matrix.      
* [0.x.34]*
        [2.x.50]  Exceptions        [2.x.51]       
* [0.x.35]*
       Exception      
* [0.x.36]*
       Return the square of the norm of the vector  [2.x.52]  with respect to the       norm induced by this matrix, i.e.  [2.x.53] . This is       useful, e.g. in the finite element context, where the  [2.x.54]  norm of a       function equals the matrix norm with respect to the mass matrix of       the vector representing the nodal values of the finite element       function.             Obviously, the matrix needs to be quadratic for this operation.             The implementation of this function is not as efficient as the one in       the  [2.x.55]  class used in deal.II (i.e. the original one, not       the PETSc wrapper class) since PETSc doesn't support this operation       and needs a temporary vector.      
* [0.x.37]*
       Compute the matrix scalar product  [2.x.56] .             The implementation of this function is not as efficient as the one in       the  [2.x.57]  class used in deal.II (i.e. the original one, not       the PETSc wrapper class) since PETSc doesn't support this operation       and needs a temporary vector.      
* [0.x.38]*
       Return the partitioning of the domain space of this matrix, i.e., the       partitioning of the vectors this matrix has to be multiplied with.      
* [0.x.39]*
       Return the partitioning of the range space of this matrix, i.e., the       partitioning of the vectors that result from matrix-vector       products.      
* [0.x.40]*
       Perform the matrix-matrix multiplication  [2.x.58] , or,        [2.x.59]  given a compatible vector  [2.x.60] .             This function calls  [2.x.61]  to do the actual work.      
* [0.x.41]*
       Perform the matrix-matrix multiplication with the transpose of       <tt>this</tt>, i.e.,  [2.x.62] , or,        [2.x.63]  given a compatible vector  [2.x.64] .             This function calls  [2.x.65]  to do the actual work.      
* [0.x.42]*
       Copy of the communicator object to be used for this parallel vector.      
* [0.x.43]*
       Same as previous functions.      
* [0.x.44]*
       Same as previous functions.      
* [0.x.45]

include/deal.II-translator/lac/petsc_vector_0.txt
[0.x.0]!  [2.x.0]  PETScWrappers [2.x.1] 

* 
* [0.x.1]*
   Namespace for PETSc classes that work in parallel over MPI, such as   distributed vectors and matrices.    
*  [2.x.2]   
* [0.x.2]*
     Implementation of a parallel vector class based on PETSC and using MPI     communication to synchronize distributed operations. All the     functionality is actually in the base class, except for the calls to     generate a parallel vector. This is possible since PETSc only works on     an abstract vector type and internally distributes to functions that do     the actual work depending on the actual vector type (much like using     virtual functions). Only the functions creating a vector of specific     type differ, and are implemented in this particular class.             [1.x.0]         The parallel functionality of PETSc is built on top of the Message     Passing Interface (MPI). MPI's communication model is built on     collective communications: if one process wants something from another,     that other process has to be willing to accept this communication. A     process cannot query data from another process by calling a remote     function, without that other process expecting such a transaction. The     consequence is that most of the operations in the base class of this     class have to be called collectively. For example, if you want to     compute the l2 norm of a parallel vector,  [2.x.3]  all processes across     which this vector is shared have to call the  [2.x.4]  function. If     you don't do this, but instead only call the  [2.x.5]  function on one     process, then the following happens: This one process will call one of     the collective MPI functions and wait for all the other processes to     join in on this. Since the other processes don't call this function,     you will either get a time-out on the first process, or, worse, by the     time the next a call to a PETSc function generates an MPI message on     the other processes, you will get a cryptic message that only a subset     of processes attempted a communication. These bugs can be very hard to     figure out, unless you are well-acquainted with the communication model     of MPI, and know which functions may generate MPI messages.         One particular case, where an MPI message may be generated unexpectedly     is discussed below.             [1.x.1]         PETSc does allow read access to individual elements of a vector, but in     the distributed case only to elements that are stored locally. We     implement this through calls like <tt>d=vec(i)</tt>. However, if you     access an element outside the locally stored range, an exception is     generated.         In contrast to read access, PETSc (and the respective deal.II wrapper     classes) allow to write (or add) to individual elements of vectors,     even if they are stored on a different process. You can do this     writing, for example, <tt>vec(i)=d</tt> or <tt>vec(i)+=d</tt>, or     similar operations. There is one catch, however, that may lead to very     confusing error messages: PETSc requires application programs to call     the compress() function when they switch from adding, to elements to     writing to elements. The reasoning is that all processes might     accumulate addition operations to elements, even if multiple processes     write to the same elements. By the time we call compress() the next     time, all these additions are executed. However, if one process adds to     an element, and another overwrites to it, the order of execution would     yield non-deterministic behavior if we don't make sure that a     synchronization with compress() happens in between.         In order to make sure these calls to compress() happen at the     appropriate time, the deal.II wrappers keep a state variable that store     which is the presently allowed operation: additions or writes. If it     encounters an operation of the opposite kind, it calls compress() and     flips the state. This can sometimes lead to very confusing behavior, in     code that may for example look like this:    
* [1.x.2]
*          This code can run into trouble: by the time we see the first addition     operation, we need to flush the overwrite buffers for the vector, and     the deal.II library will do so by calling compress(). However, it will     only do so for all processes that actually do an addition
* 
*  -  if the     condition is never true for one of the processes, then this one will     not get to the actual compress() call, whereas all the other ones do.     This gets us into trouble, since all the other processes hang in the     call to flush the write buffers, while the one other process advances     to the call to compute the l2 norm. At this time, you will get an error     that some operation was attempted by only a subset of processes. This     behavior may seem surprising, unless you know that write/addition     operations on single elements may trigger this behavior.         The problem described here may be avoided by placing additional calls     to compress(), or making sure that all processes do the same type of     operations at the same time, for example by placing zero additions if     necessary.          [2.x.6]       [2.x.7]  "vectors with ghost elements"        
*  [2.x.8]     
*  [2.x.9]     
* [0.x.3]*
       Declare type for container size.      
* [0.x.4]*
       Default constructor. Initialize the vector as empty.      
* [0.x.5]*
       Constructor. Set dimension to  [2.x.10]  and initialize all elements with       zero.              [2.x.11]  locally_owned_size denotes the size of the chunk that shall be       stored on the present process.              [2.x.12]  communicator denotes the MPI communicator over which the       different parts of the vector shall communicate             The constructor is made explicit to avoid accidents like this:       <tt>v=0;</tt>. Presumably, the user wants to set every element of the       vector to zero, but instead, what happens is this call:       <tt>v=Vector [2.x.13]  i.e. the vector is replaced by one       of length zero.      
* [0.x.6]*
       Copy-constructor from deal.II vectors. Sets the dimension to that of       the given vector, and copies all elements.              [2.x.14]  locally_owned_size denotes the size of the chunk that shall be       stored on the present process.              [2.x.15]  communicator denotes the MPI communicator over which the       different parts of the vector shall communicate      
* [0.x.7]*
       Copy-constructor the values from a PETSc wrapper vector class.              [2.x.16]  local_size denotes the size of the chunk that shall be stored on       the present process.              [2.x.17]  communicator denotes the MPI communicator over which the       different parts of the vector shall communicate              [2.x.18]  The use of objects that are explicitly of type VectorBase       is deprecated: use  [2.x.19]  instead.      
* [0.x.8]*
       Construct a new parallel ghosted PETSc vector from IndexSets.             Note that  [2.x.20]  must be ascending and 1:1, see        [2.x.21]   In particular, the DoFs in        [2.x.22]  need to be contiguous, meaning you can only create vectors       from a DoFHandler with several finite element components if they are       not reordered by component (use a  [2.x.23]        otherwise).  The global size of the vector is determined by       local.size(). The global indices in  [2.x.24]  are supplied as ghost       indices so that they can be read locally.             Note that the  [2.x.25]  IndexSet may be empty and that any indices       already contained in  [2.x.26]  are ignored during construction. That       way, the ghost parameter can equal the set of locally relevant       degrees of freedom, see  [2.x.27] .            
*  [2.x.28]  This operation always creates a ghosted vector, which is considered       read-only.              [2.x.29]         [2.x.30]  "vectors with ghost elements"      
* [0.x.9]*
       Construct a new parallel PETSc vector without ghost elements from an       IndexSet.             Note that  [2.x.31]  must be ascending and 1:1, see        [2.x.32]   In particular, the DoFs in        [2.x.33]  need to be contiguous, meaning you can only create vectors       from a DoFHandler with several finite element components if they are       not reordered by component (use a  [2.x.34]        otherwise).      
* [0.x.10]*
       Copy constructor.      
* [0.x.11]*
       Release all memory and return to a state just like after having       called the default constructor.      
* [0.x.12]*
       Copy the given vector. Resize the present vector if necessary. Also       take over the MPI communicator of  [2.x.35]       
* [0.x.13]*
       Set all components of the vector to the given number  [2.x.36]  Simply       pass this down to the base class, but we still need to declare this       function to make the example given in the discussion about making the       constructor explicit work.      
* [0.x.14]*
       Copy the values of a deal.II vector (as opposed to those of the PETSc       vector wrapper class) into this object.             Contrary to the case of sequential vectors, this operators requires       that the present vector already has the correct size, since we need       to have a partition and a communicator present which we otherwise       can't get from the source vector.      
* [0.x.15]*
       Change the dimension of the vector to  [2.x.37]  It is unspecified how       resizing the vector affects the memory allocation of this object;       i.e., it is not guaranteed that resizing it to a smaller size       actually also reduces memory consumption, or if for efficiency the       same amount of memory is used              [2.x.38]  denotes how many of the  [2.x.39]  values shall be       stored locally on the present process. for less data.              [2.x.40]  denotes the MPI communicator henceforth to be used       for this vector.             If  [2.x.41]  is false, the vector is filled by zeros.       Otherwise, the elements are left an unspecified state.      
* [0.x.16]*
       Change the dimension to that of the vector  [2.x.42]  and also take over       the partitioning into local sizes as well as the MPI communicator.       The same applies as for the other  [2.x.43]  function.             The elements of  [2.x.44]  are not copied, i.e. this function is the same       as calling <tt>reinit(v.size(), v.locally_owned_size(),       omit_zeroing_entries)</tt>.      
* [0.x.17]*
       Reinit as a vector with ghost elements. See the constructor with       same signature for more details.              [2.x.45]         [2.x.46]  "vectors with ghost elements"      
* [0.x.18]*
       Reinit as a vector without ghost elements. See constructor with same       signature for more details.              [2.x.47]         [2.x.48]  "vectors with ghost elements"      
* [0.x.19]*
       Return a reference to the MPI communicator object in use with this       vector.      
* [0.x.20]*
       Print to a stream.  [2.x.49]  denotes the desired precision with       which values shall be printed,  [2.x.50]  whether scientific       notation shall be used. If  [2.x.51]  is  [2.x.52]  then the vector is       printed in a line, while if  [2.x.53]  then the elements are printed on       a separate line each.            
*  [2.x.54]  This function overloads the one in the base class to ensure       that the right thing happens for parallel vectors that are       distributed across processors.      
* [0.x.21]*
        [2.x.55]   [2.x.56]             
*  [2.x.57]  This function overloads the one in the base class to make this       a collective operation.      
* [0.x.22]*
       Create a vector of length  [2.x.58]  For this class, we create a parallel       vector.  [2.x.59]  denotes the total size of the vector to be created.  [2.x.60]        locally_owned_size denotes how many of these elements shall be stored       locally.      
* [0.x.23]*
       Create a vector of global length  [2.x.61]  local size  [2.x.62]        locally_owned_size and with the specified ghost indices. Note that       you need to call update_ghost_values() before accessing those.      
* [0.x.24]*
       Copy of the communicator object to be used for this parallel vector.      
* [0.x.25]*
     Global function  [2.x.63]  which overloads the default implementation of     the C++ standard library which uses a temporary object. The function     simply exchanges the data of the two vectors.          [2.x.64]   [2.x.65]     
* [0.x.26]*
     A helper class used internally in linear_operator.h. Specialization for      [2.x.66]     
* [0.x.27]*
 Declare  [2.x.67]  as distributed vector.

* 
* [0.x.28]

include/deal.II-translator/lac/petsc_vector_base_0.txt
[0.x.0]*
 A namespace in which wrapper classes for PETSc objects reside.
* 

* 
*  [2.x.0] 

* 
*  [2.x.1] 

* 
* [0.x.1]*
    [2.x.2]  internal  
* [0.x.2]*
   A namespace for internal implementation details of the PETScWrapper   members.  
*  [2.x.3]   
* [0.x.3]*
     Since access to PETSc vectors only goes through functions, rather than     by obtaining a reference to a vector element, we need a wrapper class     that acts as if it was a reference, and basically redirects all     accesses (read and write) to member functions of this class.         This class implements such a wrapper: it is initialized with a vector     and an element within it, and has a conversion operator to extract the     scalar value of this element. It also has a variety of assignment     operator for writing to this one element.    
*  [2.x.4]     
* [0.x.4]*
       Declare type for container size.      
* [0.x.5]*
       Constructor. It is made private so as to only allow the actual vector       class to create it.      
* [0.x.6]       Copy constructor.      
* [0.x.7]*
       This looks like a copy operator, but does something different than       usual. In particular, it does not copy the member variables of this       reference. Rather, it handles the situation where we have two vectors        [2.x.5]  and  [2.x.6]  and assign elements like in <tt>v(i)=w(i)</tt>. Here,       both left and right hand side of the assignment have data type       VectorReference, but what we really mean is to assign the vector       elements represented by the two references. This operator implements       this operation. Note also that this allows us to make the assignment       operator const.      
* [0.x.8]*
       The same function as above, but for non-const reference objects. The       function is needed since the compiler might otherwise automatically       generate a copy operator for non-const objects.      
* [0.x.9]*
       Set the referenced element of the vector to <tt>s</tt>.      
* [0.x.10]*
       Add <tt>s</tt> to the referenced element of the vector.      
* [0.x.11]*
       Subtract <tt>s</tt> from the referenced element of the vector.      
* [0.x.12]*
       Multiply the referenced element of the vector by <tt>s</tt>.      
* [0.x.13]*
       Divide the referenced element of the vector by <tt>s</tt>.      
* [0.x.14]*
       Return the real part of the value of the referenced element.      
* [0.x.15]*
       Return the imaginary part of the value of the referenced element.            
*  [2.x.7]  This operation is not defined for real numbers and an exception       is thrown.      
* [0.x.16]*
       Convert the reference to an actual value, i.e. return the value of       the referenced element of the vector.      
* [0.x.17]*
       Exception      
* [0.x.18]*
       Exception.      
* [0.x.19]*
       Point to the vector we are referencing.      
* [0.x.20]*
       Index of the referenced element of the vector.      
* [0.x.21]*
    [2.x.8]   
* [0.x.22]*
   Base class for all vector classes that are implemented on top of the   PETSc vector types. Since in PETSc all vector types (i.e. sequential and   parallel ones) are built by filling the contents of an abstract object   that is only referenced through a pointer of a type that is independent   of the actual vector type, we can implement almost all functionality of   vectors in this base class. As such, this class can also be used as a   deal.II-compatible wrapper for a PETSc  [2.x.9]  object of any   type. Derived classes will then only have to provide the functionality to   create one or the other kind of vector.     The interface of this class is modeled after the existing Vector class in   deal.II. It has almost the same member functions, and is often   exchangeable. However, since PETSc only supports a single scalar type   (either double, float, or a complex data type), it is not templated, and   only works with whatever your PETSc installation has defined the data   type  [2.x.10]  to.     Note that PETSc only guarantees that operations do what you expect if the   functions  [2.x.11]  and  [2.x.12]  have been called   after vector assembly. Therefore, you need to call  [2.x.13]    before you actually use the vector.    
*  [2.x.14]   
* [0.x.23]*
     Declare some of the standard types used in all containers. These types     parallel those in the <tt>C++</tt> standard libraries     <tt>vector<...></tt> class.    
* [0.x.24]*
     Default constructor. It doesn't do anything, derived classes will have     to initialize the data.    
* [0.x.25]*
     Copy constructor. Sets the dimension to that of the given vector, and     copies all elements.    
* [0.x.26]*
     Initialize a Vector from a PETSc Vec object. Note that we do not copy     the vector and we do not obtain ownership, so we do not destroy the     PETSc object in the destructor.    
* [0.x.27]*
     The copy assignment operator is deleted to avoid accidental usage with     unexpected behavior.    
* [0.x.28]*
     Destructor.    
* [0.x.29]*
     Release all memory and return to a state just like after having called     the default constructor.    
* [0.x.30]*
     Compress the underlying representation of the PETSc object, i.e. flush     the buffers of the vector object if it has any. This function is     necessary after writing into a vector element-by-element and before     anything else can be done on it.         See      [2.x.15]  "Compressing distributed objects"     for more information.    
* [0.x.31]*
     Set all components of the vector to the given number  [2.x.16]  Simply pass     this down to the individual block objects, but we still need to declare     this function to make the example given in the discussion about making     the constructor explicit work.             Since the semantics of assigning a scalar to a vector are not     immediately clear, this operator should really only be used if you want     to set the entire vector to zero. This allows the intuitive notation     <tt>v=0</tt>. Assigning other values is deprecated and may be     disallowed in the future.    
* [0.x.32]*
     Test for equality. This function assumes that the present vector and     the one to compare with have the same size already, since comparing     vectors of different sizes makes not much sense anyway.    
* [0.x.33]*
     Test for inequality. This function assumes that the present vector and     the one to compare with have the same size already, since comparing     vectors of different sizes makes not much sense anyway.    
* [0.x.34]*
     Return the global dimension of the vector.    
* [0.x.35]*
     Return the local dimension of the vector, i.e. the number of elements     stored on the present MPI process. For sequential vectors, this number     is the same as size(), but for parallel vectors it may be smaller.         To figure out which elements exactly are stored locally, use     local_range() or locally_owned_elements().          [2.x.17]  use locally_owned_size() instead.    
* [0.x.36]*
     Return the local dimension of the vector, i.e. the number of elements     stored on the present MPI process. For sequential vectors, this number     is the same as size(), but for parallel vectors it may be smaller.         To figure out which elements exactly are stored locally, use     local_range() or locally_owned_elements().    
* [0.x.37]*
     Return a pair of indices indicating which elements of this vector are     stored locally. The first number is the index of the first element     stored, the second the index of the one past the last one that is     stored locally. If this is a sequential vector, then the result will be     the pair (0,N), otherwise it will be a pair (i,i+n), where     <tt>n=locally_owned_size()</tt>.    
* [0.x.38]*
     Return whether  [2.x.18]  is in the local range or not, see also     local_range().    
* [0.x.39]*
     Return an index set that describes which elements of this vector are     owned by the current processor. Note that this index set does not     include elements this vector may store locally as ghost elements but     that are in fact owned by another processor. As a consequence, the     index sets returned on different processors if this is a distributed     vector will form disjoint sets that add up to the complete index set.     Obviously, if a vector is created on only one processor, then the     result would satisfy    
* [1.x.0]
*     
* [0.x.40]*
     Return if the vector contains ghost elements.          [2.x.19]       [2.x.20]  "vectors with ghost elements"    
* [0.x.41]*
     This function only exists for compatibility with the  [2.x.21]       [2.x.22]  class and does nothing: this class     implements ghost value updates in a different way that is a better fit     with the underlying PETSc vector object.    
* [0.x.42]*
     Provide access to a given element, both read and write.    
* [0.x.43]*
     Provide read-only access to an element.    
* [0.x.44]*
     Provide access to a given element, both read and write.         Exactly the same as operator().    
* [0.x.45]*
     Provide read-only access to an element.         Exactly the same as operator().    
* [0.x.46]*
     A collective set operation: instead of setting individual elements of a     vector, this function allows to set a whole set of elements at once.     The indices of the elements to be set are stated in the first argument,     the corresponding values in the second.    
* [0.x.47]*
     Instead of getting individual elements of a vector via operator(),     this function allows getting a whole set of elements at once. The     indices of the elements to be read are stated in the first argument, the     corresponding values are returned in the second.         If the current vector is called  [2.x.23]  then this function is the equivalent     to the code    
* [1.x.1]
*           [2.x.24]  The sizes of the  [2.x.25]  and  [2.x.26]  arrays must be identical.    
* [0.x.48]*
     Instead of getting individual elements of a vector via operator(),     this function allows getting a whole set of elements at once. In     contrast to the previous function, this function obtains the     indices of the elements by dereferencing all elements of the iterator     range provided by the first two arguments, and puts the vector     values into memory locations obtained by dereferencing a range     of iterators starting at the location pointed to by the third     argument.         If the current vector is called  [2.x.27]  then this function is the equivalent     to the code    
* [1.x.2]
*           [2.x.28]  It must be possible to write into as many memory locations       starting at  [2.x.29]  as there are iterators between        [2.x.30]  and  [2.x.31]     
* [0.x.49]*
     A collective add operation: This function adds a whole set of values     stored in  [2.x.32]  to the vector components specified by  [2.x.33]     
* [0.x.50]*
     This is a second collective add operation. As a difference, this     function takes a deal.II vector of values.    
* [0.x.51]*
     Take an address where <tt>n_elements</tt> are stored contiguously and     add them into the vector. Handles all cases which are not covered by     the other two <tt>add()</tt> functions above.    
* [0.x.52]*
     Return the scalar product of two vectors. The vectors must have the     same size.         For complex valued vector, this gives [2.x.34] .    
* [0.x.53]*
     Return the square of the  [2.x.35] -norm.    
* [0.x.54]*
     Return the mean value of the elements of this vector.    
* [0.x.55]*
      [2.x.36] -norm of the vector. The sum of the absolute values.        
*  [2.x.37]  In complex-valued PETSc priori to 3.7.0 this norm is implemented     as the sum of absolute values of real and imaginary parts of elements     of a complex vector.    
* [0.x.56]*
      [2.x.38] -norm of the vector.  The square root of the sum of the squares of     the elements.    
* [0.x.57]*
      [2.x.39] -norm of the vector. The pth root of the sum of the pth powers of     the absolute values of the elements.    
* [0.x.58]*
      [2.x.40] -norm of the vector. Return the value of the vector element     with the maximum absolute value.    
* [0.x.59]*
     Performs a combined operation of a vector addition and a subsequent     inner product, returning the value of the inner product. In other     words, the result of this function is the same as if the user called    
* [1.x.3]
*          The reason this function exists is for compatibility with deal.II's own     vector classes which can implement this functionality with less memory     transfer. However, for PETSc vectors such a combined operation is not     natively supported and thus the cost is completely equivalent as     calling the two methods separately.         For complex-valued vectors, the scalar product in the second step is     implemented as      [2.x.41] .    
* [0.x.60]*
     Return the value of the vector element with the largest negative value.          [2.x.42]  This function has been deprecated to improve compatibility     with other classes inheriting from VectorSpaceVector. If you need to     use this functionality then use the PETSc function VecMin instead.    
* [0.x.61]*
     Return the value of the vector element with the largest positive value.          [2.x.43]  This function has been deprecated to improve compatibility     with other classes inheriting from VectorSpaceVector. If you need to     use this functionality then use the PETSc function VecMax instead.    
* [0.x.62]*
     Return whether the vector contains only elements with value zero. This     is a collective operation. This function is expensive, because     potentially all elements have to be checked.    
* [0.x.63]*
     Return  [2.x.44]  if the vector has no negative entries, i.e. all entries     are zero or positive. This function is used, for example, to check     whether refinement indicators are really all positive (or zero).          [2.x.45]  This function has been deprecated to improve compatibility     with other classes inheriting from VectorSpaceVector.    
* [0.x.64]*
     Multiply the entire vector by a fixed factor.    
* [0.x.65]*
     Divide the entire vector by a fixed factor.    
* [0.x.66]*
     Add the given vector to the present one.    
* [0.x.67]*
     Subtract the given vector from the present one.    
* [0.x.68]*
     Addition of  [2.x.46]  to all components. Note that  [2.x.47]  is a scalar and not     a vector.    
* [0.x.69]*
     Simple addition of a multiple of a vector, i.e. <tt>*this += a*V</tt>.    
* [0.x.70]*
     Multiple addition of scaled vectors, i.e. <tt>*this += a*V+b*W</tt>.    
* [0.x.71]*
     Scaling and simple vector addition, i.e. <tt>*this = s*(*this)+V</tt>.    
* [0.x.72]*
     Scaling and simple addition, i.e. <tt>*this = s*(*this)+a*V</tt>.    
* [0.x.73]*
     Scale each element of this vector by the corresponding element in the     argument. This function is mostly meant to simulate multiplication (and     immediate re-assignment) by a diagonal scaling matrix.    
* [0.x.74]*
     Assignment <tt>*this = a*V</tt>.    
* [0.x.75]*
     Prints the PETSc vector object values using PETSc internal vector     viewer function <tt>VecView</tt>. The default format prints the     vector's contents, including indices of vector elements. For other     valid view formats, consult     http://www.mcs.anl.gov/petsc/petsc-current/docs/manualpages/Vec/VecView.html    
* [0.x.76]*
     Print to a stream.  [2.x.48]  denotes the desired precision with     which values shall be printed,  [2.x.49]  whether scientific     notation shall be used. If  [2.x.50]  is  [2.x.51]  then the vector is     printed in a line, while if  [2.x.52]  then the elements are printed on a     separate line each.    
* [0.x.77]*
     Swap the contents of this vector and the other vector  [2.x.53]  One could     do this operation with a temporary variable and copying over the data     elements, but this function is significantly more efficient since it     only swaps the pointers to the data of the two vectors and therefore     does not need to allocate temporary storage and move data around.         This function is analogous to the  [2.x.54]  function of all C++     standard containers. Also, there is a global function     <tt>swap(u,v)</tt> that simply calls <tt>u.swap(v)</tt>, again in     analogy to standard functions.    
* [0.x.78]*
     Conversion operator to gain access to the underlying PETSc type. If you     do this, you cut this class off some information it may need, so this     conversion operator should only be used if you know what you do. In     particular, it should only be used for read-only operations into the     vector.    
* [0.x.79]*
     Estimate for the memory consumption (not implemented for this class).    
* [0.x.80]*
     Return a reference to the MPI communicator object in use with this     object.    
* [0.x.81]*
     A generic vector object in PETSc. The actual type, a sequential vector,     is set in the constructor.    
* [0.x.82]*
     Denotes if this vector has ghost indices associated with it. This means     that at least one of the processes in a parallel program has at least     one ghost index.    
* [0.x.83]*
     This vector contains the global indices of the ghost values. The     location in this vector denotes the local numbering, which is used in     PETSc.    
* [0.x.84]*
     Store whether the last action was a write or add operation. This     variable is  [2.x.55]  so that the accessor classes can write to it,     even though the vector object they refer to is constant.    
* [0.x.85]*
     Specifies if the vector is the owner of the PETSc Vec. This is true if     it got created by this class and determines if it gets destroyed in     the destructor.    
* [0.x.86]*
     Collective set or add operation: This function is invoked by the     collective  [2.x.56]  and  [2.x.57]  with the  [2.x.58]  flag set to the     corresponding value.    
* [0.x.87]*
   Global function  [2.x.59]  which overloads the default implementation of the   C++ standard library which uses a temporary object. The function simply   exchanges the data of the two vectors.      [2.x.60]   [2.x.61]   
* [0.x.88]

include/deal.II-translator/lac/precondition_0.txt
[0.x.0]!  [2.x.0]  Preconditioners [2.x.1] 

* 
* [0.x.1]*
 No preconditioning.  This class helps you, if you want to use a linear solver without preconditioning. All solvers in LAC require a preconditioner. Therefore, you must use the identity provided here to avoid preconditioning. It can be used in the following way:
* 

* 
* [1.x.0]
* 
*  See the  [2.x.2]  tutorial program for an example and additional explanations.
*  Alternatively, the IdentityMatrix class can be used to precondition in this way.

* 
* [0.x.2]*
   Declare type for container size.  
* [0.x.3]*
   This function is only present to provide the interface of a   preconditioner to be handed to a smoother.  This does nothing.  
* [0.x.4]*
     Constructor.    
* [0.x.5]*
   Constructor, sets the domain and range sizes to their defaults.  
* [0.x.6]*
   The matrix argument is ignored and here just for compatibility with more   complex preconditioners.  
* [0.x.7]*
   Apply preconditioner.  
* [0.x.8]*
   Apply transpose preconditioner. Since this is the identity, this function   is the same as vmult().  
* [0.x.9]*
   Apply preconditioner, adding to the previous value.  
* [0.x.10]*
   Apply transpose preconditioner, adding. Since this is the identity, this   function is the same as vmult_add().  
* [0.x.11]*
   This function is only present to provide the interface of a   preconditioner to be handed to a smoother.  This does nothing.  
* [0.x.12]*
   Return the dimension of the codomain (or range) space. Note that the   matrix is of dimension  [2.x.3] .    
*  [2.x.4]  This function should only be called if the preconditioner has been   initialized.  
* [0.x.13]*
   Return the dimension of the domain space. Note that the matrix is of   dimension  [2.x.5] .    
*  [2.x.6]  This function should only be called if the preconditioner has been   initialized.  
* [0.x.14]*
   The dimension of the range space.  
* [0.x.15]*
   The dimension of the domain space.  
* [0.x.16]*
 Preconditioning with Richardson's method. This preconditioner just scales the vector with a constant relaxation factor provided by the AdditionalData object.
*  In Krylov-space methods, this preconditioner should not have any effect. Using SolverRichardson, the two relaxation parameters will be just multiplied. Still, this class is useful in multigrid smoother objects (MGSmootherRelaxation).

* 
* [0.x.17]*
   Declare type for container size.  
* [0.x.18]*
   Parameters for Richardson preconditioner.  
* [0.x.19]*
     Constructor. Block size must be given since there is no reasonable     default parameter.    
* [0.x.20]*
     Relaxation parameter.    
* [0.x.21]*
   Constructor, sets the relaxation parameter, domain and range sizes to   their default.  
* [0.x.22]*
   Change the relaxation parameter.  
* [0.x.23]*
   Change the relaxation parameter in a way consistent with other   preconditioners. The matrix argument is ignored and here just for   compatibility with more complex preconditioners.  
* [0.x.24]*
   Apply preconditioner.  
* [0.x.25]*
   Apply transpose preconditioner. Since this is the identity, this function   is the same as vmult().  
* [0.x.26]*
   Apply preconditioner, adding to the previous value.  
* [0.x.27]*
   Apply transpose preconditioner, adding. Since this is the identity, this   function is the same as vmult_add().  
* [0.x.28]*
   This function is only present to provide the interface of a   preconditioner to be handed to a smoother.  This does nothing.  
* [0.x.29]*
   Return the dimension of the codomain (or range) space. Note that the   matrix is of dimension  [2.x.7] .    
*  [2.x.8]  This function should only be called if the preconditioner has been   initialized.  
* [0.x.30]*
   Return the dimension of the domain space. Note that the matrix is of   dimension  [2.x.9] .    
*  [2.x.10]  This function should only be called if the preconditioner has been   initialized.  
* [0.x.31]*
   The relaxation parameter multiplied with the vectors.  
* [0.x.32]*
   The dimension of the range space.  
* [0.x.33]*
   The dimension of the domain space.  
* [0.x.34]*
 Preconditioner using a matrix-builtin function.  This class forms a preconditioner suitable for the LAC solver classes. Since many preconditioning methods are based on matrix entries, these have to be implemented as member functions of the underlying matrix implementation. This class now is intended to allow easy access to these member functions from LAC solver classes.
*  It seems that all builtin preconditioners have a relaxation parameter, so please use PreconditionRelaxation for these.
*  You will usually not want to create a named object of this type, although possible. The most common use is like this:

* 
* [1.x.1]
*  This creates an unnamed object to be passed as the fourth parameter to the solver function of the SolverGMRES class. It assumes that the SparseMatrix class has a function <tt>precondition_Jacobi</tt> taking two vectors (source and destination) as parameters (Actually, there is no function like that, the existing function takes a third parameter, denoting the relaxation parameter; this example is therefore only meant to illustrate the general idea).
*  Note that due to the default template parameters, the above example could be written shorter as follows:

* 
* [1.x.2]
* 

* 
* [0.x.35]*
   Type of the preconditioning function of the matrix.  
* [0.x.36]*
   Constructor.  This constructor stores a reference to the matrix object   for later use and selects a preconditioning method, which must be a   member function of that matrix.  
* [0.x.37]*
   Execute preconditioning. Calls the function passed to the constructor of   this object with the two arguments given here.  
* [0.x.38]*
   Pointer to the matrix in use.  
* [0.x.39]*
   Pointer to the preconditioning function.  
* [0.x.40]*
 Base class for other preconditioners. Here, only some common features Jacobi, SOR and SSOR preconditioners are implemented. For preconditioning, refer to derived classes.

* 
* [0.x.41]*
   Declare type for container size.  
* [0.x.42]*
   Class for parameters.  
* [0.x.43]*
     Constructor.    
* [0.x.44]*
     Relaxation parameter.    
* [0.x.45]*
   Initialize matrix and relaxation parameter. The matrix is just stored in   the preconditioner object. The relaxation parameter should be larger than   zero and smaller than 2 for numerical reasons. It defaults to 1.  
* [0.x.46]*
   Release the matrix and reset its pointer.  
* [0.x.47]*
   Return the dimension of the codomain (or range) space. Note that the   matrix is of dimension  [2.x.11] .  
* [0.x.48]*
   Return the dimension of the domain space. Note that the matrix is of   dimension  [2.x.12] .  
* [0.x.49]*
   Pointer to the matrix object.  
* [0.x.50]*
   Relaxation parameter.  
* [0.x.51]*
 Jacobi preconditioner using matrix built-in function.  The <tt>MatrixType</tt> class used is required to have a function <tt>precondition_Jacobi(VectorType&, const VectorType&, double</tt>). This class satisfies the  [2.x.13]  "relaxation concept".
* 

* 
* [1.x.3]
* 

* 
* [0.x.52]*
   An alias to the base class AdditionalData.  
* [0.x.53]*
   Apply preconditioner.  
* [0.x.54]*
   Apply transpose preconditioner. Since this is a symmetric preconditioner,   this function is the same as vmult().  
* [0.x.55]*
   Perform one step of the preconditioned Richardson iteration.  
* [0.x.56]*
   Perform one transposed step of the preconditioned Richardson iteration.  
* [0.x.57]*
 SOR preconditioner using matrix built-in function.
*  Assuming the matrix [1.x.4] is split into its diagonal [1.x.5] as well as the strict lower and upper triangles [1.x.6] and [1.x.7], then the SOR preconditioner with relaxation parameter [1.x.8] is [1.x.9] It is this operator [1.x.10], which is implemented by vmult() through forward substitution. Analogously, Tvmult() implements the operation of [1.x.11].
*  The SOR iteration itself can be directly written as [1.x.12] Using the right hand side [1.x.13] and the previous iterate [1.x.14], this is the operation implemented by step().
*  The MatrixType class used is required to have functions <tt>precondition_SOR(VectorType&, const VectorType&, double)</tt> and <tt>precondition_TSOR(VectorType&, const VectorType&, double)</tt>. This class satisfies the  [2.x.14]  "relaxation concept".
* 

* 
* [1.x.15]
* 

* 
* [0.x.58]*
   An alias to the base class AdditionalData.  
* [0.x.59]*
   Apply preconditioner.  
* [0.x.60]*
   Apply transpose preconditioner.  
* [0.x.61]*
   Perform one step of the preconditioned Richardson iteration.  
* [0.x.62]*
   Perform one transposed step of the preconditioned Richardson iteration.  
* [0.x.63]*
 SSOR preconditioner using matrix built-in function.  The <tt>MatrixType</tt> class used is required to have a function <tt>precondition_SSOR(VectorType&, const VectorType&, double)</tt>. This class satisfies the  [2.x.15]  "relaxation concept".
* 

* 
* [1.x.16]
* 

* 
* [0.x.64]*
   An alias to the base class AdditionalData.  
* [0.x.65]*
   Declare type for container size.  
* [0.x.66]*
   An alias to the base class.  
* [0.x.67]*
   Initialize matrix and relaxation parameter. The matrix is just stored in   the preconditioner object. The relaxation parameter should be larger than   zero and smaller than 2 for numerical reasons. It defaults to 1.  
* [0.x.68]*
   Apply preconditioner.  
* [0.x.69]*
   Apply transpose preconditioner. Since this is a symmetric preconditioner,   this function is the same as vmult().  
* [0.x.70]*
   Perform one step of the preconditioned Richardson iteration  
* [0.x.71]*
   Perform one transposed step of the preconditioned Richardson iteration.  
* [0.x.72]*
   An array that stores for each matrix row where the first position after   the diagonal is located.  
* [0.x.73]*
 Permuted SOR preconditioner using matrix built-in function.  The <tt>MatrixType</tt> class used is required to have functions <tt>PSOR(VectorType&, const VectorType&, double)</tt> and <tt>TPSOR(VectorType&, const VectorType&, double)</tt>.
* 

* 
* [1.x.17]
* 

* 
* [0.x.74]*
   Declare type for container size.  
* [0.x.75]*
   Parameters for PreconditionPSOR.  
* [0.x.76]*
     Constructor. For the parameters' description, see below.         The permutation vectors are stored as a reference. Therefore, it has to     be assured that the lifetime of the vector exceeds the lifetime of the     preconditioner.         The relaxation parameter should be larger than zero and smaller than 2     for numerical reasons. It defaults to 1.    
* [0.x.77]*
     Storage for the permutation vector.    
* [0.x.78]*
     Storage for the inverse permutation vector.    
* [0.x.79]*
     Relaxation parameters    
* [0.x.80]*
   Initialize matrix and relaxation parameter. The matrix is just stored in   the preconditioner object.     The permutation vector is stored as a pointer. Therefore, it has to be   assured that the lifetime of the vector exceeds the lifetime of the   preconditioner.     The relaxation parameter should be larger than zero and smaller than 2   for numerical reasons. It defaults to 1.  
* [0.x.81]*
   Initialize matrix and relaxation parameter. The matrix is just stored in   the preconditioner object.     For more detail about possible parameters, see the class documentation   and the documentation of the  [2.x.16]  class.     After this function is called the preconditioner is ready to be used   (using the  [2.x.17]  function of derived classes).  
* [0.x.82]*
   Apply preconditioner.  
* [0.x.83]*
   Apply transpose preconditioner.  
* [0.x.84]*
   Storage for the permutation vector.  
* [0.x.85]*
   Storage for the inverse permutation vector.  
* [0.x.86]*
 Preconditioning with a Chebyshev polynomial for symmetric positive definite matrices. This preconditioner is based on an iteration of an inner preconditioner of type  [2.x.18]  with coefficients that are adapted to optimally cover an eigenvalue range between the largest eigenvalue  [2.x.19]  down to a given lower eigenvalue  [2.x.20]  specified by the optional parameter  [2.x.21]  The algorithm is based on the following three-term recurrence: [1.x.18] where the parameter  [2.x.22]  is set to  [2.x.23]  for the maximal eigenvalue  [2.x.24]  and updated via  [2.x.25] . The Chebyshev polynomial is constructed to strongly damp the eigenvalue range between  [2.x.26]  and  [2.x.27]  and is visualized e.g. in  [2.x.28] 
*  The typical use case for the preconditioner is a Jacobi preconditioner specified through DiagonalMatrix, which is also the default value for the preconditioner. Note that if the degree variable is set to one, the Chebyshev iteration corresponds to a Jacobi preconditioner (or the underlying preconditioner type) with relaxation parameter according to the specified smoothing range.
*  Besides the default choice of a pointwise Jacobi preconditioner, this class also allows for more advanced types of preconditioners, for example iterating block-Jacobi preconditioners in DG methods.
*  Apart from the inner preconditioner object, this iteration does not need access to matrix entries, which makes it an ideal ingredient for matrix-free computations. In that context, this class can be used as a multigrid smoother that is trivially %parallel (assuming that matrix-vector products are %parallel and the inner preconditioner is %parallel). Its use is demonstrated in the  [2.x.29]  and  [2.x.30]  tutorial programs.
*  [1.x.19]
*  The Chebyshev method relies on an estimate of the eigenvalues of the matrix which are computed during the first invocation of vmult(). The algorithm invokes a conjugate gradient solver (i.e., Lanczos iteration) so symmetry and positive definiteness of the (preconditioned) matrix system are required. The eigenvalue algorithm can be controlled by  [2.x.31]  specifying how many iterations should be performed. The iterations are started from an initial vector that depends on the vector type. For the classes  [2.x.32]  or  [2.x.33]  which have fast element access, it is a vector with entries `(-5.5,
* 
*  - .5,
* 
*  - .5,
* 

* 
* 
*  - .5, ..., 3.5, 4.5, 5.5)` with appropriate epilogue and adjusted such that its mean is always zero, which works well for the Laplacian. This setup is stable in parallel in the sense that for a different number of processors but the same ordering of unknowns, the same initial vector and thus eigenvalue distribution will be computed, apart from roundoff errors. For other vector types, the initial vector contains all ones, scaled by the length of the vector, except for the very first entry that is zero, triggering high-frequency content again.
*  The computation of eigenvalues happens the first time one of the vmult(), Tvmult(), step() or Tstep() functions is called or when estimate_eigenvalues() is called directly. In the latter case, it is necessary to provide a temporary vector of the same layout as the source and destination vectors used during application of the preconditioner.
*  The estimates for minimum and maximum eigenvalue are taken from SolverCG (even if the solver did not converge in the requested number of iterations). Finally, the maximum eigenvalue is multiplied by a safety factor of 1.2.
*  Due to the cost of the eigenvalue estimate, this class is most appropriate if it is applied repeatedly, e.g. in a smoother for a geometric multigrid solver, that can in turn be used to solve several linear systems.
*  [1.x.20]
*  In some contexts, the automatic eigenvalue computation of this class may result in bad quality, or it may be unstable when used in parallel with different enumerations of the degrees of freedom, making computations strongly dependent on the parallel configuration. It is possible to bypass the automatic eigenvalue computation by setting  [2.x.34]  to zero, and provide the variable  [2.x.35]  instead. The minimal eigenvalue is implicitly specified via `max_eigenvalue/smoothing_range`.
*  [1.x.21]
*  If the range <tt>[max_eigenvalue/smoothing_range, max_eigenvalue]</tt> contains all eigenvalues of the preconditioned matrix system and the degree (i.e., number of iterations) is high enough, this class can also be used as a direct solver. For an error estimation of the Chebyshev iteration that can be used to determine the number of iteration, see Varga (2009).
*  In order to use Chebyshev as a solver, set the degree to  [2.x.36]  to force the automatic computation of the number of iterations needed to reach a given target tolerance. In this case, the target tolerance is read from the variable  [2.x.37]  (it needs to be a number less than one to force any iterations obviously).
*  For details on the algorithm, see section 5.1 of

* 
* [1.x.22]
* 
*  [1.x.23]
*  The class MatrixType must be derived from Subscriptor because a SmartPointer to MatrixType is held in the class. In particular, this means that the matrix object needs to persist during the lifetime of PreconditionChebyshev. The preconditioner is held in a shared_ptr that is copied into the AdditionalData member variable of the class, so the variable used for initialization can safely be discarded after calling initialize(). Both the matrix and the preconditioner need to provide  [2.x.38]  functions for the matrix-vector product and  [2.x.39]  functions for accessing the number of rows in the (square) matrix. Furthermore, the matrix must provide <tt>el(i,i)</tt> methods for accessing the matrix diagonal in case the preconditioner type is DiagonalMatrix. Even though it is highly recommended to pass the inverse diagonal entries inside a separate preconditioner object for implementing the Jacobi method (which is the only possible way to operate this class when computing in %parallel with MPI because there is no knowledge about the locally stored range of entries that would be needed from the matrix alone), there is a backward compatibility function that can extract the diagonal in case of a serial computation.

* 
* [0.x.87]*
   Declare type for container size.  
* [0.x.88]*
   Standardized data struct to pipe additional parameters to the   preconditioner.  
* [0.x.89]*
     Constructor.    
* [0.x.90]*
      Copy assignment operator.    
* [0.x.91]*
     This determines the degree of the Chebyshev polynomial. The degree of     the polynomial gives the number of matrix-vector products to be     performed for one application of the vmult() operation. Degree one     corresponds to a damped Jacobi method.         If the degree is set to  [2.x.40]  the algorithm     will automatically determine the number of necessary iterations based     on the usual Chebyshev error formula as mentioned in the discussion of     the main class.    
* [0.x.92]*
     This sets the range between the largest eigenvalue in the matrix and     the smallest eigenvalue to be treated. If the parameter is set to a     number less than 1, an estimate for the largest and for the smallest     eigenvalue will be calculated internally. For a smoothing range larger     than one, the Chebyshev polynomial will act in the interval      [2.x.41] ,     where  [2.x.42]  is an estimate of the maximum eigenvalue     of the matrix. A choice of <tt>smoothing_range</tt> between 5 and 20 is     useful in case the preconditioner is used as a smoother in multigrid.    
* [0.x.93]*
     Maximum number of CG iterations performed for finding the maximum     eigenvalue. If set to zero, no computations are performed. Instead, the     user must supply a largest eigenvalue via the variable      [2.x.43]     
* [0.x.94]*
     Tolerance for CG iterations performed for finding the maximum     eigenvalue.    
* [0.x.95]*
     Maximum eigenvalue to work with. Only in effect if  [2.x.44]      eig_cg_n_iterations is set to zero, otherwise this parameter is     ignored.    
* [0.x.96]*
     Constraints to be used for the operator given. This variable is used to     zero out the correct entries when creating an initial guess.    
* [0.x.97]*
     Stores the preconditioner object that the Chebyshev is wrapped around.    
* [0.x.98]*
   Constructor.  
* [0.x.99]*
   Initialize function. Takes the matrix which is used to form the   preconditioner, and additional flags if there are any. This function   works only if the input matrix has an operator <tt>el(i,i)</tt> for   accessing all the elements in the diagonal. Alternatively, the diagonal   can be supplied with the help of the AdditionalData field.     This function calculates an estimate of the eigenvalue range of the   matrix weighted by its diagonal using a modified CG iteration in case the   given number of iterations is positive.  
* [0.x.100]*
   Compute the action of the preconditioner on <tt>src</tt>, storing the   result in <tt>dst</tt>.  
* [0.x.101]*
   Compute the action of the transposed preconditioner on <tt>src</tt>,   storing the result in <tt>dst</tt>.  
* [0.x.102]*
   Perform one step of the preconditioned Richardson iteration.  
* [0.x.103]*
   Perform one transposed step of the preconditioned Richardson iteration.  
* [0.x.104]*
   Resets the preconditioner.  
* [0.x.105]*
   Return the dimension of the codomain (or range) space. Note that the   matrix is of dimension  [2.x.45] .  
* [0.x.106]*
   Return the dimension of the domain space. Note that the matrix is of   dimension  [2.x.46] .  
* [0.x.107]*
   A struct that contains information about the eigenvalue estimation   performed by the PreconditionChebychev class.  
* [0.x.108]*
     Estimate for the smallest eigenvalue.    
* [0.x.109]*
     Estimate for the largest eigenvalue.    
* [0.x.110]*
     Number of CG iterations performed or 0.    
* [0.x.111]*
     The degree of the Chebyshev polynomial (either as set using      [2.x.47]  or estimated as described there).    
* [0.x.112]*
     Constructor initializing with invalid values.    
* [0.x.113]*
   Compute eigenvalue estimates required for the preconditioner.     This function is called automatically on first use of the preconditioner   if it is not called by the user. The layout of the vector  [2.x.48]  is used   to create internal temporary vectors and its content does not matter.     Initializes the factors theta and delta based on an eigenvalue   computation. If the user set provided values for the largest eigenvalue   in AdditionalData, no computation is performed and the information given   by the user is used.  
* [0.x.114]*
   A pointer to the underlying matrix.  
* [0.x.115]*
   Internal vector used for the <tt>vmult</tt> operation.  
* [0.x.116]*
   Internal vector used for the <tt>vmult</tt> operation.  
* [0.x.117]*
   Internal vector used for the <tt>vmult</tt> operation.  
* [0.x.118]*
   Stores the additional data passed to the initialize function, obtained   through a copy operation.  
* [0.x.119]*
   Average of the largest and smallest eigenvalue under consideration.  
* [0.x.120]*
   Half the interval length between the largest and smallest eigenvalue   under consideration.  
* [0.x.121]*
   Stores whether the preconditioner has been set up and eigenvalues have   been computed.  
* [0.x.122]*
   A mutex to avoid that multiple vmult() invocations by different threads   overwrite the temporary vectors.  
* [0.x.123]

include/deal.II-translator/lac/precondition_block_0.txt
[0.x.0]!  [2.x.0]  Preconditioners [2.x.1] 

* 
* [0.x.1]*
 Base class for actual block preconditioners. This class assumes the <tt>MatrixType</tt> consisting of invertible blocks of  [2.x.2]  on the diagonal and provides the inversion of the diagonal blocks of the matrix. It is not necessary for this class that the matrix be block diagonal; rather, it applies to matrices of arbitrary structure with the minimal property of having invertible blocks on the diagonal. Still the matrix must have access to single matrix entries. Therefore, BlockMatrixArray and similar classes are not a possible matrix class template arguments.
*  The block matrix structure used by this class is given, e.g., for the DG method for the transport equation. For a downstream numbering the matrices even have got a block lower left matrix structure, i.e. the matrices are empty above the diagonal blocks.
* 

* 
*  [2.x.3]  This class is intended to be used for matrices whose structure is given by local contributions from disjoint cells, such as for DG methods. It is not intended for problems where the block structure results from different physical variables such as in the Stokes equations considered in  [2.x.4] .
*  For all matrices that are empty above and below the diagonal blocks (i.e. for all block diagonal matrices) the  [2.x.5]  preconditioner is a direct solver. For all matrices that are empty only above the diagonal blocks (e.g. the matrices one gets by the DG method with downstream numbering)  [2.x.6]  is a direct solver.
*  This first implementation of the  [2.x.7]  assumes the matrix has blocks each of the same block size. Varying block sizes within the matrix must still be implemented if needed.
*  The first template parameter denotes the type of number representation in the sparse matrix, the second denotes the type of number representation in which the inverted diagonal block matrices are stored within this class by <tt>invert_diagblocks()</tt>. If you don't want to use the block inversion as an exact solver, but rather as a preconditioner, you may probably want to store the inverted blocks with less accuracy than the original matrix; for example, <tt>number==double, inverse_type=float</tt> might be a viable choice.
*   [2.x.8]   [2.x.9]  "Block (linear algebra)"

* 
* [0.x.2]*
   Define number type of matrix.  
* [0.x.3]*
   Value type for inverse matrices.  
* [0.x.4]*
   Declare type for container size.  
* [0.x.5]*
   Parameters for block preconditioners.  
* [0.x.6]*
     Constructor. Block size must be given since there is no reasonable     default parameter.    
* [0.x.7]*
     Relaxation parameter.    
* [0.x.8]*
     Block size.    
* [0.x.9]*
     Invert diagonal during initialization.    
* [0.x.10]*
     Assume all diagonal blocks are equal to save memory.    
* [0.x.11]*
     Choose the inversion method for the blocks.    
* [0.x.12]*
     The if #inversion is SVD, the threshold below which a singular value     will be considered zero and thus not inverted. This parameter is used     in the call to  [2.x.10]     
* [0.x.13]*
   Constructor.  
* [0.x.14]*
   Destructor.  
* [0.x.15]*
   Initialize matrix and block size.  We store the matrix and the block size   in the preconditioner object. In a second step, the inverses of the   diagonal blocks may be computed.     Additionally, a relaxation parameter for derived classes may be provided.  
* [0.x.16]*
   Initialize matrix and block size for permuted preconditioning.   Additionally to the parameters of the other initialize() function, we   hand over two index vectors with the permutation and its inverse. For the   meaning of these vectors see PreconditionBlockSOR.     In a second step, the inverses of the diagonal blocks may be computed.   Make sure you use invert_permuted_diagblocks() to yield consistent data.     Additionally, a relaxation parameter for derived classes may be provided.  
* [0.x.17]*
   Set either the permutation of rows or the permutation of blocks,   depending on the size of the vector.     If the size of the permutation vectors is equal to the dimension of the   linear system, it is assumed that rows are permuted individually. In this   case, set_permutation() must be called before initialize(), since the   diagonal blocks are built from the permuted entries of the matrix.     If the size of the permutation vector is not equal to the dimension of   the system, the diagonal blocks are computed from the unpermuted entries.   Instead, the relaxation methods step() and Tstep() are executed applying   the blocks in the order given by the permutation vector. They will throw   an exception if length of this vector is not equal to the number of   blocks.    
*  [2.x.11]  Permutation of blocks can only be applied to the relaxation   operators step() and Tstep(), not to the preconditioning operators   vmult() and Tvmult().    
*  [2.x.12]  It is safe to call set_permutation() before initialize(), while the   other order is only admissible for block permutation.  
* [0.x.18]*
   Replacement of invert_diagblocks() for permuted preconditioning.  
* [0.x.19]*
   Deletes the inverse diagonal block matrices if existent, sets the   blocksize to 0, hence leaves the class in the state that it had directly   after calling the constructor.  
* [0.x.20]*
   Check whether the object is empty.  
* [0.x.21]*
   Read-only access to entries. This function is only possible if the   inverse diagonal blocks are stored.  
* [0.x.22]*
   Stores the inverse of the diagonal blocks in  [2.x.13]  This costs some   additional memory
* 
*  - for DG methods about 1/3 (for double inverses) or 1/6   (for float inverses) of that used for the matrix
* 
*  - but it makes the   preconditioning much faster.     It is not allowed to call this function twice (will produce an error)   before a call of <tt>clear(...)</tt>  because at the second time there   already exist the inverse matrices.     After this function is called, the lock on the matrix given through the    [2.x.14]  function is released, i.e. you may overwrite of delete it.   You may want to do this in case you use this matrix to precondition   another matrix.  
* [0.x.23]*
   Perform one block relaxation step in forward numbering.     Depending on the arguments  [2.x.15]  and  [2.x.16]  this performs an SOR step   (both reference the same vector) of a Jacobi step (both different   vectors). For the Jacobi step, the calling function must copy  [2.x.17]  to    [2.x.18]  after this.    
*  [2.x.19]  If a permutation is set, it is automatically honored by this   function.  
* [0.x.24]*
   Perform one block relaxation step in backward numbering.     Depending on the arguments  [2.x.20]  and  [2.x.21]  this performs an SOR step   (both reference the same vector) of a Jacobi step (both different   vectors). For the Jacobi step, the calling function must copy  [2.x.22]  to    [2.x.23]  after this.    
*  [2.x.24]  If a permutation is set, it is automatically honored by this   function.  
* [0.x.25]*
   Return the size of the blocks.  
* [0.x.26]*
   Determine an estimate for the memory consumption (in bytes) of this   object.  
* [0.x.27]*
    [2.x.25]  Exceptions    [2.x.26]   
* [0.x.28]*
   For non-overlapping block preconditioners, the block size must divide the   matrix size. If not, this exception gets thrown.  
* [0.x.29]*
   Exception  
* [0.x.30]*
   Size of the blocks. Each diagonal block is assumed to be of the same   size.  
* [0.x.31]*
   Pointer to the matrix. Make sure that the matrix exists as long as this   class needs it, i.e. until calling  [2.x.27]  or (if the   inverse matrices should not be stored) until the last call of the   preconditoining  [2.x.28]  function of the derived classes.  
* [0.x.32]*
   Relaxation parameter to be used by derived classes.  
* [0.x.33]*
   The permutation vector  
* [0.x.34]*
   The inverse permutation vector  
* [0.x.35]*
 Block Jacobi preconditioning. See PreconditionBlock for requirements on the matrix. This class satisfies the  [2.x.29]  "relaxation concept".
* 

* 
*  [2.x.30]  Instantiations for this template are provided for <tt> [2.x.31]  and  [2.x.32]  others can be generated in application programs (see the section on  [2.x.33]  in the manual).

* 
* [0.x.36]*
   Define number type of matrix.  
* [0.x.37]*
   Declare type for container size.  
* [0.x.38]*
   Standard-conforming iterator.  
* [0.x.39]*
     Accessor class for iterators    
* [0.x.40]*
       Constructor. Since we use accessors only for read access, a const       matrix pointer is sufficient.      
* [0.x.41]*
       Row number of the element represented by this object.      
* [0.x.42]*
       Column number of the element represented by this object.      
* [0.x.43]*
       Value of this matrix entry.      
* [0.x.44]*
       The matrix accessed.      
* [0.x.45]*
       Save block size here for further reference.      
* [0.x.46]*
       Current block number.      
* [0.x.47]*
       Iterator inside block.      
* [0.x.48]*
       End of current block.      
* [0.x.49]*
     Constructor.    
* [0.x.50]*
     Prefix increment.    
* [0.x.51]*
     Dereferencing operator.    
* [0.x.52]*
     Dereferencing operator.    
* [0.x.53]*
     Comparison. True, if both iterators point to the same matrix position.    
* [0.x.54]*
     Inverse of <tt>==</tt>.    
* [0.x.55]*
     Comparison operator. Result is true if either the first row number is     smaller or if the row numbers are equal and the first index is smaller.    
* [0.x.56]*
     Store an object of the accessor class.    
* [0.x.57]*
   import functions from private base class  
* [0.x.58]*
   Execute block Jacobi preconditioning.     This function will automatically use the inverse matrices if they exist,   if not then BlockJacobi will need much time inverting the diagonal block   matrices in each preconditioning step.  
* [0.x.59]*
   Same as  [2.x.34]  since Jacobi is symmetric.  
* [0.x.60]*
   Execute block Jacobi preconditioning, adding to  [2.x.35]      This function will automatically use the inverse matrices if they exist,   if not then BlockJacobi will need much time inverting the diagonal block   matrices in each preconditioning step.  
* [0.x.61]*
   Same as  [2.x.36]  since Jacobi is symmetric.  
* [0.x.62]*
   Perform one step of the Jacobi iteration.  
* [0.x.63]*
   Perform one step of the Jacobi iteration.  
* [0.x.64]*
   Iterator starting at the first entry.  
* [0.x.65]*
   Final iterator.  
* [0.x.66]*
   Iterator starting at the first entry of row  [2.x.37]   
* [0.x.67]*
   Final iterator of row  [2.x.38]   
* [0.x.68]*
   Actual implementation of the preconditioner.     Depending on  [2.x.39]  the result of preconditioning is added to the   destination vector.  
* [0.x.69]*
 Block SOR preconditioning. This class satisfies the  [2.x.40]  "relaxation concept".
*  The functions  [2.x.41]  and  [2.x.42]  execute a (transposed) block-SOR step, based on the blocks in PreconditionBlock. The elements outside the diagonal blocks may be distributed arbitrarily.
*  See PreconditionBlock for requirements on the matrix. The blocks used in this class must be contiguous and non-overlapping. An overlapping Schwarz relaxation method can be found in RelaxationBlockSOR; that class does not offer preconditioning, though.
*  [1.x.0]
*  Optionally, the entries of the source vector can be treated in the order of the indices in the permutation vector set by #set_permutation (or the opposite order for Tvmult()). The inverse permutation is used for storing elements back into this vector. This functionality is automatically enabled after a call to set_permutation() with vectors of nonzero size.
* 

* 
*  [2.x.43]  The diagonal blocks, like the matrix, are not permuted! Therefore, the permutation vector can only swap whole blocks. It may not change the order inside blocks or swap single indices between blocks.
*  [1.x.1]
* 

* 
*  [2.x.44]  Instantiations for this template are provided for <tt> [2.x.45]  and  [2.x.46]  others can be generated in application programs (see the section on  [2.x.47]  in the manual).

* 
* [0.x.70]*
   Declare type for container size.  
* [0.x.71]*
   Default constructor.  
* [0.x.72]*
   Define number type of matrix.  
* [0.x.73]*
   import types and functions from protected base class.  
* [0.x.74]*
   Execute block SOR preconditioning.     This function will automatically use the inverse matrices if they exist,   if not then BlockSOR will waste much time inverting the diagonal block   matrices in each preconditioning step.     For matrices which are empty above the diagonal blocks BlockSOR is a   direct solver.  
* [0.x.75]*
   Execute block SOR preconditioning.     Warning: this function performs normal  [2.x.48]  without adding. The   reason for its existence is that BlockMatrixArray requires the adding   version by default. On the other hand, adding requires an additional   auxiliary vector, which is not desirable.      [2.x.49]  vmult  
* [0.x.76]*
   Backward application of vmult().     In the current implementation, this is not the transpose of vmult(). It   is a transposed Gauss-Seidel algorithm applied to the whole matrix, but   the diagonal blocks being inverted are not transposed. Therefore, it is   the transposed, if the diagonal blocks are symmetric.  
* [0.x.77]*
   Execute backward block SOR preconditioning.     Warning: this function performs normal  [2.x.50]  without adding. The   reason for its existence is that BlockMatrixArray requires the adding   version by default. On the other hand, adding requires an additional   auxiliary vector, which is not desirable.      [2.x.51]  vmult  
* [0.x.78]*
   Perform one step of the SOR iteration.  
* [0.x.79]*
   Perform one step of the transposed SOR iteration.  
* [0.x.80]*
   Constructor to be used by PreconditionBlockSSOR.  
* [0.x.81]*
   Implementation of the forward substitution loop called by vmult() and   vmult_add().     If a #permutation is set by set_permutation(), it will automatically be   honored by this function.     The parameter  [2.x.52]  does not have any function, yet.  
* [0.x.82]*
   Implementation of the backward substitution loop called by Tvmult() and   Tvmult_add().     If a #permutation is set by set_permutation(), it will automatically be   honored by this function.     The parameter  [2.x.53]  does not have any function, yet.  
* [0.x.83]*
 Block SSOR preconditioning. This class satisfies the  [2.x.54]  "relaxation concept".
*  The functions  [2.x.55]  and  [2.x.56]  execute a block-SSOR step, based on the implementation in PreconditionBlockSOR.  This class requires storage of the diagonal blocks and their inverses.
*  See PreconditionBlock for requirements on the matrix. The blocks used in this class must be contiguous and non-overlapping. An overlapping Schwarz relaxation method can be found in RelaxationBlockSSOR; that class does not offer preconditioning, though.
* 

* 
*  [2.x.57]  Instantiations for this template are provided for <tt> [2.x.58]  and  [2.x.59]  others can be generated in application programs (see the section on  [2.x.60]  in the manual).

* 
* [0.x.84]*
   Declare type for container size.  
* [0.x.85]*
   Define number type of matrix.  
* [0.x.86]*
   Constructor.  
* [0.x.87]*
   Make initialization function publicly available.  
* [0.x.88]*
   Execute block SSOR preconditioning.     This function will automatically use the inverse matrices if they exist,   if not then BlockSOR will waste much time inverting the diagonal block   matrices in each preconditioning step.  
* [0.x.89]*
   Same as vmult()  
* [0.x.90]*
   Perform one step of the SOR iteration.  
* [0.x.91]*
   Perform one step of the transposed SOR iteration.  
* [0.x.92]

include/deal.II-translator/lac/precondition_block_base_0.txt
[0.x.0]*
 A class storing the inverse diagonal blocks for block preconditioners and block relaxation methods.
*  This class does the book keeping for preconditioners and relaxation methods based on inverting blocks on the diagonal of a matrix. It allows us to either store all diagonal blocks and their inverses or the same block for each entry, and it keeps track of the choice. Thus, after initializing it and filling the inverse diagonal blocks correctly, a derived class can use inverse() with an integer argument referring to the block number.
*  Additionally, it allows the storage of the original diagonal blocks, not only the inverses. These are for instance used in the intermediate step of the SSOR preconditioner.

* 
* [0.x.1]*
   Declare type for container size.  
* [0.x.2]*
   Choose a method for inverting the blocks, and thus the data type for the   inverses.  
* [0.x.3]*
     Use the standard Gauss-Jacobi method implemented in      [2.x.0]     
* [0.x.4]*
     Use QR decomposition of the Householder class.    
* [0.x.5]*
     Use the singular value decomposition of LAPACKFullMatrix.    
* [0.x.6]*
   Constructor initializing default values.  
* [0.x.7]*
   The virtual destructor  
* [0.x.8]*
   Deletes the inverse diagonal block matrices if existent hence leaves the   class in the state that it had directly after calling the constructor.  
* [0.x.9]*
   Resize to this number of diagonal blocks with the given block size. If   <tt>compress</tt> is true, then only one block will be stored.  
* [0.x.10]*
   Tell the class that inverses are computed.  
* [0.x.11]*
   Does the matrix use only one diagonal block?  
* [0.x.12]*
   Check, whether diagonal blocks (not their inverses) should be stored.  
* [0.x.13]*
   Return true, if inverses are ready for use.  
* [0.x.14]*
   The number of blocks.  
* [0.x.15]*
   Multiply with the inverse block at position <tt>i</tt>.  
* [0.x.16]*
   Multiply with the transposed inverse block at position <tt>i</tt>.  
* [0.x.17]*
   Access to the inverse diagonal blocks if Inversion is #gauss_jordan.  
* [0.x.18]*
   Access to the inverse diagonal blocks if Inversion is #householder.  
* [0.x.19]*
   Access to the inverse diagonal blocks if Inversion is #householder.  
* [0.x.20]*
   Access to the inverse diagonal blocks.  
* [0.x.21]*
   Access to the inverse diagonal blocks if Inversion is #householder.  
* [0.x.22]*
   Access to the inverse diagonal blocks if Inversion is #householder.  
* [0.x.23]*
   Access to the diagonal blocks.  
* [0.x.24]*
   Access to the diagonal blocks.  
* [0.x.25]*
   Print some statistics about the inverses to  [2.x.1]  Output depends on   #Inversion. It is richest for svd, where we obtain statistics on extremal   singular values and condition numbers.  
* [0.x.26]*
   Determine an estimate for the memory consumption (in bytes) of this   object.  
* [0.x.27]*
   You are trying to access a diagonal block (not its inverse), but you   decided not to store the diagonal blocks.  
* [0.x.28]*
   You are accessing a diagonal block, assuming that it has a certain type.   But, the method used for inverting the diagonal blocks does not use this   type  
* [0.x.29]*
   The method used for inverting blocks.  
* [0.x.30]*
   The number of (inverse) diagonal blocks, if only one is stored.  
* [0.x.31]*
   Storage of the inverse matrices of the diagonal blocks matrices as   <tt>FullMatrix<number></tt> matrices, if Inversion #gauss_jordan is used.   Using <tt>number=float</tt> saves memory in comparison with   <tt>number=double</tt>, but may introduce numerical instability.  
* [0.x.32]*
   Storage of the inverse matrices of the diagonal blocks matrices as   <tt>Householder</tt> matrices if Inversion #householder is used. Using   <tt>number=float</tt> saves memory in comparison with   <tt>number=double</tt>, but may introduce numerical instability.  
* [0.x.33]*
   Storage of the inverse matrices of the diagonal blocks matrices as   <tt>LAPACKFullMatrix</tt> matrices if Inversion #svd is used. Using   <tt>number=float</tt> saves memory in comparison with   <tt>number=double</tt>, but may introduce numerical instability.  
* [0.x.34]*
   Storage of the original diagonal blocks.     Used by the blocked SSOR method.  
* [0.x.35]*
   This is true, if the field #var_diagonal is to be used.  
* [0.x.36]*
   This is true, if only one inverse is stored.  
* [0.x.37]*
   The inverse matrices are usable. Set by the parent class via   inverses_computed().  
* [0.x.38]

include/deal.II-translator/lac/precondition_block.templates_0.txt
[0.x.0]

include/deal.II-translator/lac/precondition_selector_0.txt
[0.x.0]!  [2.x.0]  Preconditioners [2.x.1] 

* 
* [0.x.1]*
 Selects the preconditioner. The constructor of this class takes the name of the preconditioning and the damping parameter  [2.x.2]  of the preconditioning and the  [2.x.3]  function takes the matrix that is used by the matrix-builtin precondition functions. Each time, the <tt>operator()</tt> function is called, this preselected preconditioner, this matrix and this  [2.x.4]  is used for the preconditioning. This class is designed for being used as argument of the  [2.x.5]  function of a  [2.x.6]  Solver and it covers the selection of all matrix-builtin precondition functions. The selection of other preconditioners, like BlockSOR or ILU should be handled in derived classes by the user.
*  [1.x.0] The simplest use of this class is the following:

* 
* [1.x.1]
*  The same example where also the  [2.x.7]  class is used reads

* 
* [1.x.2]
*  Now the use of the  [2.x.8]  in combination with the  [2.x.9]  PreconditionSelector allows the user to select both, the solver and the preconditioner, at the beginning of their program and each time the solver is started (that is several times e.g. in a nonlinear iteration) this preselected solver and preconditioner is called.

* 
* [0.x.2]*
   Declare type for container size.  
* [0.x.3]*
   Constructor.  [2.x.10]  denotes the damping parameter of the   preconditioning.  
* [0.x.4]*
   Destructor.  
* [0.x.5]*
   Takes the matrix that is needed for preconditionings that involves a   matrix. e.g. for  [2.x.11]  <tt>~_sor</tt>, <tt>~_ssor</tt>.  
* [0.x.6]*
   Return the dimension of the codomain (or range) space. Note that the   matrix is of dimension  [2.x.12] .  
* [0.x.7]*
   Return the dimension of the domain space. Note that the matrix is of   dimension  [2.x.13] .  
* [0.x.8]*
   Precondition procedure. Calls the preconditioning that was specified in   the constructor.  
* [0.x.9]*
   Transpose precondition procedure. Calls the preconditioning that was   specified in the constructor.  
* [0.x.10]*
   Get the names of all implemented preconditionings. The list of possible   options includes:    [2.x.14]     [2.x.15]   "none"  [2.x.16]     [2.x.17]   "jacobi"  [2.x.18]     [2.x.19]   "sor"  [2.x.20]     [2.x.21]   "ssor"  [2.x.22]     [2.x.23]   
* [0.x.11]*
    [2.x.24]  Exceptions    [2.x.25]   
* [0.x.12]*
   Exception.  
* [0.x.13]*
   Stores the name of the preconditioning.  
* [0.x.14]*
   Matrix that is used for the matrix-builtin preconditioning function. cf.   also  [2.x.26]   
* [0.x.15]*
   Stores the damping parameter of the preconditioner.  
* [0.x.16]

include/deal.II-translator/lac/qr_0.txt
[0.x.0]*
 A base class for thin QR implementations.
*  This class and classes derived from it are meant to build  [2.x.0]  and  [2.x.1]  matrices one row/column at a time, i.e., by growing  [2.x.2]  matrix from an empty  [2.x.3]  matrix to  [2.x.4] , where  [2.x.5]  is the number of added column vectors.
*  As a consequence, matrices which have the same number of rows as each vector (i.e.  [2.x.6]  matrix) is stored as a collection of vectors of `VectorType`.

* 
* [0.x.1]*
   Number type for R matrix.  
* [0.x.2]*
   Default private constructor.  
* [0.x.3]*
   Destructor.  
* [0.x.4]*
   Append  [2.x.7]  to the QR factorization.   Returns  [2.x.8]  if the result is successful, i.e.   the columns are linearly independent. Otherwise the  [2.x.9]    is rejected and the return value is  [2.x.10] .  
* [0.x.5]*
   Remove a column  [2.x.11]  and update QR factorization.  
* [0.x.6]*
   Return size of the subspace.  
* [0.x.7]*
   Return the current upper triangular matrix R.  
* [0.x.8]*
   Solve  [2.x.12] . Vectors  [2.x.13]  and  [2.x.14]  should be consistent   with the current size of the subspace.   If  [2.x.15]  is  [2.x.16] ,  [2.x.17]  is solved instead.  
* [0.x.9]*
   Set  [2.x.18] . The size of  [2.x.19]  should be consistent with the   size of the R matrix.  
* [0.x.10]*
   Set  [2.x.20] . The size of  [2.x.21]  should be consistent with the   size of column vectors.  
* [0.x.11]*
   Set  [2.x.22] . The size of  [2.x.23]  should be consistent with the   size of the R matrix.  
* [0.x.12]*
   Set  [2.x.24] . The size of  [2.x.25]  should be consistent with the   size of column vectors.  
* [0.x.13]*
   Connect a slot to retrieve a notification when the Givens rotations   are performed.     The function takes two indices,  [2.x.26]  and  [2.x.27]  describing the plane of   rotation, and a triplet of numbers  [2.x.28]  (cosine, sine and radius, see    [2.x.29]  which represents the rotation   matrix.  
* [0.x.14]*
   Compute  [2.x.30]  where  [2.x.31]  is the matrix formed by the column vectors stored   by this object.  
* [0.x.15]*
   Multiply with transpose columns stored in the object.  
* [0.x.16]*
   A vector of unique pointers to store columns.  
* [0.x.17]*
   Matrix to store R.  
* [0.x.18]*
   Current size (number of columns in Q).  
* [0.x.19]*
   Signal used to retrieve a notification   when Givens rotations are performed in the `(i,j)`-plane.  
* [0.x.20]*
 A class to compute and store the QR factorization of a matrix represented by a set of column vectors.
*  The class is design to update a given (possibly empty) QR factorization of a matrix  [2.x.32]  (constructed incrementally by providing its columns) due to the addition of a new column vector to  [2.x.33] . This is equivalent to constructing an orthonormal basis by the Gram-Schmidt procedure. The class also provides update functionality when the first column is removed.
*  The `VectorType` template argument may either be a parallel and serial vector, and only need to have basic operations such as additions, scalar product, etc. It also needs to have a copy-constructor.
*  See sections 6.5.2-6.5.3 on pp. 335-338 in

* 
* [1.x.0]
*  as well as

* 
* [1.x.1]
* 

* 
* [0.x.21]*
   Number type for R matrix.  
* [0.x.22]*
   Default constructor.  
* [0.x.23]*
   Destructor.  
* [0.x.24]*
    [2.x.34]   [2.x.35]     
*  [2.x.36]  Currently this function always returns  [2.x.37] .  
* [0.x.25]*
   Remove first column and update QR factorization.     Starting from the given QR decomposition    [2.x.38]    we aim at computing factorization of    [2.x.39] .     The standard approach is to partition  [2.x.40]  as   [1.x.2]   It then follows that   [1.x.3]   is upper Hessenberg where unwanted sub-diagonal elements can be   zeroed by a sequence of Givens rotations.     Note that  [2.x.41] ,   where the RHS is included in  [2.x.42] . Therefore    [2.x.43]  can be obtained by Cholesky decomposition.  
* [0.x.26]*
   Apply givens rotation in the `(i,j)`-plane to  [2.x.44]  and  [2.x.45]  so that    [2.x.46]  is zeroed.     See Chapter 5.1.9 of Golub 2013, Matrix computations.  
* [0.x.27]*
   Temporary vector needed to do Givens rotation of Q.  
* [0.x.28]*
 A class to obtain the triangular  [2.x.47]  matrix of the  [2.x.48]  factorization together with the matrix  [2.x.49]  itself. The orthonormal matrix  [2.x.50]  is not stored explicitly, the name of the class. The multiplication with  [2.x.51]  can be represented as  [2.x.52] , whereas the multiplication with  [2.x.53]  is given by  [2.x.54] .
*  The class is designed to update a given (possibly empty) QR factorization due to the addition of a new column vector. This is equivalent to constructing an orthonormal basis by the Gram-Schmidt procedure. The class also provides update functionality when the column is removed.
*  The `VectorType` template argument may either be a parallel and serial vector, and only need to have basic operations such as additions, scalar product, etc. It also needs to have a copy-constructor.

* 
* [0.x.29]*
   Number type for R matrix.  
* [0.x.30]*
   Default constructor.  
* [0.x.31]*
   Destructor.  
* [0.x.32]*
   Remove column and update QR factorization.     Starting from the given QR decomposition    [2.x.55]    we aim at computing factorization of    [2.x.56] .     Note that  [2.x.57] ,   where the RHS is included in  [2.x.58] . Therefore    [2.x.59]  can be obtained by Cholesky decomposition.  
* [0.x.33]*
   Connect a slot to implement a custom check of linear dependency   during addition of a column.     Here,  [2.x.60]  is the last column of the to-be R matrix,  [2.x.61]    is its diagonal and  [2.x.62]  is the square of the  [2.x.63]  norm of the column.   The function should return  [2.x.64]  if the new column is   linearly independent.  
* [0.x.34]*
   Apply givens rotation in the `(i,k)`-plane to zero out  [2.x.65] .  
* [0.x.35]*
   Signal used to decide if the new column is linear dependent.     Here,  [2.x.66]  is the last column of the to-be R matrix,  [2.x.67]    is its diagonal and  [2.x.68]  is the square of the  [2.x.69]  norm of the column.   The function should return  [2.x.70]  if the new column is   linearly independent.  
* [0.x.36]

include/deal.II-translator/lac/read_write_vector_0.txt
[0.x.0]!  [2.x.0]  Vectors   [2.x.1]   
* [0.x.1]*
   ReadWriteVector is intended to represent vectors in  [2.x.2]  for   which it stores all or a subset of elements. The latter case in important   in parallel computations, where  [2.x.3]  may be so large that no processor can   actually all elements of a solution vector, but where this is also not   necessary: one typically only has to store the values of degrees of   freedom that live on cells that are locally owned plus potentially those   degrees of freedom that live on ghost cells.     This class allows to access individual elements to be read or written.   However, it does not allow global operations such as taking the norm.   ReadWriteVector can be used to read and write elements in vectors derived   from VectorSpaceVector such as  [2.x.4]  and    [2.x.5]      [1.x.0] Most of the time, one will simply read from or   write into a vector of the current class using the global numbers of   these degrees of freedom. This is done using operator()() or operator[]()   which call global_to_local() to transform the [1.x.1] index into a   [1.x.2] one. In such cases, it is clear that one can only access   elements of the vector that the current object indeed stores.     However, it is also possible to access elements in the order in which   they are stored by the current object. In other words, one is not   interested in accessing elements with their [1.x.3] indices, but   instead using an enumeration that only takes into account the elements   that are actually stored. This is facilitated by the local_element()   function. To this end, it is necessary to know [1.x.4] the   current class stores its element. The elements of all the consecutive   ranges are stored in ascending order of the first index of each range.   The function  [2.x.6]  can be used to   get the first index of the largest range.  
* [0.x.2]*
     Declare standard types used in all containers. These types parallel     those in the <tt>C++</tt> standard libraries <tt>vector<...></tt>     class.    
* [0.x.3]*
      [2.x.7]  1: Basic Object-handling    
* [0.x.4]*
     Empty constructor.    
* [0.x.5]*
     Copy constructor.    
* [0.x.6]*
     Construct a vector given the size, the stored elements have their     index in [0,size).    
* [0.x.7]*
     Construct a vector whose stored elements indices are given by the     IndexSet  [2.x.8]     
* [0.x.8]*
     Destructor.    
* [0.x.9]*
     Set the global size of the vector to  [2.x.9]  The stored elements have     their index in [0,size).         If the flag  [2.x.10]  is set to false, the memory will be     initialized with zero, otherwise the memory will be untouched (and the     user must make sure to fill it with reasonable data before using it).    
* [0.x.10]*
     Uses the same IndexSet as the one of the input vector  [2.x.11]  and     allocates memory for this vector.         If the flag  [2.x.12]  is set to false, the memory will be     initialized with zero, otherwise the memory will be untouched (and the     user must make sure to fill it with reasonable data before using it).    
* [0.x.11]*
     Initializes the vector. The indices are specified by  [2.x.13]      locally_stored_indices.         If the flag  [2.x.14]  is set to false, the memory will be     initialized with zero, otherwise the memory will be untouched (and the     user must make sure to fill it with reasonable data before using it).     locally_stored_indices.    
* [0.x.12]*
     Initialize this ReadWriteVector by supplying access to all locally     available entries in the given ghosted or non-ghosted vector.        
*  [2.x.15]  This function currently copies the values from the argument into     the ReadWriteVector, so modifications here will not modify  [2.x.16]          This function is mainly written for backwards-compatibility to get     element access to a ghosted  [2.x.17]  inside the     library.    
* [0.x.13]*
     Apply the functor  [2.x.18]  to each element of the vector. The functor     should look like    
* [1.x.5]
*         
*  [2.x.19]  This function requires that the header read_write_vector.templates.h     be included.    
* [0.x.14]*
     Swap the contents of this vector and the other vector  [2.x.20]  One could     do this operation with a temporary variable and copying over the data     elements, but this function is significantly more efficient since it     only swaps the pointers to the data of the two vectors and therefore     does not need to allocate temporary storage and move data around.         This function is analogous to the  [2.x.21]  function of all C++     standard containers. Also, there is a global function     <tt>swap(u,v)</tt> that simply calls <tt>u.swap(v)</tt>, again in     analogy to standard functions.    
* [0.x.15]*
     Copies the data and the IndexSet of the input vector  [2.x.22]     
* [0.x.16]*
     Copies the data and the IndexSet of the input vector  [2.x.23]     
* [0.x.17]*
     Sets all elements of the vector to the scalar  [2.x.24]  This operation is     only allowed if  [2.x.25]  is equal to zero.    
* [0.x.18]*
     Imports all the elements present in the vector's IndexSet from the     input vector  [2.x.26]   [2.x.27]   [2.x.28]      is used to decide if the elements in  [2.x.29]  should be added to the     current vector or replace the current elements.        
*  [2.x.30]  The parameter  [2.x.31]  is ignored since we are       dealing with a serial vector here.    
* [0.x.19]*
     Imports all the elements present in the vector's IndexSet from the     input vector  [2.x.32]   [2.x.33]   [2.x.34]      is used to decide if the elements in  [2.x.35]  should be added to the     current vector or replace the current elements.        
*  [2.x.36]  The parameter  [2.x.37]  is ignored since we are       dealing with a serial vector here.    
* [0.x.20]*
     Imports all the elements present in the vector's IndexSet from the     input vector  [2.x.38]   [2.x.39]   [2.x.40]      is used to decide if the elements in  [2.x.41]  should be added to the     current vector or replace the current elements. The last parameter can     be used if the same communication pattern is used multiple times. This     can be used to improve performance.    
* [0.x.21]*
     Imports all the elements present in the vector's IndexSet from the input     vector  [2.x.42]   [2.x.43]   [2.x.44]  is used to decide     if the elements in  [2.x.45]  should be added to the current vector or replace     the current elements. The last parameter can be used if the same     communication pattern is used multiple times. This can be used to improve     performance.    
* [0.x.22]*
     Imports all the elements present in the vector's IndexSet from the input     vector  [2.x.46]   [2.x.47]   [2.x.48]  is used to     decide if the elements in  [2.x.49]  should be added to the current vector or     replace the current elements. The last parameter can be used if the same     communication pattern is used multiple times. This can be used to improve     performance.        
*  [2.x.50]  The  [2.x.51]  is not allowed to have ghost entries.    
* [0.x.23]*
     Imports all the elements present in the vector's IndexSet from the input     vector  [2.x.52]   [2.x.53]   [2.x.54]  is used to     decide if the elements in  [2.x.55]  should be added to the current vector or     replace the current elements. The last parameter can be used if the same     communication pattern is used multiple times. This can be used to improve     performance.    
* [0.x.24]*
     Imports all the elements present in the vector's IndexSet from the input     vector  [2.x.56]   [2.x.57]   [2.x.58]  is used to     decide if the elements in  [2.x.59]  should be added to the current vector or     replace the current elements. The last parameter can be used if the same     communication pattern is used multiple times. This can be used to improve     performance.    
* [0.x.25]*
     Import all the elements present in the vector's IndexSet from the input     vector  [2.x.60]   [2.x.61]   [2.x.62]  is used to     decide if the elements in  [2.x.63]  should be added to the current vector or     replace the current elements. The last parameter is not used.    
* [0.x.26]*
     The value returned by this function denotes the dimension of the vector     spaces that are modeled by objects of this kind. However, objects of     the current class do not actually stores all elements of vectors of     this space but may, in fact store only a subset. The number of elements     stored is returned by n_elements() and is smaller or equal to the     number returned by the current function.    
* [0.x.27]*
     This function returns the number of elements stored. It is smaller or     equal to the dimension of the vector space that is modeled by an object     of this kind. This dimension is return by size().          [2.x.64]  use locally_owned_size() instead.    
* [0.x.28]*
     Return the local size of the vector, i.e., the number of indices     owned locally.    
* [0.x.29]*
     Return the IndexSet that represents the indices of the elements stored.    
* [0.x.30]*
     Make the  [2.x.65]  class a bit like the <tt>vector<></tt>     class of the C++ standard library by returning iterators to the start     and end of the [1.x.6] elements of this vector.    
* [0.x.31]*
     Return constant iterator to the start of the locally stored elements     of the vector.    
* [0.x.32]*
     Return an iterator pointing to the element past the end of the array     of locally stored entries.    
* [0.x.33]*
     Return a constant iterator pointing to the element past the end of the     array of the locally stored entries.    
* [0.x.34]*
      [2.x.66]  2: Data-Access    
* [0.x.35]*
     Read access to the data in the position corresponding to  [2.x.67]      global_index. An exception is thrown if  [2.x.68]  is not stored     by the current object.    
* [0.x.36]*
     Read and write access to the data in the position corresponding to  [2.x.69]      global_index. An exception is thrown if  [2.x.70]  is not stored     by the current object.    
* [0.x.37]*
     Read access to the data in the position corresponding to  [2.x.71]      global_index. An exception is thrown if  [2.x.72]  is not stored     by the current object.         This function does the same thing as operator().    
* [0.x.38]*
     Read and write access to the data in the position corresponding to  [2.x.73]      global_index. An exception is thrown if  [2.x.74]  is not stored     by the current object.         This function does the same thing as operator().    
* [0.x.39]*
     Instead of getting individual elements of a vector via operator(),     this function allows getting a whole set of elements at once. The     indices of the elements to be read are stated in the first argument, the     corresponding values are returned in the second.         If the current vector is called  [2.x.75]  then this function is the equivalent     to the code    
* [1.x.7]
*           [2.x.76]  The sizes of the  [2.x.77]  and  [2.x.78]  arrays must be identical.    
* [0.x.40]*
     Instead of getting individual elements of a vector via operator(),     this function allows getting a whole set of elements at once. In     contrast to the previous function, this function obtains the     indices of the elements by dereferencing all elements of the iterator     range provided by the first two arguments, and puts the vector     values into memory locations obtained by dereferencing a range     of iterators starting at the location pointed to by the third     argument.         If the current vector is called  [2.x.79]  then this function is the equivalent     to the code    
* [1.x.8]
*           [2.x.80]  It must be possible to write into as many memory locations       starting at  [2.x.81]  as there are iterators between        [2.x.82]  and  [2.x.83]     
* [0.x.41]*
     Read access to the data field specified by  [2.x.84]  When you     access elements in the order in which they are stored, it is necessary     that you know in which they are stored. In other words, you need to     know the map between the global indices of the elements this class     stores, and the local indices into the contiguous array of these global     elements. For this, see the general documentation of this class.         Performance: Direct array access (fast).    
* [0.x.42]*
     Read and write access to the data field specified by  [2.x.85]      When you access elements in the order in which they are stored, it is     necessary that you know in which they are stored. In other words, you     need to know the map between the global indices of the elements this     class stores, and the local indices into the contiguous array of these     global elements. For this, see the general documentation of this class.         Performance: Direct array access (fast).    
* [0.x.43]*
      [2.x.86]  3: Modification of vectors    
* [0.x.44]*
     This function adds a whole set of values stored in  [2.x.87]  to the     vector components specified by  [2.x.88]     
* [0.x.45]*
     This function is similar to the previous one but takes a     ReadWriteVector of values.    
* [0.x.46]*
     Take an address where <tt>n_elements</tt> are stored contiguously and     add them into the vector. Handles all cases which are not covered by     the other two <tt>add()</tt> functions above.    
* [0.x.47]*
     Prints the vector to the output stream  [2.x.89]     
* [0.x.48]*
     Return the memory consumption of this class in bytes.    
* [0.x.49]*
     Import all the elements present in the vector's IndexSet from the input     vector  [2.x.90]  This is an helper function and it should not be     used directly.    
* [0.x.50]*
     Import all the elements present in the vector's IndexSet from the input     vector  [2.x.91]  This is an helper function and it should not be     used directly.    
* [0.x.51]*
     Return the local position of  [2.x.92]     
* [0.x.52]*
     A helper function that is used to resize the val array.    
* [0.x.53]*
     Return a  [2.x.93]  and store it for future     use.    
* [0.x.54]*
     Return a  [2.x.94]  and store it for future     use.    
* [0.x.55]*
     Indices of the elements stored.    
* [0.x.56]*
     IndexSet of the elements of the last imported vector;    
* [0.x.57]*
     CommunicationPattern for the communication between the     source_stored_elements IndexSet and the current vector.    
* [0.x.58]*
     Pointer to the array of local elements of this vector.    
* [0.x.59]*
     For parallel loops with TBB, this member variable stores the affinity     information of loops.    
* [0.x.60]*
     This class provides a wrapper around a Functor which acts on     single elements of the vector. This is necessary to use      [2.x.95]  which requires a TBBForFunctor.    
* [0.x.61]*
       Constructor. Take a functor and store a copy of it.      
* [0.x.62]*
       Evaluate the element with the stored copy of the functor.      
* [0.x.63]*
       Alias to the ReadWriteVector object that owns the FunctorTemplate.      
* [0.x.64]*
       Copy of the functor.      
* [0.x.65]*
 Global function  [2.x.96]  which overloads the default implementation of the C++ standard library which uses a temporary object. The function simply exchanges the data of the two vectors.
*   [2.x.97]  Vector

* 
* [0.x.66]

include/deal.II-translator/lac/read_write_vector.templates_0.txt
[0.x.0]

include/deal.II-translator/lac/relaxation_block_0.txt
[0.x.0]*
 Base class for the implementation of overlapping, multiplicative Schwarz relaxation methods and smoothers.
*  This class uses the infrastructure provided by PreconditionBlockBase. It adds functions to initialize with a block list and to do the relaxation step. The actual relaxation method with the interface expected by SolverRelaxation and MGSmootherRelaxation is in the derived classes.
*  This class allows for more general relaxation methods than PreconditionBlock, since the index sets may be arbitrary and overlapping, while there only contiguous, disjoint sets of equal size are allowed. As a drawback, this class cannot be used as a preconditioner, since its implementation relies on a straight forward implementation of the Gauss- Seidel process.
*  Parallel computations require you to specify an initialized ghost vector in  [2.x.0] 
* 

* 
*  [2.x.1] 

* 
* [0.x.1]*
   Define number type of matrix.  
* [0.x.2]*
   Value type for inverse matrices.  
* [0.x.3]*
   Declare type for container size.  
* [0.x.4]*
   Parameters for block relaxation methods. In addition to typical control   parameters like #relaxation, this object also contains the block   structure in #block_list and an optional ordering of the blocks in   #order.  
* [0.x.5]*
     Constructor.    
* [0.x.6]*
     The mapping from indices to blocks. Each row of this pattern enumerates     the indices constituting a diagonal block to be inverted.    
* [0.x.7]*
     Relaxation parameter.    
* [0.x.8]*
     Invert diagonal during initialization. Alternatively, diagonal blocks     are inverted on the fly, whenever they are used. While inverting blocks     in advance requires more memory, it usually saves a lot of computation.     See #same_diagonal on how you can avoid memory overhead.    
* [0.x.9]*
     Assume all diagonal blocks are equal to save memory. If this flag is     true, then only the first diagonal block of the matrix is inverted and     stored. It is then used for all other blocks.         \note Avoid setting this true if your blocks are not equal, in     particular if their sizes differ.    
* [0.x.10]*
     Choose the inversion method for the blocks.    
* [0.x.11]*
     If #inversion is SVD, we can compute the Penrose-Moore inverse of the     blocks. In order to do so, we can specify here the threshold below     which a singular value will be considered zero and thus not inverted.     Setting this parameter to a value greater than zero takes precedence over     threshold, i.e. kernel_size must be zero if you want to use threshold.     This parameter is used in the call to      [2.x.2]     
* [0.x.12]*
     If #inversion is SVD, we can compute the Penrose-Moore inverse of the     blocks. In order to do so, we can specify here the size of the kernel     that will not be inverted but considered zero. Setting this parameter     to a value greater than zero takes precedence over threshold, i.e.     kernel_size must be zero if you want to use threshold.     This parameter is used in the call to      [2.x.3]     
* [0.x.13]*
     The order in which blocks should be traversed. This vector can initiate     several modes of execution:          [2.x.4]           [2.x.5] If the length of the vector is zero, then the relaxation method     will be executed from first to last block. [2.x.6]           [2.x.7]  If the length is one, then the inner vector must have the same     size as the number of blocks. The relaxation method is applied in the     order given in this vector. [2.x.8]           [2.x.9]  If the outer vector has length greater one, then the relaxation     method is applied several times, each time in the order given by the     inner vector of the corresponding index. This mode can for instance be     used for ADI methods and similar direction sweeps. [2.x.10]           [2.x.11]     
* [0.x.14]*
     Temporary ghost vector that is used in the relaxation method when     performing parallel MPI computations. The user is required to have this     point to an initialized vector that contains all indices     that appear in the  [2.x.12]  sa ghost values. Typically, this the     set of locally active level DoFs. Unused when VectorType is a serial     vector type like Vector<double>.    
* [0.x.15]*
     Return the memory allocated in this object.    
* [0.x.16]*
   Initialize matrix and additional information. In a second step, the   inverses of the diagonal blocks may be computed.     Note that AdditionalData, different from other preconditioners, defines   quite large objects, and that therefore the object is not copied, but   rather a pointer is stored. Thus, the lifetime of    [2.x.13]  hast to exceed the lifetime of this object.  
* [0.x.17]*
   Deletes the inverse diagonal block matrices if existent, sets the   blocksize to 0, hence leaves the class in the state that it had directly   after calling the constructor.  
* [0.x.18]*
   Stores the inverse of the diagonal blocks in  [2.x.14]  This costs some   additional memory
* 
*  - for DG methods about 1/3 (for double inverses) or 1/6   (for float inverses) of that used for the matrix
* 
*  - but it makes the   preconditioning much faster.     It is not allowed to call this function twice (will produce an error)   before a call of <tt>clear(...)</tt> because at the second time there   already exist the inverse matrices.     After this function is called, the lock on the matrix given through the    [2.x.15]  function is released, i.e. you may overwrite of delete it.   You may want to do this in case you use this matrix to precondition   another matrix.  
* [0.x.19]*
   Perform one block relaxation step.     Depending on the arguments  [2.x.16]  and  [2.x.17]  this performs an SOR step   (both reference the same vector) or a Jacobi step (both are different   vectors). For the Jacobi step, the calling function must copy  [2.x.18]  to    [2.x.19]  after this.  
* [0.x.20]*
   Pointer to the matrix. Make sure that the matrix exists as long as this   class needs it, i.e. until calling  [2.x.20]  or (if the   inverse matrices should not be stored) until the last call of the   preconditioning  [2.x.21]  function of the derived classes.  
* [0.x.21]*
   Control information.  
* [0.x.22]*
   Computes (the inverse of) a range of blocks.  
* [0.x.23]*
 Block Jacobi (additive Schwarz) method with possibly overlapping blocks.
*  This class implements the step() and Tstep() functions expected by the  [2.x.22]  "relaxation concept". They perform an additive Schwarz method on the blocks provided in the block list of AdditionalData. Differing from PreconditionBlockJacobi, these blocks may be of varying size, non- contiguous, and overlapping. On the other hand, this class does not implement the preconditioner interface expected by Solver objects.
* 

* 
*  [2.x.23] 

* 
* [0.x.24]*
   Default constructor.  
* [0.x.25]*
   Define number type of matrix.  
* [0.x.26]*
   Make type publicly available.  
* [0.x.27]*
   Make initialization function publicly available.  
* [0.x.28]*
   Make function of base class public again.  
* [0.x.29]*
   Make function of base class public again.  
* [0.x.30]*
   Make function of base class public again.  
* [0.x.31]*
   Make function of base class public again.  
* [0.x.32]*
   Make function of base class public again.  
* [0.x.33]*
   Make function of base class public again.  
* [0.x.34]*
   Perform one step of the Jacobi iteration.  
* [0.x.35]*
   Perform one step of the Jacobi iteration.  
* [0.x.36]*
   Implements a vmult() operation, which for this class first sets the dst()   vector to zero before calling the step() method.  
* [0.x.37]*
   Implements a transpose vmult operation, which for this class first sets   the dst() vector to zero before calling the Tstep() method.  
* [0.x.38]*
 Block Gauss-Seidel method with possibly overlapping blocks.
*  This class implements the step() and Tstep() functions expected by the  [2.x.24]  "relaxation concept". They perform a multiplicative Schwarz method on the blocks provided in the block list of AdditionalData.  Differing from PreconditionBlockSOR, these blocks may be of varying size, non-contiguous, and overlapping. On the other hand, this class does not implement the preconditioner interface expected by Solver objects.
* 

* 
*  [2.x.25] 

* 
* [0.x.39]*
   Default constructor.  
* [0.x.40]*
   Define number type of matrix.  
* [0.x.41]*
   Make type publicly available.  
* [0.x.42]*
   Make initialization function publicly available.  
* [0.x.43]*
   Make function of base class public again.  
* [0.x.44]*
   Make function of base class public again.  
* [0.x.45]*
   Make function of base class public again.  
* [0.x.46]*
   Make function of base class public again.  
* [0.x.47]*
   Make function of base class public again.  
* [0.x.48]*
   Make function of base class public again.  
* [0.x.49]*
   Perform one step of the SOR iteration.  
* [0.x.50]*
   Perform one step of the transposed SOR iteration.  
* [0.x.51]*
   Implements a vmult() operation, which for this class first sets the dst()   vector to zero before calling the step() method.  
* [0.x.52]*
   Implements a transpose vmult operation, which for this class first sets   the dst() vector to zero before calling the Tstep() method.  
* [0.x.53]*
 Symmetric block Gauss-Seidel method with possibly overlapping blocks.
*  This class implements the step() and Tstep() functions expected by the  [2.x.26]  "relaxation concept". They perform a multiplicative Schwarz method on the blocks provided in the block list of AdditionalData in symmetric fashion. Differing from PreconditionBlockSSOR, these blocks may be of varying size, non-contiguous, and overlapping. On the other hand, this class does not implement the preconditioner interface expected by Solver objects.
* 

* 
*  [2.x.27] 

* 
* [0.x.54]*
   Define number type of matrix.  
* [0.x.55]*
   Make type publicly available.  
* [0.x.56]*
   Make initialization function publicly available.  
* [0.x.57]*
   Make function of base class public again.  
* [0.x.58]*
   Make function of base class public again.  
* [0.x.59]*
   Make function of base class public again.  
* [0.x.60]*
   Make function of base class public again.  
* [0.x.61]*
   Make function of base class public again.  
* [0.x.62]*
   Make function of base class public again.  
* [0.x.63]*
   Perform one step of the SSOR iteration.  
* [0.x.64]*
   Perform one step of the transposed SSOR iteration.  
* [0.x.65]*
   Implements a vmult() operation, which for this class first sets the dst()   vector to zero before calling the step() method.  
* [0.x.66]*
   Implements a transpose vmult operation, which for this class first sets   the dst() vector to zero before calling the Tstep() method.  
* [0.x.67]

include/deal.II-translator/lac/relaxation_block.templates_0.txt
[0.x.0]*
   Default implementation for serial vectors. Here we don't need to make a   copy into a ghosted vector, so just return a reference to  [2.x.0]   
* [0.x.1]*
   Specialization for Trilinos. Use the ghosted vector.  
* [0.x.2]

include/deal.II-translator/lac/scalapack_0.txt
[0.x.0]*
 A wrapper class around ScaLAPACK parallel dense linear algebra.
*  ScaLAPACK assumes that matrices are distributed according to the block-cyclic decomposition scheme. An  [2.x.0]  by  [2.x.1]  matrix is first decomposed into  [2.x.2]  by  [2.x.3]  blocks which are then uniformly distributed across the 2D process grid with  [2.x.4]  processes, where  [2.x.5]  are grid dimensions and  [2.x.6]  is the total number of processes. The parameters MB and NB are referred to as row and column block size and determine the granularity of the block-cyclic distribution.
*  In the following the block-cyclic distribution of a  [2.x.7]  matrix onto a  [2.x.8]  Cartesian process grid with block sizes  [2.x.9]  is displayed.
*  \htmlonly <style>div.image img[src="scalapack_block_cycling.png"]{width:35%;}</style> \endhtmlonly  [2.x.10] 
*  Note that the odd number of columns of the local matrices owned by the processes P2, P5 and P8 accounts for  [2.x.11]  not being an integral multiple of  [2.x.12] .
*  The choice of the block sizes is a compromise between a sufficiently large size for efficient local/serial BLAS, but one that is also small enough to achieve good parallel load balance.
*  Below we show a strong scaling example of  [2.x.13]  on up to 5 nodes each composed of two Intel Xeon 2660v2 IvyBridge sockets 2.20GHz, 10 cores/socket. Calculations are performed on square processor grids 1x1, 2x2, 3x3, 4x4, 5x5, 6x6, 7x7, 8x8, 9x9, 10x10.
*   [2.x.14] 
* 

* 
*  [2.x.15] 

* 
* [0.x.1]*
   Declare the type for container size.  
* [0.x.2]*
   Constructor for a rectangular matrix with  [2.x.16]  and  [2.x.17]    and distributed using the grid  [2.x.18]      The parameters  [2.x.19]  and  [2.x.20]  are the block sizes used   for the block-cyclic distribution of the matrix.   In general, it is recommended to use powers of  [2.x.21] , e.g.  [2.x.22] .  
* [0.x.3]*
   Constructor for a square matrix of size  [2.x.23]  and distributed   using the process grid in  [2.x.24]      The parameter  [2.x.25]  is used for the block-cyclic distribution of the matrix.   An identical block size is used for the rows and columns of the matrix.   In general, it is recommended to use powers of  [2.x.26] , e.g.  [2.x.27] .  
* [0.x.4]*
   Constructor for a general rectangular matrix that is read from   the file  [2.x.28]  and distributed using the grid  [2.x.29]      Loads the matrix from file  [2.x.30]  using HDF5.   In case that deal.II was built without HDF5   a call to this function will cause an exception to be thrown.     The parameters  [2.x.31]  and  [2.x.32]  are the block sizes used   for the block-cyclic distribution of the matrix.   In general, it is recommended to use powers of  [2.x.33] , e.g.  [2.x.34] .  
* [0.x.5]*
   Destructor  
* [0.x.6]*
   Initialize the rectangular matrix with  [2.x.35]  and  [2.x.36]    and distributed using the grid  [2.x.37]      The parameters  [2.x.38]  and  [2.x.39]  are the block sizes used   for the block-cyclic distribution of the matrix.   In general, it is recommended to use powers of  [2.x.40] , e.g.  [2.x.41] .  
* [0.x.7]*
   Initialize the square matrix of size  [2.x.42]  and distributed using the grid  [2.x.43]      The parameter  [2.x.44]  is used for the block-cyclic distribution of the matrix.   An identical block size is used for the rows and columns of the matrix.   In general, it is recommended to use powers of  [2.x.45] , e.g.  [2.x.46] .  
* [0.x.8]*
   Assign  [2.x.47]  to this matrix.  
* [0.x.9]*
   Return current  [2.x.48]  of this matrix  
* [0.x.10]*
   Return current  [2.x.49]  of this matrix  
* [0.x.11]*
   Assignment operator from a regular FullMatrix.    
*  [2.x.50]  This function should only be used for relatively small matrix   dimensions. It is primarily intended for debugging purposes.  
* [0.x.12]*
   Copies the content of the locally owned  [2.x.51]  to the distributed matrix.   The distributed matrix and  [2.x.52]  on process  [2.x.53]  must have matching dimensions.     For all processes except the process with rank  [2.x.54]  the serial  [2.x.55]  is not referenced.   The user has to ensure that all processes call this with identical  [2.x.56]    The  [2.x.57]  refers to a process of the MPI communicator used to create the process grid   of the distributed matrix.  
* [0.x.13]*
   Copy the contents of the distributed matrix into  [2.x.58]     
*  [2.x.59]  This function should only be used for relatively small matrix   dimensions. It is primarily intended for debugging purposes.  
* [0.x.14]*
   Copies the content of the distributed matrix into the locally replicated  [2.x.60]    on the process with rank  [2.x.61]  For all processes except  [2.x.62]   [2.x.63]  is not referenced.   The distributed matrix and  [2.x.64]  on the process  [2.x.65]  must have matching dimensions.     The user has to ensure that all processes call this with identical  [2.x.66]    The  [2.x.67]  refers to a process of the MPI communicator used to create the process grid   of the distributed matrix.  
* [0.x.15]*
   Copy the contents of the distributed matrix into a differently distributed matrix  [2.x.68]    The function also works for matrices with different process grids   or block-cyclic distributions.  
* [0.x.16]*
   Copy a submatrix (subset) of the distributed matrix A to a submatrix of the distributed matrix  [2.x.69] 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - The global row and column index of the first element of the submatrix A is provided by  [2.x.70]      with row index= [2.x.71]  and column   index= [2.x.72] .
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - The global row and column index of the first element of the submatrix B is provided by  [2.x.73]      with row index= [2.x.74]  and column   index= [2.x.75] .
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - The dimension of the submatrix to be copied is given by  [2.x.76]      with number of rows= [2.x.77]  and number of   columns= [2.x.78] .       If it is necessary to copy complete matrices with an identical block-cyclic   distribution, use    [2.x.79]  &dest)   with only one argument to avoid communication.     The underlying process grids of the matrices  [2.x.80]  and  [2.x.81]  must have been built   with the same MPI communicator.  
* [0.x.17]*
   Transposing assignment:  [2.x.82]      The matrices  [2.x.83]  and  [2.x.84]  must have the same process grid.     The following alignment conditions have to be fulfilled:  [2.x.85]  and    [2.x.86] .  
* [0.x.18]*
   The operations based on the input parameter  [2.x.87]  and the   alignment conditions are summarized in the following table:     | transpose_B |          Block Sizes         |                    Operation                  |   | :---------: | :--------------------------: | :-------------------------------------------: |   |   false     |  [2.x.88]   [2.x.89]   [2.x.90]  |   [2.x.91]    |   |   true      |  [2.x.92]   [2.x.93]   [2.x.94]  |  [2.x.95]   |     The matrices  [2.x.96]  and  [2.x.97]  must have the same process grid.  
* [0.x.19]*
   Matrix-addition:    [2.x.98]      The matrices  [2.x.99]  and  [2.x.100]  must have the same process grid.     The following alignment conditions have to be fulfilled:  [2.x.101]  and    [2.x.102] .  
* [0.x.20]*
   Matrix-addition:    [2.x.103]      The matrices  [2.x.104]  and  [2.x.105]  must have the same process grid.     The following alignment conditions have to be fulfilled:  [2.x.106]  and    [2.x.107] .  
* [0.x.21]*
   Matrix-matrix-multiplication:     The operations based on the input parameters and the alignment conditions   are summarized in the following table:     | transpose_A | transpose_B |                  Block Sizes                  |                             Operation                           |   | :---------: | :---------: | :-------------------------------------------: | :-------------------------------------------------------------: |   | false       |   false     |  [2.x.108]   [2.x.109]   [2.x.110]   [2.x.111]   [2.x.112]  |    [2.x.113]    |   | false       |   true      |  [2.x.114]   [2.x.115]   [2.x.116]   [2.x.117]   [2.x.118]  |   [2.x.119]   |   | true        |   false     |  [2.x.120]   [2.x.121]   [2.x.122]   [2.x.123]   [2.x.124]  |  [2.x.125]    |   | true        |   true      |  [2.x.126]   [2.x.127]   [2.x.128]   [2.x.129]   [2.x.130]  |  [2.x.131]  |     It is assumed that  [2.x.132]  and  [2.x.133]  have compatible sizes and   that    [2.x.134]  already has the right size.     The matrices  [2.x.135] ,  [2.x.136]  and  [2.x.137]  must have the same   process grid.  
* [0.x.22]*
   Matrix-matrix-multiplication.     The optional parameter  [2.x.138]  determines whether the result is   stored in  [2.x.139]  or added to  [2.x.140] .     if ( [2.x.141]   [2.x.142]      else  [2.x.143]      It is assumed that  [2.x.144]  and  [2.x.145]  have compatible sizes and   that    [2.x.146]  already has the right size.     The following alignment conditions have to be fulfilled:  [2.x.147] ,    [2.x.148]  and  [2.x.149] .  
* [0.x.23]*
   Matrix-matrix-multiplication using transpose of  [2.x.150] .     The optional parameter  [2.x.151]  determines whether the result is   stored in  [2.x.152]  or added to  [2.x.153] .     if ( [2.x.154]   [2.x.155]      else  [2.x.156]      It is assumed that  [2.x.157]  and  [2.x.158]  have compatible sizes and   that    [2.x.159]  already has the right size.     The following alignment conditions have to be fulfilled:  [2.x.160] ,    [2.x.161]  and  [2.x.162] .  
* [0.x.24]*
   Matrix-matrix-multiplication using the transpose of  [2.x.163] .     The optional parameter  [2.x.164]  determines whether the result is   stored in  [2.x.165]  or added to  [2.x.166] .     if ( [2.x.167]   [2.x.168]      else  [2.x.169]      It is assumed that  [2.x.170]  and  [2.x.171]  have compatible sizes and   that    [2.x.172]  already has the right size.     The following alignment conditions have to be fulfilled:  [2.x.173] ,    [2.x.174]  and  [2.x.175] .  
* [0.x.25]*
   Matrix-matrix-multiplication using transpose of  [2.x.176]  and    [2.x.177] .     The optional parameter  [2.x.178]  determines whether the result is   stored in  [2.x.179]  or added to  [2.x.180] .     if ( [2.x.181]   [2.x.182]      else  [2.x.183]      It is assumed that  [2.x.184]  and  [2.x.185]  have compatible sizes and   that    [2.x.186]  already has the right size.     The following alignment conditions have to be fulfilled:  [2.x.187] ,    [2.x.188]  and  [2.x.189] .  
* [0.x.26]*
   Stores the distributed matrix in  [2.x.190]  using HDF5.     In case that deal.II was built without HDF5   a call to this function will cause an exception to be thrown.     If HDF5 was built with MPI, parallel I/O is used to save the matrix.   Otherwise, just one process will do the output. This means that   internally the distributed matrix is copied to one process, which   does the output. Therefore, the matrix has to fit into the memory   of one process.     To tweak the I/O performance, especially for parallel I/O, the user may define the optional parameter  [2.x.191]    All MPI processes need to call the function with the same value.   The matrix is written in chunks to the file, therefore the properties of   the system define the optimal chunk size. Internally, HDF5 splits the   matrix into <tt>chunk_size.first</tt> x <tt>chunk_size.second</tt> sized   blocks, with <tt>chunk_size.first</tt> being the number of rows of a chunk   and <tt>chunk_size.second</tt> the number of columns.  
* [0.x.27]*
   Loads the distributed matrix from file  [2.x.192]  using HDF5.   In case that deal.II was built without HDF5   a call to this function will cause an exception to be thrown.     The matrix must have the same dimensions as the matrix stored in the file.     If HDF5 was build with MPI, parallel I/O is used to load the matrix.   Otherwise, just one process will load the matrix from storage   and distribute the content to the other processes subsequently.  
* [0.x.28]*
   Compute the Cholesky factorization of the matrix using ScaLAPACK   function  [2.x.193] . The result of the factorization is stored in   this object.  
* [0.x.29]*
   Compute the LU factorization of the matrix using ScaLAPACK   function  [2.x.194]  and partial pivoting with row interchanges.   The result of the factorization is stored in this object.  
* [0.x.30]*
   Invert the matrix by first computing a Cholesky for symmetric matrices   or a LU factorization for general matrices and then   building the actual inverse using  [2.x.195]  or    [2.x.196] . If the matrix is triangular, the LU factorization   step is skipped, and  [2.x.197]  is used directly.     If a Cholesky or LU factorization has been applied previously,    [2.x.198]  are called directly.     The inverse is stored in this object.  
* [0.x.31]*
   Computing selected eigenvalues and, optionally, the eigenvectors of the   real symmetric matrix  [2.x.199] .     The eigenvalues/eigenvectors are selected by prescribing a range of indices  [2.x.200]      If successful, the computed eigenvalues are arranged in ascending order.   The eigenvectors are stored in the columns of the matrix, thereby   overwriting the original content of the matrix.     If all eigenvalues/eigenvectors have to be computed, pass the closed interval  [2.x.201]  in  [2.x.202]      Pass the closed interval  [2.x.203]  if the  [2.x.204]  largest   eigenvalues/eigenvectors are desired.  
* [0.x.32]*
   Computing selected eigenvalues and, optionally, the eigenvectors.   The eigenvalues/eigenvectors are selected by prescribing a range of values  [2.x.205]  for the eigenvalues.     If successful, the computed eigenvalues are arranged in ascending order.   The eigenvectors are stored in the columns of the matrix, thereby   overwriting the original content of the matrix.  
* [0.x.33]*
   Computing selected eigenvalues and, optionally, the eigenvectors of the   real symmetric matrix  [2.x.206]  using the   MRRR algorithm.     The eigenvalues/eigenvectors are selected by prescribing a range of indices  [2.x.207]      If successful, the computed eigenvalues are arranged in ascending order.   The eigenvectors are stored in the columns of the matrix, thereby   overwriting the original content of the matrix.     If all eigenvalues/eigenvectors have to be computed, pass the closed interval  [2.x.208]  in  [2.x.209]      Pass the closed interval  [2.x.210]  if the  [2.x.211]  largest   eigenvalues/eigenvectors are desired.  
* [0.x.34]*
   Computing selected eigenvalues and, optionally, the eigenvectors of the   real symmetric matrix  [2.x.212]  using the   MRRR algorithm.   The eigenvalues/eigenvectors are selected by prescribing a range of values  [2.x.213]  for the eigenvalues.     If successful, the computed eigenvalues are arranged in ascending order.   The eigenvectors are stored in the columns of the matrix, thereby   overwriting the original content of the matrix.  
* [0.x.35]*
   Computing the singular value decomposition (SVD) of a   matrix  [2.x.214] , optionally computing the   left and/or right singular vectors. The SVD is written as  [2.x.215]  with  [2.x.216]  as a diagonal matrix,    [2.x.217]  and  [2.x.218]  as orthogonal matrices. The diagonal elements of    [2.x.219]  are the singular values of  [2.x.220]  and the columns of    [2.x.221]  and  [2.x.222]  are the corresponding left and right singular   vectors, respectively. The singular values are returned in decreasing order   and only the first  [2.x.223]  columns of  [2.x.224]  and rows of    [2.x.225]  are computed.     Upon return the content of the matrix is unusable.   The matrix  [2.x.226]  must have identical block cyclic distribution for   the rows and column.     If left singular vectors are required matrices  [2.x.227]  and    [2.x.228]  have to be constructed with the same process grid and block   cyclic distribution. If right singular vectors are required matrices    [2.x.229]  and  [2.x.230]  have to be constructed with the same   process grid  and block cyclic distribution.     To avoid computing the left and/or right singular vectors the function   accepts  [2.x.231]    for  [2.x.232]  and/or  [2.x.233]   
* [0.x.36]*
   Solving overdetermined or underdetermined real linear   systems involving matrix  [2.x.234] , or its   transpose  [2.x.235] , using a QR or LQ factorization of  [2.x.236]    for  [2.x.237]  RHS vectors in the columns of matrix  [2.x.238]      It is assumed that  [2.x.239]  has full rank:  [2.x.240] .     The following options are supported:
* 

* 
* 

* 
* 

* 
* 
*  - If(!transpose) and  [2.x.241] : least squares solution of overdetermined   system       [2.x.242] .\n      Upon exit the rows  [2.x.243]  to  [2.x.244]  of  [2.x.245]  contain the least square   solution vectors. The residual sum of squares for each column is given by   the sum of squares of elements  [2.x.246]  to  [2.x.247]  in that column.
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - If(!transpose) and  [2.x.248] : find minimum norm solutions of   underdetermined systems       [2.x.249] .\n      Upon exit the columns of  [2.x.250]  contain the minimum norm solution   vectors.
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - If(transpose) and  [2.x.251] : find minimum norm solutions of   underdetermined system  [2.x.252] .\n      Upon exit the columns of  [2.x.253]  contain the minimum norm solution   vectors.
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - If(transpose) and  [2.x.254] : least squares solution of overdetermined   system       [2.x.255] .\n      Upon exit the rows  [2.x.256]  to  [2.x.257]  contain the least square solution   vectors. The residual sum of squares for each column is given by the sum of   squares of elements  [2.x.258]  to  [2.x.259]  in that column.     If(!tranpose) then  [2.x.260] ,   otherwise  [2.x.261] .   The matrices  [2.x.262]  and  [2.x.263]  must have an identical block   cyclic distribution for rows and columns.  
* [0.x.37]*
   Compute the pseudoinverse  [2.x.264]    (Moore-Penrose inverse) of a real matrix  [2.x.265]  using the singular value decomposition    [2.x.266] .     Unlike the inverse, the pseudoinverse  [2.x.267]  exists for both rectangular as well   as singular matrices  [2.x.268] .     For a rectangular  [2.x.269]  the pseudoinverse is computed by taking   the reciprocal of each non-zero element on the diagonal, leaving the zeros   in place, and then transposing  [2.x.270] . For the numerical   computation only the singular values  [2.x.271]  are taken into account. Upon successful exit, the function   returns the number of singular values fulfilling that condition. That value   can be interpreted as the rank of  [2.x.272] .     Upon return this object contains the pseudoinverse  [2.x.273] .     The following alignment conditions have to be fulfilled:  [2.x.274] .  
* [0.x.38]*
   Estimate the condition number of a SPD matrix in the  [2.x.275] -norm.   The matrix has to be in the Cholesky state (see   compute_cholesky_factorization()). The reciprocal of the condition number   is returned in order to avoid the possibility of overflow when the   condition number is very large.      [2.x.276]  must contain the  [2.x.277] -norm of the matrix prior to calling   Cholesky factorization (see l1_norm()).    
*  [2.x.278]  An alternative is to compute the inverse of the matrix   explicitly and manually construct  [2.x.279] .  
* [0.x.39]*
   Compute the  [2.x.280] -norm of the matrix.  
* [0.x.40]*
   Compute the  [2.x.281]  norm of the matrix.  
* [0.x.41]*
   Compute the Frobenius norm of the matrix.  
* [0.x.42]*
   Number of rows of the  [2.x.282]  matrix.  
* [0.x.43]*
   Number of columns of the  [2.x.283]  matrix.  
* [0.x.44]*
   Number of local rows on this MPI processes.  
* [0.x.45]*
   Number of local columns on this MPI process.  
* [0.x.46]*
   Return the global row number for the given local row  [2.x.284]  .  
* [0.x.47]*
   Return the global column number for the given local column  [2.x.285]   
* [0.x.48]*
   Read access to local element.  
* [0.x.49]*
   Write access to local element.  
* [0.x.50]*
   Scale the columns of the distributed matrix by the scalars provided in the array  [2.x.286]      The array  [2.x.287]  must have as many entries as the matrix columns.     Copies of  [2.x.288]  have to be available on all processes of the underlying MPI communicator.    
*  [2.x.289]  The fundamental prerequisite for the  [2.x.290]  is that it must be possible to   create an ArrayView from it; this is satisfied by the  [2.x.291]  and Vector classes.  
* [0.x.51]*
   Scale the rows of the distributed matrix by the scalars provided in the array  [2.x.292]      The array  [2.x.293]  must have as many entries as the matrix rows.     Copies of  [2.x.294]  have to be available on all processes of the underlying MPI communicator.    
*  [2.x.295]  The fundamental prerequisite for the  [2.x.296]  is that it must be possible to   create an ArrayView from it; this is satisfied by the  [2.x.297]  and Vector classes.  
* [0.x.52]*
   Calculate the norm of a distributed symmetric dense matrix using   ScaLAPACK's internal function.  
* [0.x.53]*
   Calculate the norm of a distributed dense matrix using ScaLAPACK's   internal function.  
* [0.x.54]*
   Computing selected eigenvalues and, optionally, the eigenvectors.   The eigenvalues/eigenvectors are selected by either prescribing a range of indices  [2.x.298]    or a range of values  [2.x.299]  for the eigenvalues. The function will throw an exception   if both ranges are prescribed (meaning that both ranges differ from the   default value) as this ambiguity is prohibited. If successful, the computed   eigenvalues are arranged in ascending order. The eigenvectors are stored in   the columns of the matrix, thereby overwriting the original content of the   matrix.  
* [0.x.55]*
   Computing selected eigenvalues and, optionally, the eigenvectors of the   real symmetric matrix  [2.x.300]  using the   MRRR algorithm.   The eigenvalues/eigenvectors are selected by either prescribing a range of indices  [2.x.301]    or a range of values  [2.x.302]  for the eigenvalues. The function will throw an exception   if both ranges are prescribed (meaning that both ranges differ from the   default value) as this ambiguity is prohibited.     By calling this function the original content of the matrix will be   overwritten. If requested, the eigenvectors are stored in the columns of   the matrix. Also in the case that just the eigenvalues are required, the   content of the matrix will be overwritten.     If successful, the computed eigenvalues are arranged in ascending order.    
*  [2.x.303]  Due to a bug in Netlib-ScaLAPACK, either all or no eigenvectors can be computed.   Therefore, the input  [2.x.304]  has to be set accordingly. Using Intel-MKL this restriction is not required.  
* [0.x.56]   Stores the distributed matrix in  [2.x.305]    using serial routines  
* [0.x.57]   Loads the distributed matrix from file  [2.x.306]    using serial routines  
* [0.x.58]   Stores the distributed matrix in  [2.x.307]    using parallel routines  
* [0.x.59]   Loads the distributed matrix from file  [2.x.308]    using parallel routines  
* [0.x.60]*
   Since ScaLAPACK operations notoriously change the meaning of the matrix   entries, we record the current state after the last operation here.  
* [0.x.61]*
   Additional property of the matrix which may help to select more   efficient ScaLAPACK functions.  
* [0.x.62]*
   A shared pointer to a  [2.x.309]  object which contains a   BLACS context and a MPI communicator, as well as other necessary data   structures.  
* [0.x.63]*
   Number of rows in the matrix.  
* [0.x.64]*
   Number of columns in the matrix.  
* [0.x.65]*
   Row block size.  
* [0.x.66]*
   Column block size.  
* [0.x.67]*
   Number of rows in the matrix owned by the current process.  
* [0.x.68]*
   Number of columns in the matrix owned by the current process.  
* [0.x.69]*
   ScaLAPACK description vector.  
* [0.x.70]*
   Workspace array.  
* [0.x.71]*
   Integer workspace array.  
* [0.x.72]*
   Integer array holding pivoting information required   by ScaLAPACK's matrix factorization routines.  
* [0.x.73]*
   A character to define where elements are stored in case   ScaLAPACK operations support this.  
* [0.x.74]*
   The process row of the process grid over which the first row   of the global matrix is distributed.  
* [0.x.75]*
   The process column of the process grid over which the first column   of the global matrix is distributed.  
* [0.x.76]*
   Global row index that determines where to start a submatrix.   Currently this equals unity, as we don't use submatrices.  
* [0.x.77]*
   Global column index that determines where to start a submatrix.   Currently this equals unity, as we don't use submatrices.  
* [0.x.78]*
   Thread mutex.  
* [0.x.79]

include/deal.II-translator/lac/scalapack.templates_0.txt
[0.x.0]*
   Determine how many processes are available and the current process rank.     https://www.ibm.com/support/knowledgecenter/en/SSNR5K_4.2.0/com.ibm.cluster.pessl.v4r2.pssl100.doc/am6gr_dbpnf.htm  
* [0.x.1]*
   Return internal BLACS value in  [2.x.0]  based on the input  [2.x.1]  and  [2.x.2]    The most common use is in retrieving a default system context ( [2.x.3]  = 0,  [2.x.4]  is ignored)   to be used in BLACS_GRIDINIT or BLACS_GRIDMAP.     https://www.ibm.com/support/knowledgecenter/en/SSNR5K_4.2.0/com.ibm.cluster.pessl.v4r2.pssl100.doc/am6gr_dbget.htm  
* [0.x.2]*
   Map the processes sequentially in row-major or column-major order   into the process grid. Input arguments must be the same on every process.     On return,  [2.x.5]  is the integer handle to the BLACS context,   whereas on entry it is a system context to be used in creating the   BLACS context.     https://www.ibm.com/support/knowledgecenter/en/SSNR5K_4.2.0/com.ibm.cluster.pessl.v4r2.pssl100.doc/am6gr_dbint.htm  
* [0.x.3]*
   Return the process row and column index.     https://www.ibm.com/support/knowledgecenter/en/SSNR5K_4.2.0/com.ibm.cluster.pessl.v4r2.pssl100.doc/am6gr_dbinfo.htm  
* [0.x.4]*
   Given the system process number, return the row and column coordinates in   the BLACS' process grid.  
* [0.x.5]*
   Release a BLACS context.  
* [0.x.6]*
   This routines holds up execution of all processes within the indicated   scope until they have all called the routine.  
* [0.x.7]*
   Free all BLACS contexts and releases all allocated memory.  
* [0.x.8]*
   Receives a message from a process  [2.x.6]   [2.x.7]  into a general rectangular matrix.     https://software.intel.com/en-us/mkl-developer-reference-c-gerv2d  
* [0.x.9]*
   Sends the general rectangular matrix A to the destination   process  [2.x.8]   [2.x.9]  in the process grid.     https://software.intel.com/en-us/mkl-developer-reference-c-2018-beta-gesd2d  
* [0.x.10]*
   Get BLACS context from MPI  [2.x.10]   
* [0.x.11]*
   Compute how many rows and columns each process owns (NUMber of Rows Or   Columns).     https://www.ibm.com/support/knowledgecenter/SSNR5K_4.2.0/com.ibm.cluster.pessl.v4r2.pssl100.doc/am6gr_dnumy.htm  
* [0.x.12]*
   Compute the Cholesky factorization of an N-by-N real   symmetric positive definite distributed matrix sub( A ) denoting   A(IA:IA+N-1, JA:JA+N-1).     http://www.netlib.org/scalapack/explore-html/d5/d9e/pdpotrf_8f_source.html   https://www.ibm.com/support/knowledgecenter/SSNR5K_4.2.0/com.ibm.cluster.pessl.v4r2.pssl100.doc/am6gr_lpotrf.htm  
* [0.x.13]*
   Computes an LU factorization of a general distributed matrix sub( A )   using partial pivoting with row interchanges.     http://www.netlib.org/scalapack/explore-html/df/dfe/pdgetrf_8f_source.html   https://www.ibm.com/support/knowledgecenter/en/SSNR5K_4.2.0/com.ibm.cluster.pessl.v4r2.pssl100.doc/am6gr_lgetrf.htm  
* [0.x.14]*
   Compute the inverse of a real symmetric positive definite   distributed matrix sub( A ) = A(IA:IA+N-1,JA:JA+N-1) using the   Cholesky factorization sub( A ) = U**T*U or L*L**T computed by   PDPOTRF.     http://www.netlib.org/scalapack/explore-html/d2/d44/pdpotri_8f_source.html   https://www.ibm.com/support/knowledgecenter/SSNR5K_4.2.0/com.ibm.cluster.pessl.v4r2.pssl100.doc/am6gr_lpotri.htm   https://software.intel.com/en-us/mkl-developer-reference-c-p-potri  
* [0.x.15]*
   PDGETRI computes the inverse of a distributed matrix using the LU   factorization computed by PDGETRF. This method inverts U and then   computes the inverse of sub( A ) = A(IA:IA+N-1,JA:JA+N-1) denoted   InvA by solving the system InvA*L = inv(U) for InvA.     http://www.netlib.org/scalapack/explore-html/d3/df3/pdgetri_8f_source.html   https://www.ibm.com/support/knowledgecenter/SSNR5K_4.2.0/com.ibm.cluster.pessl.v4r2.pssl100.doc/am6gr_lgetri.htm  
* [0.x.16]*
   PDTRTRI computes the inverse of a upper or lower triangular   distributed matrix sub( A ) = A(IA:IA+N-1,JA:JA+N-1).     http://www.netlib.org/scalapack/explore-html/d9/dc0/pdtrtri_8f_source.html   https://www.ibm.com/support/knowledgecenter/SSNR5K_4.2.0/com.ibm.cluster.pessl.v4r2.pssl100.doc/am6gr_lpdtri.htm   https://software.intel.com/en-us/mkl-developer-reference-c-p-trtri  
* [0.x.17]*
   Estimate the reciprocal of the condition number (in the   l1-norm) of a real symmetric positive definite distributed matrix   using the Cholesky factorization.     https://www.ibm.com/support/knowledgecenter/SSNR5K_4.2.0/com.ibm.cluster.pessl.v4r2.pssl100.doc/am6gr_lpocon.htm#lpocon   http://www.netlib.org/scalapack/explore-html/d4/df7/pdpocon_8f.html   https://software.intel.com/en-us/mkl-developer-reference-fortran-pocon  
* [0.x.18]*
   Norm of a real symmetric matrix     http://www.netlib.org/scalapack/explore-html/dd/d12/pdlansy_8f_source.html   https://www.ibm.com/support/knowledgecenter/SSNR5K_4.2.0/com.ibm.cluster.pessl.v4r2.pssl100.doc/am6gr_pdlansy.htm#pdlansy  
* [0.x.19]*
   Compute the Least Common Multiple (LCM) of two positive integers  [2.x.11]  and  [2.x.12]    In fact the routine Compute the greatest common divisor (GCD) and   use the fact that M*N = GCD*LCM.     http://www.netlib.org/scalapack/explore-html/d0/d9b/ilcm_8f_source.html  
* [0.x.20]*
   Return the ceiling of the division of two integers.     http://www.netlib.org/scalapack/explore-html/df/d07/iceil_8f_source.html  
* [0.x.21]*
   Initialize the descriptor vector with the 8 input arguments  
* [0.x.22]*
   Compute the global index of a distributed matrix entry   pointed to by the local index  [2.x.13]  of the process indicated by    [2.x.14]       [2.x.15]  indxloc The local index of the distributed matrix entry.    [2.x.16]  nb Block size, size of the blocks the distributed matrix is split   into.    [2.x.17]  iproc The coordinate of the process whose local array row or column   is to be determined    [2.x.18]  isrcproc  The coordinate of the process that possesses the first   row/column of the distributed matrix    [2.x.19]  nprocs The total number processes over which the distributed matrix   is distributed  
* [0.x.23]*
   Compute the solution to a real system of linear equations  
* [0.x.24]*
   Perform one of the matrix-matrix operations:  
* [1.x.0]
*    where    [2.x.20]  denotes C(IC:IC+M-1,JC:JC+N-1),  and,  [2.x.21]  is one of    [2.x.22]  or  [2.x.23] .  
* [0.x.25]*
   Return the value of the one norm, or the Frobenius norm, or the infinity   norm, or the element of largest absolute value of a distributed matrix  
* [0.x.26]*
   Compute the process coordinate which possesses the entry of a   distributed matrix specified by a global index  
* [0.x.27]*
   Compute all eigenvalues and, optionally, eigenvectors of a real symmetric   matrix A by calling the recommended sequence of ScaLAPACK routines. In its   present form, the routine assumes a homogeneous system and makes no checks   for consistency of the eigenvalues or eigenvectors across the different   processes. Because of this, it is possible that a heterogeneous system may   return incorrect results without any error messages.     http://www.netlib.org/scalapack/explore-html/d0/d1a/pdsyev_8f.html   https://www.ibm.com/support/knowledgecenter/SSNR5K_4.2.0/com.ibm.cluster.pessl.v4r2.pssl100.doc/am6gr_lsyev.htm#lsyev  
* [0.x.28]*
   Copy all or a part of a distributed matrix A to another distributed matrix   B. No communication is performed, pdlacpy performs a local copy    [2.x.24] , where  [2.x.25]    denotes  [2.x.26]  and  [2.x.27]  denotes    [2.x.28] .  
* [0.x.29]*
   Copies the content of a general rectangular distributed matrix  [2.x.29]  to another distributed matrix  [2.x.30]    It is not required that the matrices A and B have the same process grid or   block size, e.g. copying a matrix from a one-dimensional to a   two-dimensional process grid    [2.x.31]  is a context which is at least a union of all processes in context   A and B  
* [0.x.30]*
   helper routines determining machine precision  
* [0.x.31]*
    psyevx computes selected eigenvalues and, optionally, eigenvectors    of a real symmetric matrix A. Eigenvalues/vectors can be selected by    specifying a range of values or a range of indices for the desired    eigenvalues.  
* [0.x.32]   PDGESVD computes the singular value decomposition (SVD) of an   M-by-N matrix A, optionally computing the left and/or right   singular vectors  
* [0.x.33]   P_GELS solves overdetermined or underdetermined real linear   systems involving an M-by-N matrix A, or its transpose,   using a QR or LQ factorization of A.  It is assumed that A has full rank.  
* [0.x.34]   Perform matrix sum:   [1.x.1]
*    where  [2.x.32]  denotes either  [2.x.33]  or  [2.x.34] .  
* [0.x.35]*
   Routine to transpose a matrix:   C = beta C + alpha A^T  
* [0.x.36]*
    psyevr computes selected eigenvalues and, optionally, eigenvectors    of a real symmetric matrix A using a parallel implementation of the MRR   algorithm. Eigenvalues/vectors can be selected by specifying a range of   values or a range of indices for the desired eigenvalues.  
* [0.x.37] In the following we have template wrappers for the ScaLAPACK routines wrappers for other numeric types can be added in the future

* 
* [0.x.38]   Netlib ScaLAPACK performs floating point tests (e.g. divide-by-zero) within   the call to pdsyevr causing floating point exceptions to be thrown (at   least in debug mode). Therefore, we wrap the calls to pdsyevr into the   following code to suppress the exception.  
* [0.x.39]   Netlib ScaLAPACK performs floating point tests (e.g. divide-by-zero) within   the call to pssyevr causing floating point exceptions to be thrown (at   least in debug mode). Therefore, we wrap the calls to pssyevr into the   following code to suppress the exception.  
* [0.x.40]

include/deal.II-translator/lac/schur_complement_0.txt
[0.x.0]*
  [2.x.0]  Creation of a LinearOperator related to the Schur Complement

* 
* [0.x.1]*
  [2.x.1]  LinearOperator
*  Return a LinearOperator that performs the operations associated with the Schur complement. There are two additional helper functions, condense_schur_rhs() and postprocess_schur_solution(), that are likely necessary to be used in order to perform any useful tasks in linear algebra with this operator.
*  We construct the definition of the Schur complement in the following way:
*  Consider a general system of linear equations that can be decomposed into two major sets of equations: [1.x.0]
*  where  [2.x.2]   represent general subblocks of the matrix  [2.x.3]  and, similarly, general subvectors of  [2.x.4]  are given by  [2.x.5]  .
*  This is equivalent to the following two statements: [1.x.1]
* 
*  Assuming that  [2.x.6]  are both square and invertible, we could then perform one of two possible substitutions, [1.x.2]
*  which amount to performing block Gaussian elimination on this system of equations.
*  For the purpose of the current implementation, we choose to substitute (3) into (2) [1.x.3]
*  This leads to the result [1.x.4] with  [2.x.7]  being the Schur complement and the modified right-hand side vector  [2.x.8]  arising from the condensation step. Note that for this choice of  [2.x.9] , submatrix  [2.x.10]  need not be invertible and may thus be the null matrix. Ideally  [2.x.11]  should be well-conditioned.
*  So for any arbitrary vector  [2.x.12] , the Schur complement performs the following operation: [1.x.5]
*  A typical set of steps needed the solve a linear system (1),(2) would be: 1. Define the inverse matrix  [2.x.13]  (using inverse_operator()). 2. Define the Schur complement  [2.x.14]  (using schur_complement()). 3. Define iterative inverse matrix  [2.x.15]  such that (6) holds. It is necessary to use a solver with a preconditioner to compute the approximate inverse operation of  [2.x.16]  since we never compute  [2.x.17]  directly, but rather the result of its operation. To achieve this, one may again use the inverse_operator() in conjunction with the Schur complement that we've just constructed. Observe that the both  [2.x.18]  and its preconditioner operate over the same space as  [2.x.19] . 4. Perform pre-processing step on the RHS of (5) using condense_schur_rhs():    [1.x.6] 5. Solve for  [2.x.20]  in (5):    [1.x.7] 6. Perform the post-processing step from (3) using postprocess_schur_solution():    [1.x.8]
*  An illustration of typical usage of this operator for a fully coupled system is given below.

* 
* [1.x.9]
* 
*  In the above example, the preconditioner for  [2.x.21]  was defined as the preconditioner for  [2.x.22] , which is valid since they operate on the same space. However, if  [2.x.23]  and  [2.x.24]  are too dissimilar, then this may lead to a large number of solver iterations as  [2.x.25]  is not a good approximation for  [2.x.26] .
*  A better preconditioner in such a case would be one that provides a more representative approximation for  [2.x.27] . One approach is shown in  [2.x.28] , where  [2.x.29]  is the null matrix and the preconditioner for  [2.x.30]  is derived from the mass matrix over this space.
*  From another viewpoint, a similar result can be achieved by first constructing an object that represents an approximation for  [2.x.31]  wherein expensive operation, namely  [2.x.32] , is approximated. Thereafter we construct the approximate inverse operator  [2.x.33]  which is then used as the preconditioner for computing  [2.x.34] .

* 
* [1.x.10]
*  Note that due to the construction of  [2.x.35]  S_inv_approx and subsequently  [2.x.36]  S_inv, there are a pair of nested iterative solvers which could collectively consume a lot of resources. Therefore care should be taken in the choices leading to the construction of the iterative inverse_operators. One might consider the use of a IterationNumberControl (or a similar mechanism) to limit the number of inner solver iterations. This controls the accuracy of the approximate inverse operation  [2.x.37]  which acts only as the preconditioner for  [2.x.38] . Furthermore, the preconditioner to  [2.x.39] , which in this example is  [2.x.40] , should ideally be computationally inexpensive.
*  However, if an iterative solver based on IterationNumberControl is used as a preconditioner then the preconditioning operation is not a linear operation. Here a flexible solver like SolverFGMRES (flexible GMRES) is best employed as an outer solver in order to deal with the variable behavior of the preconditioner. Otherwise the iterative solver can stagnate somewhere near the tolerance of the preconditioner or generally behave erratically. Alternatively, using a ReductionControl would ensure that the preconditioner always solves to the same tolerance, thereby rendering its behavior constant.
*  Further examples of this functionality can be found in the test-suite, such as  [2.x.41]  . The solution of a multi- component problem (namely  [2.x.42] ) using the schur_complement can be found in  [2.x.43]  .
*   [2.x.44]   [2.x.45]  "Block (linear algebra)"
* 

* 
*  [2.x.46] 

* 
* [0.x.2]*
  [2.x.47]  Creation of PackagedOperation objects related to the Schur Complement

* 
* [0.x.3]*
  [2.x.48]  PackagedOperation
*  For the system of equations [1.x.11]
*  this operation performs the pre-processing (condensation) step on the RHS subvector  [2.x.49]  so that the Schur complement can be used to solve this system of equations. More specifically, it produces an object that represents the condensed form of the subvector  [2.x.50]  namely [1.x.12]
*   [2.x.51]   [2.x.52]  "Block (linear algebra)"
* 

* 
*  [2.x.53] 

* 
* [0.x.4]*
  [2.x.54]  PackagedOperation
*  For the system of equations [1.x.13]
*  this operation performs the post-processing step of the Schur complement to solve for the second subvector  [2.x.55]  once subvector  [2.x.56]  is known, with the result that [1.x.14]
*   [2.x.57]   [2.x.58]  "Block (linear algebra)"
* 

* 
*  [2.x.59] 

* 
* [0.x.5]

include/deal.II-translator/lac/slepc_solver_0.txt
[0.x.0]*
 Base namespace for solver classes using the SLEPc solvers which are selected based on flags passed to the eigenvalue problem solver context. Derived classes set the right flags to set the right solver.
*  The SLEPc solvers are intended to be used for solving the generalized eigenspectrum problem  [2.x.0] , for  [2.x.1] ; where  [2.x.2]  is a system matrix,  [2.x.3]  is a mass matrix, and  [2.x.4]  are a set of eigenvalues and eigenvectors respectively. The emphasis is on methods and techniques appropriate for problems in which the associated matrices are sparse. Most of the methods offered by the SLEPc library are projection methods or other methods with similar properties; and wrappers are provided to interface to SLEPc solvers that handle both of these problem sets.
*  SLEPcWrappers can be implemented in application codes in the following way:

* 
* [1.x.0]
*  for the generalized eigenvalue problem  [2.x.5] , where the variable  [2.x.6]  tells SLEPc the number of eigenvector/eigenvalue pairs to solve for. Additional options and solver parameters can be passed to the SLEPc solvers before calling  [2.x.7] . For example, if the matrices of the general eigenspectrum problem are not hermitian and the lower eigenvalues are wanted only, the following code can be implemented before calling  [2.x.8] :

* 
* [1.x.1]
*  These options can also be set at the command line.
*  See also  [2.x.9]  for a hands-on example.
*  For cases when spectral transformations are used in conjunction with Krylov-type solvers or Davidson-type eigensolvers are employed one can additionally specify which linear solver and preconditioner to use. This can be achieved as follows

* 
* [1.x.2]
* 
*  In order to support this usage case, different from PETSc wrappers, the classes in this namespace are written in such a way that the underlying SLEPc objects are initialized in constructors. By doing so one also avoid caching of different settings (such as target eigenvalue or type of the problem); instead those are applied straight away when the corresponding functions of the wrapper classes are called.
*  An alternative implementation to the one above is to use the API internals directly within the application code. In this way the calling sequence requires calling several of SolverBase functions rather than just one. This freedom is intended for use of the SLEPcWrappers that require a greater handle on the eigenvalue problem solver context. See also the API of, for example:

* 
* [1.x.3]
*  as an example on how to do this.
*  For further information and explanations on handling the  [2.x.10]  "SLEPcWrappers", see also the  [2.x.11]  "PETScWrappers", on which they depend.
* 

* 
*  [2.x.12] 

* 
* [0.x.1]*
   Base class for solver classes using the SLEPc solvers. Since solvers in   SLEPc are selected based on flags passed to a generic solver object,   basically all the actual solver calls happen in this class, and derived   classes simply set the right flags to select one solver or another, or to   set certain parameters for individual solvers.     For examples of how this and its derived classes can be used, including   how to provide preconditioners to the matrix of which eigenvalues are   to be computed, see the documentation of the SolverBase class as well   as the extensive discussions in the documentation of the SLEPcWrappers   namespace.  
* [0.x.2]*
     Constructor. Takes the MPI communicator over which parallel     computations are to happen.    
* [0.x.3]*
     Destructor.    
* [0.x.4]*
     Composite method that solves the eigensystem  [2.x.13] . The     eigenvector sent in has to have at least one element that we can use as     a template when resizing, since we do not know the parameters of the     specific vector class used (i.e. local_dofs for MPI vectors). However,     while copying eigenvectors, at least twice the memory size of     <tt>eigenvectors</tt> is being used (and can be more). To avoid doing     this, the fairly standard calling sequence executed here is used: Set     up matrices for solving; Actually solve the system; Gather the     solution(s).        
*  [2.x.14]  Note that the number of converged eigenvectors can be larger than     the number of eigenvectors requested; this is due to a round off error     (success) of the eigenproblem solver context. If this is found to be     the case we simply do not bother with more eigenpairs than requested,     but handle that it may be more than specified by ignoring any extras.     By default one eigenvector/eigenvalue pair is computed.    
* [0.x.5]*
     Same as above, but here a composite method for solving the system  [2.x.15] , for real matrices, vectors, and values  [2.x.16] .    
* [0.x.6]*
     Same as above, but here a composite method for solving the system  [2.x.17]  with real matrices  [2.x.18]  and imaginary eigenpairs  [2.x.19] .    
* [0.x.7]*
     Set the initial vector space for the solver.         By default, SLEPc initializes the starting vector or the initial     subspace randomly.    
* [0.x.8]*
     Set the spectral transformation to be used.    
* [0.x.9]*
     Set target eigenvalues in the spectrum to be computed. By default, no     target is set.    
* [0.x.10]*
     Indicate which part of the spectrum is to be computed. By default     largest magnitude eigenvalues are computed.        
*  [2.x.20]  For other allowed values see the SLEPc documentation.    
* [0.x.11]*
     Specify the type of the eigenspectrum problem. This can be used to     exploit known symmetries of the matrices that make up the     standard/generalized eigenspectrum problem.  By default a non-Hermitian     problem is assumed.        
*  [2.x.21]  For other allowed values see the SLEPc documentation.    
* [0.x.12]*
     Take the information provided from SLEPc and checks it against     deal.II's own SolverControl objects to see if convergence has been     reached.    
* [0.x.13]*
     Exception. Standard exception.    
* [0.x.14]*
     Exception. SLEPc error with error number.    
* [0.x.15]*
     Exception. Convergence failure on the number of eigenvectors.    
* [0.x.16]*
     Access to the object that controls convergence.    
* [0.x.17]*
     Reference to the object that controls convergence of the iterative     solver.    
* [0.x.18]*
     Copy of the MPI communicator object to be used for the solver.    
* [0.x.19]*
     Solve the linear system for  [2.x.22]  eigenstates.     Parameter  [2.x.23]  contains the actual number of     eigenstates that have  converged; this can be both fewer or more than     n_eigenpairs, depending on the SLEPc eigensolver used.    
* [0.x.20]*
     Access the real parts of solutions for a solved eigenvector problem,     pair index solutions,  [2.x.24] .    
* [0.x.21]*
     Access the real and imaginary parts of solutions for a solved     eigenvector problem, pair index solutions,  [2.x.25] .    
* [0.x.22]*
     Initialize solver for the linear system  [2.x.26] . (Note: this is     required before calling solve ())    
* [0.x.23]*
     Same as above, but here initialize solver for the linear system  [2.x.27] .    
* [0.x.24]*
     Objects for Eigenvalue Problem Solver.    
* [0.x.25]*
     Convergence reason.    
* [0.x.26]*
     A function that can be used in SLEPc as a callback to check on     convergence.        
*  [2.x.28]  This function is not used currently.    
* [0.x.27]*
   An implementation of the solver interface using the SLEPc Krylov-Schur   solver. Usage: All spectrum, all problem types, complex.     For examples of how this and its sibling classes can be used, including   how to provide preconditioners to the matrix of which eigenvalues are   to be computed, see the documentation of the SolverBase class as well   as the extensive discussions in the documentation of the SLEPcWrappers   namespace.    
*  [2.x.29]   
* [0.x.28]*
     Standardized data struct to pipe additional data to the solver, should     it be needed.    
* [0.x.29]*
     SLEPc solvers will want to have an MPI communicator context over which     computations are parallelized. By default, this carries the same     behavior as the PETScWrappers, but you can change that.    
* [0.x.30]*
     Store a copy of the flags for this particular solver.    
* [0.x.31]*
   An implementation of the solver interface using the SLEPc Arnoldi solver.   Usage: All spectrum, all problem types, complex.     For examples of how this and its sibling classes can be used, including   how to provide preconditioners to the matrix of which eigenvalues are   to be computed, see the documentation of the SolverBase class as well   as the extensive discussions in the documentation of the SLEPcWrappers   namespace.    
*  [2.x.30]   
* [0.x.32]*
     Standardized data struct to pipe additional data to the solver, should     it be needed.    
* [0.x.33]*
       Constructor. By default, set the option of delayed       reorthogonalization to false, i.e. don't do it.      
* [0.x.34]*
       Flag for delayed reorthogonalization.      
* [0.x.35]*
     SLEPc solvers will want to have an MPI communicator context over which     computations are parallelized. By default, this carries the same     behavior as the PETScWrappers, but you can change that.    
* [0.x.36]*
     Store a copy of the flags for this particular solver.    
* [0.x.37]*
   An implementation of the solver interface using the SLEPc Lanczos solver.   Usage: All spectrum, all problem types, complex.     For examples of how this and its sibling classes can be used, including   how to provide preconditioners to the matrix of which eigenvalues are   to be computed, see the documentation of the SolverBase class as well   as the extensive discussions in the documentation of the SLEPcWrappers   namespace.    
*  [2.x.31]   
* [0.x.38]*
     Standardized data struct to pipe additional data to the solver, should     it be needed.    
* [0.x.39]*
       The type of reorthogonalization used during the Lanczos iteration.      
* [0.x.40]*
       Constructor. By default sets the type of reorthogonalization used       during the Lanczos iteration to full.      
* [0.x.41]*
     SLEPc solvers will want to have an MPI communicator context over which     computations are parallelized. By default, this carries the same     behavior as the PETScWrappers, but you can change that.    
* [0.x.42]*
     Store a copy of the flags for this particular solver.    
* [0.x.43]*
   An implementation of the solver interface using the SLEPc Power solver.   Usage: Largest values of spectrum only, all problem types, complex.     For examples of how this and its sibling classes can be used, including   how to provide preconditioners to the matrix of which eigenvalues are   to be computed, see the documentation of the SolverBase class as well   as the extensive discussions in the documentation of the SLEPcWrappers   namespace.    
*  [2.x.32]   
* [0.x.44]*
     Standardized data struct to pipe additional data to the solver, should     it be needed.    
* [0.x.45]*
     SLEPc solvers will want to have an MPI communicator context over which     computations are parallelized. By default, this carries the same     behavior as the PETScWrappers, but you can change that.    
* [0.x.46]*
     Store a copy of the flags for this particular solver.    
* [0.x.47]*
   An implementation of the solver interface using the SLEPc Davidson   solver. Usage: All problem types.     For examples of how this and its sibling classes can be used, including   how to provide preconditioners to the matrix of which eigenvalues are   to be computed, see the documentation of the SolverBase class as well   as the extensive discussions in the documentation of the SLEPcWrappers   namespace.    
*  [2.x.33]   
* [0.x.48]*
     Standardized data struct to pipe additional data to the solver, should     it be needed.    
* [0.x.49]*
       Use double expansion in search subspace.      
* [0.x.50]*
       Constructor. By default set double_expansion to false.      
* [0.x.51]*
     SLEPc solvers will want to have an MPI communicator context over which     computations are parallelized. By default, this carries the same     behavior as the PETScWrappers, but you can change that.    
* [0.x.52]*
     Store a copy of the flags for this particular solver.    
* [0.x.53]*
   An implementation of the solver interface using the SLEPc Jacobi-Davidson   solver. Usage: All problem types.     For examples of how this and its sibling classes can be used, including   how to provide preconditioners to the matrix of which eigenvalues are   to be computed, see the documentation of the SolverBase class as well   as the extensive discussions in the documentation of the SLEPcWrappers   namespace.    
*  [2.x.34]   
* [0.x.54]*
     Standardized data struct to pipe additional data to the solver, should     it be needed.    
* [0.x.55]*
     SLEPc solvers will want to have an MPI communicator context over which     computations are parallelized. By default, this carries the same     behavior as the PETScWrappers, but you can change that.    
* [0.x.56]*
     Store a copy of the flags for this particular solver.    
* [0.x.57]*
   An implementation of the solver interface using the SLEPc LAPACK direct   solver.     For examples of how this and its sibling classes can be used, including   how to provide preconditioners to the matrix of which eigenvalues are   to be computed, see the documentation of the SolverBase class as well   as the extensive discussions in the documentation of the SLEPcWrappers   namespace.    
*  [2.x.35]   
* [0.x.58]*
     Standardized data struct to pipe additional data to the solver, should     it be needed.    
* [0.x.59]*
     SLEPc solvers will want to have an MPI communicator context over which     computations are parallelized. By default, this carries the same     behavior as the PETScWrappers, but you can change that.    
* [0.x.60]*
     Store a copy of the flags for this particular solver.    
* [0.x.61]*
   This is declared here to make it possible to take a  [2.x.36]  of   different PETScWrappers vector types  
* [0.x.62]

include/deal.II-translator/lac/slepc_spectral_transformation_0.txt
[0.x.0]*
   Base class for spectral transformation classes using the SLEPc solvers   which are selected based on flags passed to the spectral transformation.      [2.x.0]  is   your favourite transformation type, can then be implemented in   application codes in the following way for  [2.x.1]  with   the solver object  [2.x.2] :  
* [1.x.0]
*    and later calling the  [2.x.3]  function as usual:  
* [1.x.1]
*     
*  [2.x.4]  These options can also be set at the command line.    
*  [2.x.5]   
* [0.x.1]*
     Constructor.    
* [0.x.2]*
     Destructor.    
* [0.x.3]*
     Set a flag to indicate how the transformed matrices are being stored in     the spectral transformations.         The possible values are given by the enumerator STMatMode in the SLEPc     library     http://www.grycap.upv.es/slepc/documentation/current/docs/manualpages/ST/STMatMode.html    
* [0.x.4]*
     Set solver to be used when solving a system of linear algebraic     equations inside the eigensolver.    
* [0.x.5]*
     SLEPc spectral transformation object.    
* [0.x.6]*
   An implementation of the transformation interface using the SLEPc Shift.    
*  [2.x.6]   
* [0.x.7]*
     Standardized data struct to pipe additional data to the solver.    
* [0.x.8]*
       Constructor. By default, set the shift parameter to zero.      
* [0.x.9]*
       Shift parameter.      
* [0.x.10]*
     Constructor.    
* [0.x.11]*
     Store a copy of the flags for this particular solver.    
* [0.x.12]*
   An implementation of the transformation interface using the SLEPc Shift   and Invert.    
*  [2.x.7]   
* [0.x.13]*
     Standardized data struct to pipe additional data to the solver.    
* [0.x.14]*
       Constructor. By default, set the shift parameter to zero.      
* [0.x.15]*
       Shift parameter.      
* [0.x.16]*
     Constructor.    
* [0.x.17]*
     Store a copy of the flags for this particular solver.    
* [0.x.18]*
   An implementation of the transformation interface using the SLEPc   Spectrum Folding. This transformation type has been removed in SLEPc   3.5.0 and thus cannot be used in the newer versions.    
*  [2.x.8]   
* [0.x.19]*
     Standardized data struct to pipe additional data to the solver.    
* [0.x.20]*
       Constructor. By default, set the shift parameter to zero.      
* [0.x.21]*
       Shift parameter.      
* [0.x.22]*
     Constructor.    
* [0.x.23]*
     Store a copy of the flags for this particular solver.    
* [0.x.24]*
   An implementation of the transformation interface using the SLEPc Cayley.    
*  [2.x.9]   
* [0.x.25]*
     Standardized data struct to pipe additional data to the solver.    
* [0.x.26]*
       Constructor. Requires two shift parameters      
* [0.x.27]*
       Shift parameter.      
* [0.x.28]*
       Antishift parameter.      
* [0.x.29]*
     Constructor.    
* [0.x.30]*
     Store a copy of the flags for this particular solver.    
* [0.x.31]

include/deal.II-translator/lac/solver_0.txt
[0.x.0]*
 A base class for iterative linear solvers. This class provides interfaces to a memory pool and the objects that determine whether a solver has converged.
* 

*  [1.x.0]
*  In general, iterative solvers do not rely on any special structure of matrices or the format of storage. Rather, they only require that matrices and vectors define certain operations such as matrix-vector products, or scalar products between vectors. Consequently, this class as well as the derived classes and their member functions implementing concrete linear solvers are templated on the types of matrices and vectors. However, there are some common requirements a matrix or vector type must fulfill to qualify as an acceptable type for the solvers in this hierarchy. These requirements are listed below.
*  The classes we show below are not any concrete class. Rather, they are intended to form a "signature" which a concrete class has to conform to. Note that the matrix and vector classes within this library of course conform to this interface; therefore, SparseMatrix and Vector are good examples for these classes as they provide the necessary signatures of member functions (although they also provide many more interfaces that solvers do not in fact need
* 
*  -  for example, element access). In addition, you may want to take a look at  [2.x.0] ,  [2.x.1] , or a number of classes in the LinearSolvers namespace for examples of how one can define matrix-like classes that can serve as linear operators for linear solvers.
*  Concretely, matrix and vector classes that can be passed to a linear solver need to provide the following interfaces:

* 
* [1.x.1]
* 
*  In addition, for some solvers there has to be a global function <tt>swap(VectorType &a, VectorType &b)</tt> that exchanges the values of the two vectors.
*  Finally, the solvers also expect an instantiation of GrowingVectorMemory [2.x.2]  These instantiations are provided by the deal.II library for the built-in vector types, but must be explicitly added for user-provided vector classes. Otherwise, the linker will complain that it cannot find the constructors and destructors of GrowingVectorMemory that happen in the  [2.x.3]  class.
* 

* 
* [1.x.2]
* 
*  The preconditioners used must have the same interface as matrices, i.e. in particular they have to provide a member function  [2.x.4]  which denotes the application of the preconditioner.
* 

*  [1.x.3]
*  Several solvers need additional data, like the damping parameter  [2.x.5]  of the  [2.x.6]  class or the maximum number of temporary vectors of  [2.x.7]   To have a standardized way of constructing solvers, each solver class has a <tt>struct AdditionalData</tt> as a member, and constructors of all solver classes take such an argument. Some solvers need no additional data, or may not at the current time. For these solvers the struct  [2.x.8]  is empty and calling the constructor may be done without giving the additional structure as an argument as a default  [2.x.9]  AdditionalData is set by default.
*  With this, creating a solver looks like one of the following blocks:

* 
* [1.x.4]
* 
*  Using a unified constructor parameter list for all solvers supports the  [2.x.10]  SolverSelector class; the unified interface enables us to use this class unchanged even if the number of types of parameters to a certain solver changes and it is still possible in a simple way to give these additional data to the  [2.x.11]  object for each solver which it may use.
* 

*  [1.x.5]
*  The SolverBase class, being the base class for all of the iterative solvers such as SolverCG, SolverGMRES, etc, provides the facilities by which actual solver implementations determine whether the iteration is converged, not yet converged, or has failed. Typically, this is done using an object of type SolverControl that is passed to the solver classes's constructors and from them down to the constructor of this base class. Every one of the tutorial programs that solves a linear problem (starting with  [2.x.12] ) uses this method and it is described in detail there. However, the underlying mechanism is more general and allows for many other uses to observe how the linear solver iterations progress.
*  The basic approach is that the iterative solvers invoke a [1.x.6] at the end of each iteration to determine whether the solution is converged. A signal is a class that has, conceptually, a list of pointers to functions and every time the signal is invoked, each of these functions are called. In the language of signals, the functions called are called [1.x.7] and one can attach any number of slots to a signal. (The implementation of signals and slots we use here is the one from the BOOST.signals2 library.) A number of details may clarify what is happening underneath:
* 

* 
* 
*  - In reality, the signal object does not store pointers to functions, but function objects as slots. Each slot must conform to a particular signature: here, it is an object that can be called with three arguments (the number of the current linear iteration, the current residual, and the current iterate; more specifics are discussed in the documentation of the connect() function). A pointer to a function with this argument list satisfies the requirements, but you can also pass a member function whose  [2.x.13]  argument has been bound using a lambda function (see the example below).
* 

* 
* 
*  - Each of the slots will return a value that indicates whether the iteration should continue, should stop because it has succeeded, or stop because it has failed. The return type of slots is therefore of type  [2.x.14]  The returned values from all of the slots will then have to be combined before they are returned to the iterative solver that invoked the signal. The way this works is that if at least one slot returned  [2.x.15]  then the combined value is  [2.x.16]  otherwise, if at least one slot returned  [2.x.17]  then this is going to be the return value of the signal; finally, only if all slots return  [2.x.18]  will the signal's return value be  [2.x.19] 
* 

* 
* 
*  - It may of course be that a particular slot has been connected to the signal only to observe how the solution or a specific part of it converges, but has no particular opinion on whether the iteration should continue or not. In such cases, the slot should just return  [2.x.20]  which is the weakest of all return values according to the rules laid out above.
*  Given all this, it should now be obvious how the SolverControl object fits into this scheme: when a SolverControl object is passed to the constructor of the current class, we simply connect the  [2.x.21]  function of that object as a slot to the signal we maintain here. In other words, since a SolverBase object is always constructed using a SolverControl object, there is always at least one slot associated with the signal, namely the one that determines convergence.
*  On the other hand, using the connect() member function, it is possible to connect any number of other slots to the signal to observe whatever it is you want to observe. The connect() function also returns an object that describes the connection from the signal to the slot, and the corresponding BOOST functions then allow you to disconnect the slot if you want.
*  An example may illuminate these issues. In the  [2.x.22]  tutorial program, let us add a member function as follows to the main class:

* 
* [1.x.8]
*  The function satisfies the signature necessary to be a slot for the signal discussed above, with the exception that it is a member function and consequently requires a  [2.x.23]  pointer. What the function does is to take the vector given as last argument and write it into a file in VTU format with a file name derived from the number of the iteration.
*  This function can then be hooked into the CG solver by modifying the  [2.x.24]  function as follows:

* 
* [1.x.9]
*  The use of a lambda function here ensures that we convert the member function with its three arguments plus the  [2.x.25]  pointer, to a function that only takes three arguments, by fixing the implicit  [2.x.26]  argument of the function to the  [2.x.27]  pointer in the current function.
*  It is well understood that the CG method is a smoothing iteration (in the same way as the more commonly used Jacobi or SSOR iterations are smoothers). The code above therefore allows to observe how the solution becomes smoother and smoother in every iteration. This is best observed by initializing the solution vector with randomly distributed numbers in  [2.x.28] , using code such as

* 
* [1.x.10]
*  Using this, the slot will then generate files that when visualized look like this over the course of iterations zero to five:  [2.x.29] 
* 

* 
*  [2.x.30] 

* 
* [0.x.1]*
   An alias for the underlying vector type  
* [0.x.2]*
   Constructor. Takes a control object which evaluates the conditions for   convergence, and an object that allows solvers to allocate memory for   temporary objects.     Of both objects, a reference is stored, so it is the user's   responsibility to guarantee that the lifetime of the two arguments is at   least as long as that of the solver object.  
* [0.x.3]*
   Constructor. Takes a control object which evaluates the conditions for   convergence. In contrast to the other constructor, this constructor   designates an internal object of type GrowingVectorMemory to allocate   memory.     A reference to the control object is stored, so it is the user's   responsibility to guarantee that the lifetime of the argument is at least   as long as that of the solver object.  
* [0.x.4]*
   Connect a function object that will be called periodically within   iterative solvers. This function is used to attach monitors to iterative   solvers, either to determine when convergence has happened, or simply to   observe the progress of an iteration. See the documentation of this class   for more information.      [2.x.31]  slot A function object specified here will, with each call,   receive the number of the current iteration, the value that is used to   check for convergence (typically the residual of the current iterate with   respect to the linear system to be solved) and the currently best   available guess for the current iterate. Note that some solvers do not   update the approximate solution in every iteration but only after   convergence or failure has been determined (GMRES is an example); in such   cases, the vector passed as the last argument to the signal is simply the   best approximate at the time the signal is called, but not the vector   that will be returned if the signal's return value indicates that the   iteration should be terminated. The function object must return a    [2.x.32]  value that indicates whether the iteration should   continue, has failed, or has succeeded. The results of all connected   functions will then be combined to determine what should happen with the   iteration.      [2.x.33]  A connection object that represents the connection from the   signal to the function object. It can be used to disconnect the function   object again from the signal. See the documentation of the BOOST Signals2   library for more information on connection management.  
* [0.x.5]*
   A static vector memory object to be used whenever no such object has been   given to the constructor.  
* [0.x.6]*
   A reference to an object that provides memory for auxiliary vectors.  
* [0.x.7]*
   A class whose operator() combines two states indicating whether we should   continue iterating or stop, and returns a state that dominates. The rules   are:
* 

* 
* 

* 
* 

* 
* 
*  - If one of the two states indicates failure, return failure.
* 

* 
* 

* 
* 

* 
* 
*  - Otherwise, if one of the two states indicates to continue iterating, then   continue iterating.
* 

* 
* 

* 
* 

* 
* 
*  - Otherwise, return success.  
* [0.x.8]*
   A signal that iterative solvers can execute at the end of every iteration   (or in an otherwise periodic fashion) to find out whether we should   continue iterating or not. The signal may call one or more slots that   each will make this determination by themselves, and the result over all   slots (function calls) will be determined by the StateCombiner object.     The arguments passed to the signal are (i) the number of the current   iteration; (ii) the value that is used to determine convergence   (oftentimes the residual, but in other cases other quantities may be used   as long as they converge to zero as the iterate approaches the solution   of the linear system); and (iii) a vector that corresponds to the current   best guess for the solution at the point where the signal is called. Note   that some solvers do not update the approximate solution in every   iteration but only after convergence or failure has been determined   (GMRES is an example); in such cases, the vector passed as the last   argument to the signal is simply the best approximate at the time the   signal is called, but not the vector that will be returned if the   signal's return value indicates that the iteration should be terminated.  
* [0.x.9]

include/deal.II-translator/lac/solver_bicgstab_0.txt
[0.x.0]*
   Class containing the non-parameter non-template values used by the   SolverBicgstab class.  
* [0.x.1]*
     Auxiliary value.    
* [0.x.2]*
     Auxiliary value.    
* [0.x.3]*
     Auxiliary value.    
* [0.x.4]*
     Auxiliary value.    
* [0.x.5]*
     Auxiliary value.    
* [0.x.6]*
     Current iteration step.    
* [0.x.7]*
     Residual.    
* [0.x.8]*
     Default constructor. This is protected so that only SolverBicgstab can     create instances.    
* [0.x.9]*
 Bicgstab algorithm by van der Vorst.
*  For the requirements on matrices and vectors in order to work with this class, see the documentation of the Solver base class.
*  Like all other solver classes, this class has a local structure called  [2.x.0]  AdditionalData which is used to pass additional parameters to the solver, like damping parameters or the number of temporary vectors. We use this additional structure instead of passing these values directly to the constructor because this makes the use of the  [2.x.1]  and other classes much easier and guarantees that these will continue to work even if number or type of the additional parameters for a certain solver changes.
*  The Bicgstab-method has two additional parameters: the first is a boolean, deciding whether to compute the actual residual in each step ( [2.x.2]  or to use the length of the computed orthogonal residual ( [2.x.3]  Note that computing the residual causes a third matrix-vector-multiplication, though no additional preconditioning, in each step. The reason for doing this is, that the size of the orthogonalized residual computed during the iteration may be larger by orders of magnitude than the true residual. This is due to numerical instabilities related to badly conditioned matrices. Since this instability results in a bad stopping criterion, the default for this parameter is  [2.x.4]  Whenever the user knows that the estimated residual works reasonably as well, the flag should be set to  [2.x.5]  in order to increase the performance of the solver.
*  The second parameter is the size of a breakdown criterion. It is difficult to find a general good criterion, so if things do not work for you, try to change this value.
* 

*  [1.x.0]
*  The solve() function of this class uses the mechanism described in the Solver base class to determine convergence. This mechanism can also be used to observe the progress of the iteration.

* 
* [0.x.10]*
   There are two possibilities to compute the residual: one is an estimate   using the computed value  [2.x.6]  The other is exact computation using   another matrix vector multiplication. This increases the costs of the   algorithm, so it is should be set to false whenever the problem allows   it.     Bicgstab is susceptible to breakdowns, so we need a parameter telling us,   which numbers are considered zero.  
* [0.x.11]*
     Constructor.         The default is to perform an exact residual computation and breakdown     parameter is the minimum finite value representable by the value_type of     VectorType.    
* [0.x.12]*
     Flag for exact computation of residual.    
* [0.x.13]*
     Breakdown threshold.    
* [0.x.14]*
   Constructor.  
* [0.x.15]*
   Constructor. Use an object of type GrowingVectorMemory as a default to   allocate memory.  
* [0.x.16]*
   Virtual destructor.  
* [0.x.17]*
   Solve primal problem only.  
* [0.x.18]*
   A pointer to the solution vector passed to solve().  
* [0.x.19]*
   Auxiliary vector.  
* [0.x.20]*
   Auxiliary vector.  
* [0.x.21]*
   Auxiliary vector.  
* [0.x.22]*
   Auxiliary vector.  
* [0.x.23]*
   Auxiliary vector.  
* [0.x.24]*
   Auxiliary vector.  
* [0.x.25]*
   Auxiliary vector.  
* [0.x.26]*
   A pointer to the right hand side vector passed to solve().  
* [0.x.27]*
   Computation of the stopping criterion.  
* [0.x.28]*
   Interface for derived class.  This function gets the current iteration   vector, the residual and the update vector in each step. It can be used   for graphical output of the convergence history.  
* [0.x.29]*
   Additional parameters.  
* [0.x.30]*
   A structure returned by the iterate() function representing what it found   is happening during the iteration.  
* [0.x.31]*
   The iteration loop itself. The function returns a structure indicating   what happened in this function.  
* [0.x.32]

include/deal.II-translator/lac/solver_cg_0.txt
[0.x.0]*
 This class implements the preconditioned Conjugate Gradients (CG) method that can be used to solve linear systems with a symmetric positive definite matrix. This class is used first in  [2.x.0]  and  [2.x.1] , but is used in many other tutorial programs as well. Like all other solver classes, it can work on any kind of vector and matrix as long as they satisfy certain requirements (for the requirements on matrices and vectors in order to work with this class, see the documentation of the Solver base class). The type of the solution vector must be passed as template argument, and defaults to  [2.x.2] 
* 

* 
*  [2.x.3]  This version of CG is taken from D. Braess's book "Finite Elements". It requires a symmetric preconditioner (i.e., for example, SOR is not a possible choice).
* 

*  [1.x.0]
*  The cg-method performs an orthogonal projection of the original preconditioned linear system to another system of smaller dimension. Furthermore, the projected matrix  [2.x.4]  is tri-diagonal. Since the projection is orthogonal, the eigenvalues of  [2.x.5]  approximate those of the original preconditioned matrix  [2.x.6]  In fact, after  [2.x.7]  steps, where  [2.x.8]  is the dimension of the original system, the eigenvalues of both matrices are equal. But, even for small numbers of iteration steps, the condition number of  [2.x.9]  is a good estimate for the one of  [2.x.10] 
*  After  [2.x.11]  steps the matrix T_m can be written in terms of the coefficients  [2.x.12]  and  [2.x.13]  as the tri-diagonal matrix with diagonal elements <tt>1/alpha_0</tt>, <tt>1/alpha_1 + beta_0/alpha_0</tt>, ..., <tt>1/alpha_{m-1</tt>+beta_{m-2}/alpha_{m-2}} and off-diagonal elements <tt>sqrt(beta_0)/alpha_0</tt>, ..., <tt>sqrt(beta_{m-2</tt>)/alpha_{m-2}}. The eigenvalues of this matrix can be computed by postprocessing.
*   [2.x.14]  Y. Saad: "Iterative methods for Sparse Linear Systems", section 6.7.3 for details.
*  The coefficients, eigenvalues and condition number (computed as the ratio of the largest over smallest eigenvalue) can be obtained by connecting a function as a slot to the solver using one of the functions  [2.x.15]  connect_coefficients_slot,  [2.x.16]  and  [2.x.17]  connect_condition_number_slot. These slots will then be called from the solver with the estimates as argument.
*  [1.x.1]
*  The solve() function of this class uses the mechanism described in the Solver base class to determine convergence. This mechanism can also be used to observe the progress of the iteration.

* 
* [0.x.1]*
   Declare type for container size.  
* [0.x.2]*
   Standardized data struct to pipe additional data to the solver.   Here, it doesn't store anything but just exists for consistency   with the other solver classes.  
* [0.x.3]*
   Constructor.  
* [0.x.4]*
   Constructor. Use an object of type GrowingVectorMemory as a default to   allocate memory.  
* [0.x.5]*
   Virtual destructor.  
* [0.x.6]*
   Solve the linear system  [2.x.18]  for x.  
* [0.x.7]*
   Connect a slot to retrieve the CG coefficients. The slot will be called   with alpha as the first argument and with beta as the second argument,   where alpha and beta follow the notation in Y. Saad: "Iterative methods   for Sparse Linear Systems", section 6.7. Called once per iteration  
* [0.x.8]*
   Connect a slot to retrieve the estimated condition number. Called on each   iteration if every_iteration=true, otherwise called once when iterations   are ended (i.e., either because convergence has been achieved, or because   divergence has been detected).  
* [0.x.9]*
   Connect a slot to retrieve the estimated eigenvalues. Called on each   iteration if every_iteration=true, otherwise called once when iterations   are ended (i.e., either because convergence has been achieved, or because   divergence has been detected).  
* [0.x.10]*
   Interface for derived class. This function gets the current iteration   vector, the residual and the update vector in each step. It can be used   for graphical output of the convergence history.  
* [0.x.11]*
   Estimates the eigenvalues from diagonal and offdiagonal. Uses these   estimate to compute the condition number. Calls the signals   eigenvalues_signal and cond_signal with these estimates as arguments.  
* [0.x.12]*
   Additional parameters.  
* [0.x.13]*
   Signal used to retrieve the CG coefficients. Called on each iteration.  
* [0.x.14]*
   Signal used to retrieve the estimated condition number. Called once when   all iterations are ended.  
* [0.x.15]*
   Signal used to retrieve the estimated condition numbers. Called on each   iteration.  
* [0.x.16]*
   Signal used to retrieve the estimated eigenvalues. Called once when all   iterations are ended.  
* [0.x.17]*
   Signal used to retrieve the estimated eigenvalues. Called on each   iteration.  
* [0.x.18]

include/deal.II-translator/lac/solver_control_0.txt
[0.x.0]*
 Control class to determine convergence of iterative solvers.
*  Used by iterative methods to determine whether the iteration should be continued. To this end, the virtual function <tt>check()</tt> is called in each iteration with the current iteration step and the value indicating convergence (usually the residual).
*  After the iteration has terminated, the functions last_value() and last_step() can be used to obtain information about the final state of the iteration.
*  check() can be replaced in derived classes to allow for more sophisticated tests.
* 

*  [1.x.0] The return states of the check function are of type #State, which is an enum local to this class. It indicates the state the solver is in.
*  The possible values of State are  [2.x.0]   [2.x.1]  <tt>iterate = 0</tt>: continue the iteration.  [2.x.2]   [2.x.3]  the goal is reached, the iterative method can terminate successfully.  [2.x.4]   [2.x.5]  the iterative method should stop because convergence could not be achieved or at least was not achieved within the given maximal number of iterations.  [2.x.6] 

* 
* [0.x.1]*
   Enum denoting the different states a solver can be in. See the general   documentation of this class for more information.  
* [0.x.2]*
   Class to be thrown upon failing convergence of an iterative solver, when   either the number of iterations exceeds the limit or the residual fails   to reach the desired limit, e.g. in the case of a break-down.     The residual in the last iteration, as well as the iteration number of   the last step are stored in this object and can be recovered upon   catching an exception of this class.  
* [0.x.3]*
     Iteration number of the last step.    
* [0.x.4]*
     Residual in the last step.    
* [0.x.5]*
   Constructor. The parameters  [2.x.7]  and  [2.x.8]  are the maximum number of   iteration steps before failure and the tolerance to determine success of   the iteration.      [2.x.9]  specifies whether the history (i.e. the value to be   checked and the number of the iteration step) shall be printed to  [2.x.10]    deallog stream.  Default is: do not print. Similarly,  [2.x.11]    specifies the whether the final result is logged to  [2.x.12]  Default   is yes.  
* [0.x.6]*
   Virtual destructor is needed as there are virtual functions in this   class.  
* [0.x.7]*
   Interface to parameter file.  
* [0.x.8]*
   Read parameters from file.  
* [0.x.9]*
   Decide about success or failure of an iteration.  This function gets the   current iteration step to determine, whether the allowed number of steps   has been exceeded and returns  [2.x.13]  in this case. If  [2.x.14]    is below the prescribed tolerance, it returns  [2.x.15]  In all other   cases  [2.x.16]  is returned to suggest continuation of the iterative   procedure.     The iteration is also aborted if the residual becomes a denormalized   value ( [2.x.17]      <tt>check()</tt> additionally preserves  [2.x.18]  and  [2.x.19]  These   values are accessible by <tt>last_value()</tt> and <tt>last_step()</tt>.     Derived classes may overload this function, e.g. to log the convergence   indicators ( [2.x.20]  or to do other computations.  
* [0.x.10]*
   Return the result of the last check operation.  
* [0.x.11]*
   Return the initial convergence criterion.  
* [0.x.12]*
   Return the convergence value of last iteration step for which  [2.x.21]    was called by the solver.  
* [0.x.13]*
   Number of last iteration step.  
* [0.x.14]*
   Maximum number of steps.  
* [0.x.15]*
   Change maximum number of steps.  
* [0.x.16]*
   Enables the failure check. Solving is stopped with  [2.x.22]   [2.x.23]    failure if <tt>residual>failure_residual</tt> with   <tt>failure_residual := rel_failure_residual*first_residual</tt>.  
* [0.x.17]*
   Disables failure check and resets  [2.x.24]  and  [2.x.25]    failure_residual to zero.  
* [0.x.18]*
   Tolerance.  
* [0.x.19]*
   Change tolerance.  
* [0.x.20]*
   Enables writing residuals of each step into a vector for later analysis.  
* [0.x.21]*
   Provide read access to the collected residual data.  
* [0.x.22]*
   Average error reduction over all steps.     Requires enable_history_data()  
* [0.x.23]*
   Error reduction of the last step; for stationary iterations, this   approximates the norm of the iteration matrix.     Requires enable_history_data()  
* [0.x.24]*
   Error reduction of any iteration step.     Requires enable_history_data()  
* [0.x.25]*
   Log each iteration step. Use  [2.x.26]  for skipping steps.  
* [0.x.26]*
   Return the  [2.x.27]  flag.  
* [0.x.27]*
   Set logging frequency.  
* [0.x.28]*
   Log start and end step.  
* [0.x.29]*
   Return the  [2.x.28]  flag.  
* [0.x.30]*
   This exception is thrown if a function operating on the vector of history   data of a SolverControl object id called, but storage of history data was   not enabled by enable_history_data().  
* [0.x.31]*
   Maximum number of steps.  
* [0.x.32]*
   Prescribed tolerance to be achieved.  
* [0.x.33]*
   Result of last check operation.  
* [0.x.34]*
   Initial value.  
* [0.x.35]*
   Last value of the convergence criterion.  
* [0.x.36]*
   Last step.  
* [0.x.37]*
   Is set to  [2.x.29]  by  [2.x.30]  and enables failure   checking.  
* [0.x.38]*
   Stores the  [2.x.31]  set by  [2.x.32]   
* [0.x.39]*
    [2.x.33]  equals the first residual multiplied by  [2.x.34]    relative_crit set by  [2.x.35]  (see there).     Until the first residual is known it is 0.  
* [0.x.40]*
   Log convergence history to  [2.x.36]   
* [0.x.41]*
   Log only every nth step.  
* [0.x.42]*
   Log iteration result to  [2.x.37]   If true, after finishing the   iteration, a statement about failure or success together with  [2.x.38]    and  [2.x.39]  are logged.  
* [0.x.43]*
   Control over the storage of history data. Set by enable_history_data().  
* [0.x.44]*
   Vector storing the result after each iteration step for later statistical   analysis.     Use of this vector is enabled by enable_history_data().  
* [0.x.45]*
 Specialization of  [2.x.40]  which returns  [2.x.41]  if either the specified tolerance is achieved or if the initial residual (or whatever criterion was chosen by the solver class) is reduced by a given factor. This is useful in cases where you don't want to solve exactly, but rather want to gain two digits or if the maximal number of iterations is achieved. For example: The maximal number of iterations is 20, the reduction factor is 1% and the tolerance is 0.1%. The initial residual is 2.5. The process will break if 20 iteration are completed or the new residual is less then 2.5*1% or if it is less then 0.1%.

* 
* [0.x.46]*
   Constructor.  Provide the reduction factor in addition to arguments that   have the same meaning as those of the constructor of the SolverControl   constructor.  
* [0.x.47]*
   Initialize with a SolverControl object. The result will emulate   SolverControl by setting  [2.x.42]  to zero.  
* [0.x.48]*
   Assign a SolverControl object to ReductionControl. The result of the   assignment will emulate SolverControl by setting  [2.x.43]  to zero.  
* [0.x.49]*
   Virtual destructor is needed as there are virtual functions in this   class.  
* [0.x.50]*
   Interface to parameter file.  
* [0.x.51]*
   Read parameters from file.  
* [0.x.52]*
   Decide about success or failure of an iteration.  This function calls the   one in the base class, but sets the tolerance to <tt>reduction initial   value</tt> upon the first iteration.  
* [0.x.53]*
   Reduction factor.  
* [0.x.54]*
   Change reduction factor.  
* [0.x.55]*
   Desired reduction factor.  
* [0.x.56]*
   Reduced tolerance. Stop iterations if either this value is achieved or if   the base class indicates success.  
* [0.x.57]*
 Specialization of  [2.x.44]  which returns  [2.x.45]  if a given number of iteration was performed, irrespective of the actual residual. This is useful in cases where you don't want to solve exactly, but rather want to perform a fixed number of iterations, e.g. in an inner solver. The arguments given to this class are exactly the same as for the SolverControl class and the solver terminates similarly when one of the given tolerance or the maximum iteration count were reached. The only difference to SolverControl is that the solver returns success in the latter case.

* 
* [0.x.58]*
   Constructor.  Provide exactly the same arguments as the constructor of   the SolverControl class.  
* [0.x.59]*
   Initialize with a SolverControl object. The result will emulate   SolverControl by setting the reduction target to zero.  
* [0.x.60]*
   Assign a SolverControl object to ReductionControl. The result of the   assignment will emulate SolverControl by setting the reduction target to   zero.  
* [0.x.61]*
   Virtual destructor is needed as there are virtual functions in this   class.  
* [0.x.62]*
   Decide about success or failure of an iteration. This function bases   success solely on the fact if a given number of iterations was reached or   the check value reached exactly zero.  
* [0.x.63]*
 Specialization of  [2.x.46]  which returns  [2.x.47]  if and only if a certain positive number of consecutive iterations satisfy the specified tolerance. This is useful in cases when solving nonlinear problems using inexact Hessian.
*  For example: The requested number of consecutively converged iterations is 2, the tolerance is 0.2. The ConsecutiveControl will return  [2.x.48]  only at the last step in the sequence 0.5, 0.0005, 1.0, 0.05, 0.01.

* 
* [0.x.64]*
   Constructor.  [2.x.49]  is the number of   consecutive iterations which should satisfy the prescribed tolerance for   convergence. Other arguments have the same meaning as those of the   constructor of the SolverControl.  
* [0.x.65]*
   Initialize with a SolverControl object. The result will emulate   SolverControl by setting  [2.x.50]  to one.  
* [0.x.66]*
   Assign a SolverControl object to ConsecutiveControl. The result of the   assignment will emulate SolverControl by setting  [2.x.51]    to one.  
* [0.x.67]*
   Virtual destructor is needed as there are virtual functions in this   class.  
* [0.x.68]*
   Decide about success or failure of an iteration, see the class description   above.  
* [0.x.69]*
   The number of consecutive iterations which should satisfy the prescribed   tolerance for convergence.  
* [0.x.70]*
   Counter for the number of consecutively converged iterations.  
* [0.x.71]

include/deal.II-translator/lac/solver_fire_0.txt
[0.x.0]*
 FIRE (Fast Inertial Relaxation Engine) for minimization of (potentially non-linear) objective function  [2.x.0] ,  [2.x.1]  is a vector of  [2.x.2]  variables ( [2.x.3]  is the number of variables of the objective function). Like all other solver classes, it can work on any kind of vector and matrix as long as they satisfy certain requirements (for the requirements on matrices and vectors in order to work with this class, see the documentation of the Solver base class). The type of the solution vector must be passed as template argument, and defaults to  [2.x.4] 
*  FIRE is a damped dynamics method described in [1.x.0] by Bitzek et al. 2006, typically used to find stable equilibrium configurations of atomistic systems in computational material science. Starting from a given initial configuration of the atomistic system, the algorithm relies on inertia to obtain (nearest) configuration with least potential energy.
*  Notation:
* 

* 
* 

* 
* 
*  - The global vector of unknown variables:  [2.x.5] .
* 

* 
* 

* 
* 
*  - Objective function:                      [2.x.6] .
* 

* 
* 

* 
* 
*  - Rate of change of unknowns:              [2.x.7] .
* 

* 
* 

* 
* 
*  - Gradient of the objective    function w.r.t unknowns:                 [2.x.8] .
* 

* 
* 

* 
* 
*  - Mass matrix:                             [2.x.9] .
* 

* 
* 

* 
* 
*  - Initial guess of unknowns:               [2.x.10] .
* 

* 
* 

* 
* 
*  - Time step:                               [2.x.11] .
*  Given initial values for  [2.x.12] ,  [2.x.13] ,  [2.x.14] ,  [2.x.15]  and  [2.x.16]  along with a given mass matrix  [2.x.17] , FIRE algorithm is as follows, 1. Calculate  [2.x.18]  and check for convergence    ( [2.x.19] ). 2. Update  [2.x.20]  and  [2.x.21]  using simple (forward) Euler integration step,    <BR>         [2.x.22] ,                 <BR>         [2.x.23] . 3. Calculate  [2.x.24] . 4. Set  [2.x.25] . 5. If  [2.x.26]  and number of steps since  [2.x.27]  was last negative is larger    than certain value, then increase time step  [2.x.28]  and decrease     [2.x.29] . 6. If  [2.x.30] , then decrease the time step, freeze the system i.e.,     [2.x.31]  and reset  [2.x.32] . 7. Return to 1.
*  Also see [1.x.1] by Eidel et al. 2011.

* 
* [0.x.1]*
   Standardized data struct to pipe additional data to the solver.  
* [0.x.2]*
     Constructor. By default, set the initial time step for the (forward)     Euler integration step to 0.1, the maximum time step to 1 and the     maximum change allowed in any variable (per iteration) to 1.    
* [0.x.3]*
     Initial time step for the (forward) Euler integration step.    
* [0.x.4]*
     Maximum time step for the (forward) Euler integration step.    
* [0.x.5]*
     Maximum change allowed in any variable of the objective function.    
* [0.x.6]*
   Constructor.  
* [0.x.7]*
   Constructor. Use an object of type GrowingVectorMemory as a default to   allocate memory.  
* [0.x.8]*
   Obtain a set of variables  [2.x.33]  that minimize an objective function   described by the polymorphic function wrapper  [2.x.34]  with a given   preconditioner  [2.x.35]  and initial  [2.x.36]  values.   The function  [2.x.37]  returns the objective function's value and updates   the objective function's gradient (with respect to the variables) when   passed in as first argument based on the second argument-- the state of   variables.  
* [0.x.9]*
   Solve for x that minimizes  [2.x.38]  for the <EM>special case</EM>   when  [2.x.39] .  
* [0.x.10]*
   Interface for derived class. This function gets the current iteration    [2.x.40]  (variables),  [2.x.41]  (x's time derivative) and  [2.x.42]  (the gradient) in   each step.   It can be used for graphical output of the convergence history.  
* [0.x.11]*
   Additional data to the solver.  
* [0.x.12]

include/deal.II-translator/lac/solver_gmres_0.txt
[0.x.0]*
   A namespace for a helper class to the GMRES solver.  
* [0.x.1]*
     Class to hold temporary vectors.  This class automatically allocates a     new vector, once it is needed.         A future version should also be able to shift through vectors     automatically, avoiding restart.    
* [0.x.2]*
       Constructor. Prepares an array of  [2.x.0]  of length  [2.x.1]        max_size.      
* [0.x.3]*
       Destructor. Delete all allocated vectors.      
* [0.x.4]*
       Get vector number  [2.x.2]  If this vector was unused before, an error       occurs.      
* [0.x.5]*
       Get vector number  [2.x.3]  Allocate it if necessary.             If a vector must be allocated,  [2.x.4]  is used to reinit it to the       proper dimensions.      
* [0.x.6]*
       Return size of data vector. It is used in the solver to store       the Arnoldi vectors.      
* [0.x.7]*
       Pool where vectors are obtained from.      
* [0.x.8]*
       Field for storing the vectors.      
* [0.x.9]*
 Implementation of the Restarted Preconditioned Direct Generalized Minimal Residual Method. The stopping criterion is the norm of the residual.
*  The AdditionalData structure contains the number of temporary vectors used. The size of the Arnoldi basis is this number minus three. Additionally, it allows you to choose between right or left preconditioning. The default is left preconditioning. Finally it includes a flag indicating whether or not the default residual is used as stopping criterion.
* 

*  [1.x.0]
*   [2.x.5]  allows you to choose between left and right preconditioning. As expected, this switches between solving for the systems [1.x.1] and [1.x.2], respectively.
*  A second consequence is the type of residual which is used to measure convergence. With left preconditioning, this is the [1.x.3] residual, while with right preconditioning, it is the residual of the unpreconditioned system.
*  Optionally, this behavior can be overridden by using the flag  [2.x.6]  A <tt>true</tt> value refers to the behavior described in the previous paragraph, while <tt>false</tt> reverts it. Be aware though that additional residuals have to be computed in this case, impeding the overall performance of the solver.
* 

*  [1.x.4]
*  The maximal basis size is controlled by  [2.x.7]  and it is this number minus 2. If the number of iteration steps exceeds this number, all basis vectors are discarded and the iteration starts anew from the approximation obtained so far.
*  Note that the minimizing property of GMRes only pertains to the Krylov space spanned by the Arnoldi basis. Therefore, restarted GMRes is [1.x.5] minimizing anymore. The choice of the basis length is a trade- off between memory consumption and convergence speed, since a longer basis means minimization over a larger space.
*  For the requirements on matrices and vectors in order to work with this class, see the documentation of the Solver base class.
* 

*  [1.x.6]
*  The solve() function of this class uses the mechanism described in the Solver base class to determine convergence. This mechanism can also be used to observe the progress of the iteration.
* 

*  [1.x.7]
*  This class can estimate eigenvalues and condition number during the solution process. This is done by creating the Hessenberg matrix during the inner iterations. The eigenvalues are estimated as the eigenvalues of the Hessenberg matrix and the condition number is estimated as the ratio of the largest and smallest singular value of the Hessenberg matrix. The estimates can be obtained by connecting a function as a slot using  [2.x.8]  connect_condition_number_slot and  [2.x.9]  These slots will then be called from the solver with the estimates as argument.

* 
* [0.x.10]*
   Standardized data struct to pipe additional data to the solver.  
* [0.x.11]*
     Constructor. By default, set the number of temporary vectors to 30,     i.e. do a restart every 28 iterations. Also set preconditioning from     left, the residual of the stopping criterion to the default residual,     and re-orthogonalization only if necessary.    
* [0.x.12]*
     Maximum number of temporary vectors. This parameter controls the size     of the Arnoldi basis, which for historical reasons is     #max_n_tmp_vectors-2. SolverGMRES assumes that there are at least three     temporary vectors, so this value must be greater than or equal to three.    
* [0.x.13]*
     Flag for right preconditioning.        
*  [2.x.10]  Change between left and right preconditioning will also change     the way residuals are evaluated. See the corresponding section in the     SolverGMRES.    
* [0.x.14]*
     Flag for the default residual that is used to measure convergence.    
* [0.x.15]*
     Flag to force re-orthogonalization of orthonormal basis in every step.     If set to false, the solver automatically checks for loss of     orthogonality every 5 iterations and enables re-orthogonalization only     if necessary.    
* [0.x.16]*
   Constructor.  
* [0.x.17]*
   Constructor. Use an object of type GrowingVectorMemory as a default to   allocate memory.  
* [0.x.18]*
   The copy constructor is deleted.  
* [0.x.19]*
   Solve the linear system  [2.x.11]  for x.  
* [0.x.20]*
   Connect a slot to retrieve the estimated condition number. Called on each   outer iteration if every_iteration=true, otherwise called once when   iterations are ended (i.e., either because convergence has been achieved,   or because divergence has been detected).  
* [0.x.21]*
   Connect a slot to retrieve the estimated eigenvalues. Called on each   outer iteration if every_iteration=true, otherwise called once when   iterations are ended (i.e., either because convergence has been achieved,   or because divergence has been detected).  
* [0.x.22]*
   Connect a slot to retrieve the Hessenberg matrix obtained by the   projection of the initial matrix on the Krylov basis. Called on each   outer iteration if every_iteration=true, otherwise called once when   iterations are ended (i.e., either because convergence has been achieved,   or because divergence has been detected).  
* [0.x.23]*
   Connect a slot to retrieve the basis vectors of the Krylov space   generated by the Arnoldi algorithm. Called at once when iterations   are completed (i.e., either because convergence has been achieved,   or because divergence has been detected).  
* [0.x.24]*
   Connect a slot to retrieve a notification when the vectors are   re-orthogonalized.  
* [0.x.25]*
   Includes the maximum number of tmp vectors.  
* [0.x.26]*
   Signal used to retrieve the estimated condition number. Called once when   all iterations are ended.  
* [0.x.27]*
   Signal used to retrieve the estimated condition numbers. Called on each   outer iteration.  
* [0.x.28]*
   Signal used to retrieve the estimated eigenvalues. Called once when all   iterations are ended.  
* [0.x.29]*
   Signal used to retrieve the estimated eigenvalues. Called on each outer   iteration.  
* [0.x.30]*
   Signal used to retrieve the Hessenberg matrix. Called once when   all iterations are ended.  
* [0.x.31]*
   Signal used to retrieve the Hessenberg matrix. Called on each outer   iteration.  
* [0.x.32]*
   Signal used to retrieve the Krylov space basis vectors. Called once   when all iterations are ended.  
* [0.x.33]*
   Signal used to retrieve a notification   when the vectors are re-orthogonalized.  
* [0.x.34]*
   Implementation of the computation of the norm of the residual.  
* [0.x.35]*
   Transformation of an upper Hessenberg matrix into tridiagonal structure   by givens rotation of the last column  
* [0.x.36]*
   Orthogonalize the vector  [2.x.12]  against the  [2.x.13]  (orthogonal) vectors   given by the first argument using the modified Gram-Schmidt algorithm.   The factors used for orthogonalization are stored in  [2.x.14]  The boolean  [2.x.15]    re_orthogonalize specifies whether the modified Gram-Schmidt algorithm   should be applied twice. The algorithm checks loss of orthogonality in   the procedure every fifth step and sets the flag to true in that case.   All subsequent iterations use re-orthogonalization.   Calls the signal re_orthogonalize_signal if it is connected.  
* [0.x.37]*
   Estimates the eigenvalues from the Hessenberg matrix, H_orig, generated   during the inner iterations. Uses these estimate to compute the condition   number. Calls the signals eigenvalues_signal and cond_signal with these   estimates as arguments.  
* [0.x.38]*
   Projected system matrix  
* [0.x.39]*
   Auxiliary matrix for inverting  [2.x.16]   
* [0.x.40]*
 Implementation of the Generalized minimal residual method with flexible preconditioning (flexible GMRES or FGMRES).
*  This flexible version of the GMRES method allows for the use of a different preconditioner in each iteration step. Therefore, it is also more robust with respect to inaccurate evaluation of the preconditioner. An important application is the use of a Krylov space method inside the preconditioner. As opposed to SolverGMRES which allows one to choose between left and right preconditioning, this solver always applies the preconditioner from the right.
*  FGMRES needs two vectors in each iteration steps yielding a total of  [2.x.17]  auxiliary vectors. Otherwise, FGMRES requires roughly the same number of operations per iteration compared to GMRES, except one application of the preconditioner less at each restart and at the end of solve().
*  For more details see  [2.x.18] .

* 
* [0.x.41]*
   Standardized data struct to pipe additional data to the solver.  
* [0.x.42]*
     Constructor. By default, set the maximum basis size to 30.    
* [0.x.43]*
     Maximum basis size.    
* [0.x.44]*
   Constructor.  
* [0.x.45]*
   Constructor. Use an object of type GrowingVectorMemory as a default to   allocate memory.  
* [0.x.46]*
   Solve the linear system  [2.x.19]  for x.  
* [0.x.47]*
   Additional flags.  
* [0.x.48]*
   Projected system matrix  
* [0.x.49]*
   Auxiliary matrix for inverting  [2.x.20]   
* [0.x.50]

include/deal.II-translator/lac/solver_idr_0.txt
[0.x.0]*
   A namespace for a helper class to the IDR(s) solver.  
* [0.x.1]*
     Class to hold temporary vectors whose size depends on     the solver parameter s.    
* [0.x.2]*
       Constructor. Prepares an array of  [2.x.0]  of length  [2.x.1]       
* [0.x.3]*
       Destructor. Delete all allocated vectors.      
* [0.x.4]*
       Get vector number  [2.x.2]  If this vector was unused before, an error       occurs.      
* [0.x.5]*
       Get vector number  [2.x.3]  Allocate it if necessary.             If a vector must be allocated,  [2.x.4]  is used to reinit it to the       proper dimensions.      
* [0.x.6]*
       Pool where vectors are obtained from.      
* [0.x.7]*
       Field for storing the vectors.      
* [0.x.8]*
 This class implements the IDR(s) method used for solving nonsymmetric, indefinite linear systems, developed in [1.x.0]. The implementation here is the preconditioned version from [1.x.1]. The local structure  [2.x.5]  takes the value for the parameter s which can be any integer greater than or equal to 1. For  [2.x.6] , this method has similar convergence to BiCGStab.
* 

* 
*  [2.x.7]  Each iteration of IDR(s) requires  [2.x.8]  preconditioning steps and matrix-vector products. In this implementation the residual is updated and convergence is checked after each of these inner steps inside the outer iteration. If the user enables the history data, the residual at each of these steps is stored and therefore there will be multiple values per iteration.

* 
* [0.x.9]*
   Structure for storing additional data needed by the solver.  
* [0.x.10]*
     Constructor. By default, an IDR(2) method is used.    
* [0.x.11]*
   Constructor.  
* [0.x.12]*
   Constructor. Use an object of type GrowingVectorMemory as a default to   allocate memory.  
* [0.x.13]*
   Virtual destructor.  
* [0.x.14]*
   Solve the linear system  [2.x.9]  for x.  
* [0.x.15]*
   Interface for derived class. This function gets the current iteration   vector, the residual and the update vector in each step. It can be used   for graphical output of the convergence history.  
* [0.x.16]*
   Additional solver parameters.  
* [0.x.17]

include/deal.II-translator/lac/solver_minres_0.txt
[0.x.0]*
 Minimal residual method for symmetric matrices.
*  For the requirements on matrices and vectors in order to work with this class, see the documentation of the Solver base class.
*  Like all other solver classes, this class has a local structure called  [2.x.0]  AdditionalData which is used to pass additional parameters to the solver, like damping parameters or the number of temporary vectors. We use this additional structure instead of passing these values directly to the constructor because this makes the use of the  [2.x.1]  and other classes much easier and guarantees that these will continue to work even if number or type of the additional parameters for a certain solver changes.
*  However, since the MinRes method does not need additional data, the respective structure is empty and does not offer any functionality. The constructor has a default argument, so you may call it without the additional parameter.
*  The preconditioner has to be positive definite and symmetric
*  The algorithm is taken from the Master thesis of Astrid Battermann with some changes. The full text can be found at http://scholar.lib.vt.edu/theses/public/etd-12164379662151/etd-title.html
* 

*  [1.x.0]
*  The solve() function of this class uses the mechanism described in the Solver base class to determine convergence. This mechanism can also be used to observe the progress of the iteration.

* 
* [0.x.1]*
   Standardized data struct to pipe additional data to the solver. This   solver does not need additional data yet.  
* [0.x.2]*
   Constructor.  
* [0.x.3]*
   Constructor. Use an object of type GrowingVectorMemory as a default to   allocate memory.  
* [0.x.4]*
   Virtual destructor.  
* [0.x.5]*
   Solve the linear system  [2.x.2]  for x.  
* [0.x.6]*
    [2.x.3]  Exceptions    [2.x.4]   
* [0.x.7]*
   Exception  
* [0.x.8]*
   Implementation of the computation of the norm of the residual.  
* [0.x.9]*
   Interface for derived class. This function gets the current iteration   vector, the residual and the update vector in each step. It can be used   for graphical output of the convergence history.  
* [0.x.10]*
   Within the iteration loop, the square of the residual vector is stored in   this variable. The function  [2.x.5]  uses this variable to compute   the convergence value, which in this class is the norm of the residual   vector and thus the square root of the  [2.x.6]  value.  
* [0.x.11]

include/deal.II-translator/lac/solver_qmrs_0.txt
[0.x.0]*
 [1.x.0]
*  The SQMR (symmetric quasi-minimal residual) method is supposed to solve symmetric indefinite linear systems with symmetric, not necessarily definite preconditioners. It is a variant of the original quasi-minimal residual method (QMR) and produces the same iterative solution. This version of SQMR is adapted from the respective symmetric QMR-from-BiCG algorithm given by both Freund/Nachtigal: A new Krylov-subspace method for symmetric indefinite linear systems, NASA STI/Recon Technical Report N, 95 (1994) and Freund/Nachtigal: Software for simplified Lanczos and QMR algorithms, Appl. Num. Math. 19 (1995), pp. 319-341 and provides both right and left (but not split) preconditioning.
* 

*  [1.x.1]
*  Note, that the QMR implementation that the given algorithm is based on is derived from classical BiCG. It can be shown (Freund/Szeto: A transpose-free quasi-minimal residual squared algorithm for non-Hermitian linear systems, Advances in Computer Methods for Partial Differential Equations VII (IMACS, New Brunswick, NJ, 1992) pp. 258-264) that the QMR iterates can be generated from the BiCG iteration through one additional vector and some scalar updates. Possible breakdowns (or precisely, divisions by zero) of BiCG therefore obviously transfer to this simple no-look-ahead algorithm.
*  In return the algorithm is cheap compared to classical QMR or BiCGStab, using only one matrix-vector product with the system matrix and one application of the preconditioner per iteration respectively.
*  The residual used for measuring convergence is only approximately calculated by an upper bound. If this value comes below a threshold prescribed within the AdditionalData struct, then the exact residual of the current QMR iterate will be calculated using another multiplication with the system matrix. By experience (according to Freund and Nachtigal) this technique is useful for a threshold that is ten times the solving tolerance, and in that case will be only used in the last one or two steps of the complete iteration.
*  For the requirements on matrices and vectors in order to work with this class, see the documentation of the Solver base class.
*  Like all other solver classes, this class has a local structure called  [2.x.0]  AdditionalData which is used to pass additional parameters to the solver, like damping parameters or the number of temporary vectors. We use this additional structure instead of passing these values directly to the constructor because this makes the use of the  [2.x.1]  and other classes much easier and guarantees that these will continue to work even if number or type of the additional parameters for a certain solver changes.
* 

*  [1.x.2]
*  The solve() function of this class uses the mechanism described in the Solver base class to determine convergence. This mechanism can also be used to observe the progress of the iteration.

* 
* [0.x.1]*
   Standardized data struct to pipe additional data to the solver.     The user is able to switch between right and left preconditioning, that   means solving the systems [1.x.3] and [1.x.4]   respectively, using the corresponding parameter. Note that left   preconditioning means to employ the preconditioned (BiCG-)residual and   otherwise the unpreconditioned one. The default is the application from the   right side.     The  [2.x.2]  threshold is used to define the said bound below which the residual   is computed exactly. See the class documentation for more information. The   default value is 1e-9, that is the default solving precision multiplied by   ten.     SQMR is susceptible to breakdowns (divisions by zero), so we need a   parameter telling us which numbers are considered zero. The proper   breakdown criterion is very unclear, so experiments may be necessary here.   It is even possible to achieve convergence despite of dividing through by   small numbers. There are even cases in which it is advantageous to accept   such divisions because the cheap iteration cost makes the algorithm the   fastest of all available indefinite iterative solvers. Nonetheless, the   default breakdown threshold value is 1e-16.  
* [0.x.2]*
     Constructor.         The default is right preconditioning, with the  [2.x.3]  chosen to be 1e-9 and     the  [2.x.4]  set at 1e-16.    
* [0.x.3]*
     Flag for using a left-preconditioned version.    
* [0.x.4]*
     The threshold below which the current residual is computed exactly.    
* [0.x.5]*
     Flag for breakdown testing.    
* [0.x.6]*
     Breakdown threshold. Scalars measured to this bound are used for     divisions.    
* [0.x.7]*
   Constructor.  
* [0.x.8]*
   Constructor. Use an object of type GrowingVectorMemory as a default to   allocate memory.  
* [0.x.9]*
   Solve the linear system  [2.x.5]  for x.  
* [0.x.10]*
   Interface for derived class. This function gets the current iteration   vector, the residual and the update vector in each step. It can be used   for a graphical output of the convergence history.  
* [0.x.11]*
   Additional parameters.  
* [0.x.12]*
   A structure returned by the iterate() function representing what it found   is happening during the iteration.  
* [0.x.13]*
   The iteration loop itself. The function returns a structure indicating   what happened in this function.  
* [0.x.14]*
   Number of the current iteration (accumulated over restarts)  
* [0.x.15]

include/deal.II-translator/lac/solver_relaxation_0.txt
[0.x.0]*
 Implementation of an iterative solver based on relaxation methods. The stopping criterion is the norm of the residual.
*  For the requirements on matrices and vectors in order to work with this class, see the documentation of the Solver base class.
*  Like all other solver classes, this class has a local structure called  [2.x.0]  AdditionalData which is used to pass additional parameters to the solver, like damping parameters or the number of temporary vectors. We use this additional structure instead of passing these values directly to the constructor because this makes the use of the  [2.x.1]  and other classes much easier and guarantees that these will continue to work even if number or type of the additional parameters for a certain solver changes. AdditionalData of this class currently does not contain any data.
* 

*  [1.x.0]
*  The solve() function of this class uses the mechanism described in the Solver base class to determine convergence. This mechanism can also be used to observe the progress of the iteration.
* 

* 

* 
*  [2.x.2] 

* 
* [0.x.1]*
   Standardized data struct to pipe additional data to the solver. There is   no data in here for relaxation methods.  
* [0.x.2]*
   Constructor.  
* [0.x.3]*
   Solve the system  [2.x.3]  using the relaxation method  [2.x.4] . The matrix [1.x.1] itself is only used to compute the   residual.  
* [0.x.4]

include/deal.II-translator/lac/solver_richardson_0.txt
[0.x.0]*
 Implementation of the preconditioned Richardson iteration method. The stopping criterion is the norm of the residual.
*  For the requirements on matrices and vectors in order to work with this class, see the documentation of the Solver base class.
*  Like all other solver classes, this class has a local structure called  [2.x.0]  AdditionalData which is used to pass additional parameters to the solver, like damping parameters or the number of temporary vectors. We use this additional structure instead of passing these values directly to the constructor because this makes the use of the  [2.x.1]  and other classes much easier and guarantees that these will continue to work even if number or type of the additional parameters for a certain solver changes.
*  For the Richardson method, the additional data is the damping parameter, which is the only content of the  [2.x.2]  structure. By default, the constructor of the structure sets it to one.
* 

*  [1.x.0]
*  The solve() function of this class uses the mechanism described in the Solver base class to determine convergence. This mechanism can also be used to observe the progress of the iteration.

* 
* [0.x.1]*
   Standardized data struct to pipe additional data to the solver.  
* [0.x.2]*
     Constructor. By default, set the damping parameter to one.    
* [0.x.3]*
     Relaxation parameter.    
* [0.x.4]*
     Parameter for stopping criterion.    
* [0.x.5]*
   Constructor.  
* [0.x.6]*
   Constructor. Use an object of type GrowingVectorMemory as a default to   allocate memory.  
* [0.x.7]*
   Virtual destructor.  
* [0.x.8]*
   Solve the linear system  [2.x.3]  for x.  
* [0.x.9]*
   Solve  [2.x.4]  for  [2.x.5] .  
* [0.x.10]*
   Set the damping-coefficient. Default is 1., i.e. no damping.  
* [0.x.11]*
   Interface for derived class. This function gets the current iteration   vector, the residual and the update vector in each step. It can be used   for graphical output of the convergence history.  
* [0.x.12]*
   Implementation of the computation of the norm of the residual.   Depending on the flags given to the solver, the default   implementation of this function uses either the actual   residual,  [2.x.6]  or the preconditioned residual,  [2.x.7]   
* [0.x.13]*
   Control parameters.  
* [0.x.14]

include/deal.II-translator/lac/solver_selector_0.txt
[0.x.0]*
 Selects a solver by changing a parameter.
*  By calling the  [2.x.0]  function of this  [2.x.1]  it selects the  [2.x.2]  function of that  [2.x.3]  that was specified in the constructor of this class.
*  [1.x.0] The simplest use of this class is the following:

* 
* [1.x.1]
*  But the full usefulness of the  [2.x.4]  class is not clear until the presentation of the following example that assumes the user using the  [2.x.5]  class and having declared a "solver" entry, e.g. with

* 
* [1.x.2]
*  Assuming that in the users parameter file there exists the line

* 
* [1.x.3]
*  then the constructor call in the above example can be written as

* 
* [1.x.4]
* 
* 

*  If at some time there exists a new solver "xyz" then the user does not need to change their program. Only in the implementation of the  [2.x.6]  the calling of this solver has to be added and each user with program lines quoted above only needs to 'set solver = xyz' in their parameter file to get access to that new solver.

* 
* [0.x.1]*
   An alias for the underlying vector type  
* [0.x.2]*
   Constructor, filling in default values  
* [0.x.3]*
   Constructor, selecting the solver  [2.x.7]    and the SolverControl object  [2.x.8]  already.  
* [0.x.4]*
   Destructor  
* [0.x.5]*
   Solver procedure. Calls the  [2.x.9]  function of the  [2.x.10]  whose  [2.x.11]    SolverName was specified in the constructor.  
* [0.x.6]*
   Select a new solver. Note that all solver names used in this class are   all lower case.  
* [0.x.7]*
   Set a new SolverControl. This needs to be set before solving.  
* [0.x.8]*
   Set the additional data. For more information see the  [2.x.12]  class.  
* [0.x.9]*
   Set the additional data. For more information see the  [2.x.13]  class.  
* [0.x.10]*
   Set the additional data. For more information see the  [2.x.14]  class.  
* [0.x.11]*
   Set the additional data. For more information see the  [2.x.15]  class.  
* [0.x.12]*
   Set the additional data. For more information see the  [2.x.16]  class.  
* [0.x.13]*
   Set the additional data. For more information see the  [2.x.17]  class.  
* [0.x.14]*
   Get the names of all implemented solvers. The list of possible   options includes:    [2.x.18]     [2.x.19]   "richardson"  [2.x.20]     [2.x.21]   "cg"  [2.x.22]     [2.x.23]   "bicgstab"  [2.x.24]     [2.x.25]   "gmres"  [2.x.26]     [2.x.27]   "fgmres"  [2.x.28]     [2.x.29]   "minres"  [2.x.30]     [2.x.31]   
* [0.x.15]*
   Exception.  
* [0.x.16]*
   Stores the  [2.x.32]  that is needed in the constructor of each  [2.x.33]    Solver class. This can be changed with  [2.x.34]   
* [0.x.17]*
   Stores the name of the solver.  
* [0.x.18]*
   Stores the additional data.  
* [0.x.19]*
   Stores the additional data.  
* [0.x.20]*
   Stores the additional data.  
* [0.x.21]*
   Stores the additional data.  
* [0.x.22]*
   Stores the additional data.  
* [0.x.23]*
   Stores the additional data.  
* [0.x.24]

include/deal.II-translator/lac/sparse_decomposition_0.txt
[0.x.0]!  [2.x.0]  Preconditioners [2.x.1] 

* 
* [0.x.1]*
 Abstract base class for incomplete decompositions of a sparse matrix into sparse factors. This class can't be used by itself, but only as the base class of derived classes that actually implement particular decompositions such as SparseILU or SparseMIC.
*  The decomposition is stored as a sparse matrix which is why this class is derived from the SparseMatrix. Since it is not a matrix in the usual sense (the stored entries are not those of a matrix, but of the two factors of the original matrix), the derivation is <tt>protected</tt> rather than <tt>public</tt>.
* 

*  [1.x.0]
*  Sparse decompositions are frequently used with additional fill-in, i.e., the sparsity structure of the decomposition is denser than that of the matrix to be decomposed. The initialize() function of this class allows this fill-in via the AdditionalData object as long as all entries present in the original matrix are present in the decomposition also, i.e. the sparsity pattern of the decomposition is a superset of the sparsity pattern in the original matrix.
*  Such fill-in can be accomplished by various ways, one of which is the copy- constructor of the SparsityPattern class that allows the addition of side- diagonals to a given sparsity structure.
* 

*  [1.x.1]
*  While objects of this class can not be used directly (this class is only a base class for others implementing actual decompositions), derived classes such as SparseILU and SparseMIC can be used in the usual form as preconditioners. For example, this works:

* 
* [1.x.2]
* 
*  Through the AdditionalData object it is possible to specify additional parameters of the LU decomposition.
*  1/ The matrix diagonal can be strengthened by adding  [2.x.2]  times the sum of the absolute row entries of each row to the respective diagonal entries. By default no strengthening is performed.
*  2/ By default, each initialize() function call creates its own sparsity. For that, it copies the sparsity of  [2.x.3]  and adds a specific number of extra off diagonal entries specified by  [2.x.4] .
*  3/ By setting  [2.x.5]  the sparsity is not recreated but the sparsity of the previous initialize() call is reused (recycled). This might be useful when several linear problems on the same sparsity need to solved, as for example several Newton iteration steps on the same triangulation. The default is  [2.x.6] .
*  4/ It is possible to give a user defined sparsity to  [2.x.7] . Then, no sparsity is created but  [2.x.8]  is used to store the decomposed matrix. For restrictions on the sparsity see section `Fill-in' above).
* 

*  [1.x.3]
*  It is enough to override the initialize() and vmult() methods to implement particular LU decompositions, like the true LU, or the Cholesky decomposition. Additionally, if that decomposition needs fine tuned diagonal strengthening on a per row basis, it may override the get_strengthen_diagonal() method.

* 
* [0.x.2]*
   Constructor. Does nothing.     Call the initialize() function before using this object as preconditioner   (vmult()).  
* [0.x.3]*
   Declare type for container size.  
* [0.x.4]*
   Destruction. Mark the destructor pure to ensure that this class isn't   used directly, but only its derived classes.  
* [0.x.5]*
   Deletes all member variables. Leaves the class in the state that it had   directly after calling the constructor  
* [0.x.6]*
   Parameters for SparseDecomposition.  
* [0.x.7]*
     Constructor. For the parameters' description, see below.    
* [0.x.8]*
      [2.x.9]  times the sum of absolute row entries is     added to the diagonal entries.         Per default, this value is zero, i.e. the diagonal is not strengthened.    
* [0.x.9]*
     By default, the  [2.x.10]  function creates     its own sparsity. This sparsity has the same SparsityPattern as      [2.x.11]  with some extra off diagonals the number of which     is specified by  [2.x.12] .         The user can give a SparsityPattern to  [2.x.13] .     Then this sparsity is used and the  [2.x.14]      argument is ignored.    
* [0.x.10]*
     If this flag is true the initialize() function uses the same sparsity     that was used during the previous initialize() call.         This might be useful when several linear problems on the same sparsity     need to solved, as for example several Newton iteration steps on the     same triangulation.    
* [0.x.11]*
     When a SparsityPattern is given to this argument, the initialize()     function calls  [2.x.15]  causing this     sparsity to be used.         Note that the sparsity structures of  [2.x.16]      and the matrix passed to the initialize function need not be equal.     Fill-in is allowed, as well as filtering out some elements in the     matrix.    
* [0.x.12]*
   This function needs to be called before an object of this class is used   as preconditioner.     For more detail about possible parameters, see the class documentation   and the documentation of the  [2.x.17]  class.     According to the  [2.x.18] , this function creates a new   SparsityPattern or keeps the previous sparsity or takes the sparsity   given by the user to  [2.x.19] . Then, this function performs the   LU decomposition.     After this function is called the preconditioner is ready to be used   (using the  [2.x.20]  function of derived classes).  
* [0.x.13]*
   Return whether the object is empty. It calls the inherited    [2.x.21]  function.  
* [0.x.14]*
   Return the dimension of the codomain (or range) space. It calls the   inherited  [2.x.22]  function. Note that the matrix is of   dimension  [2.x.23] .  
* [0.x.15]*
   Return the dimension of the domain space. It calls the  inherited    [2.x.24]  function. Note that the matrix is of dimension  [2.x.25] .  
* [0.x.16]*
   Adding Matrix-vector multiplication. Add [1.x.4] on [1.x.5] with   [1.x.6] being this matrix.     Source and destination must not be the same vector.  
* [0.x.17]*
   Adding Matrix-vector multiplication. Add [1.x.7] to   [1.x.8] with [1.x.9] being this matrix. This function does the same   as vmult_add() but takes the transposed matrix.     Source and destination must not be the same vector.  
* [0.x.18]*
   Determine an estimate for the memory consumption (in bytes) of this   object.  
* [0.x.19]*
    [2.x.26]  Exceptions    [2.x.27]   
* [0.x.20]*
   Exception  
* [0.x.21]*
   Copies the passed SparseMatrix onto this object. This object's sparsity   pattern remains unchanged.  
* [0.x.22]*
   Performs the strengthening loop. For each row calculates the sum of   absolute values of its elements, determines the strengthening factor   (through get_strengthen_diagonal()) sf and multiplies the diagonal entry   with  [2.x.28] .  
* [0.x.23]*
   In the decomposition phase, computes a strengthening factor for the   diagonal entry in row  [2.x.29]  with sum of absolute values of its   elements  [2.x.30] .    
*  [2.x.31]  The default implementation in SparseLUDecomposition returns    [2.x.32] 's value. This variable is set to   a nonzero value in several of the derived classes.  
* [0.x.24]*
   The default strengthening value, returned by get_strengthen_diagonal().  
* [0.x.25]*
   For every row in the underlying SparsityPattern, this array contains a   pointer to the row's first afterdiagonal entry. Becomes available after   invocation of prebuild_lower_bound().  
* [0.x.26]*
   Fills the #prebuilt_lower_bound array.  
* [0.x.27]*
   In general this pointer is zero except for the case that no   SparsityPattern is given to this class. Then, a SparsityPattern is   created and is passed down to the SparseMatrix base class.     Nevertheless, the SparseLUDecomposition needs to keep ownership of this   sparsity. It keeps this pointer to it enabling it to delete this sparsity   at destruction time.  
* [0.x.28]

include/deal.II-translator/lac/sparse_decomposition.templates_0.txt
[0.x.0]

include/deal.II-translator/lac/sparse_direct_0.txt
[0.x.0]*
   Index type for UMFPACK. SuiteSparse_long has to be used here for the   Windows 64 build.  
* [0.x.1]*
 This class provides an interface to the sparse direct solver UMFPACK, which is part of the SuiteSparse library (see [1.x.0]). UMFPACK is a set of routines for solving non-symmetric sparse linear systems, Ax=b, using the Unsymmetric-pattern MultiFrontal method and direct sparse LU factorization. Matrices may have symmetric or unsymmetric sparsity patterns, and may have unsymmetric entries. The use of this class is explained in the  [2.x.0]  and  [2.x.1]  tutorial programs.
*  This matrix class implements the usual interface of preconditioners, that is a function initialize(const SparseMatrix<double>&matrix,const AdditionalData) for initializing and the whole set of vmult() functions common to all matrices. Implemented here are only vmult() and vmult_add(), which perform multiplication with the inverse matrix. Furthermore, this class provides an older interface, consisting of the functions factorize() and solve(). Both interfaces are interchangeable.
* 

* 
*  [2.x.2]  This class exists if the [1.x.1] interface was not explicitly disabled during configuration.
* 

* 
*  [2.x.3]  UMFPACK has its own license, independent of that of deal.II. If you want to use the UMFPACK you have to accept that license. It is linked to from the deal.II ReadMe file. UMFPACK is included courtesy of its author, [1.x.2].
* 

*  [1.x.3]
*  There are instantiations of this class for SparseMatrix<double>, SparseMatrix<float>, SparseMatrixEZ<float>, SparseMatrixEZ<double>, BlockSparseMatrix<double>, and BlockSparseMatrix<float>.
* 

* 
*  [2.x.4] 

* 
* [0.x.2]*
   Declare type for container size.  
* [0.x.3]*
   Dummy class needed for the usual initialization interface of   preconditioners.  
* [0.x.4]*
   Constructor. See the documentation of this class for the meaning of the   parameters to this function.  
* [0.x.5]*
   Destructor.  
* [0.x.6]*
    [2.x.5]  Setting up a sparse factorization  
* [0.x.7]*
    [2.x.6]   
* [0.x.8]*
   This function does nothing. It is only here to provide a interface   consistent with other sparse direct solvers.  
* [0.x.9]*
   Factorize the matrix. This function may be called multiple times for   different matrices, after the object of this class has been initialized   for a certain sparsity pattern. You may therefore save some computing   time if you want to invert several matrices with the same sparsity   pattern. However, note that the bulk of the computing time is actually   spent in the factorization, so this functionality may not always be of   large benefit.     In contrast to the other direct solver classes, the initialization method   does nothing. Therefore initialize is not automatically called by this   method, when the initialization step has not been performed yet.     This function copies the contents of the matrix into its own storage; the   matrix can therefore be deleted after this operation, even if subsequent   solves are required.  
* [0.x.10]*
   Initialize memory and call  [2.x.7]   
* [0.x.11]*
    [2.x.8]   
* [0.x.12]*
    [2.x.9]  Functions that represent the inverse of a matrix  
* [0.x.13]*
    [2.x.10]   
* [0.x.14]*
   Preconditioner interface function. Usually, given the source vector, this   method returns an approximate solution of [1.x.4]. As this class   provides a wrapper to a direct solver, here it is actually the exact   solution (exact within the range of numerical accuracy of course).     In other words, this function actually multiplies with the exact inverse   of the matrix,  [2.x.11] .  
* [0.x.15]*
   Same as before, but for block vectors.  
* [0.x.16]*
   Same as before, but uses the transpose of the matrix, i.e. this function   multiplies with  [2.x.12] .  
* [0.x.17]*
   Same as before, but for block vectors  
* [0.x.18]*
   Return the dimension of the codomain (or range) space. Note that the   matrix is of dimension  [2.x.13] .  
* [0.x.19]*
   Return the dimension of the domain space. Note that the matrix is of   dimension  [2.x.14] .  
* [0.x.20]*
    [2.x.15]   
* [0.x.21]*
    [2.x.16]  Functions that solve linear systems  
* [0.x.22]*
    [2.x.17]   
* [0.x.23]*
   Solve for a certain right hand side vector. This function may be called   multiple times for different right hand side vectors after the matrix has   been factorized. This yields substantial savings in computing time, since   the actual solution is fast, compared to the factorization of the matrix.     The solution will be returned in place of the right hand side vector.      [2.x.18]  rhs_and_solution A vector that contains the right hand side      [2.x.19]  of a linear system  [2.x.20]  upon calling this function, and that     contains the solution  [2.x.21]  of the linear system after calling this     function.    [2.x.22]  transpose If set to true, this function solves the linear      [2.x.23]  instead of  [2.x.24] .      [2.x.25]  You need to call factorize() before this function can be called.  
* [0.x.24]*
   Like the previous function, but for a complex-valued right hand side   and solution vector.     If the matrix that was previously factorized had complex-valued entries,   then the `rhs_and_solution` vector will, upon return from this function,   simply contain the solution of the linear system  [2.x.26] . If the matrix   was real-valued, then this is also true, but the solution will simply   be computed by applying the factorized  [2.x.27]  to both the   real and imaginary parts of the right hand side vector.  
* [0.x.25]*
   Same as before, but for block vectors.  
* [0.x.26]*
   Same as before, but for complex-valued block vectors.  
* [0.x.27]*
   Call the two functions factorize() and solve() in that order, i.e.   perform the whole solution process for the given right hand side vector.     The solution will be returned in place of the right hand side vector.  
* [0.x.28]*
   Same as before, but for complex-valued solution vectors.  
* [0.x.29]*
   Same as before, but for block vectors.  
* [0.x.30]*
   Same as before, but for complex-valued block vectors.  
* [0.x.31]*
    [2.x.28]   
* [0.x.32]*
   One of the UMFPack routines threw an error. The error code is included in   the output and can be looked up in the UMFPack user manual. The name of   the routine is included for reference.  
* [0.x.33]*
   The dimension of the range space, i.e., the number of rows of the matrix.  
* [0.x.34]*
   The dimension of the domain space, i.e., the number of columns of the   matrix.  
* [0.x.35]*
   The UMFPACK routines allocate objects in which they store information   about symbolic and numeric values of the decomposition. The actual data   type of these objects is opaque, and only passed around as void pointers.  
* [0.x.36]*
   Free all memory that hasn't been freed yet.  
* [0.x.37]*
   Make sure that the arrays Ai and Ap are sorted in each row. UMFPACK wants   it this way. We need to have three versions of this function, one for the   usual SparseMatrix, one for the SparseMatrixEZ, and one for the   BlockSparseMatrix classes  
* [0.x.38]*
   The arrays in which we store the data for the solver. These are documented   in the descriptions of the umfpack_*_symbolic() and umfpack_*_numeric()   functions, but in short:
* 

* 
* 

* 
* 

* 
* 
*  - `Ap` is the array saying which row starts where in `Ai`
* 

* 
* 

* 
* 

* 
* 
*  - `Ai` is the array that stores the column indices of nonzero entries
* 

* 
* 

* 
* 

* 
* 
*  - `Ax` is the array that stores the values of nonzero entries; if the     matrix is complex-valued, then it stores the real parts
* 

* 
* 

* 
* 

* 
* 
*  - `Az` is the array that stores the imaginary parts of nonzero entries,     and is used only if the matrix is complex-valued.  
* [0.x.39]*
   Control and work arrays for the solver routines.  
* [0.x.40]

include/deal.II-translator/lac/sparse_ilu_0.txt
[0.x.0]!  [2.x.0]  Preconditioners [2.x.1] 

* 
* [0.x.1]*
 This class computes an Incomplete LU (ILU) decomposition of a sparse matrix, using either the same sparsity pattern or a different one. By incomplete we mean that unlike the exact decomposition, the incomplete one is also computed using sparse factors, and entries in the decomposition that do not fit into the given sparsity structure are discarded.
*  The algorithm used by this class is essentially a copy of the algorithm given in the book Y. Saad: "Iterative methods for sparse linear systems", second edition, in section 10.3.2.
* 

*  [1.x.0]
*  Refer to SparseLUDecomposition documentation for suggested usage and state management. This class is used in the  [2.x.2]  " [2.x.3] " tutorial program.
* 

* 
*  [2.x.4]  Instantiations for this template are provided for <tt> [2.x.5]  and  [2.x.6]  others can be generated in application programs (see the section on  [2.x.7]  in the manual).

* 
* [0.x.2]*
   Declare type for container size.  
* [0.x.3]*
   Constructor. Does nothing.     Call the  [2.x.8]  function before using this object as   preconditioner.  
* [0.x.4]*
   Make  [2.x.9]  accessible to this class as   well.  
* [0.x.5]*
   Perform the incomplete LU factorization of the given matrix.     This function needs to be called before an object of this class is used   as preconditioner.     For more details about possible parameters, see the class documentation   of SparseLUDecomposition and the documentation of the  [2.x.10]     [2.x.11]  class.     According to the  [2.x.12]  this function creates a new   SparsityPattern or keeps the previous sparsity or takes the sparsity   given by the user to  [2.x.13]  Then, this function performs the LU   decomposition.     After this function is called the preconditioner is ready to be used.  
* [0.x.6]*
   Apply the incomplete decomposition, i.e. do one forward-backward step    [2.x.14] .     The initialize() function needs to be called before.  
* [0.x.7]*
   Apply the transpose of the incomplete decomposition, i.e. do one forward-   backward step  [2.x.15] .     The initialize() function needs to be called before.  
* [0.x.8]*
   Determine an estimate for the memory consumption (in bytes) of this   object.  
* [0.x.9]*
    [2.x.16]  Exceptions    [2.x.17]   
* [0.x.10]*
   Exception  
* [0.x.11]*
   Exception  
* [0.x.12]

include/deal.II-translator/lac/sparse_ilu.templates_0.txt
[0.x.0]

include/deal.II-translator/lac/sparse_matrix_0.txt
[0.x.0]*
  [2.x.0]  Matrix1  [2.x.1] 

* 
* [0.x.1]*
 A namespace in which we declare iterators over the elements of sparse matrices.

* 
* [0.x.2]*
   Declare type for container size.  
* [0.x.3]*
   General template for sparse matrix accessors. The first template argument   denotes the underlying numeric type, the second the constness of the   matrix.     The general template is not implemented, only the specializations for the   two possible values of the second template argument. Therefore, the   interface listed here only serves as a template provided since doxygen   does not link the specializations.  
* [0.x.4]*
     Value of this matrix entry.    
* [0.x.5]*
     Value of this matrix entry.    
* [0.x.6]*
     Return a reference to the matrix into which this accessor points. Note     that in the present case, this is a constant reference.    
* [0.x.7]*
   Accessor class for constant matrices, used in the const_iterators. This   class builds on the accessor classes used for sparsity patterns to loop   over all nonzero entries, and only adds the accessor functions to gain   access to the actual value stored at a certain location.  
* [0.x.8]*
     Typedef for the type (including constness) of the matrix to be used     here.    
* [0.x.9]*
     Constructor.    
* [0.x.10]*
     Constructor. Construct the end accessor for the given matrix.    
* [0.x.11]*
     Copy constructor to get from a non-const accessor to a const accessor.    
* [0.x.12]*
     Value of this matrix entry.    
* [0.x.13]*
     Return a reference to the matrix into which this accessor points. Note     that in the present case, this is a constant reference.    
* [0.x.14]*
     Pointer to the matrix we use.    
* [0.x.15]*
     Make the advance function of the base class available.    
* [0.x.16]*
   Accessor class for non-constant matrices, used in the iterators. This   class builds on the accessor classes used for sparsity patterns to loop   over all nonzero entries, and only adds the accessor functions to gain   access to the actual value stored at a certain location.  
* [0.x.17]*
     Reference class. This is what the accessor class returns when you call     the value() function. The reference acts just as if it were a reference     to the actual value of a matrix entry, i.e. you can read and write it,     you can add and multiply to it, etc, but since the matrix does not give     away the address of this matrix entry, we have to go through functions     to do all this.         The constructor takes a pointer to an accessor object that describes     which element of the matrix it points to. This creates an ambiguity     when one writes code like iterator->value()=0 (instead of     iterator->value()=0.0), since the right hand side is an integer that     can both be converted to a <tt>number</tt> (i.e., most commonly a     double) or to another object of type <tt>Reference</tt>. The compiler     then complains about not knowing which conversion to take.         For some reason, adding another overload operator=(int) doesn't seem to     cure the problem. We avoid it, however, by adding a second, dummy     argument to the Reference constructor, that is unused, but makes sure     there is no second matching conversion sequence using a one-argument     right hand side.         The testcase oliver_01 checks that this actually works as intended.    
* [0.x.18]*
       Constructor. For the second argument, see the general class       documentation.      
* [0.x.19]*
       Conversion operator to the data type of the matrix.      
* [0.x.20]*
       Set the element of the matrix we presently point to to  [2.x.2]       
* [0.x.21]*
       Add  [2.x.3]  to the element of the matrix we presently point to.      
* [0.x.22]*
       Subtract  [2.x.4]  from the element of the matrix we presently point to.      
* [0.x.23]*
       Multiply the element of the matrix we presently point to by  [2.x.5]       
* [0.x.24]*
       Divide the element of the matrix we presently point to by  [2.x.6]       
* [0.x.25]*
       Pointer to the accessor that denotes which element we presently point       to.      
* [0.x.26]*
     Typedef for the type (including constness) of the matrix to be used     here.    
* [0.x.27]*
     Constructor.    
* [0.x.28]*
     Constructor. Construct the end accessor for the given matrix.    
* [0.x.29]*
     Value of this matrix entry, returned as a read- and writable reference.    
* [0.x.30]*
     Return a reference to the matrix into which this accessor points. Note     that in the present case, this is a non-constant reference.    
* [0.x.31]*
     Pointer to the matrix we use.    
* [0.x.32]*
     Make the advance function of the base class available.    
* [0.x.33]*
   Iterator for constant and non-constant matrices.     The typical use for these iterators is to iterate over the elements of a   sparse matrix or over the elements of individual rows. Note that there is   no guarantee that the elements of a row are actually traversed in an   order in which columns monotonically increase. See the documentation of   the SparsityPattern class for more information.     The first template argument denotes the underlying numeric type, the   second the constness of the matrix.     Since there is a specialization of this class for   <tt>Constness=false</tt>, this class is for iterators to constant   matrices.    
*  [2.x.7]  This class operates directly on the internal data structures of the   SparsityPattern and SparseMatrix classes. As a consequence, some   operations are cheap and some are not. In particular, it is cheap to   access the column index and the value of an entry pointed to. On the   other hand, it is expensive to access the row index (this requires    [2.x.8]  operations for a matrix with  [2.x.9]  row). As a consequence,   when you design algorithms that use these iterators, it is common   practice to not loop over [1.x.0] elements of a sparse matrix at once,   but to have an outer loop over all rows and within this loop iterate over   the elements of this row. This way, you only ever need to dereference the   iterator to obtain the column indices and values whereas the (expensive)   lookup of the row index can be avoided by using the loop index instead.  
* [0.x.34]*
     Typedef for the matrix type (including constness) we are to operate on.    
* [0.x.35]*
     An alias for the type you get when you dereference an iterator of the     current kind.    
* [0.x.36]*
     Constructor. Create an iterator into the matrix  [2.x.10]  for the given     index in the complete matrix (counting from the zeroth entry).    
* [0.x.37]*
     Constructor. Create the end iterator for the given matrix.    
* [0.x.38]*
     Conversion constructor to get from a non-const iterator to a const     iterator.    
* [0.x.39]*
     Copy assignment operator from a non-const iterator to a const iterator.    
* [0.x.40]*
     Prefix increment.    
* [0.x.41]*
     Postfix increment.    
* [0.x.42]*
     Dereferencing operator.    
* [0.x.43]*
     Dereferencing operator.    
* [0.x.44]*
     Comparison. True, if both iterators point to the same matrix position.    
* [0.x.45]*
     Inverse of <tt>==</tt>.    
* [0.x.46]*
     Comparison operator. Result is true if either the first row number is     smaller or if the row numbers are equal and the first index is smaller.         This function is only valid if both iterators point into the same     matrix.    
* [0.x.47]*
     Comparison operator. Works in the same way as above operator, just the     other way round.    
* [0.x.48]*
     Return the distance between the current iterator and the argument. The     distance is given by how many times one has to apply operator++ to the     current iterator to get the argument (for a positive return value), or     operator-- (for a negative return value).    
* [0.x.49]*
     Return an iterator that is  [2.x.11]  ahead of the current one.    
* [0.x.50]*
     Store an object of the accessor class.    
* [0.x.51]*
  [2.x.12] 

* 
* [0.x.52]*
 Sparse matrix. This class implements the functionality to store matrix entry values in the locations denoted by a SparsityPattern. See  [2.x.13]  for a discussion about the separation between sparsity patterns and matrices.
*  The elements of a SparseMatrix are stored in the same order in which the SparsityPattern class stores its entries. Within each row, elements are generally stored left-to-right in increasing column index order; the exception to this rule is that if the matrix is square (m() == n()), then the diagonal entry is stored as the first element in each row to make operations like applying a Jacobi or SSOR preconditioner faster. As a consequence, if you traverse the elements of a row of a SparseMatrix with the help of iterators into this object (using  [2.x.14]  and  [2.x.15]  you will find that the elements are not sorted by column index within each row whenever the matrix is square.
* 

* 
*  [2.x.16]  Instantiations for this template are provided for <tt> [2.x.17]  and  [2.x.18]  others can be generated in application programs (see the section on  [2.x.19]  in the manual).
* 

* 
*  [2.x.20] 

* 
* [0.x.53]*
   Declare type for container size.  
* [0.x.54]*
   Type of the matrix entries. This alias is analogous to   <tt>value_type</tt> in the standard library containers.  
* [0.x.55]*
   Declare a type that has holds real-valued numbers with the same precision   as the template argument to this class. If the template argument of this   class is a real data type, then real_type equals the template argument.   If the template argument is a  [2.x.21]  type then real_type equals the   type underlying the complex numbers.     This alias is used to represent the return type of norms.  
* [0.x.56]*
   Typedef of an iterator class walking over all the nonzero entries of this   matrix. This iterator cannot change the values of the matrix.  
* [0.x.57]*
   Typedef of an iterator class walking over all the nonzero entries of this   matrix. This iterator  [2.x.22]  can change the values of the matrix, but of   course can't change the sparsity pattern as this is fixed once a sparse   matrix is attached to it.  
* [0.x.58]*
   A structure that describes some of the traits of this class in terms of   its run-time behavior. Some other classes (such as the block matrix   classes) that take one or other of the matrix classes as its template   parameters can tune their behavior based on the variables in this class.  
* [0.x.59]*
     It is safe to elide additions of zeros to individual elements of this     matrix.    
* [0.x.60]*
    [2.x.23]  Constructors and initialization  
* [0.x.61]*
   Constructor; initializes the matrix to be empty, without any structure,   i.e.  the matrix is not usable at all. This constructor is therefore only   useful for matrices which are members of a class. All other matrices   should be created at a point in the data flow where all necessary   information is available.     You have to initialize the matrix before usage with reinit(const   SparsityPattern&).  
* [0.x.62]*
   Copy constructor. This constructor is only allowed to be called if the   matrix to be copied is empty. This is for the same reason as for the   SparsityPattern, see there for the details.     If you really want to copy a whole matrix, you can do so by using the   copy_from() function.  
* [0.x.63]*
   Move constructor. Construct a new sparse matrix by transferring the   internal data of the matrix  [2.x.24]  into a new object.     Move construction allows an object to be returned from a function or   packed into a tuple even when the class cannot be copy-constructed.  
* [0.x.64]*
   Constructor. Takes the given matrix sparsity structure to represent the   sparsity pattern of this matrix. You can change the sparsity pattern   later on by calling the reinit(const SparsityPattern&) function.     You have to make sure that the lifetime of the sparsity structure is at   least as long as that of this matrix or as long as reinit(const   SparsityPattern&) is not called with a new sparsity pattern.     The constructor is marked explicit so as to disallow that someone passes   a sparsity pattern in place of a sparse matrix to some function, where an   empty matrix would be generated then.  
* [0.x.65]*
   Copy constructor: initialize the matrix with the identity matrix. This   constructor will throw an exception if the sizes of the sparsity pattern   and the identity matrix do not coincide, or if the sparsity pattern does   not provide for nonzero entries on the entire diagonal.  
* [0.x.66]*
   Destructor. Free all memory, but do not release the memory of the   sparsity structure.  
* [0.x.67]*
   Copy operator. Since copying entire sparse matrices is a very expensive   operation, we disallow doing so except for the special case of empty   matrices of size zero. This doesn't seem particularly useful, but is   exactly what one needs if one wanted to have a    [2.x.25] : in that case, one   can create a vector (which needs the ability to copy objects) of empty   matrices that are then later filled with something useful.  
* [0.x.68]*
   Move assignment operator. This operator replaces the present matrix with    [2.x.26]  by transferring the internal data of  [2.x.27]   
* [0.x.69]*
   Copy operator: initialize the matrix with the identity matrix. This   operator will throw an exception if the sizes of the sparsity pattern and   the identity matrix do not coincide, or if the sparsity pattern does not   provide for nonzero entries on the entire diagonal.  
* [0.x.70]*
   This operator assigns a scalar to a matrix. Since this does usually not   make much sense (should we set all matrix entries to this value?  Only   the nonzero entries of the sparsity pattern?), this operation is only   allowed if the actual value to be assigned is zero. This operator only   exists to allow for the obvious notation <tt>matrix=0</tt>, which sets   all elements of the matrix to zero, but keep the sparsity pattern   previously used.      [2.x.28]   
* [0.x.71]*
   Reinitialize the sparse matrix with the given sparsity pattern. The   latter tells the matrix how many nonzero elements there need to be   reserved.     Regarding memory allocation, the same applies as said above.     You have to make sure that the lifetime of the sparsity structure is at   least as long as that of this matrix or as long as reinit(const   SparsityPattern &) is not called with a new sparsity structure.     The elements of the matrix are set to zero by this function.  
* [0.x.72]*
   Release all memory and return to a state just like after having called   the default constructor. It also forgets the sparsity pattern it was   previously tied to.  
* [0.x.73]*
    [2.x.29]  Information on the matrix  
* [0.x.74]*
   Return whether the object is empty. It is empty if either both dimensions   are zero or no SparsityPattern is associated.  
* [0.x.75]*
   Return the dimension of the codomain (or range) space. Note that the   matrix is of dimension  [2.x.30] .  
* [0.x.76]*
   Return the dimension of the domain space. Note that the matrix is of   dimension  [2.x.31] .  
* [0.x.77]*
   Return the number of entries in a specific row.  
* [0.x.78]*
   Return the number of nonzero elements of this matrix. Actually, it   returns the number of entries in the sparsity pattern; if any of the   entries should happen to be zero, it is counted anyway.  
* [0.x.79]*
   Return the number of actually nonzero elements of this matrix. It is   possible to specify the parameter <tt>threshold</tt> in order to count   only the elements that have absolute value greater than the threshold.     Note, that this function does (in contrary to n_nonzero_elements()) not   count all entries of the sparsity pattern but only the ones that are   nonzero (or whose absolute value is greater than threshold).  
* [0.x.80]*
   Return a (constant) reference to the underlying sparsity pattern of this   matrix.     Though the return value is declared <tt>const</tt>, you should be aware   that it may change if you call any nonconstant function of objects which   operate on it.  
* [0.x.81]*
   Determine an estimate for the memory consumption (in bytes) of this   object. See MemoryConsumption.  
* [0.x.82]*
   Dummy function for compatibility with distributed, parallel matrices.  
* [0.x.83]*
    [2.x.32]  Modifying entries  
* [0.x.84]*
   Set the element ([1.x.1]) to <tt>value</tt>. Throws an error if the   entry does not exist or if <tt>value</tt> is not a finite number. Still,   it is allowed to store zero values in non-existent fields.  
* [0.x.85]*
   Set all elements given in a FullMatrix into the sparse matrix locations   given by <tt>indices</tt>. In other words, this function writes the   elements in <tt>full_matrix</tt> into the calling matrix, using the   local-to-global indexing specified by <tt>indices</tt> for both the rows   and the columns of the matrix. This function assumes a quadratic sparse   matrix and a quadratic full_matrix, the usual situation in FE   calculations.     The optional parameter <tt>elide_zero_values</tt> can be used to specify   whether zero values should be set anyway or they should be filtered away   (and not change the previous content in the respective element if it   exists). The default value is <tt>false</tt>, i.e., even zero values are   treated.  
* [0.x.86]*
   Same function as before, but now including the possibility to use   rectangular full_matrices and different local-to-global indexing on rows   and columns, respectively.  
* [0.x.87]*
   Set several elements in the specified row of the matrix with column   indices as given by <tt>col_indices</tt> to the respective value.     The optional parameter <tt>elide_zero_values</tt> can be used to specify   whether zero values should be set anyway or they should be filtered away   (and not change the previous content in the respective element if it   exists). The default value is <tt>false</tt>, i.e., even zero values are   treated.  
* [0.x.88]*
   Set several elements to values given by <tt>values</tt> in a given row in   columns given by col_indices into the sparse matrix.     The optional parameter <tt>elide_zero_values</tt> can be used to specify   whether zero values should be inserted anyway or they should be filtered   away. The default value is <tt>false</tt>, i.e., even zero values are   inserted/replaced.  
* [0.x.89]*
   Add <tt>value</tt> to the element ([1.x.2]).  Throws an error if the   entry does not exist or if <tt>value</tt> is not a finite number. Still,   it is allowed to store zero values in non-existent fields.  
* [0.x.90]*
   Add all elements given in a FullMatrix<double> into sparse matrix   locations given by <tt>indices</tt>. In other words, this function adds   the elements in <tt>full_matrix</tt> to the respective entries in calling   matrix, using the local-to-global indexing specified by <tt>indices</tt>   for both the rows and the columns of the matrix. This function assumes a   quadratic sparse matrix and a quadratic full_matrix, the usual situation   in FE calculations.     The optional parameter <tt>elide_zero_values</tt> can be used to specify   whether zero values should be added anyway or these should be filtered   away and only non-zero data is added. The default value is <tt>true</tt>,   i.e., zero values won't be added into the matrix.  
* [0.x.91]*
   Same function as before, but now including the possibility to use   rectangular full_matrices and different local-to-global indexing on rows   and columns, respectively.  
* [0.x.92]*
   Set several elements in the specified row of the matrix with column   indices as given by <tt>col_indices</tt> to the respective value.     The optional parameter <tt>elide_zero_values</tt> can be used to specify   whether zero values should be added anyway or these should be filtered   away and only non-zero data is added. The default value is <tt>true</tt>,   i.e., zero values won't be added into the matrix.  
* [0.x.93]*
   Add an array of values given by <tt>values</tt> in the given global   matrix row at columns specified by col_indices in the sparse matrix.     The optional parameter <tt>elide_zero_values</tt> can be used to specify   whether zero values should be added anyway or these should be filtered   away and only non-zero data is added. The default value is <tt>true</tt>,   i.e., zero values won't be added into the matrix.  
* [0.x.94]*
   Multiply the entire matrix by a fixed factor.  
* [0.x.95]*
   Divide the entire matrix by a fixed factor.  
* [0.x.96]*
   Symmetrize the matrix by forming the mean value between the existing   matrix and its transpose,  [2.x.33] .     This operation assumes that the underlying sparsity pattern represents a   symmetric object. If this is not the case, then the result of this   operation will not be a symmetric matrix, since it only explicitly   symmetrizes by looping over the lower left triangular part for efficiency   reasons; if there are entries in the upper right triangle, then these   elements are missed in the symmetrization. Symmetrization of the sparsity   pattern can be obtain by  [2.x.34]   
* [0.x.97]*
   Copy the matrix given as argument into the current object.     Copying matrices is an expensive operation that we do not want to happen   by accident through compiler generated code for  [2.x.35] .   (This would happen, for example, if one accidentally declared a function   argument of the current type [1.x.3] rather than [1.x.4].) The functionality of copying matrices is implemented in   this member function instead. All copy operations of objects of this type   therefore require an explicit function call.     The source matrix may be a matrix of arbitrary type, as long as its data   type is convertible to the data type of this matrix.     The function returns a reference to <tt>*this</tt>.  
* [0.x.98]*
   This function is complete analogous to the  [2.x.36]    function in that it allows to initialize a whole matrix in one step. See   there for more information on argument types and their meaning. You can   also find a small example on how to use this function there.     The only difference to the cited function is that the objects which the   inner iterator points to need to be of type  [2.x.37]  int,   value</tt>, where <tt>value</tt> needs to be convertible to the element   type of this class, as specified by the <tt>number</tt> template   argument.     Previous content of the matrix is overwritten. Note that the entries   specified by the input parameters need not necessarily cover all elements   of the matrix. Elements not covered remain untouched.  
* [0.x.99]*
   Copy the nonzero entries of a full matrix into this object. Previous   content is deleted.     Note that the underlying sparsity pattern must be appropriate to   hold the nonzero entries of the full matrix. This can be achieved   using that version of  [2.x.38]  that takes a   FullMatrix as argument.  
* [0.x.100]*
   Copy the given Trilinos matrix to this one. The operation triggers an   assertion if the sparsity patterns of the current object does not contain   the location of a non-zero entry of the given argument.     This function assumes that the two matrices have the same sizes.     The function returns a reference to <tt>*this</tt>.  
* [0.x.101]*
   Add <tt>matrix</tt> scaled by <tt>factor</tt> to this matrix, i.e. the   matrix <tt>factor*matrix</tt> is added to <tt>this</tt>. This function   throws an error if the sparsity patterns of the two involved matrices do   not point to the same object, since in this case the operation is   cheaper.     The source matrix may be a sparse matrix over an arbitrary underlying   scalar type, as long as its data type is convertible to the data type of   this matrix.  
* [0.x.102]*
    [2.x.39]  Entry Access  
* [0.x.103]*
   Return the value of the entry ([1.x.5]).  This may be an expensive   operation and you should always take care where to call this function. In   order to avoid abuse, this function throws an exception if the required   element does not exist in the matrix.     In case you want a function that returns zero instead (for entries that   are not in the sparsity pattern of the matrix), use the el() function.     If you are looping over all elements, consider using one of the iterator   classes instead, since they are tailored better to a sparse matrix   structure.  
* [0.x.104]*
   In contrast to the one above, this function allows modifying the object.  
* [0.x.105]*
   This function is mostly like operator()() in that it returns the value of   the matrix entry ([1.x.6]). The only difference is that if this entry   does not exist in the sparsity pattern, then instead of raising an   exception, zero is returned. While this may be convenient in some cases,   note that it is simple to write algorithms that are slow compared to an   optimal solution, since the sparsity of the matrix is not used.     If you are looping over all elements, consider using one of the iterator   classes instead, since they are tailored better to a sparse matrix   structure.  
* [0.x.106]*
   Return the main diagonal element in the [1.x.7]th row. This function   throws an error if the matrix is not quadratic.     This function is considerably faster than the operator()(), since for   quadratic matrices, the diagonal entry may be the first to be stored in   each row and access therefore does not involve searching for the right   column number.  
* [0.x.107]*
   Same as above, but return a writeable reference. You're sure you know   what you do?  
* [0.x.108]*
    [2.x.40]  Multiplications  
* [0.x.109]*
   Matrix-vector multiplication: let [1.x.8] with [1.x.9] being   this matrix.     Note that while this function can operate on all vectors that offer   iterator classes, it is only really effective for objects of type    [2.x.41] .   For all classes for which iterating over elements, or random member   access is expensive, this function is not efficient. In particular, if   you want to multiply with BlockVector objects, you should consider using   a BlockSparseMatrix as well.     Source and destination must not be the same vector.      [2.x.42]   
* [0.x.110]*
   Matrix-vector multiplication: let [1.x.10] with   [1.x.11] being this matrix. This function does the same as vmult() but   takes the transposed matrix.     Note that while this function can operate on all vectors that offer   iterator classes, it is only really effective for objects of type    [2.x.43] .   For all classes for which iterating over elements, or random member   access is expensive, this function is not efficient. In particular, if   you want to multiply with BlockVector objects, you should consider using   a BlockSparseMatrix as well.     Source and destination must not be the same vector.  
* [0.x.111]*
   Adding Matrix-vector multiplication. Add [1.x.12] on [1.x.13] with   [1.x.14] being this matrix.     Note that while this function can operate on all vectors that offer   iterator classes, it is only really effective for objects of type    [2.x.44] .   For all classes for which iterating over elements, or random member   access is expensive, this function is not efficient. In particular, if   you want to multiply with BlockVector objects, you should consider using   a BlockSparseMatrix as well.     Source and destination must not be the same vector.      [2.x.45]   
* [0.x.112]*
   Adding Matrix-vector multiplication. Add [1.x.15] to   [1.x.16] with [1.x.17] being this matrix. This function does the same   as vmult_add() but takes the transposed matrix.     Note that while this function can operate on all vectors that offer   iterator classes, it is only really effective for objects of type    [2.x.46] .   For all classes for which iterating over elements, or random member   access is expensive, this function is not efficient. In particular, if   you want to multiply with BlockVector objects, you should consider using   a BlockSparseMatrix as well.     Source and destination must not be the same vector.  
* [0.x.113]*
   Return the square of the norm of the vector  [2.x.47]  with respect to the norm   induced by this matrix, i.e.  [2.x.48] . This is useful, e.g. in   the finite element context, where the  [2.x.49]  norm of a function equals the   matrix norm with respect to the mass matrix of the vector representing   the nodal values of the finite element function.     Obviously, the matrix needs to be quadratic for this operation, and for   the result to actually be a norm it also needs to be either real   symmetric or complex hermitian.     The underlying template types of both this matrix and the given vector   should either both be real or complex-valued, but not mixed, for this   function to make sense.      [2.x.50]   
* [0.x.114]*
   Compute the matrix scalar product  [2.x.51] .      [2.x.52]   
* [0.x.115]*
   Compute the residual of an equation [1.x.18], where the residual is   defined to be [1.x.19]. Write the residual into <tt>dst</tt>. The   [1.x.20] norm of the residual vector is returned.     Source [1.x.21] and destination [1.x.22] must not be the same vector.      [2.x.53]   
* [0.x.116]*
   Perform the matrix-matrix multiplication <tt>C = A B</tt>, or, if an   optional vector argument is given, <tt>C = A diag(V) B</tt>, where   <tt>diag(V)</tt> defines a diagonal matrix with the vector entries.     This function assumes that the calling matrix  [2.x.54]  and the argument  [2.x.55]    have compatible sizes. By default, the output matrix  [2.x.56]  will be   resized appropriately.     By default, i.e., if the optional argument  [2.x.57]    is  [2.x.58]  the sparsity pattern of the matrix C will be   changed to ensure that all entries that result from the product  [2.x.59]    can be stored in  [2.x.60] . This is an expensive operation, and if there is   a way to predict the sparsity pattern up front, you should probably   build it yourself before calling this function with  [2.x.61]  as last   argument. In this case, the rebuilding of the sparsity pattern is   bypassed.     When setting  [2.x.62]  to  [2.x.63]  (i.e., leaving it   at the default value), it is important to realize that the matrix    [2.x.64]  passed as first argument still has to be initialized with a   sparsity pattern (either at the time of creation of the SparseMatrix   object, or via the  [2.x.65]  function). This is because   we could create a sparsity pattern inside the current function, and   then associate  [2.x.66]  with it, but there would be no way to transfer   ownership of this sparsity pattern to anyone once the current function   finishes. Consequently, the function requires that  [2.x.67]  be already   associated with a sparsity pattern object, and this object is then   reset to fit the product of  [2.x.68]  and  [2.x.69]      As a consequence of this, however, it is also important to realize   that the sparsity pattern of  [2.x.70]  is modified and that this would   render invalid [1.x.23] that happen   to [1.x.24] use that sparsity pattern object.  
* [0.x.117]*
   Perform the matrix-matrix multiplication with the transpose of   <tt>this</tt>, i.e., <tt>C = A<sup>T</sup> B</tt>, or, if an optional   vector argument is given, <tt>C = A<sup>T</sup> diag(V) B</tt>, where   <tt>diag(V)</tt> defines a diagonal matrix with the vector entries.     This function assumes that the calling matrix <tt>A</tt> and <tt>B</tt>   have compatible sizes. The size of <tt>C</tt> will be set within this   function.     The content as well as the sparsity pattern of the matrix C will be   changed by this function, so make sure that the sparsity pattern is not   used somewhere else in your program. This is an expensive operation, so   think twice before you use this function.     There is an optional flag <tt>rebuild_sparsity_pattern</tt> that can be   used to bypass the creation of a new sparsity pattern and instead uses   the sparsity pattern stored in <tt>C</tt>. In that case, make sure that   it really fits. The default is to rebuild the sparsity pattern.    
*  [2.x.71]  Rebuilding the sparsity pattern requires changing it. This means   that all other matrices that are associated with this sparsity pattern   will then have invalid entries.  
* [0.x.118]*
    [2.x.72]  Matrix norms  
* [0.x.119]*
   Return the  [2.x.73] -norm of the matrix, that is  [2.x.74] , (max. sum of   columns).  This is the natural matrix norm that is compatible to the    [2.x.75] -norm for vectors, i.e.   [2.x.76] . (cf. Haemmerlin-   Hoffmann: Numerische Mathematik)  
* [0.x.120]*
   Return the  [2.x.77] -norm of the matrix, that is    [2.x.78] , (max. sum of rows).  This is the natural matrix norm that is   compatible to the  [2.x.79] -norm of vectors, i.e.   [2.x.80] .  (cf. Haemmerlin-Hoffmann: Numerische Mathematik)  
* [0.x.121]*
   Return the frobenius norm of the matrix, i.e. the square root of the sum   of squares of all entries in the matrix.  
* [0.x.122]*
    [2.x.81]  Preconditioning methods  
* [0.x.123]*
   Apply the Jacobi preconditioner, which multiplies every element of the   <tt>src</tt> vector by the inverse of the respective diagonal element and   multiplies the result with the relaxation factor <tt>omega</tt>.  
* [0.x.124]*
   Apply SSOR preconditioning to <tt>src</tt> with damping <tt>omega</tt>.   The optional argument <tt>pos_right_of_diagonal</tt> is supposed to   provide an array where each entry specifies the position just right of   the diagonal in the global array of nonzeros.  
* [0.x.125]*
   Apply SOR preconditioning matrix to <tt>src</tt>.  
* [0.x.126]*
   Apply transpose SOR preconditioning matrix to <tt>src</tt>.  
* [0.x.127]*
   Perform SSOR preconditioning in-place.  Apply the preconditioner matrix   without copying to a second vector.  <tt>omega</tt> is the relaxation   parameter.  
* [0.x.128]*
   Perform an SOR preconditioning in-place.  <tt>omega</tt> is the   relaxation parameter.  
* [0.x.129]*
   Perform a transpose SOR preconditioning in-place.  <tt>omega</tt> is the   relaxation parameter.  
* [0.x.130]*
   Perform a permuted SOR preconditioning in-place.     The standard SOR method is applied in the order prescribed by   <tt>permutation</tt>, that is, first the row <tt>permutation[0]</tt>,   then <tt>permutation[1]</tt> and so on. For efficiency reasons, the   permutation as well as its inverse are required.     <tt>omega</tt> is the relaxation parameter.  
* [0.x.131]*
   Perform a transposed permuted SOR preconditioning in-place.     The transposed SOR method is applied in the order prescribed by   <tt>permutation</tt>, that is, first the row <tt>permutation[m()-1]</tt>,   then <tt>permutation[m()-2]</tt> and so on. For efficiency reasons, the   permutation as well as its inverse are required.     <tt>omega</tt> is the relaxation parameter.  
* [0.x.132]*
   Do one Jacobi step on <tt>v</tt>.  Performs a direct Jacobi step with   right hand side <tt>b</tt>. This function will need an auxiliary vector,   which is acquired from GrowingVectorMemory.  
* [0.x.133]*
   Do one SOR step on <tt>v</tt>.  Performs a direct SOR step with right   hand side <tt>b</tt>.  
* [0.x.134]*
   Do one adjoint SOR step on <tt>v</tt>.  Performs a direct TSOR step with   right hand side <tt>b</tt>.  
* [0.x.135]*
   Do one SSOR step on <tt>v</tt>.  Performs a direct SSOR step with right   hand side <tt>b</tt> by performing TSOR after SOR.  
* [0.x.136]*
    [2.x.82]  Iterators  
* [0.x.137]*
   Return an iterator pointing to the first element of the matrix.     Note the discussion in the general documentation of this class about the   order in which elements are accessed.  
* [0.x.138]*
   Like the function above, but for non-const matrices.  
* [0.x.139]*
   Return an iterator pointing the element past the last one of this matrix.  
* [0.x.140]*
   Like the function above, but for non-const matrices.  
* [0.x.141]*
   Return an iterator pointing to the first element of row  [2.x.83]      Note that if the given row is empty, i.e. does not contain any nonzero   entries, then the iterator returned by this function equals   <tt>end(r)</tt>. The returned iterator may not be dereferenceable in that   case if neither row  [2.x.84]  nor any of the following rows contain any   nonzero entries.  
* [0.x.142]*
   Like the function above, but for non-const matrices.  
* [0.x.143]*
   Return an iterator pointing the element past the last one of row  [2.x.85]  ,   or past the end of the entire sparsity pattern if none of the rows after    [2.x.86]  contain any entries at all.     Note that the end iterator is not necessarily dereferenceable. This is in   particular the case if it is the end iterator for the last row of a   matrix.  
* [0.x.144]*
   Like the function above, but for non-const matrices.  
* [0.x.145]*
    [2.x.87]  Input/Output  
* [0.x.146]*
   Print the matrix to the given stream, using the format <tt>(row,column)   value</tt>, i.e. one nonzero entry of the matrix per line. If   <tt>across</tt> is true, print all entries on a single line, using the   format row,column:value.     If the argument <tt>diagonal_first</tt> is true, diagonal elements of   quadratic matrices are printed first in their row, corresponding to the   internal storage scheme. If it is false, the elements in a row are   written in ascending column order.  
* [0.x.147]*
   Print the matrix in the usual format, i.e. as a matrix and not as a list   of nonzero elements. For better readability, elements not in the matrix   are displayed as empty space, while matrix elements which are explicitly   set to zero are displayed as such.     The parameters allow for a flexible setting of the output format:   <tt>precision</tt> and <tt>scientific</tt> are used to determine the   number format, where <tt>scientific = false</tt> means fixed point   notation.  A zero entry for <tt>width</tt> makes the function compute a   width, but it may be changed to a positive value, if output is crude.     Additionally, a character for an empty value may be specified.     Finally, the whole matrix can be multiplied with a common denominator to   produce more readable output, even integers.      [2.x.88]  This function may produce [1.x.25] amounts of output if   applied to a large matrix!  
* [0.x.148]*
   Print the actual pattern of the matrix. For each entry with an absolute   value larger than threshold, a '*' is printed, a ':' for every value   smaller and a '.' for every entry not allocated.  
* [0.x.149]*
   Print the matrix to the output stream  [2.x.89]  in a format that can be   read by  [2.x.90]  To load the matrix in python just do   <code>    [data, row, column] = numpy.loadtxt('my_matrix.txt')    sparse_matrix = scipy.sparse.csr_matrix((data, (row, column)))   </code>  
* [0.x.150]*
   Write the data of this object en bloc to a file. This is done in a binary   mode, so the output is neither readable by humans nor (probably) by other   computers using a different operating system of number format.     The purpose of this function is that you can swap out matrices and   sparsity pattern if you are short of memory, want to communicate between   different programs, or allow objects to be persistent across different   runs of the program.  
* [0.x.151]*
   Read data that has previously been written by block_write() from a file.   This is done using the inverse operations to the above function, so it is   reasonably fast because the bitstream is not interpreted except for a few   numbers up front.     The object is resized on this operation, and all previous contents are   lost. Note, however, that no checks are performed whether new data and   the underlying SparsityPattern object fit together. It is your   responsibility to make sure that the sparsity pattern and the data to be   read match.     A primitive form of error checking is performed which will recognize the   bluntest attempts to interpret some data as a matrix stored bitwise to a   file that wasn't actually created that way, but not more.  
* [0.x.152]*
    [2.x.91]  Exceptions    [2.x.92]   
* [0.x.153]*
   Exception  
* [0.x.154]*
   Exception  
* [0.x.155]*
   Exception  
* [0.x.156]*
   Exception  
* [0.x.157]*
   For some matrix storage formats, in particular for the PETSc distributed   blockmatrices, set and add operations on individual elements can not be   freely mixed. Rather, one has to synchronize operations when one wants to   switch from setting elements to adding to elements.  BlockMatrixBase   automatically synchronizes the access by calling this helper function for   each block.  This function ensures that the matrix is in a state that   allows adding elements; if it previously already was in this state, the   function does nothing.  
* [0.x.158]*
   Same as prepare_add() but prepare the matrix for setting elements if the   representation of elements in this class requires such an operation.  
* [0.x.159]*
   Pointer to the sparsity pattern used for this matrix. In order to   guarantee that it is not deleted while still in use, we subscribe to it   using the SmartPointer class.  
* [0.x.160]*
   Array of values for all the nonzero entries. The position of an   entry within the matrix, i.e., the row and column number for a   given value in this array can only be deduced using the sparsity   pattern. The same holds for the more common operation of finding   an entry by its coordinates.  
* [0.x.161]*
   Allocated size of #val. This can be larger than the actually used part if   the size of the matrix was reduced sometime in the past by associating a   sparsity pattern with a smaller size to this object, using the reinit()   function.  
* [0.x.162]

include/deal.II-translator/lac/sparse_matrix_ez_0.txt
[0.x.0]*
  [2.x.0]  Matrix1  [2.x.1] 

* 
* [0.x.1]*
 Sparse matrix without sparsity pattern.
*  Instead of using a pre-assembled sparsity pattern, this matrix builds the pattern on the fly. Filling the matrix may consume more time than for  [2.x.2]  since large memory movements may be involved when new matrix elements are inserted somewhere in the middle of the matrix and no currently unused memory locations are available for the row into which the new entry is to be inserted. To help optimize things, an expected row-length may be provided to the constructor, as well as an increment size for rows.
*  This class uses a storage structure that, similar to the usual sparse matrix format, only stores non-zero elements. These are stored in a single data array for the entire matrix, and are ordered by row and, within each row, by column number. A separate array describes where in the long data array each row starts and how long it is.
*  Due to this structure, gaps may occur between rows. Whenever a new entry must be created, an attempt is made to use the gap in its row. If no gap is left, the row must be extended and all subsequent rows must be shifted backwards. This is a very expensive operation and explains the inefficiency of this data structure and why it is useful to pre-allocate a sparsity pattern as the SparseMatrix class does.
*  This is where the optimization parameters, provided to the constructor or to the reinit() functions come in.  [2.x.3]  is the number of entries that will be allocated for each row on initialization (the actual length of the rows is still zero). This means, that  [2.x.4]  entries can be added to this row without shifting other rows. If fewer entries are added, the additional memory will of course be wasted.
*  If the space for a row is not sufficient, then it is enlarged by  [2.x.5]  entries. This way, subsequent rows are not shifted by single entries very often.
*  Finally, the  [2.x.6]  allocates extra space at the end of the data array. This space is used whenever any row must be enlarged. It is important because otherwise not only the following rows must be moved, but in fact [1.x.0] rows after allocating sufficiently much space for the entire data array.
*  Suggested settings:  [2.x.7]  should be the length of a typical row, for instance the size of the stencil in regular parts of the grid. Then,  [2.x.8]  may be the expected amount of entries added to the row by having one hanging node. This way, a good compromise between memory consumption and speed should be achieved.  [2.x.9]  should then be an estimate for the number of hanging nodes times  [2.x.10]  default_increment.
*  Letting  [2.x.11]  be zero causes an exception whenever a row overflows.
*  If the rows are expected to be filled more or less from first to last, using a  [2.x.12]  of zero may not be such a bad idea.
* 

* 
*  [2.x.13]  The name of the class makes sense by pronouncing it the American way,   where "EZ" is pronounced the same way as the word "easy".

* 
* [0.x.2]*
   Declare type for container size.  
* [0.x.3]*
   The class for storing the column number of an entry together with its   value.  
* [0.x.4]*
     Standard constructor. Sets  [2.x.14]  to  [2.x.15]     
* [0.x.5]*
     Constructor. Fills column and value.    
* [0.x.6]*
     The column number.    
* [0.x.7]*
     The value there.    
* [0.x.8]*
     Non-existent column number.    
* [0.x.9]*
   Structure for storing information on a matrix row. One object for each   row will be stored in the matrix.  
* [0.x.10]*
     Constructor.    
* [0.x.11]*
     Index of first entry of the row in the data field.    
* [0.x.12]*
     Number of entries in this row.    
* [0.x.13]*
     Position of the diagonal element relative tor the start index.    
* [0.x.14]*
     Value for non-existing diagonal.    
* [0.x.15]*
   Standard-conforming iterator.  
* [0.x.16]*
     Accessor class for iterators    
* [0.x.17]*
       Constructor. Since we use accessors only for read access, a const       matrix pointer is sufficient.      
* [0.x.18]*
       Row number of the element represented by this object.      
* [0.x.19]*
       Index in row of the element represented by this object.      
* [0.x.20]*
       Column number of the element represented by this object.      
* [0.x.21]*
       Value of this matrix entry.      
* [0.x.22]*
       The matrix accessed.      
* [0.x.23]*
       Current row number.      
* [0.x.24]*
       Current index in row.      
* [0.x.25]*
     Constructor.    
* [0.x.26]*
     Prefix increment. This always returns a valid entry or <tt>end()</tt>.    
* [0.x.27]*
     Dereferencing operator.    
* [0.x.28]*
     Dereferencing operator.    
* [0.x.29]*
     Comparison. True, if both iterators point to the same matrix position.    
* [0.x.30]*
     Inverse of <tt>==</tt>.    
* [0.x.31]*
     Comparison operator. Result is true if either the first row number is     smaller or if the row numbers are equal and the first index is smaller.    
* [0.x.32]*
     Store an object of the accessor class.    
* [0.x.33]*
   Type of matrix entries. This alias is analogous to <tt>value_type</tt>   in the standard library containers.  
* [0.x.34]*
    [2.x.16]  Constructors and initialization  
* [0.x.35]*
   Constructor. Initializes an empty matrix of dimension zero times zero.  
* [0.x.36]*
   Dummy copy constructor. This is here for use in containers. It may only   be called for empty objects.     If you really want to copy a whole matrix, you can do so by using the  [2.x.17]    copy_from function.  
* [0.x.37]*
   Constructor. Generates a matrix of the given size, ready to be filled.   The optional parameters  [2.x.18]  and  [2.x.19]    allow for preallocating memory. Providing these properly is essential for   an efficient assembling of the matrix.  
* [0.x.38]*
   Destructor. Free all memory.  
* [0.x.39]*
   Pseudo operator only copying empty objects.  
* [0.x.40]*
   This operator assigns a scalar to a matrix. Since this does usually not   make much sense (should we set all matrix entries to this value? Only the   nonzero entries of the sparsity pattern?), this operation is only allowed   if the actual value to be assigned is zero. This operator only exists to   allow for the obvious notation <tt>matrix=0</tt>, which sets all elements   of the matrix to zero, but keep the sparsity pattern previously used.  
* [0.x.41]*
   Reinitialize the sparse matrix to the dimensions provided. The matrix   will have no entries at this point. The optional parameters  [2.x.20]    default_row_length,  [2.x.21]  and  [2.x.22]  allow for   preallocating memory. Providing these properly is essential for an   efficient assembling of the matrix.  
* [0.x.42]*
   Release all memory and return to a state just like after having called   the default constructor. It also forgets its sparsity pattern.  
* [0.x.43]*
    [2.x.23]  Information on the matrix  
* [0.x.44]*
   Return whether the object is empty. It is empty if both dimensions are   zero.  
* [0.x.45]*
   Return the dimension of the codomain (or range) space. Note that the   matrix is of dimension  [2.x.24] .  
* [0.x.46]*
   Return the dimension of the domain space. Note that the matrix is of   dimension  [2.x.25] .  
* [0.x.47]*
   Return the number of entries in a specific row.  
* [0.x.48]*
   Return the number of nonzero elements of this matrix.  
* [0.x.49]*
   Determine an estimate for the memory consumption (in bytes) of this   object.  
* [0.x.50]*
   Print statistics. If  [2.x.26]  is  [2.x.27]  prints a histogram of all   existing row lengths and allocated row lengths. Otherwise, just the   relation of allocated and used entries is shown.  
* [0.x.51]*
   Compute numbers of entries.     In the first three arguments, this function returns the number of entries   used, allocated and reserved by this matrix.     If the final argument is true, the number of entries in each line is   printed as well.  
* [0.x.52]*
    [2.x.28]  Modifying entries  
* [0.x.53]*
   Set the element <tt>(i,j)</tt> to  [2.x.29]      If <tt>value</tt> is not a finite number an exception is thrown.     The optional parameter <tt>elide_zero_values</tt> can be used to specify   whether zero values should be added anyway or these should be filtered   away and only non-zero data is added. The default value is <tt>true</tt>,   i.e., zero values won't be added into the matrix.     If this function sets the value of an element that does not yet exist,   then it allocates an entry for it. (Unless `elide_zero_values` is `true`   as mentioned above.)    
*  [2.x.30]  You may need to insert zero elements if you want to keep a symmetric   sparsity pattern for the matrix.  
* [0.x.54]*
   Add  [2.x.31]  to the element <tt>(i,j)</tt>.     If this function adds to the value of an element that does not yet exist,   then it allocates an entry for it.     The function filters out zeroes automatically, i.e., it does not create   new entries when adding zero to a matrix element for which no entry   currently exists.  
* [0.x.55]*
   Add all elements given in a FullMatrix<double> into sparse matrix   locations given by <tt>indices</tt>. In other words, this function adds   the elements in <tt>full_matrix</tt> to the respective entries in calling   matrix, using the local-to-global indexing specified by <tt>indices</tt>   for both the rows and the columns of the matrix. This function assumes a   quadratic sparse matrix and a quadratic full_matrix, the usual situation   in FE calculations.     The optional parameter <tt>elide_zero_values</tt> can be used to specify   whether zero values should be added anyway or these should be filtered   away and only non-zero data is added. The default value is <tt>true</tt>,   i.e., zero values won't be added into the matrix.  
* [0.x.56]*
   Same function as before, but now including the possibility to use   rectangular full_matrices and different local-to-global indexing on rows   and columns, respectively.  
* [0.x.57]*
   Set several elements in the specified row of the matrix with column   indices as given by <tt>col_indices</tt> to the respective value.     The optional parameter <tt>elide_zero_values</tt> can be used to specify   whether zero values should be added anyway or these should be filtered   away and only non-zero data is added. The default value is <tt>true</tt>,   i.e., zero values won't be added into the matrix.  
* [0.x.58]*
   Add an array of values given by <tt>values</tt> in the given global   matrix row at columns specified by col_indices in the sparse matrix.     The optional parameter <tt>elide_zero_values</tt> can be used to specify   whether zero values should be added anyway or these should be filtered   away and only non-zero data is added. The default value is <tt>true</tt>,   i.e., zero values won't be added into the matrix.  
* [0.x.59]*
   Copy the matrix given as argument into the current object.     Copying matrices is an expensive operation that we do not want to happen   by accident through compiler generated code for  [2.x.32] .   (This would happen, for example, if one accidentally declared a function   argument of the current type [1.x.1] rather than [1.x.2].) The functionality of copying matrices is implemented in   this member function instead. All copy operations of objects of this type   therefore require an explicit function call.     The source matrix may be a matrix of arbitrary type, as long as its data   type is convertible to the data type of this matrix.     The optional parameter <tt>elide_zero_values</tt> can be used to specify   whether zero values should be added anyway or these should be filtered   away and only non-zero data is added. The default value is <tt>true</tt>,   i.e., zero values won't be added into the matrix.     The function returns a reference to  [2.x.33]   
* [0.x.60]*
   Add  [2.x.34]  scaled by  [2.x.35]  to this matrix.     The source matrix may be a matrix of arbitrary type, as long as its data   type is convertible to the data type of this matrix and it has the   standard  [2.x.36]   
* [0.x.61]*
    [2.x.37]  Entry Access  
* [0.x.62]*
   Return the value of the entry (i,j).  This may be an expensive operation   and you should always take care where to call this function.  In order to   avoid abuse, this function throws an exception if the required element   does not exist in the matrix.     In case you want a function that returns zero instead (for entries that   are not in the sparsity pattern of the matrix), use the  [2.x.38]  function.  
* [0.x.63]*
   Return the value of the entry (i,j). Returns zero for all non-existing   entries.  
* [0.x.64]*
    [2.x.39]  Multiplications  
* [0.x.65]*
   Matrix-vector multiplication: let  [2.x.40]  with  [2.x.41]  being this   matrix.  
* [0.x.66]*
   Matrix-vector multiplication: let  [2.x.42]  with  [2.x.43]  being this   matrix. This function does the same as  [2.x.44]  but takes the transposed   matrix.  
* [0.x.67]*
   Adding Matrix-vector multiplication. Add  [2.x.45]  on  [2.x.46]  with  [2.x.47]  being   this matrix.  
* [0.x.68]*
   Adding Matrix-vector multiplication. Add  [2.x.48]  to  [2.x.49]  with  [2.x.50]    being this matrix. This function does the same as  [2.x.51]  but takes   the transposed matrix.  
* [0.x.69]*
    [2.x.52]  Matrix norms  
* [0.x.70]*
   Frobenius-norm of the matrix.  
* [0.x.71]*
    [2.x.53]  Preconditioning methods  
* [0.x.72]*
   Apply the Jacobi preconditioner, which multiplies every element of the  [2.x.54]    src vector by the inverse of the respective diagonal element and   multiplies the result with the damping factor  [2.x.55]   
* [0.x.73]*
   Apply SSOR preconditioning to  [2.x.56]   
* [0.x.74]*
   Apply SOR preconditioning matrix to  [2.x.57]  The result of this method is    [2.x.58] .  
* [0.x.75]*
   Apply transpose SOR preconditioning matrix to  [2.x.59]  The result of this   method is  [2.x.60] .  
* [0.x.76]*
   Add the matrix  [2.x.61]  conjugated by  [2.x.62]  that is,  [2.x.63]  to this   object. If the parameter  [2.x.64]  is true, compute  [2.x.65] .     This function requires that  [2.x.66]  has a  [2.x.67]  traversing all   matrix entries and that  [2.x.68]  has a function <tt>el(i,j)</tt> for access   to a specific entry.  
* [0.x.77]*
    [2.x.69]  Iterators  
* [0.x.78]*
   Iterator starting at the first existing entry.  
* [0.x.79]*
   Final iterator.  
* [0.x.80]*
   Iterator starting at the first entry of row  [2.x.70]  If this row is empty,   the result is <tt>end(r)</tt>, which does NOT point into row  [2.x.71]   
* [0.x.81]*
   Final iterator of row  [2.x.72]  The result may be different from   <tt>end()</tt>!  
* [0.x.82]*
    [2.x.73]  Input/Output  
* [0.x.83]*
   Print the matrix to the given stream, using the format <tt>(line,col)   value</tt>, i.e. one nonzero entry of the matrix per line.  
* [0.x.84]*
   Print the matrix in the usual format, i.e. as a matrix and not as a list   of nonzero elements. For better readability, elements not in the matrix   are displayed as empty space, while matrix elements which are explicitly   set to zero are displayed as such.     The parameters allow for a flexible setting of the output format:  [2.x.74]    precision and  [2.x.75]  are used to determine the number format,   where  [2.x.76]  =  [2.x.77]  means fixed point notation.  A zero entry   for  [2.x.78]  makes the function compute a width, but it may be changed to   a positive value, if output is crude.     Additionally, a character for an empty value may be specified.     Finally, the whole matrix can be multiplied with a common denominator to   produce more readable output, even integers.     This function may produce  [2.x.79]  large amounts of output if applied to a   large matrix!  
* [0.x.85]*
   Write the data of this object in binary mode to a file.     Note that this binary format is platform dependent.  
* [0.x.86]*
   Read data that has previously been written by  [2.x.80]      The object is resized on this operation, and all previous contents are   lost.     A primitive form of error checking is performed which will recognize the   bluntest attempts to interpret some data as a vector stored bitwise to a   file, but not more.  
* [0.x.87]*
    [2.x.81]  Exceptions    [2.x.82]   
* [0.x.88]*
   Exception for missing diagonal entry.  
* [0.x.89]*
   Exception  
* [0.x.90]*
   Find an entry and return a const pointer. Return a zero-pointer if the   entry does not exist.  
* [0.x.91]*
   Find an entry and return a writable pointer. Return a zero-pointer if the   entry does not exist.  
* [0.x.92]*
   Find an entry or generate it.  
* [0.x.93]*
   Version of  [2.x.83]  which only performs its actions on the region defined   by <tt>[begin_row,end_row)</tt>. This function is called by  [2.x.84]  in   the case of enabled multithreading.  
* [0.x.94]*
   Version of  [2.x.85]  which only performs its actions on the   region defined by <tt>[begin_row,end_row)</tt>. This function is called   by  [2.x.86]  in the case of enabled multithreading.  
* [0.x.95]*
   Version of  [2.x.87]  which only performs its actions on   the region defined by <tt>[begin_row,end_row)</tt>. This function is   called by  [2.x.88]  in the case of enabled multithreading.  
* [0.x.96]*
   Number of columns. This is used to check vector dimensions only.  
* [0.x.97]*
   Info structure for each row.  
* [0.x.98]*
   Data storage.  
* [0.x.99]*
   Increment when a row grows.  
* [0.x.100]*
   Remember the user provided default row length.  
* [0.x.101]*
  [2.x.89] 

* 
* [0.x.102]

include/deal.II-translator/lac/sparse_matrix_ez.templates_0.txt
[0.x.0]

include/deal.II-translator/lac/sparse_matrix.templates_0.txt
[0.x.0]*
     Perform a vmult using the SparseMatrix data structures, but only using     a subinterval for the row indices.         In the sequential case, this function is called on all rows, in the     parallel case it may be called on a subrange, at the discretion of the     task scheduler.    
* [0.x.1]*
     Perform a vmult using the SparseMatrix data structures, but only using     a subinterval for the row indices.         In the sequential case, this function is called on all rows, in the     parallel case it may be called on a subrange, at the discretion of the     task scheduler.    
* [0.x.2]*
     Perform a vmult using the SparseMatrix data structures, but only using     a subinterval for the row indices.         In the sequential case, this function is called on all rows, in the     parallel case it may be called on a subrange, at the discretion of the     task scheduler.    
* [0.x.3]*
     Perform a vmult using the SparseMatrix data structures, but only using     a subinterval for the row indices.         In the sequential case, this function is called on all rows, in the     parallel case it may be called on a subrange, at the discretion of the     task scheduler.    
* [0.x.4]

include/deal.II-translator/lac/sparse_mic_0.txt
[0.x.0]!  [2.x.0]  Preconditioners [2.x.1] 

* 
* [0.x.1]*
 Implementation of the Modified Incomplete Cholesky (MIC(0)) preconditioner for symmetric matrices. This class conforms to the state and usage specification in SparseLUDecomposition.
* 

*  [1.x.0]
*  Let a symmetric, positive-definite, sparse matrix  [2.x.2]  be in the form  [2.x.3] , where  [2.x.4]  is the diagonal part of  [2.x.5]  and  [2.x.6]  is a strictly lower triangular matrix. The MIC(0) decomposition of the matrix  [2.x.7]  is defined by  [2.x.8] , where  [2.x.9]  is a diagonal matrix defined by the condition  [2.x.10] .

* 
* [0.x.2]*
   Declare type for container size.  
* [0.x.3]*
   Constructor. Does nothing, so you have to call  [2.x.11]  sometimes   afterwards.  
* [0.x.4]*
   Destructor.  
* [0.x.5]*
   Deletes all member variables. Leaves the class in the state that it had   directly after calling the constructor  
* [0.x.6]*
   Make the  [2.x.12]  type in the base class accessible to this   class as well.  
* [0.x.7]*
   Perform the incomplete LU factorization of the given matrix.     This function needs to be called before an object of this class is used   as preconditioner.     For more details about possible parameters, see the class documentation   of SparseLUDecomposition and the documentation of the  [2.x.13]     [2.x.14]  class.     According to the  [2.x.15]  this function creates a new   SparsityPattern or keeps the previous sparsity or takes the sparsity   given by the user to  [2.x.16]  Then, this function performs the MIC   decomposition.     After this function is called the preconditioner is ready to be used.  
* [0.x.8]*
   Apply the incomplete decomposition, i.e. do one forward-backward step    [2.x.17] .     Call  [2.x.18]  before calling this function.  
* [0.x.9]*
   Apply the transpose of the incomplete decomposition, i.e. do one forward-   backward step  [2.x.19] .     Call  [2.x.20]  before calling this function.    
*  [2.x.21]  This function has not yet been implemented  
* [0.x.10]*
   Determine an estimate for the memory consumption (in bytes) of this   object.  
* [0.x.11]*
    [2.x.22]  Exceptions    [2.x.23]   
* [0.x.12]*
   Exception  
* [0.x.13]*
   Exception  
* [0.x.14]*
   Exception  
* [0.x.15]*
   Values of the computed diagonal.  
* [0.x.16]*
   Inverses of the diagonal: precomputed for faster vmult.  
* [0.x.17]*
   Values of the computed "inner sums", i.e. per-row sums of the elements   laying on the right side of the diagonal.  
* [0.x.18]*
   Compute the row-th "inner sum".  
* [0.x.19]

include/deal.II-translator/lac/sparse_mic.templates_0.txt
[0.x.0]

include/deal.II-translator/lac/sparse_vanka_0.txt
[0.x.0]!  [2.x.0]  Preconditioners [2.x.1] 

* 
* [0.x.1]*
 Point-wise Vanka preconditioning. This class does Vanka preconditioning  on a point-wise base. Vanka preconditioners are used for saddle point problems like Stokes' problem or problems arising in optimization where Lagrange multipliers occur and the Newton method matrix has a zero block. With these matrices the application of Jacobi or Gauss-Seidel methods is impossible, because some diagonal elements are zero in the rows of the Lagrange multiplier. The approach of Vanka is to solve a small (usually indefinite) system of equations for each Langrange multiplier variable (we will also call the pressure in Stokes' equation a Langrange multiplier since it can be interpreted as such).
*  Objects of this class are constructed by passing a vector of indices of the degrees of freedom of the Lagrange multiplier. In the actual preconditioning method, these rows are traversed in the order in which the appear in the matrix. Since this is a Gauß-Seidel like procedure, remember to have a good ordering in advance (for transport dominated problems, Cuthill-McKee algorithms are a good means for this, if points on the inflow boundary are chosen as starting points for the renumbering).
*  For each selected degree of freedom, a local system of equations is built by the degree of freedom itself and all other values coupling immediately, i.e. the set of degrees of freedom considered for the local system of equations for degree of freedom  [2.x.2]  is  [2.x.3]  itself and all  [2.x.4]  such that the element <tt>(i,j)</tt> is a nonzero entry in the sparse matrix under consideration. The elements <tt>(j,i)</tt> are not considered. We now pick all matrix entries from rows and columns out of the set of degrees of freedom just described out of the global matrix and put it into a local matrix, which is subsequently inverted. This system may be of different size for each degree of freedom, depending for example on the local neighborhood of the respective node on a computational grid.
*  The right hand side is built up in the same way, i.e. by copying all entries that coupled with the one under present consideration, but it is augmented by all degrees of freedom coupling with the degrees from the set described above (i.e. the DoFs coupling second order to the present one). The reason for this is, that the local problems to be solved shall have Dirichlet boundary conditions on the second order coupling DoFs, so we have to take them into account but eliminate them before actually solving; this elimination is done by the modification of the right hand side, and in the end these degrees of freedom do not occur in the matrix and solution vector any more at all.
*  This local system is solved and the values are updated into the destination vector.
*  Remark: the Vanka method is a non-symmetric preconditioning method.
* 

*  [1.x.0] This little example is taken from a program doing parameter optimization. The Lagrange multiplier is the third component of the finite element used. The system is solved by the GMRES method.

* 
* [1.x.1]
* 
* 

*  [1.x.2] At present, the local matrices are built up such that the degree of freedom associated with the local Lagrange multiplier is the first one. Thus, usually the upper left entry in the local matrix is zero. It is not clear to me (W.B.) whether this might pose some problems in the inversion of the local matrices. Maybe someone would like to check this.
* 

* 
*  [2.x.5]  Instantiations for this template are provided for <tt> [2.x.6]  and  [2.x.7]  others can be generated in application programs (see the section on  [2.x.8]  in the manual).

* 
* [0.x.2]*
   Declare type for container size.  
* [0.x.3]*
   Constructor. Does nothing.     Call the initialize() function before using this object as preconditioner   (vmult()).  
* [0.x.4]*
   Constructor which also takes two deprecated inputs.      [2.x.9]  The use of the last two parameters is deprecated. They are   currently ignored.  
* [0.x.5]*
   Constructor. Gets the matrix for preconditioning and a bit vector with   entries  [2.x.10]  for all rows to be updated. A reference to this vector   will be stored, so it must persist longer than the Vanka object. The same   is true for the matrix.     The matrix  [2.x.11]  which is passed here may or may not be the same matrix   for which this object shall act as preconditioner. In particular, it is   conceivable that the preconditioner is build up for one matrix once, but   is used for subsequent steps in a nonlinear process as well, where the   matrix changes in each step slightly.  
* [0.x.6]*
   Destructor. Delete all allocated matrices.  
* [0.x.7]*
   Parameters for SparseVanka.  
* [0.x.8]*
     Constructor. For the parameters' description, see below.    
* [0.x.9]*
     Constructor. For the parameters' description, see below.          [2.x.12]  The use of this constructor is deprecated
* 
*  - the second and     third parameters are ignored.    
* [0.x.10]*
     Indices of those degrees of freedom that we shall work on.    
* [0.x.11]*
   If the default constructor is used then this function needs to be called   before an object of this class is used as preconditioner.     For more detail about possible parameters, see the class documentation   and the documentation of the  [2.x.13]  class.     After this function is called the preconditioner is ready to be used   (using the  [2.x.14]  function of derived classes).  
* [0.x.12]*
   Do the preconditioning. This function takes the residual in  [2.x.15]  and   returns the resulting update vector in  [2.x.16]   
* [0.x.13]*
   Apply transpose preconditioner. This function takes the residual in  [2.x.17]    src  and returns the resulting update vector in  [2.x.18]   
* [0.x.14]*
   Return the dimension of the codomain (or range) space. Note that the   matrix is of dimension  [2.x.19] .    
*  [2.x.20]  This function should only be called if the preconditioner has been   initialized.  
* [0.x.15]*
   Return the dimension of the domain space. Note that the matrix is of   dimension  [2.x.21] .    
*  [2.x.22]  This function should only be called if the preconditioner has been   initialized.  
* [0.x.16]*
   Apply the inverses corresponding to those degrees of freedom that have a    [2.x.23]  value in  [2.x.24]  to the  [2.x.25]  vector and move the result   into  [2.x.26]  Actually, only values for allowed indices are written to  [2.x.27]    dst, so the application of this function only does what is announced in   the general documentation if the given mask sets all values to zero     The reason for providing the mask anyway is that in derived classes we   may want to apply the preconditioner to parts of the matrix only, in   order to parallelize the application. Then, it is important to only write   to some slices of  [2.x.28]  in order to eliminate the dependencies of   threads of each other.     If a null pointer is passed instead of a pointer to the  [2.x.29]  (as   is the default value), then it is assumed that we shall work on all   degrees of freedom. This is then equivalent to calling the function with   a <tt>vector<bool>(n_dofs,true)</tt>.     The  [2.x.30]  of this class of course calls this function with a null   pointer  
* [0.x.17]*
   Determine an estimate for the memory consumption (in bytes) of this   object.  
* [0.x.18]*
   Pointer to the matrix.  
* [0.x.19]*
   Indices of those degrees of freedom that we shall work on.  
* [0.x.20]*
   Array of inverse matrices, one for each degree of freedom. Only those   elements will be used that are tagged in  [2.x.31]   
* [0.x.21]*
   The dimension of the range space.  
* [0.x.22]*
   The dimension of the domain space.  
* [0.x.23]*
   Compute the inverses of all selected diagonal elements.  
* [0.x.24]*
   Compute the inverses at positions in the range <tt>[begin,end)</tt>. In   non-multithreaded mode, <tt>compute_inverses()</tt> calls this function   with the whole range, but in multithreaded mode, several copies of this   function are spawned.  
* [0.x.25]*
   Compute the inverse of the block located at position  [2.x.32]  Since the   vector is used quite often, it is generated only once in the caller of   this function and passed to this function which first clears it. Reusing   the vector makes the process significantly faster than in the case where   this function re-creates it each time.  
* [0.x.26]*
 Block version of the sparse Vanka preconditioner. This class divides the matrix into blocks and works on the diagonal blocks only, which of course reduces the efficiency as preconditioner, but is perfectly parallelizable. The constructor takes a parameter into how many blocks the matrix shall be subdivided and then lets the underlying class do the work. Division of the matrix is done in several ways which are described in detail below.
*  This class is probably useless if you don't have a multiprocessor system, since then the amount of work per preconditioning step is the same as for the  [2.x.33]  class, but preconditioning properties are worse. On the other hand, if you have a multiprocessor system, the worse preconditioning quality (leading to more iterations of the linear solver) usually is well balanced by the increased speed of application due to the parallelization, leading to an overall decrease in elapsed wall-time for solving your linear system. It should be noted that the quality as preconditioner reduces with growing number of blocks, so there may be an optimal value (in terms of wall-time per linear solve) for the number of blocks.
*  To facilitate writing portable code, if the number of blocks into which the matrix is to be subdivided, is set to one, then this class acts just like the  [2.x.34]  class. You may therefore want to set the number of blocks equal to the number of processors you have.
*  Note that the parallelization is done if <tt>deal.II</tt> was configured for multithread use and that the number of threads which is spawned equals the number of blocks. This is reasonable since you will not want to set the number of blocks unnecessarily large, since, as mentioned, this reduces the preconditioning properties.
* 

*  [1.x.3]
*  Splitting the matrix into blocks is always done in a way such that the blocks are not necessarily of equal size, but such that the number of selected degrees of freedom for which a local system is to be solved is equal between blocks. The reason for this strategy to subdivision is load- balancing for multithreading. There are several possibilities to actually split the matrix into blocks, which are selected by the flag  [2.x.35]  blocking_strategy that is passed to the constructor. By a block, we will in the sequel denote a list of indices of degrees of freedom; the algorithm will work on each block separately, i.e. the solutions of the local systems corresponding to a degree of freedom of one block will only be used to update the degrees of freedom belonging to the same block, but never to update degrees of freedom of other blocks. A block can be a consecutive list of indices, as in the first alternative below, or a nonconsecutive list of indices. Of course, we assume that the intersection of each two blocks is empty and that the union of all blocks equals the interval <tt>[0,N)</tt>, where  [2.x.36]  is the number of degrees of freedom of the system of equations.
*   [2.x.37]   [2.x.38]   [2.x.39]  Here, we chose the blocks to be intervals <tt>[a_i,a_{i+1</tt>)}, i.e. consecutive degrees of freedom are usually also within the same block. This is a reasonable strategy, if the degrees of freedom have, for example, be re-numbered using the Cuthill-McKee algorithm, in which spatially neighboring degrees of freedom have neighboring indices. In that case, coupling in the matrix is usually restricted to the vicinity of the diagonal as well, and we can simply cut the matrix into blocks.
*  The bounds of the intervals, i.e. the  [2.x.40]  above, are chosen such that the number of degrees of freedom on which we shall work (i.e. usually the degrees of freedom corresponding to Lagrange multipliers) is about the same in each block; this does not mean, however, that the sizes of the blocks are equal, since the blocks also comprise the other degrees of freedom for which no local system is solved. In the extreme case, consider that all Lagrange multipliers are sorted to the end of the range of DoF indices, then the first block would be very large, since it comprises all other DoFs and some Lagrange multipliers, while all other blocks are rather small and comprise only Langrange multipliers. This strategy therefore does not only depend on the order in which the Lagrange DoFs are sorted, but also on the order in which the other DoFs are sorted. It is therefore necessary to note that this almost renders the capability as preconditioner useless if the degrees of freedom are numbered by component, i.e. all Lagrange multipliers en bloc.
*   [2.x.41]   [2.x.42]  This strategy is a bit more clever in cases where the Langrange DoFs are clustered, as in the example above. It works as follows: it first groups the Lagrange DoFs into blocks, using the same strategy as above. However, instead of grouping the other DoFs into the blocks of Lagrange DoFs with nearest DoF index, it decides for each non-Lagrange DoF to put it into the block of Lagrange DoFs which write to this non-Lagrange DoF most often. This makes it possible to even sort the Lagrange DoFs to the end and still associate spatially neighboring non-Lagrange DoFs to the same blocks where the respective Lagrange DoFs are, since they couple to each other while spatially distant DoFs don't couple.
*  The additional computational effort to sorting the non-Lagrange DoFs is not very large compared with the inversion of the local systems and applying the preconditioner, so this strategy might be reasonable if you want to sort your degrees of freedom by component. If the degrees of freedom are not sorted by component, the results of the both strategies outlined above does not differ much. However, unlike the first strategy, the performance of the second strategy does not deteriorate if the DoFs are renumbered by component.  [2.x.43] 
* 

*  [1.x.4]
*  As a prototypical test case, we use a nonlinear problem from optimization, which leads to a series of saddle point problems, each of which is solved using GMRES with Vanka as preconditioner. The equation had approx. 850 degrees of freedom. With the non-blocked version  [2.x.44]  (or  [2.x.45]  SparseBlockVanka with <tt>n_blocks==1</tt>), the following numbers of iterations is needed to solver the linear system in each nonlinear step:

* 
* [1.x.5]
* 
*  With four blocks, we need the following numbers of iterations

* 
* [1.x.6]
*  As can be seen, more iterations are needed. However, in terms of computing time, the first version needs 72 seconds wall time (and 79 seconds CPU time, which is more than wall time since some other parts of the program were parallelized as well), while the second version needed 53 second wall time (and 110 seconds CPU time) on a four processor machine. The total time is in both cases dominated by the linear solvers. In this case, it is therefore worth while using the blocked version of the preconditioner if wall time is more important than CPU time.
*  The results with the block version above were obtained with the first blocking strategy and the degrees of freedom were not numbered by component. Using the second strategy does not much change the numbers of iterations (at most by one in each step) and they also do not change when the degrees of freedom are sorted by component, while the first strategy significantly deteriorated.

* 
* [0.x.27]*
   Declare type for container size.  
* [0.x.28]*
   Enumeration of the different methods by which the DoFs are distributed to   the blocks on which we are to work.  
* [0.x.29]*
     Block by index intervals.    
* [0.x.30]*
     Block with an adaptive strategy.    
* [0.x.31]*
   Constructor. Pass all arguments except for  [2.x.46]  to the base class.      [2.x.47]  This constructor is deprecated. The values passed to the last   two arguments are ignored.  
* [0.x.32]*
   Constructor. Pass all arguments except for  [2.x.48]  to the base class.  
* [0.x.33]*
   Apply the preconditioner.  
* [0.x.34]*
   Determine an estimate for the memory consumption (in bytes) of this   object.  
* [0.x.35]*
   Store the number of blocks.  
* [0.x.36]*
   In this field, we precompute for each block which degrees of freedom   belong to it. Thus, if <tt>dof_masks[i][j]==true</tt>, then DoF  [2.x.49]    belongs to block  [2.x.50]  Of course, no other <tt>dof_masks[l][j]</tt> may   be  [2.x.51]  for <tt>l!=i</tt>. This computation is done in the   constructor, to avoid recomputing each time the preconditioner is called.  
* [0.x.37]*
   Compute the contents of the field  [2.x.52]  This function is called   from the constructor.  
* [0.x.38]

include/deal.II-translator/lac/sparse_vanka.templates_0.txt
[0.x.0]

include/deal.II-translator/lac/sparsity_pattern_0.txt
[0.x.0]!  [2.x.0]  Sparsity [2.x.1] 

* 
* [0.x.1]*
     Declare type for container size.    
* [0.x.2]*
     Helper function to get the column index from a dereferenced iterator in     the copy_from() function, if the inner iterator type points to plain     unsigned integers.    
* [0.x.3]*
     Helper function to get the column index from a dereferenced iterator in     the copy_from() function, if the inner iterator type points to pairs of     unsigned integers and some other value.    
* [0.x.4]*
     Likewise, but sometimes needed for certain types of containers that     make the first element of the pair constant (such as      [2.x.2]     
* [0.x.5]*
 Iterators on objects of type SparsityPattern.

* 
* [0.x.6]*
   Declare type for container size.  
* [0.x.7]*
   Accessor class for iterators into sparsity patterns. This class is also   the base class for both const and non-const accessor classes into sparse   matrices.     Note that this class only allows read access to elements, providing their   row and column number (or alternatively the index within the complete   sparsity pattern). It does not allow modifying the sparsity pattern   itself.  
* [0.x.8]*
     Size type of SparsityPattern.    
* [0.x.9]*
     Constructor.    
* [0.x.10]*
     Constructor. Construct the end accessor for the given sparsity pattern.    
* [0.x.11]*
     Default constructor creating a dummy accessor. This constructor is here     only to be able to store accessors in STL containers such as      [2.x.3]     
* [0.x.12]*
     Row number of the element represented by this object. This function can     only be called for entries for which is_valid_entry() is true.    
* [0.x.13]*
     Index within the current row of the element represented by this object.     This function can only be called for entries for which is_valid_entry()     is true.    
* [0.x.14]*
     This function returns the how-many'th entry within the entire sparsity     pattern the current iterator points to. While the order of entries in     a sparsity pattern is generally not important, this function allows     indexing entries of the sparsity pattern using a linear index.         This function can only be called for entries for which is_valid_entry()     is true.    
* [0.x.15]*
     Column number of the element represented by this object. This function     can only be called for entries for which is_valid_entry() is true.    
* [0.x.16]*
     Return whether the sparsity pattern entry pointed to by this iterator     is valid or not. Note that after compressing the sparsity pattern, all     entries are valid. However, before compression, the sparsity pattern     allocated some memory to be used while still adding new nonzero     entries; if you create iterators in this phase of the sparsity     pattern's lifetime, you will iterate over elements that are not valid.     If this is so, then this function will return false.    
* [0.x.17]*
     Comparison. True, if both iterators point to the same matrix position.    
* [0.x.18]*
     Comparison operator. Result is true if either the first row number is     smaller or if the row numbers are equal and the first index is smaller.         This function is only valid if both iterators point into the same     sparsity pattern.    
* [0.x.19]*
     The sparsity pattern we operate on accessed.    
* [0.x.20]*
     Index in global sparsity pattern. This index represents the location     the iterator/accessor points to within the array of the SparsityPattern     class that stores the column numbers. It is also the index within the     values array of a sparse matrix that stores the corresponding value of     this site.    
* [0.x.21]*
     Move the accessor to the next nonzero entry in the matrix.    
* [0.x.22]*
   An iterator class for walking over the elements of a sparsity pattern.     The typical use for these iterators is to iterate over the elements of a   sparsity pattern (or, since they also serve as the basis for iterating   over the elements of an associated matrix, over the elements of a sparse   matrix), or over the elements of individual rows. There is no guarantee   that the elements of a row are actually traversed in an order in which   column numbers monotonically increase. See the documentation of the   SparsityPattern class for more information.    
*  [2.x.4]  This class operates directly on the internal data structures of the   SparsityPatternBase class. As a consequence, some operations are cheap and   some are not. In particular, it is cheap to access the column index of   the sparsity pattern entry pointed to. On the other hand, it is expensive   to access the row index (this requires  [2.x.5]  operations for a   matrix with  [2.x.6]  row). As a consequence, when you design algorithms that   use these iterators, it is common practice to not loop over [1.x.0]   elements of a sparsity pattern at once, but to have an outer loop over   all rows and within this loop iterate over the elements of this row. This   way, you only ever need to dereference the iterator to obtain the column   indices whereas the (expensive) lookup of the row index can be avoided by   using the loop index instead.  
* [0.x.23]*
     Size type.    
* [0.x.24]*
     Type of the stored pointer.    
* [0.x.25]*
     Constructor. Create an iterator into the sparsity pattern  [2.x.7]  for the     given global index (i.e., the index of the given element counting from     the zeroth row).    
* [0.x.26]*
     Constructor. Create an iterator into the sparsity pattern  [2.x.8]  for     a given accessor.    
* [0.x.27]*
 A class that can store which elements of a matrix are nonzero (or, in fact, [1.x.1] be nonzero) and for which we have to allocate memory to store their values. This class is an example of the "static" type of sparsity patters (see  [2.x.9] ). It uses the [1.x.2] format to store data, and is used as the basis for the derived SparsityPattern class and SparseMatrix class.
*  The elements of a SparsityPatternBase, corresponding to the places where SparseMatrix objects can store nonzero entries, are stored row-by-row. The ordering of non-zero elements within each row (i.e. increasing column index order) depends on the derived classes.

* 
* [0.x.28]*
   Declare type for container size.  
* [0.x.29]*
   Typedef an iterator class that allows to walk over all nonzero elements   of a sparsity pattern.  
* [0.x.30]*
   Typedef an iterator class that allows to walk over all nonzero elements   of a sparsity pattern.     Since the iterator does not allow to modify the sparsity pattern, this   type is the same as that for  [2.x.10]   
* [0.x.31]*
   Define a value which is used to indicate that a certain value in the   #colnums array is unused, i.e. does not represent a certain column number   index.     Indices with this invalid value are used to insert new entries to the   sparsity pattern using the add() member function, and are removed when   calling compress().     You should not assume that the variable declared here has a certain   value. The initialization is given here only to enable the compiler to   perform some optimizations, but the actual value of the variable may   change over time.  
* [0.x.32]*
    [2.x.11]  Construction and Initialization     Constructors, destructor, functions initializing, copying and filling an   object.  
* [0.x.33]*
   Initialize the matrix empty, that is with no memory allocated. This is   useful if you want such objects as member variables in other classes. You   can make the structure usable by calling the reinit() function.  
* [0.x.34]*
   Destructor.  
* [0.x.35]*
   Reallocate memory and set up data structures for a new matrix with  [2.x.12]    rows and  [2.x.13]  columns, with at most  [2.x.14]    nonzero entries per row.     This function simply maps its operations to the other reinit()   function.  
* [0.x.36]*
   Reallocate memory for a matrix of size  [2.x.15]  times  [2.x.16]  The number of   entries for each row is taken from the array  [2.x.17]  which   has to give this number of each row  [2.x.18] .     If <tt>m*n==0</tt> all memory is freed, resulting in a total   reinitialization of the object. If it is nonzero, new memory is only   allocated if the new size extends the old one. This is done to save time   and to avoid fragmentation of the heap.  
* [0.x.37]*
   Same as above, but with an ArrayView argument instead.     The derived classes are responsible for implementation of this function.  
* [0.x.38]*
   Make the sparsity pattern symmetric by adding the sparsity pattern of the   transpose object.     This function throws an exception if the sparsity pattern does not   represent a quadratic matrix.  
* [0.x.39]*
   Add a nonzero entry to the matrix.  This function may only be called for   non-compressed sparsity patterns.     If the entry already exists, nothing bad happens.  
* [0.x.40]*
    [2.x.19]  Iterators  
* [0.x.41]*
   Iterator starting at the first entry of the matrix. The resulting   iterator can be used to walk over all nonzero entries of the sparsity   pattern.     The order in which elements are accessed depends on the storage scheme   implemented by derived classes.  
* [0.x.42]*
   Final iterator.  
* [0.x.43]*
   Iterator starting at the first entry of row <tt>r</tt>.     Note that if the given row is empty, i.e. does not contain any nonzero   entries, then the iterator returned by this function equals   <tt>end(r)</tt>. Note also that the iterator may not be dereferenceable in   that case.     The order in which elements are accessed depends on the storage scheme   implemented by derived classes.  
* [0.x.44]*
   Final iterator of row <tt>r</tt>. It points to the first element past the   end of line  [2.x.20]  or past the end of the entire sparsity pattern.     Note that the end iterator is not necessarily dereferenceable. This is in   particular the case if it is the end iterator for the last row of a   matrix.  
* [0.x.45]*
    [2.x.21]  Querying information  
* [0.x.46]*
   Test for equality of two SparsityPatterns.  
* [0.x.47]*
   Return whether the object is empty. It is empty if no memory is   allocated, which is the same as that both dimensions are zero.  
* [0.x.48]*
   Check if a value at a certain position may be non-zero.  
* [0.x.49]*
   Return the maximum number of entries per row. Before compression, this   equals the number given to the constructor, while after compression, it   equals the maximum number of entries actually allocated by the user.  
* [0.x.50]*
   Compute the bandwidth of the matrix represented by this structure. The   bandwidth is the maximum of  [2.x.22]  for which the index pair  [2.x.23]    represents a nonzero entry of the matrix. Consequently, the maximum   bandwidth a  [2.x.24]  matrix can have is  [2.x.25] , a diagonal   matrix has bandwidth 0, and there are at most  [2.x.26]  entries per row if   the bandwidth is  [2.x.27] . The returned quantity is sometimes called "half   bandwidth" in the literature.  
* [0.x.51]*
   Return the number of nonzero elements of this matrix. Actually, it   returns the number of entries in the sparsity pattern; if any of the   entries should happen to be zero, it is counted anyway.     This function may only be called if the matrix struct is compressed. It   does not make too much sense otherwise anyway.  
* [0.x.52]*
   Return whether the structure is compressed or not.  
* [0.x.53]*
   Return number of rows of this matrix, which equals the dimension of the   image space.  
* [0.x.54]*
   Return number of columns of this matrix, which equals the dimension of   the range space.  
* [0.x.55]*
   Number of entries in a specific row.  
* [0.x.56]*
   Determine an estimate for the memory consumption (in bytes) of this   object. See MemoryConsumption.  
* [0.x.57]*
    [2.x.28]  Accessing entries  
* [0.x.58]*
   Access to column number field.  Return the column number of the   <tt>index</tt>th entry in <tt>row</tt>. Note that if diagonal elements   are optimized, the first element in each row is the diagonal element,   i.e. <tt>column_number(row,0)==row</tt>.     If the sparsity pattern is already compressed, then (except for the   diagonal element), the entries are sorted by columns, i.e.   <tt>column_number(row,i)</tt> <tt><</tt> <tt>column_number(row,i+1)</tt>.  
* [0.x.59]*
   The index of a global matrix entry in its row.     This function is analogous to operator(), but it computes the index not   with respect to the total field, but only with respect to the row   <tt>j</tt>.  
* [0.x.60]*
   This is the inverse operation to operator()(): given a global index, find   out row and column of the matrix entry to which it belongs. The returned   value is the pair composed of row and column index.     This function may only be called if the sparsity pattern is closed. The   global index must then be between zero and n_nonzero_elements().     If <tt>N</tt> is the number of rows of this matrix, then the complexity   of this function is [1.x.3].  
* [0.x.61]*
    [2.x.29]  Input/Output  
* [0.x.62]*
   Print the sparsity of the matrix. The output consists of one line per row   of the format <tt>[i,j1,j2,j3,...]</tt>. [1.x.4] is the row number and   [1.x.5] are the allocated columns in this row.  
* [0.x.63]*
   Print the sparsity of the matrix in a format that <tt>gnuplot</tt>   understands and which can be used to plot the sparsity pattern in a   graphical way. The format consists of pairs <tt>i j</tt> of nonzero   elements, each representing one entry of this matrix, one per line of the   output file. Indices are counted from zero on, as usual. Since sparsity   patterns are printed in the same way as matrices are displayed, we print   the negative of the column index, which means that the <tt>(0,0)</tt>   element is in the top left rather than in the bottom left corner.     Print the sparsity pattern in gnuplot by setting the data style to dots   or points and use the <tt>plot</tt> command.  
* [0.x.64]*
   Prints the sparsity of the matrix in a .svg file which can be opened in a   web browser. The .svg file contains squares which correspond to the   entries in the matrix. An entry in the matrix which contains a non-zero   value corresponds with a red square while a zero-valued entry in the   matrix correspond with a white square.  
* [0.x.65]*
   Write the data of this object to a stream for the purpose of   serialization using the [BOOST serialization   library](https://www.boost.org/doc/libs/1_74_0/libs/serialization/doc/index.html).  
* [0.x.66]*
   Read the data of this object from a stream for the purpose of   serialization using the [BOOST serialization   library](https://www.boost.org/doc/libs/1_74_0/libs/serialization/doc/index.html).  
* [0.x.67]*
   Write and read the data of this object from a stream for the purpose   of serialization using the [BOOST serialization   library](https://www.boost.org/doc/libs/1_74_0/libs/serialization/doc/index.html).  
* [0.x.68]*
    [2.x.30]  Exceptions    [2.x.31]   
* [0.x.69]*
   The operation is only allowed after the SparsityPattern has been set up   and compress() was called.  
* [0.x.70]*
   You tried to add an element to a row, but there was no space left.  
* [0.x.71]*
   This operation changes the structure of the SparsityPattern and is not   possible after compress() has been called.  
* [0.x.72]*
   Maximum number of rows that can be stored in the #rowstart array.  Since   reallocation of that array only happens if the present one is too small,   but never when the size of this matrix structure shrinks, #max_dim might   be larger than #rows and in this case #rowstart has more elements than   are used.  
* [0.x.73]*
   Number of rows that this sparsity structure shall represent.  
* [0.x.74]*
   Number of columns that this sparsity structure shall represent.  
* [0.x.75]*
   Size of the actually allocated array #colnums. Here, the same applies as   for the #rowstart array, i.e. it may be larger than the actually used   part of the array.  
* [0.x.76]*
   Maximum number of elements per row. This is set to the value given to the   reinit() function (or to the constructor), or to the maximum row length   computed from the vectors in case the more flexible constructors or   reinit versions are called. Its value is more or less meaningless after   compress() has been called.  
* [0.x.77]*
   Array which hold for each row which is the first element in #colnums   belonging to that row. Note that the size of the array is one larger than   the number of rows, because the last element is used for   <tt>row</tt>=#rows, i.e. the row past the last used one. The value of   #rowstart[#rows]} equals the index of the element past the end in   #colnums; this way, we are able to write loops like <tt>for   (i=rowstart[k]; i<rowstart[k+1]; ++i)</tt> also for the last row.     Note that the actual size of the allocated memory may be larger than the   region that is used. The actual number of elements that was allocated is   stored in #max_dim.  
* [0.x.78]*
   Array of column numbers. In this array, we store for each non-zero   element its column number. The column numbers for the elements in row   [1.x.6] are stored within the index range   #rowstart[[1.x.7]]...#rowstart[[1.x.8]]. Therefore to find out   whether a given element ([1.x.9]) exists, we have to check whether the   column number [1.x.10] exists in the above-mentioned range within this   array. If it exists, say at position [1.x.11] within this array, the   value of the respective element in the sparse matrix will also be at   position [1.x.12] of the values array of that class.     At the beginning, all elements of this array are set to  [2.x.32] 
* 
*  -  indicating   invalid (unused) column numbers (diagonal elements are preset if   optimized storage is requested, though). Now, if nonzero elements are   added, one column number in the row's respective range after the other is   set to the column number of the added element. When compress is called,   unused elements (indicated by column numbers  [2.x.33] 
* 
*  - ) are eliminated by   copying the column number of subsequent rows and the column numbers   within each row (with possible exception of the diagonal element) are   sorted, such that finding whether an element exists and determining its   position can be done by a binary search.  
* [0.x.79]*
   Store whether the compress() function was called for this object.  
* [0.x.80]*
 This class stores a sparsity pattern in the [1.x.13] format to store data, and is used as the basis for the SparseMatrix class.
*  The elements of a SparsityPattern, corresponding to the places where SparseMatrix objects can store nonzero entries, are stored row-by-row. Within each row, elements are generally stored left-to-right in increasing column index order; the exception to this rule is that if the matrix is square (n_rows() == n_columns()), then the diagonal entry is stored as the first element in each row to make operations like applying a Jacobi or SSOR preconditioner faster. As a consequence, if you traverse the elements of a row of a SparsityPattern with the help of iterators into this object (using  [2.x.34]  and  [2.x.35]  you will find that the elements are not sorted by column index within each row whenever the matrix is square (the first item will be the diagonal, followed by the other entries sorted by column index).
*  While this class forms the basis upon which SparseMatrix objects base their storage format, and thus plays a central role in setting up linear systems, it is rarely set up directly due to the way it stores its information. Rather, one typically goes through an intermediate format first, see for example the  [2.x.36]  tutorial program as well as the documentation module  [2.x.37] .
*  You can iterate over entries in the pattern using begin(), end(), begin(row), and end(row). These functions return an iterator of type  [2.x.38]  When dereferencing an iterator  [2.x.39]  you have access to the member functions in  [2.x.40]  like <tt>it->column()</tt> and <tt>it->row()</tt>.

* 
* [0.x.81]*
   Declare type for container size.  
* [0.x.82]*
   Typedef an iterator class that allows to walk over all nonzero elements   of a sparsity pattern.  
* [0.x.83]*
   Typedef an iterator class that allows to walk over all nonzero elements   of a sparsity pattern.     Since the iterator does not allow to modify the sparsity pattern, this   type is the same as that for  [2.x.41]   
* [0.x.84]*
   Since this class has to implement only one reinit() function, we need to   bring all base reinit() functions into the scope so that the compiler can   find them.  
* [0.x.85]*
    [2.x.42]  Construction and setup     Constructors, destructor, functions initializing, copying and filling an   object.  
* [0.x.86]*
   Initialize the matrix empty, that is with no memory allocated. This is   useful if you want such objects as member variables in other classes. You   can make the structure usable by calling the reinit() function.  
* [0.x.87]*
   Copy constructor. This constructor is only allowed to be called if the   matrix structure to be copied is empty. This is so in order to prevent   involuntary copies of objects for temporaries, which can use large   amounts of computing time. However, copy constructors are needed if one   wants to place a SparsityPattern in a container, e.g., to write such   statements like <tt>v.push_back (SparsityPattern());</tt>, with   <tt>v</tt> a  [2.x.43]  of SparsityPattern objects.     Usually, it is sufficient to use the explicit keyword to disallow   unwanted temporaries, but this does not work for  [2.x.44]    Since copying a structure like this is not useful anyway because multiple   matrices can use the same sparsity structure, copies are only allowed for   empty objects, as described above.  
* [0.x.88]*
   Initialize a rectangular pattern of size <tt>m x n</tt>.      [2.x.45]  m The number of rows.    [2.x.46]  n The number of columns.    [2.x.47]  max_per_row Maximum number of nonzero entries per row.  
* [0.x.89]*
   Initialize a rectangular pattern of size <tt>m x n</tt>.      [2.x.48]  m The number of rows.    [2.x.49]  n The number of columns.    [2.x.50]  row_lengths Possible number of nonzero entries for each row.   This vector must have one entry for each row.  
* [0.x.90]*
   Initialize a quadratic pattern of dimension <tt>m</tt> with at most   <tt>max_per_row</tt> nonzero entries per row.     This constructor automatically enables optimized storage of diagonal   elements. To avoid this, use the constructor taking row and column   numbers separately.  
* [0.x.91]*
   Initialize a quadratic pattern of size <tt>m x m</tt>.      [2.x.51]  m The number of rows and columns.    [2.x.52]  row_lengths Maximum number of nonzero entries for each row.   This vector must have one entry for each row.  
* [0.x.92]*
   Make a copy with extra off-diagonals.     This constructs objects intended for the application of the ILU(n)-method   or other incomplete decompositions.  Therefore, additional to the   original entry structure, space for <tt>extra_off_diagonals</tt> side-   diagonals is provided on both sides of the main diagonal.     <tt>max_per_row</tt> is the maximum number of nonzero elements per row   which this structure is to hold. It is assumed that this number is   sufficiently large to accommodate both the elements in <tt>original</tt>   as well as the new off-diagonal elements created by this constructor. You   will usually want to give the same number as you gave for   <tt>original</tt> plus the number of side diagonals times two. You may   however give a larger value if you wish to add further nonzero entries   for the decomposition based on other criteria than their being on side-   diagonals.     This function requires that <tt>original</tt> refers to a quadratic   matrix structure.  It must be compressed. The matrix structure is not   compressed after this function finishes.  
* [0.x.93]*
   Destructor.  
* [0.x.94]*
   Copy operator. For this the same holds as for the copy constructor: it is   declared, defined and fine to be called, but the latter only for empty   objects.  
* [0.x.95]*
   Reallocate memory for a matrix of size  [2.x.53]  times  [2.x.54]  The number of   entries for each row is taken from the ArrayView  [2.x.55]  which   has to give this number of each row  [2.x.56] .  
* [0.x.96]*
   This function compresses the sparsity structure that this object   represents.  It does so by eliminating unused entries and sorting the   remaining ones to allow faster access by usage of binary search   algorithms. A special sorting scheme is used for the diagonal entry of   quadratic matrices, which is always the first entry of each row.     The memory which is no more needed is released.     SparseMatrix objects require the SparsityPattern objects they are   initialized with to be compressed, to reduce memory requirements.  
* [0.x.97]*
   This function can be used as a replacement for reinit(), subsequent calls   to add() and a final call to close() if you know exactly in advance the   entries that will form the matrix sparsity pattern.     The first two parameters determine the size of the matrix. For the two   last ones, note that a sparse matrix can be described by a sequence of   rows, each of which is represented by a sequence of pairs of column   indices and values. In the present context, the begin() and end()   parameters designate iterators (of forward iterator type) into a   container, one representing one row. The distance between begin() and   end() should therefore be equal to n_rows(). These iterators may be   iterators of  [2.x.57]   [2.x.58]  pointers into a   C-style array, or any other iterator satisfying the requirements of a   forward iterator. The objects pointed to by these iterators (i.e. what we   get after applying <tt>operator*</tt> or <tt>operator-></tt> to one of   these iterators) must be a container itself that provides functions   <tt>begin</tt> and <tt>end</tt> designating a range of iterators that   describe the contents of one line. Dereferencing these inner iterators   must either yield a pair of an unsigned integer as column index and a   value of arbitrary type (such a type would be used if we wanted to   describe a sparse matrix with one such object), or simply an unsigned   integer (of we only wanted to describe a sparsity pattern). The function   is able to determine itself whether an unsigned integer or a pair is what   we get after dereferencing the inner iterators, through some template   magic.     While the order of the outer iterators denotes the different rows of the   matrix, the order of the inner iterator denoting the columns does not   matter, as they are sorted internal to this function anyway.     Since that all sounds very complicated, consider the following example   code, which may be used to fill a sparsity pattern:  
* [1.x.14]
*      Note that this example works since the iterators dereferenced yield   containers with functions <tt>begin</tt> and <tt>end</tt> (namely    [2.x.59]  and the inner iterators dereferenced yield   unsigned integers as column indices. Note that we could have replaced   each of the two  [2.x.60]  occurrences by  [2.x.61]    and the inner one by  [2.x.62]  as well.     Another example would be as follows, where we initialize a whole matrix,   not only a sparsity pattern:  
* [1.x.15]
*      This example works because dereferencing iterators of the inner type   yields a pair of unsigned integers and a value, the first of which we   take as column index. As previously, the outer  [2.x.63]  could   be replaced by  [2.x.64]  and the inner  [2.x.65]    int,double></tt> could be replaced by  [2.x.66]    int,double> ></tt>, or a list or set of such pairs, as they all return   iterators that point to such pairs.  
* [0.x.98]*
   Copy data from a DynamicSparsityPattern. Previous content of this object   is lost, and the sparsity pattern is in compressed mode afterwards.  
* [0.x.99]*
   Copy data from a SparsityPattern. Previous content of this object is   lost, and the sparsity pattern is in compressed mode afterwards.  
* [0.x.100]*
   Take a full matrix and use its nonzero entries to generate a sparse   matrix entry pattern for this object.     Previous content of this object is lost, and the sparsity pattern is in   compressed mode afterwards.     Once you have built a sparsity pattern with this function, you   probably want to attach a SparseMatrix object to it. The original   `matrix` object can then be copied into this SparseMatrix object   using the version of  [2.x.67]  that takes a   FullMatrix object as argument. Through this procedure, you can   convert a FullMatrix into a SparseMatrix.  
* [0.x.101]*
   Add several nonzero entries to the specified matrix row.  This function   may only be called for non-compressed sparsity patterns.     If some of the entries already exist, nothing bad happens.  
* [0.x.102]*
    [2.x.68]  Querying information  
* [0.x.103]*
   Test for equality of two SparsityPatterns.  
* [0.x.104]*
   Return whether this object stores only those entries that have been added   explicitly, or if the sparsity pattern contains elements that have been   added through other means (implicitly) while building it. For the current   class, the result is false if and only if it is square because it then   unconditionally stores the diagonal entries whether they have been added   explicitly or not.     This function mainly serves the purpose of describing the current class   in cases where several kinds of sparsity patterns can be passed as   template arguments.  
* [0.x.105]*
   Determine an estimate for the memory consumption (in bytes) of this   object. See MemoryConsumption.  
* [0.x.106]*
    [2.x.69]  Accessing entries  
* [0.x.107]*
   Return the index of the matrix element with row number <tt>i</tt> and   column number <tt>j</tt>. If the matrix element is not a nonzero one,   return  [2.x.70]      This function is usually called by the  [2.x.71]  It may   only be called for compressed sparsity patterns, since in this case   searching whether the entry exists can be done quite fast with a binary   sort algorithm because the column numbers are sorted.     If <tt>m</tt> is the number of entries in <tt>row</tt>, then the   complexity of this function is [1.x.16] if the sparsity pattern is   compressed.    
*  [2.x.72]  This function is not cheap since it has to search through all of   the elements of the given row <tt>i</tt> to find whether index <tt>j</tt>   exists. Thus, it is more expensive than necessary in cases where you want   to loop over all of the nonzero elements of this sparsity pattern (or of   a sparse matrix associated with it) or of a single row. In such cases, it   is more efficient to use iterators over the elements of the sparsity   pattern or of the sparse matrix.  
* [0.x.108]*
    [2.x.73]  Input/Output  
* [0.x.109]*
   Write the data of this object en bloc to a file. This is done in a binary   mode, so the output is neither readable by humans nor (probably) by other   computers using a different operating system or number format.     The purpose of this function is that you can swap out matrices and   sparsity pattern if you are short of memory, want to communicate between   different programs, or allow objects to be persistent across different   runs of the program.  
* [0.x.110]*
   Read data that has previously been written by block_write() from a file.   This is done using the inverse operations to the above function, so it is   reasonably fast because the bitstream is not interpreted except for a few   numbers up front.     The object is resized on this operation, and all previous contents are   lost.     A primitive form of error checking is performed which will recognize the   bluntest attempts to interpret some data as a vector stored bitwise to a   file, but not more.  
* [0.x.111]*
   Write the data of this object to a stream for the purpose of   serialization  
* [0.x.112]*
   Read the data of this object from a stream for the purpose of   serialization  
* [0.x.113]*
   Write and read the data of this object from a stream for the purpose   of serialization.  
* [0.x.114]*
    [2.x.74]  Exceptions    [2.x.75]   
* [0.x.115]*
   Exception  
* [0.x.116]*
   Exception  
* [0.x.117]*
   Is special treatment of diagonals enabled?  
* [0.x.118]*
     Declare type for container size.    
* [0.x.119]

include/deal.II-translator/lac/sparsity_tools_0.txt
[0.x.0]!  [2.x.0]  Sparsity [2.x.1] 

* 
* [0.x.1]*
 A namespace for functions that deal with things that one can do on sparsity patterns, such as renumbering rows and columns (or degrees of freedom if you want) according to the connectivity, or partitioning degrees of freedom.

* 
* [0.x.2]*
   Enumerator with options for partitioner  
* [0.x.3]*
     Use METIS partitioner.    
* [0.x.4]*
     Use ZOLTAN partitioner.    
* [0.x.5]*
   Use a partitioning algorithm to generate a partitioning of the degrees of   freedom represented by this sparsity pattern. In effect, we view this   sparsity pattern as a graph of connections between various degrees of   freedom, where each nonzero entry in the sparsity pattern corresponds to   an edge between two nodes in the connection graph. The goal is then to   decompose this graph into groups of nodes so that a minimal number of   edges are cut by the boundaries between node groups. This partitioning is   done by METIS or ZOLTAN, depending upon which partitioner is chosen   in the fourth argument. The default is METIS. Note that METIS and   ZOLTAN can only partition symmetric sparsity patterns, and that of   course the sparsity pattern has to be square. We do not check for   symmetry of the sparsity pattern, since this is an expensive operation,   but rather leave this as the responsibility of caller of this function.     After calling this function, the output array will have values between   zero and  [2.x.2]  for each node (i.e. row or column of the   matrix).     If deal.II was not installed with packages ZOLTAN or METIS, this   function will generate an error when corresponding partition method   is chosen, unless  [2.x.3]  is one. I.e., you can write a program   so that it runs in the single-processor single-partition case without   the packages installed, and only requires them installed when   multiple partitions are required.     Note that the sparsity pattern itself is not changed by calling this   function. However, you will likely use the information generated by   calling this function to renumber degrees of freedom, after which you   will of course have to regenerate the sparsity pattern.     This function will rarely be called separately, since in finite element   methods you will want to partition the mesh, not the matrix. This can be   done by calling  [2.x.4]   
* [0.x.6]*
   This function performs the same operation as the one above, except that   it takes into consideration a set of  [2.x.5]  which allow the   partitioner to balance the graph while taking into consideration the   computational effort expended on each cell.    
*  [2.x.6]  If the  [2.x.7]  vector is empty, then no weighting is taken   into consideration. If not then the size of this vector must equal to the   number of active cells in the triangulation.  
* [0.x.7]*
   Using a coloring algorithm provided by ZOLTAN to color nodes whose   connections are represented using a SparsityPattern object. In effect,   we view this sparsity pattern as a graph of connections between nodes,   where each nonzero entry in the  [2.x.8]  corresponds to   an edge between two nodes. The goal is to assign each node a color index   such that no two directly connected nodes have the same color.   The assigned colors are listed in  [2.x.9]  indexed from one and   the function returns total number of colors used. ZOLTAN coloring   algorithm is run in serial by each processor. Hence all processors have   coloring information of all the nodes. A wrapper function to this   function is available in GraphColoring namespace as well.     Note that this function requires that SparsityPattern be symmetric   (and hence square) i.e an undirected graph representation. We do not   check for symmetry of the sparsity pattern, since this is an expensive   operation, but rather leave this as the responsibility of caller of   this function.      [2.x.10]    The usage of the function is illustrated by the image above, showing five   nodes numbered from 0 to 4. The connections shown are bidirectional.   After coloring, it is clear that no two directly connected nodes are   assigned the same color.     If deal.II was not installed with package ZOLTAN, this function will   generate an error.    
*  [2.x.11]  The current function is an alternative to    [2.x.12]  which is tailored to graph   coloring arising in shared-memory parallel assembly of matrices.  
* [0.x.8]*
   For a given sparsity pattern, compute a re-enumeration of row/column   indices based on the algorithm by Cuthill-McKee.     This algorithm is a graph renumbering algorithm in which we attempt to   find a new numbering of all nodes of a graph based on their connectivity   to other nodes (i.e. the edges that connect nodes). This connectivity is   here represented by the sparsity pattern. In many cases within the   library, the nodes represent degrees of freedom and edges are nonzero   entries in a matrix, i.e. pairs of degrees of freedom that couple through   the action of a bilinear form.     The algorithms starts at a node, searches the other nodes for those which   are coupled with the one we started with and numbers these in a certain   way. It then finds the second level of nodes, namely those that couple   with those of the previous level (which were those that coupled with the   initial node) and numbers these. And so on. For the details of the   algorithm, especially the numbering within each level, we refer the   reader to the book of Schwarz (H. R. Schwarz: Methode der finiten   Elemente).     These algorithms have one major drawback: they require a good starting   node, i.e. node that will have number zero in the output array. A   starting node forming the initial level of nodes can thus be given by the   user, e.g. by exploiting knowledge of the actual topology of the domain.   It is also possible to give several starting indices, which may be used   to simulate a simple upstream numbering (by giving the inflow nodes as   starting values) or to make preconditioning faster (by letting the   Dirichlet boundary indices be starting points).     If no starting index is given, one is chosen automatically, namely one   with the smallest coordination number (the coordination number is the   number of other nodes this node couples with). This node is usually   located on the boundary of the domain. There is, however, large ambiguity   in this when using the hierarchical meshes used in this library, since in   most cases the computational domain is not approximated by tilting and   deforming elements and by plugging together variable numbers of elements   at vertices, but rather by hierarchical refinement. There is therefore a   large number of nodes with equal coordination numbers. The renumbering   algorithms will therefore not give optimal results.     If the graph has two or more unconnected components and if no starting   indices are given, the algorithm will number each component   consecutively. However, this requires the determination of a starting   index for each component; as a consequence, the algorithm will produce an   exception if starting indices are given, taking the latter as an   indication that the caller of the function would like to override the   part of the algorithm that chooses starting indices.  
* [0.x.9]*
   For a given sparsity pattern, compute a re-enumeration of row/column   indices in a hierarchical way, similar to what    [2.x.13]  does for degrees of freedom on   hierarchically refined meshes.     This algorithm first selects a node with the minimum number of neighbors   and puts that node and its direct neighbors into one chunk. Next, it   selects one of the neighbors of the already selected nodes, adds the node   and its direct neighbors that are not part of one of the previous chunks,   into the next. After this sweep, neighboring nodes are grouped together.   To ensure a similar grouping on a more global level, this grouping is   called recursively on the groups so formed. The recursion stops when no   further grouping is possible. Eventually, the ordering obtained by this   method passes through the indices represented in the sparsity pattern in   a z-like way.     If the graph has two or more unconnected components, the algorithm will   number each component consecutively, starting with the components with   the lowest number of nodes.  
* [0.x.10]*
   Communicate rows in a dynamic sparsity pattern over MPI.      [2.x.14]  dsp A dynamic sparsity pattern that has been built locally   and for which we need to exchange entries with other processors to make   sure that each processor knows all the elements of the rows of a matrix   it stores and that may eventually be written to. This sparsity pattern   will be changed as a result of this function: All entries in rows that   belong to a different processor are sent to them and added there.      [2.x.15]  locally_owned_rows An IndexSet describing the rows owned by the   calling MPI process. The index set shall be one-to-one among the   processors in the communicator.      [2.x.16]  mpi_comm The MPI communicator shared between the processors that   participate in this operation.      [2.x.17]  locally_relevant_rows The range of elements stored on the local   MPI process. This should be the one used in the constructor of the   DynamicSparsityPattern, and should also be the locally relevant set. Only   rows contained in this set are checked in dsp for transfer. This function   needs to be used with  [2.x.18]  for it to work   correctly in a parallel computation.  
* [0.x.11]*
   Communicate rows in a dynamic sparsity pattern over MPI, similar to the   one above but using a vector `rows_per_cpu` containing the number of   rows per CPU for determining ownership. This is typically the value   returned by  [2.x.19]     [2.x.20] 
* 
*  -  given that the construction of the   input to this function involves all-to-all communication, it is typically   slower than the function above for more than a thousand of processes (and   quick enough also for small sizes).  
* [0.x.12]*
   Similar to the function above, but for BlockDynamicSparsityPattern   instead.      [2.x.21]  dsp The locally built sparsity pattern to be modified.      [2.x.22]  locally_owned_rows An IndexSet describing the rows owned by the   calling MPI process. The index set shall be one-to-one among the   processors in the communicator.      [2.x.23]  mpi_comm The MPI communicator to use.      [2.x.24]  locally_relevant_rows Typically the locally relevant DoFs.  
* [0.x.13]*
    [2.x.25]  Use the distribute_sparsity_pattern() with a single index set   for the present MPI process only.  
* [0.x.14]*
   Gather rows in a dynamic sparsity pattern over MPI.   The function is similar to  [2.x.26]    however instead of distributing sparsity stored in non-owned   rows on this MPI process, this function will gather sparsity   from other MPI processes and will add this to the local   DynamicSparsityPattern.      [2.x.27]  dsp A dynamic sparsity pattern that has been built locally and which   we need to extend according to the sparsity of rows stored on other MPI   processes.      [2.x.28]  locally_owned_rows An IndexSet describing the rows owned by the   calling MPI process. The index set shall be one-to-one among the   processors in the communicator.      [2.x.29]  mpi_comm The MPI communicator shared between the processors that   participate in this operation.      [2.x.30]  locally_relevant_rows The range of rows this MPI process needs to   gather. Only the part which is not included in the locally owned rows will   be used.  
* [0.x.15]*
    [2.x.31]  Use the gather_sparsity_pattern() method with the index set   for the present processor only.  
* [0.x.16]*
   Exception  
* [0.x.17]*
   Exception  
* [0.x.18]*
   Exception  
* [0.x.19]*
   Exception  
* [0.x.20]*
   Exception  
* [0.x.21]*
  [2.x.32] 

* 
* [0.x.22]

include/deal.II-translator/lac/tensor_product_matrix_0.txt
[0.x.0]*
 This is an abstract base class used for a special matrix class, namely the TensorProductMatrixSymmetricSum.
*  First, the base class acts like a container storing 1D mass matrices and 1D derivative matrices as well as the generalized eigenvalues and eigenvectors for each tensor direction. For a detailed definition of these matrices and corresponding generalized eigenproblems we refer to the main documentation of TensorProductMatrixSymmetricSum.
* 

* 
*  [2.x.0]  This base class has no functionality to calculate eigenvalues and eigenvectors for mass and derivative matrices given. The responsibility of initializing the data members completely lies with the derived class.
*  Second, it implements the matrix-vector product with the tensor product matrix (vmult()) and its inverse (apply_inverse()) as described in the main documentation of TensorProductMatrixSymmetricSum.
* 

* 
*  [2.x.1]  This class uses a temporary array for storing intermediate results that is a class member. A mutex is used to protect access to this array and ensure correct results. If several threads run parallel instances of this class, it is recommended that each threads holds its own matrix version.
*   [2.x.2]  dim Dimension of the problem. Currently, 1D, 2D, and 3D codes are implemented.
*   [2.x.3]  Number Arithmetic type of the underlying array elements.
*   [2.x.4]  n_rows_1d Compile-time number of rows of 1D matrices (only valid if the number of rows and columns coincide for each dimension). By default at
* 
*  - , which means that the number of rows is determined at run-time by means of the matrices passed to the reinit() function.

* 
* [0.x.1]*
   Type of matrix entries. This alias is analogous to <tt>value_type</tt>   in the standard library containers.  
* [0.x.2]*
   The static number of rows of the 1D matrices. For more details,   see the description of the template parameter <tt>n_rows_1d</tt>.  
* [0.x.3]*
   Return the number of rows of the tensor product matrix   resulting from the Kronecker product of 1D matrices, which is described   in the main documentation of TensorProductMatrixSymmetricSum.  
* [0.x.4]*
   Return the number of columns of the tensor product matrix   resulting from the Kronecker product of 1D matrices, which is described   in the main documentation of TensorProductMatrixSymmetricSum.  
* [0.x.5]*
   Implements a matrix-vector product with the underlying matrix as   described in the main documentation of TensorProductMatrixSymmetricSum.   This function is operating on ArrayView to allow checks of   array bounds with respect to  [2.x.5]  and  [2.x.6]   
* [0.x.6]*
   Implements a matrix-vector product with the underlying matrix as   described in the main documentation of TensorProductMatrixSymmetricSum.   This function is operating on ArrayView to allow checks of   array bounds with respect to  [2.x.7]  and  [2.x.8]   
* [0.x.7]*
   Default constructor.  
* [0.x.8]*
   An array containing a mass matrix for each tensor direction.  
* [0.x.9]*
   An array containing a derivative matrix for each tensor direction.  
* [0.x.10]*
   An array storing the generalized eigenvalues   for each tensor direction.  
* [0.x.11]*
   An array storing the generalized eigenvectors   for each tensor direction.  
* [0.x.12]*
   An array for temporary data.  
* [0.x.13]*
   A mutex that guards access to the array  [2.x.9]   
* [0.x.14]*
 This is a special matrix class defined as the tensor product (or Kronecker product) of 1D matrices of the type

* 
* [1.x.0]
*  in 2D and

* 
* [1.x.1]
*  in 3D. The typical application setting is a discretization of the Laplacian  [2.x.10]  on a Cartesian (axis-aligned) geometry, where it can be exactly represented by the Kronecker or tensor product of a 1D mass matrix  [2.x.11]  and a 1D Laplace matrix  [2.x.12]  in each tensor direction (due to symmetry  [2.x.13]  and  [2.x.14]  are the same in each dimension). The dimension of the resulting class is the product of the one-dimensional matrices.
*  This class implements two basic operations, namely the usual multiplication by a vector and the inverse. For both operations, fast tensorial techniques can be applied that implement the operator evaluation in  [2.x.15]  arithmetic operations, considerably less than  [2.x.16]  for the naive forward transformation and  [2.x.17]  for setting up the inverse of  [2.x.18] .
*  Interestingly, the exact inverse of the matrix  [2.x.19]  can be found through tensor products due to an article by [1.x.2] from 1964,

* 
* [1.x.3]
*  where  [2.x.20]  is the matrix of eigenvectors to the generalized eigenvalue problem in the given tensor direction  [2.x.21] :

* 
* [1.x.4]
*  and  [2.x.22]  is the diagonal matrix representing the generalized eigenvalues  [2.x.23] . Note that the vectors  [2.x.24]  are such that they simultaneously diagonalize  [2.x.25]  and  [2.x.26] , i.e.  [2.x.27]  and  [2.x.28] . This method of matrix inversion is called fast diagonalization method.
*  This class requires LAPACK support.
*  Note that this class allows for two modes of usage. The first is a use case with run time constants for the matrix dimensions that is achieved by setting the optional template parameter <tt>n_rows_1d</tt> to
* 
*  - . The second mode of usage that is faster allows to set the template parameter as a compile time constant, giving significantly faster code in particular for small sizes of the matrix.
*   [2.x.29]  dim Dimension of the problem. Currently, 1D, 2D, and 3D codes are implemented.
*   [2.x.30]  Number Arithmetic type of the underlying array elements. Note that the underlying LAPACK implementation supports only float and double numbers, so only these two types are currently supported by the generic class. Nevertheless, a template specialization for the vectorized types VectorizedArray<float> and VectorizedArray<double> exists. This is necessary to perform LAPACK calculations for each vectorization lane, i.e. for the supported float and double numbers.
*   [2.x.31]  n_rows_1d Compile-time number of rows of 1D matrices (only valid if the number of rows and columns coincide for each dimension). By default at
* 
*  - , which means that the number of rows is determined at run-time by means of the matrices passed to the reinit() function.

* 
* [0.x.15]*
   Default constructor.  
* [0.x.16]*
   Constructor that is equivalent to the empty constructor and   immediately calling   reinit(const  [2.x.32]  dim>&,const    [2.x.33]  dim>&).  
* [0.x.17]*
   Constructor that is equivalent to the empty constructor and   immediately calling   reinit(const  [2.x.34]     [2.x.35]   
* [0.x.18]*
   Constructor that is equivalent to the empty constructor and   immediately calling reinit(const Table<2,Number>&,const Table<2,Number>&).  
* [0.x.19]*
   Initializes the tensor product matrix by copying the arrays of 1D mass   matrices  [2.x.36]  and 1D derivative matrices  [2.x.37]  into its   base class counterparts, respectively, and by assembling the regarding   generalized eigenvalues and eigenvectors in    [2.x.38]    and  [2.x.39]  respectively.   Note that the current implementation requires each  [2.x.40]  to be symmetric   and positive definite and every  [2.x.41]  to be symmetric and invertible but   not necessarily positive definite.  
* [0.x.20]*
   This function is equivalent to the previous reinit() except that   the 1D matrices in  [2.x.42]  and  [2.x.43]  are   passed in terms of a FullMatrix, respectively.  
* [0.x.21]*
   This function is equivalent to the first reinit() except that   we consider the same 1D mass matrix  [2.x.44]  and the same 1D   derivative matrix  [2.x.45]  for each tensor direction.  
* [0.x.22]*
   A generic implementation of all reinit() functions based on   perfect forwarding, that allows to pass lvalue as well   as rvalue arguments.    [2.x.46]  MatrixArray Has to be convertible to the underlying   type of  [2.x.47]  and    [2.x.48]   
* [0.x.23]*
 This is the template specialization for VectorizedArray<Number> being the arithmetic template. For a detailed description see the main documentation of the generic TensorProductMatrixSymmetricSum class.

* 
* [0.x.24]*
   Default constructor.  
* [0.x.25]*
   Constructor that is equivalent to the empty constructor and   immediately calling   reinit(const  [2.x.49]  >, dim>&,const    [2.x.50]  >, dim>&).  
* [0.x.26]*
   Constructor that is equivalent to the empty constructor and   immediately calling   reinit(const Table<2,VectorizedArray<Number> >&,const   Table<2,VectorizedArray<Number> >&).  
* [0.x.27]*
   Initializes the tensor product matrix by copying the arrays of 1D mass   matrices  [2.x.51]  and 1D derivative matrices  [2.x.52]  into its   base class counterparts, respectively, and by assembling the regarding   generalized eigenvalues and eigenvectors in    [2.x.53]    and  [2.x.54]  respectively.   Note that the current implementation requires each  [2.x.55]  to be symmetric   and positive definite and every  [2.x.56]  to be symmetric and invertible but   not necessarily positive definite.  
* [0.x.28]*
   This function is equivalent to the previous reinit() except that   we consider the same 1D mass matrix  [2.x.57]  and the same 1D   derivative matrix  [2.x.58]  for each tensor direction.  
* [0.x.29]*
   A generic implementation of all reinit() functions based on   perfect forwarding, that allows to pass lvalue as well   as rvalue arguments.    [2.x.59]  MatrixArray Has to be convertible to the underlying   type of  [2.x.60]  and    [2.x.61]   
* [0.x.30]*
     Compute generalized eigenvalues and eigenvectors of the real     generalized symmetric eigenproblem  [2.x.62] . Since we are     operating on plain pointers we require the size of the matrices     beforehand. Note that the data arrays for the eigenvalues and     eigenvectors have to be initialized to a proper size, too. (no check of     array bounds possible)    
* [0.x.31]

include/deal.II-translator/lac/tridiagonal_matrix_0.txt
[0.x.0]!  [2.x.0]  Matrix1 [2.x.1] 

* 
* [0.x.1]*
 A quadratic tridiagonal matrix. That is, a matrix where all entries are zero, except the diagonal and the entries left and right of it.
*  The matrix has an additional symmetric mode, in which case only the upper triangle of the matrix is stored and mirrored to the lower one for matrix vector operations.
* 

* 
*  [2.x.2] 

* 
* [0.x.2]*
   Declare type for container size.  
* [0.x.3]*
    [2.x.3]  Constructors and initialization  
* [0.x.4]*
   Constructor generating an empty matrix of dimension <tt>n</tt>.  
* [0.x.5]*
   Reinitialize the matrix to a new size and reset all entries to zero. The   symmetry properties may be set as well.  
* [0.x.6]*
   Number of rows of this matrix. Note that the matrix is an [1.x.0] matrix.  
* [0.x.7]*
   Number of columns of this matrix. Note that the matrix is an [1.x.1] matrix.  
* [0.x.8]*
   Return whether the matrix contains only elements with value zero. This   function is mainly for internal consistency checks and should seldom be   used when not in debug mode since it uses quite some time.  
* [0.x.9]*
   Read-only access to a value. This is restricted to the case where   [1.x.2].  
* [0.x.10]*
   Read-write access to a value. This is restricted to the case where   [1.x.3].    
*  [2.x.4]  In case of symmetric storage technique, the entries [1.x.4]   and [1.x.5] are identified and [1.x.6] exist. This must be taken   into account if adding up is used for matrix assembling in order not to   obtain doubled entries.  
* [0.x.11]*
   Matrix-vector-multiplication. Multiplies <tt>v</tt> from the right and   stores the result in <tt>w</tt>.     If the optional parameter <tt>adding</tt> is <tt>true</tt>, the result is   added to <tt>w</tt>.     Source and destination must not be the same vector.  
* [0.x.12]*
   Adding Matrix-vector-multiplication. Same as vmult() with parameter   <tt>adding=true</tt>, but widely used in <tt>deal.II</tt> classes.     Source and destination must not be the same vector.  
* [0.x.13]*
   Transpose matrix-vector-multiplication. Multiplies <tt>v<sup>T</sup></tt>   from the left and stores the result in <tt>w</tt>.     If the optional parameter <tt>adding</tt> is <tt>true</tt>, the result is   added to <tt>w</tt>.     Source and destination must not be the same vector.  
* [0.x.14]*
   Adding transpose matrix-vector-multiplication. Same as Tvmult() with   parameter <tt>adding=true</tt>, but widely used in <tt>deal.II</tt>   classes.     Source and destination must not be the same vector.  
* [0.x.15]*
   Build the matrix scalar product <tt>u^T M v</tt>. This function is mostly   useful when building the cellwise scalar product of two functions in the   finite element context.  
* [0.x.16]*
   Return the square of the norm of the vector <tt>v</tt> with respect to   the norm induced by this matrix, i.e. [1.x.7]. This is useful, e.g.   in the finite element context, where the [1.x.8] norm of a   function equals the matrix norm with respect to the mass matrix of the   vector representing the nodal values of the finite element function.     Obviously, the matrix needs to be quadratic for this operation.  
* [0.x.17]*
   Compute the eigenvalues of the symmetric tridiagonal matrix.    
*  [2.x.5]  This function requires configuration of deal.II with LAPACK   support. Additionally, the matrix must use symmetric storage technique.  
* [0.x.18]*
   After calling compute_eigenvalues(), you can access each eigenvalue here.  
* [0.x.19]*
   Output of the matrix in user-defined format.  
* [0.x.20]*
   The diagonal entries.  
* [0.x.21]*
   The entries left of the diagonal. The entry with index zero is always   zero, since the first row has no entry left of the diagonal. Therefore,   the length of this vector is the same as that of #diagonal.     The length of this vector is zero for symmetric storage. In this case,   the second element of #left is identified with the first element of   #right.  
* [0.x.22]*
   The entries right of the diagonal. The last entry is always zero, since   the last row has no entry right of the diagonal. Therefore, the length of   this vector is the same as that of #diagonal.  
* [0.x.23]*
   If this flag is true, only the entries to the right of the diagonal are   stored and the matrix is assumed symmetric.  
* [0.x.24]*
   The state of the matrix. Normally, the state is  [2.x.6]    indicating that the object can be used for regular matrix operations.     See explanation of this data type for details.  
* [0.x.25]

include/deal.II-translator/lac/trilinos_block_sparse_matrix_0.txt
[0.x.0]!  [2.x.0]  TrilinosWrappers   [2.x.1]   
* [0.x.1]*
   Blocked sparse matrix based on the  [2.x.2]  class.   This class implements the functions that are specific to the Trilinos   SparseMatrix base objects for a blocked sparse matrix, and leaves the   actual work relaying most of the calls to the individual blocks to the   functions implemented in the base class. See there also for a description   of when this class is useful.     In contrast to the deal.II-type SparseMatrix class, the Trilinos matrices   do not have external objects for the sparsity patterns. Thus, one does   not determine the size of the individual blocks of a block matrix of this   type by attaching a block sparsity pattern, but by calling reinit() to   set the number of blocks and then by setting the size of each block   separately. In order to fix the data structures of the block matrix, it   is then necessary to let it know that we have changed the sizes of the   underlying matrices. For this, one has to call the collect_sizes()   function, for much the same reason as is documented with the   BlockSparsityPattern class.    
*  [2.x.3]     [2.x.4]  "Block (linear algebra)"  
* [0.x.2]*
     Typedef the base class for simpler access to its own alias.    
* [0.x.3]*
     Typedef the type of the underlying matrix.    
* [0.x.4]*
     Import the alias from the base class.    
* [0.x.5]*
     Constructor; initializes the matrix to be empty, without any structure,     i.e.  the matrix is not usable at all. This constructor is therefore     only useful for matrices which are members of a class. All other     matrices should be created at a point in the data flow where all     necessary information is available.         You have to initialize the matrix before usage with     reinit(BlockSparsityPattern). The number of blocks per row and column     are then determined by that function.    
* [0.x.6]*
     Destructor.    
* [0.x.7]*
     Pseudo copy operator only copying empty objects. The sizes of the block     matrices need to be the same.    
* [0.x.8]*
     This operator assigns a scalar to a matrix. Since this does usually not     make much sense (should we set all matrix entries to this value? Only     the nonzero entries of the sparsity pattern?), this operation is only     allowed if the actual value to be assigned is zero. This operator only     exists to allow for the obvious notation <tt>matrix=0</tt>, which sets     all elements of the matrix to zero, but keep the sparsity pattern     previously used.    
* [0.x.9]*
     Resize the matrix, by setting the number of block rows and columns.     This deletes all blocks and replaces them with uninitialized ones, i.e.     ones for which also the sizes are not yet set. You have to do that by     calling the  [2.x.5]  functions of the blocks themselves. Do not forget     to call collect_sizes() after that on this object.         The reason that you have to set sizes of the blocks yourself is that     the sizes may be varying, the maximum number of elements per row may be     varying, etc. It is simpler not to reproduce the interface of the  [2.x.6]      SparsityPattern class here but rather let the user call whatever     function they desire.    
* [0.x.10]*
     Resize the matrix, by using an array of index sets to determine the     %parallel distribution of the individual matrices. This function     assumes that a quadratic block matrix is generated.    
* [0.x.11]*
     Resize the matrix and initialize it by the given sparsity pattern.     Since no distribution map is given, the result is a block matrix for     which all elements are stored locally.    
* [0.x.12]*
     This function initializes the Trilinos matrix using the deal.II sparse     matrix and the entries stored therein. It uses a threshold to copy only     elements whose modulus is larger than the threshold (so zeros in the     deal.II matrix can be filtered away).    
* [0.x.13]*
     This function initializes the Trilinos matrix using the deal.II sparse     matrix and the entries stored therein. It uses a threshold to copy only     elements whose modulus is larger than the threshold (so zeros in the     deal.II matrix can be filtered away). Since no Epetra_Map is given, all     the elements will be locally stored.    
* [0.x.14]*
     Return the state of the matrix, i.e., whether compress() needs to be     called after an operation requiring data exchange. Does only return     non-true values when used in <tt>debug</tt> mode, since it is quite     expensive to keep track of all operations that lead to the need for     compress().    
* [0.x.15]*
     This function collects the sizes of the sub-objects and stores them in     internal arrays, in order to be able to relay global indices into the     matrix to indices into the subobjects. Youmust* call this function     each time after you have changed the size of the sub-objects. Note that     this is a collective operation, i.e., it needs to be called on all MPI     processes. This command internally calls the method     <tt>compress()</tt>, so you don't need to call that function in case     you use <tt>collect_sizes()</tt>.    
* [0.x.16]*
     Return the total number of nonzero elements of this matrix (summed     over all MPI processes).    
* [0.x.17]*
     Return the MPI communicator object in use with this matrix.    
* [0.x.18]*
     Return the partitioning of the domain space for the individual blocks of     this matrix, i.e., the partitioning of the block vectors this matrix has     to be multiplied with.    
* [0.x.19]*
     Return the partitioning of the range space for the individual blocks of     this matrix, i.e., the partitioning of the block vectors that result     from matrix-vector products.    
* [0.x.20]*
     Matrix-vector multiplication: let  [2.x.7]  with  [2.x.8]  being this     matrix. The vector types can be block vectors or non-block vectors     (only if the matrix has only one row or column, respectively), and need     to define  [2.x.9]     
* [0.x.21]*
     Matrix-vector multiplication: let  [2.x.10]  with  [2.x.11]  being this     matrix. This function does the same as vmult() but takes the transposed     matrix.    
* [0.x.22]*
     Compute the residual of an equation [1.x.0], where the residual is     defined to be [1.x.1]. Write the residual into  [2.x.12]  The     [1.x.2] norm of the residual vector is returned.         Source [1.x.3] and destination [1.x.4] must not be the same vector.         Note that both vectors have to be distributed vectors generated using     the same Map as was used for the matrix.         This function only applicable if the matrix only has one block row.    
* [0.x.23]*
     Compute the residual of an equation [1.x.5], where the residual is     defined to be [1.x.6]. Write the residual into  [2.x.13]  The     [1.x.7] norm of the residual vector is returned.         This function is only applicable if the matrix only has one block row.    
* [0.x.24]*
     Compute the residual of an equation [1.x.8], where the residual is     defined to be [1.x.9]. Write the residual into  [2.x.14]  The     [1.x.10] norm of the residual vector is returned.         This function is only applicable if the matrix only has one block column.    
* [0.x.25]*
     Compute the residual of an equation [1.x.11], where the residual is     defined to be [1.x.12]. Write the residual into  [2.x.15]  The     [1.x.13] norm of the residual vector is returned.         This function is only applicable if the matrix only has one block.    
* [0.x.26]*
     Make the clear() function in the base class visible, though it is     protected.    
* [0.x.27]*
      [2.x.16]  Exceptions      [2.x.17]     
* [0.x.28]*
     Exception    
* [0.x.29]*
     Exception    
* [0.x.30]*
     Internal version of (T)vmult with two block vectors    
* [0.x.31]*
     Internal version of (T)vmult where the source vector is a block vector     but the destination vector is a non-block vector    
* [0.x.32]*
     Internal version of (T)vmult where the source vector is a non-block     vector but the destination vector is a block vector    
* [0.x.33]*
     Internal version of (T)vmult where both source vector and the     destination vector are non-block vectors (only defined if the matrix     consists of only one block)    
* [0.x.34]*
       This is an extension class to BlockLinearOperators for Trilinos block       sparse matrices.            
*  [2.x.18]  This class does very little at the moment other than to check       that the correct Payload type for each subblock has been chosen       correctly. Further extensions to the class may be necessary in the       future in order to add further functionality to BlockLinearOperators       while retaining compatibility with the Trilinos sparse matrix and       preconditioner classes.                  
*  [2.x.19]       
* [0.x.35]*
         Type of payload held by each subblock        
* [0.x.36]*
         Default constructor                 This simply checks that the payload for each block has been chosen         correctly (i.e. is of type TrilinosPayload). Apart from this, this         class does not do anything in particular and needs no special         configuration, we have only one generic constructor that can be         called under any conditions.        
* [0.x.37]

include/deal.II-translator/lac/trilinos_epetra_communication_pattern_0.txt
[0.x.0]*
     This class implements a wrapper to a Trilinos Epetra_Import object,     for use in places where a  [2.x.0]  object     is required.    
* [0.x.1]*
       Initialize the communication pattern. The first argument  [2.x.1]        vector_space_vector_index_set is the index set associated to a       VectorSpaceVector object. The second argument  [2.x.2]        read_write_vector_index_set is the index set associated to a       ReadWriteVector object.      
* [0.x.2]*
       Reinitialize the object.      
* [0.x.3]*
       Return the underlying MPI communicator.      
* [0.x.4]*
       Return the underlying Epetra_Import object.      
* [0.x.5]*
       Shared pointer to the MPI communicator used.      
* [0.x.6]*
       Shared pointer to the Epetra_Import object used.      
* [0.x.7]

include/deal.II-translator/lac/trilinos_epetra_vector_0.txt
[0.x.0]*
   A namespace for classes that provide wrappers for Trilinos' Epetra vectors.     This namespace provides wrappers for the Epetra_FEVector class from the   Epetra package (https://trilinos.github.io/epetra.html) that is part of   Trilinos.  
* [0.x.1]*
     This class implements a wrapper to the Trilinos distributed vector     class Epetra_FEVector. This class is derived from the      [2.x.0]  class. Note however that Epetra only     works with Number = double. This class requires Trilinos to be compiled     with MPI support.        
*  [2.x.1]     
*  [2.x.2]     
* [0.x.2]*
       Constructor. Create a vector of dimension zero.      
* [0.x.3]*
       Copy constructor. Sets the dimension and the partitioning to that of       the given vector and copies all elements.      
* [0.x.4]*
       This constructor takes an IndexSet that defines how to distribute the       individual components among the MPI processors. Since it also       includes information about the size of the vector, this is all we       need to generate a %parallel vector.      
* [0.x.5]*
       Reinit functionality. This function destroys the old vector content       and generates a new one based on the input partitioning. The flag       <tt>omit_zeroing_entries</tt> determines whether the vector should be       filled with zero (false) or left untouched (true).      
* [0.x.6]*
       Change the dimension to that of the vector  [2.x.3]  The elements of  [2.x.4]  are not       copied.      
* [0.x.7]*
       Copy function. This function takes a Vector and copies all the       elements. The Vector will have the same parallel distribution as  [2.x.5]        V.      
* [0.x.8]*
       Sets all elements of the vector to the scalar  [2.x.6]  This operation is       only allowed if  [2.x.7]  is equal to zero.      
* [0.x.9]*
       Imports all the elements present in the vector's IndexSet from the       input vector  [2.x.8]   [2.x.9]   [2.x.10]  is used to decide if       the elements in  [2.x.11]  should be added to the current vector or replace the       current elements. The last parameter can be used if the same       communication pattern is used multiple times. This can be used to       improve performance.      
* [0.x.10]*
       Multiply the entire vector by a fixed factor.      
* [0.x.11]*
       Divide the entire vector by a fixed factor.      
* [0.x.12]*
       Add the vector  [2.x.12]  to the present one.      
* [0.x.13]*
       Subtract the vector  [2.x.13]  from the present one.      
* [0.x.14]*
       Return the scalar product of two vectors. The vectors need to have the       same layout.      
* [0.x.15]*
       Add  [2.x.14]  to all components. Note that  [2.x.15]  a scalar not a vector.      
* [0.x.16]*
       Simple addition of a multiple of a vector, i.e. <tt>*this +=       a*V</tt>. The vectors need to have the same layout.      
* [0.x.17]*
       Multiple addition of multiple of a vector, i.e. <tt>*this> +=       a*V+b*W</tt>. The vectors need to have the same layout.      
* [0.x.18]*
       Scaling and simple addition of a multiple of a vector, i.e. <tt>*this       = s*(*this)+a*V</tt>.      
* [0.x.19]*
       Scale each element of this vector by the corresponding element in the       argument. This function is mostly meant to simulate multiplication       (and immediate re-assignment) by a diagonal scaling matrix. The       vectors need to have the same layout.      
* [0.x.20]*
       Assignment <tt>*this = a*V</tt>.      
* [0.x.21]*
       Return whether the vector contains only elements with value zero.      
* [0.x.22]*
       Return the mean value of the element of this vector.      
* [0.x.23]*
       Return the l<sub>1</sub> norm of the vector (i.e., the sum of the       absolute values of all entries among all processors).      
* [0.x.24]*
       Return the l<sub>2</sub> norm of the vector (i.e., the square root of       the sum of the square of all entries among all processors).      
* [0.x.25]*
       Return the maximum norm of the vector (i.e., the maximum absolute value       among all entries and among all processors).      
* [0.x.26]*
       Performs a combined operation of a vector addition and a subsequent       inner product, returning the value of the inner product. In other       words, the result of this function is the same as if the user called      
* [1.x.0]
*              The reason this function exists is that this operation involves less       memory transfer than calling the two functions separately. This       method only needs to load three vectors,  [2.x.16]   [2.x.17]   [2.x.18]  whereas       calling separate methods means to load the calling vector  [2.x.19]        twice. Since most vector operations are memory transfer limited, this       reduces the time by 25\% (or 50\% if  [2.x.20]  equals  [2.x.21]              The vectors need to have the same layout.             For complex-valued vectors, the scalar product in the second step is       implemented as        [2.x.22] .      
* [0.x.27]*
       This function always returns false and is present only for backward       compatibility.      
* [0.x.28]*
       Return the global size of the vector, equal to the sum of the number of       locally owned indices among all processors.      
* [0.x.29]*
       Return the local size of the vector, i.e., the number of indices       owned locally.      
* [0.x.30]*
       Return the MPI communicator object in use with this object.      
* [0.x.31]*
       Return an index set that describes which elements of this vector are       owned by the current processor. As a consequence, the index sets       returned on different processors if this is a distributed vector will       form disjoint sets that add up to the complete index set. Obviously, if       a vector is created on only one processor, then the result would       satisfy      
* [1.x.1]
*       
* [0.x.32]*
       Return a const reference to the underlying Trilinos       Epetra_FEVector class.      
* [0.x.33]*
       Return a (modifiable) reference to the underlying Trilinos       Epetra_FEVector class.      
* [0.x.34]*
       Prints the vector to the output stream  [2.x.23]       
* [0.x.35]*
       Return the memory consumption of this class in bytes.      
* [0.x.36]*
       The vectors have different partitioning, i.e. their IndexSet objects       don't represent the same indices.      
* [0.x.37]*
       Attempt to perform an operation between two incompatible vector types.            
*  [2.x.24]       
* [0.x.38]*
       Exception thrown by an error in Trilinos.            
*  [2.x.25]       
* [0.x.39]*
       Create the CommunicationPattern for the communication between the       IndexSet  [2.x.26]  and the current vector based       on the communicator  [2.x.27]       
* [0.x.40]*
       Pointer to the actual Epetra vector object.      
* [0.x.41]*
       IndexSet of the elements of the last imported vector.      
* [0.x.42]*
       CommunicationPattern for the communication between the       source_stored_elements IndexSet and the current vector.      
* [0.x.43]*
 Declare  [2.x.28]  as distributed vector.

* 
* [0.x.44]

include/deal.II-translator/lac/trilinos_index_access_0.txt
[0.x.0]*
   A helper function that queries the size of an Epetra_BlockMap object   and calls either the 32 or 64 bit function to get the number of global   elements in the map.  
* [0.x.1]*
   A helper function that finds the minimum global index value on the   calling processor by calling either the 32 or 64 bit function.  
* [0.x.2]*
   A helper function that finds the maximum global index value on the   calling processor by calling either the 32 or 64 bit function.  
* [0.x.3]*
   A helper function that converts a local index to a global one calling   either the 32 or 64 bit function.  
* [0.x.4]*
   A helper function that returns a pointer to the array containing the   global indices assigned to the current process by calling either the 32   or 64 bit function.  
* [0.x.5]*
   A helper function that finds the global number of rows by calling   either the 32 or 64 bit function.  
* [0.x.6]*
   A helper function that finds the global number of columns by calling   either the 32 or 64 bit function.  
* [0.x.7]*
   A helper function that finds the number of global entries by calling   either the 32 or 64 bit function.  
* [0.x.8]*
   A helper function that finds the global row index by calling   either the 32 or 64 bit function.  
* [0.x.9]*
   A helper function that finds the global column index by calling   either the 32 or 64 bit function.  
* [0.x.10]*
   A helper function that finds the global length of a vector by calling   either the 32 or 64 bit function.  
* [0.x.11]*
   A helper function that finds the global number of rows by calling   either the 32 or 64 bit function.  
* [0.x.12]

include/deal.II-translator/lac/trilinos_linear_operator_0.txt
[0.x.0]*
    [2.x.0]  Creation of a LinearOperator  
* [0.x.1]*
    [2.x.1]  LinearOperator     A function that encapsulates generic  [2.x.2]  objects, based on an    [2.x.3]  that act on a compatible Vector type into a   LinearOperator.     This function is the equivalent of the  [2.x.4]  but   ensures full compatibility with Trilinos operations by preselecting the   appropriate template parameters.      
*  [2.x.5]   
* [0.x.2]*
    [2.x.6]  LinearOperator     A function that encapsulates generic  [2.x.7]  objects that act on a   compatible Vector type into a LinearOperator.     This function is the equivalent of the  [2.x.8]  but   ensures full compatibility with Trilinos operations by preselecting the   appropriate template parameters.      
*  [2.x.9]   
* [0.x.3]*
    [2.x.10]  Creation of a BlockLinearOperator  
* [0.x.4]*
    [2.x.11]  BlockLinearOperator     A function that encapsulates a  [2.x.12]  into a BlockLinearOperator.     This function is the equivalent of the  [2.x.13]  but   ensures full compatibility with Trilinos operations by preselecting the   appropriate template parameters.      
*  [2.x.14]   
* [0.x.5]*
    [2.x.15]  BlockLinearOperator     A variant of above function that builds up a block diagonal linear operator   from an array  [2.x.16]  of diagonal elements (off-diagonal blocks are assumed   to be 0).     This function is the equivalent of the  [2.x.17]  but   ensures full compatibility with Trilinos operations by preselecting the   appropriate template parameters.      
*  [2.x.18]   
* [0.x.6]*
    [2.x.19]  BlockLinearOperator     This function extracts the diagonal blocks of  [2.x.20]  (either a   block matrix type or a BlockLinearOperator) and creates a   BlockLinearOperator with the diagonal. Off-diagonal elements are   initialized as null_operator (with correct reinit_range_vector and   reinit_domain_vector methods).     This function is the equivalent of the  [2.x.21]  but   ensures full compatibility with Trilinos operations by preselecting the   appropriate template parameters.      
*  [2.x.22]   
* [0.x.7]*
    [2.x.23]  BlockLinearOperator     A variant of above function that builds up a block diagonal linear operator   from an array  [2.x.24]  of diagonal elements (off-diagonal blocks are assumed   to be 0).     This function is the equivalent of the  [2.x.25]  but   ensures full compatibility with Trilinos operations by preselecting the   appropriate template parameters.      
*  [2.x.26]   
* [0.x.8]

include/deal.II-translator/lac/trilinos_parallel_block_vector_0.txt
[0.x.0]!  [2.x.0]  TrilinosWrappers [2.x.1] 

* 
* [0.x.1]*
     An implementation of block vectors based on the vector class     implemented in TrilinosWrappers. While the base class provides for most     of the interface, this class handles the actual allocation of vectors     and provides functions that are specific to the underlying vector type.         The model of distribution of data is such that each of the blocks is     distributed across all MPI processes named in the MPI communicator.     I.e. we don't just distribute the whole vector, but each component. In     the constructors and reinit() functions, one therefore not only has to     specify the sizes of the individual blocks, but also the number of     elements of each of these blocks to be stored on the local process.        
*  [2.x.2]     
*  [2.x.3]       [2.x.4]  "Block (linear algebra)"    
* [0.x.2]*
       Typedef the base class for simpler access to its own alias.      
* [0.x.3]*
       Typedef the type of the underlying vector.      
* [0.x.4]*
       Import the alias from the base class.      
* [0.x.5]*
       Default constructor. Generate an empty vector without any blocks.      
* [0.x.6]*
       Constructor. Generate a block vector with as many blocks as there are       entries in  [2.x.5]   Each IndexSet together with the MPI       communicator contains the layout of the distribution of data among       the MPI processes.      
* [0.x.7]*
       Creates a BlockVector with ghost elements. See the respective       reinit() method for more details.  [2.x.6]  may contain any       elements in  [2.x.7]  they will be ignored.      
* [0.x.8]*
       Copy-Constructor. Set all the properties of the parallel vector to       those of the given argument and copy the elements.      
* [0.x.9]*
       Move constructor. Creates a new vector by stealing the internal data       of the vector  [2.x.8]       
* [0.x.10]*
       Creates a block vector consisting of <tt>num_blocks</tt> components,       but there is no content in the individual components and the user has       to fill appropriate data using a reinit of the blocks.      
* [0.x.11]*
       Destructor. Clears memory      
* [0.x.12]*
       Copy operator: fill all components of the vector that are locally       stored with the given scalar value.      
* [0.x.13]*
       Copy operator for arguments of the same type.      
* [0.x.14]*
       Move the given vector. This operator replaces the present vector with        [2.x.9]  by efficiently swapping the internal data structures.      
* [0.x.15]*
       Another copy function. This one takes a deal.II block vector and       copies it into a TrilinosWrappers block vector. Note that the number       of blocks has to be the same in the vector as in the input vector.       Use the reinit() command for resizing the BlockVector or for changing       the internal structure of the block components.             Since Trilinos only works on doubles, this function is limited to       accept only one possible number type in the deal.II vector.      
* [0.x.16]*
       Reinitialize the BlockVector to contain as many blocks as there are       index sets given in the input argument, according to the parallel       distribution of the individual components described in the maps.             If <tt>omit_zeroing_entries==false</tt>, the vector is filled with       zeros.      
* [0.x.17]*
       Reinit functionality. This function destroys the old vector content       and generates a new one based on the input partitioning. In addition       to just specifying one index set as in all the other methods above,       this method allows to supply an additional set of ghost entries.       There are two different versions of a vector that can be created. If       the flag  [2.x.10]  is set to  [2.x.11]  the vector only       allows read access to the joint set of  [2.x.12]  and        [2.x.13]  The effect of the reinit method is then equivalent       to calling the other reinit method with an index set containing both       the locally owned entries and the ghost entries.             If the flag  [2.x.14]  is set to true, this creates an       alternative storage scheme for ghost elements that allows multiple       threads to write into the vector (for the other reinit methods, only       one thread is allowed to write into the ghost entries at a time).      
* [0.x.18]*
       Change the dimension to that of the vector <tt>V</tt>. The same       applies as for the other reinit() function.             The elements of <tt>V</tt> are not copied, i.e.  this function is the       same as calling <tt>reinit (V.size(), omit_zeroing_entries)</tt>.             Note that you must call this (or the other reinit() functions)       function, rather than calling the reinit() functions of an individual       block, to allow the block vector to update its caches of vector       sizes. If you call reinit() on one of the blocks, then subsequent       actions on this object may yield unpredictable results since they may       be routed to the wrong block.      
* [0.x.19]*
       Change the number of blocks to <tt>num_blocks</tt>. The individual       blocks will get initialized with zero size, so it is assumed that the       user resizes the individual blocks by herself in an appropriate way,       and calls <tt>collect_sizes</tt> afterwards.      
* [0.x.20]*
       This reinit function is meant to be used for parallel calculations       where some non-local data has to be used. The typical situation where       one needs this function is the call of the        [2.x.15]  function (or of some derivatives)       in parallel. Since it is usually faster to retrieve the data in       advance, this function can be called before the assembly forks out to       the different processors. What this function does is the following:       It takes the information in the columns of the given matrix and looks       which data couples between the different processors. That data is       then queried from the input vector. Note that you should not write to       the resulting vector any more, since the some data can be stored       several times on different processors, leading to unpredictable       results. In particular, such a vector cannot be used for matrix-       vector products as for example done during the solution of linear       systems.      
* [0.x.21]*
       Return if this Vector contains ghost elements.              [2.x.16]         [2.x.17]  "vectors with ghost elements"      
* [0.x.22]*
       Swap the contents of this vector and the other vector <tt>v</tt>. One       could do this operation with a temporary variable and copying over       the data elements, but this function is significantly more efficient       since it only swaps the pointers to the data of the two vectors and       therefore does not need to allocate temporary storage and move data       around.             Limitation: right now this function only works if both vectors have       the same number of blocks. If needed, the numbers of blocks should be       exchanged, too.             This function is analogous to the swap() function of all C++       standard containers. Also, there is a global function swap(u,v) that       simply calls <tt>u.swap(v)</tt>, again in analogy to standard       functions.      
* [0.x.23]*
       Print to a stream.      
* [0.x.24]*
       Exception      
* [0.x.25]*
       Exception      
* [0.x.26]*
     Global function which overloads the default implementation of the C++     standard library which uses a temporary object. The function simply     exchanges the data of the two vectors.          [2.x.18]   [2.x.19]     
* [0.x.27]*
     A helper class used internally in linear_operator.h. Specialization for      [2.x.20]     
* [0.x.28]*
 Declare  [2.x.21]  as distributed vector.

* 
* [0.x.29]

include/deal.II-translator/lac/trilinos_precondition_0.txt
[0.x.0]!  [2.x.0]  TrilinosWrappers [2.x.1] 

* 
* [0.x.1]*
   The base class for all preconditioners based on Trilinos sparse matrices.    
*  [2.x.2]   
*  [2.x.3]   
* [0.x.2]*
     Declare the type for container size.    
* [0.x.3]*
     Standardized data struct to pipe additional flags to the     preconditioner.    
* [0.x.4]*
     Constructor. Does not do anything. The <tt>initialize</tt> function of     the derived classes will have to create the preconditioner from a given     sparse matrix.    
* [0.x.5]*
     Copy constructor.    
* [0.x.6]*
     Destructor.    
* [0.x.7]*
     Destroys the preconditioner, leaving an object like just after having     called the constructor.    
* [0.x.8]*
     Return the MPI communicator object in use with this matrix.    
* [0.x.9]*
     Sets an internal flag so that all operations performed by the matrix,     i.e., multiplications, are done in transposed order. However, this does     not reshape the matrix to transposed form directly, so care should be     taken when using this flag.        
*  [2.x.4]  Calling this function any even number of times in succession will     return the object to its original state.    
* [0.x.10]*
     Apply the preconditioner.    
* [0.x.11]*
     Apply the transpose preconditioner.    
* [0.x.12]*
     Apply the preconditioner on deal.II data structures instead of the ones     provided in the Trilinos wrapper class.    
* [0.x.13]*
     Apply the transpose preconditioner on deal.II data structures instead     of the ones provided in the Trilinos wrapper class.    
* [0.x.14]*
     Apply the preconditioner on deal.II parallel data structures instead of     the ones provided in the Trilinos wrapper class.    
* [0.x.15]*
     Apply the transpose preconditioner on deal.II parallel data structures     instead of the ones provided in the Trilinos wrapper class.    
* [0.x.16]*
      [2.x.5]  Access to underlying Trilinos data    
* [0.x.17]*
         Calling this function from an uninitialized object will cause an     exception.    
* [0.x.18]*
      [2.x.6]  Partitioners    
* [0.x.19]*
     Return the partitioning of the domain space of this matrix, i.e., the     partitioning of the vectors this matrix has to be multiplied with.    
* [0.x.20]*
     Return the partitioning of the range space of this matrix, i.e., the     partitioning of the vectors that are result from matrix-vector     products.    
* [0.x.21]*
      [2.x.7]  Exceptions    
* [0.x.22]*
     Exception.    
* [0.x.23]*
     This is a pointer to the preconditioner object that is used when     applying the preconditioner.    
* [0.x.24]*
     Internal communication pattern in case the matrix needs to be copied     from deal.II format.    
* [0.x.25]*
     Internal Trilinos map in case the matrix needs to be copied from     deal.II format.    
* [0.x.26]*
   A wrapper class for a (pointwise) Jacobi preconditioner for Trilinos   matrices. This preconditioner works both in serial and in parallel,   depending on the matrix it is based on.     The AdditionalData data structure allows to set preconditioner options.   For the Jacobi preconditioner, these options are the damping parameter   <tt>omega</tt> and a <tt>min_diagonal</tt> argument that can be used to   make the preconditioner work even if the matrix contains some zero   elements on the diagonal. The default settings are 1 for the damping   parameter and zero for the diagonal augmentation.    
*  [2.x.8]   
*  [2.x.9]   
* [0.x.27]*
     Standardized data struct to pipe additional flags to the     preconditioner. The parameter <tt>omega</tt> specifies the relaxation     parameter in the Jacobi preconditioner. The parameter     <tt>min_diagonal</tt> can be used to make the application of the     preconditioner also possible when some diagonal elements are zero. In a     default application this would mean that we divide by zero, so by     setting the parameter <tt>min_diagonal</tt> to a small nonzero value     the SOR will work on a matrix that is not too far away from the one we     want to treat.    
* [0.x.28]*
       Constructor. By default, set the damping parameter to one, and do not       modify the diagonal.      
* [0.x.29]*
       This specifies the relaxation parameter in the Jacobi preconditioner.      
* [0.x.30]*
       This specifies the minimum value the diagonal elements should have.       This might be necessary when the Jacobi preconditioner is used on       matrices with zero diagonal elements. In that case, a straight-       forward application would not be possible since we would divide by       zero.      
* [0.x.31]*
       Sets how many times the given operation should be applied during the       vmult() operation.      
* [0.x.32]*
     Take the sparse matrix the preconditioner object should be built of,     and additional flags (damping parameter, etc.) if there are any.    
* [0.x.33]*
   A wrapper class for a (pointwise) SSOR preconditioner for Trilinos   matrices. This preconditioner works both in serial and in parallel,   depending on the matrix it is based on.     The AdditionalData data structure allows to set preconditioner options.   For the SSOR preconditioner, these options are the damping/relaxation   parameter <tt>omega</tt>, a <tt>min_diagonal</tt> argument that can be   used to make the preconditioner work even if the matrix contains some   zero elements on the diagonal, and a parameter <tt>overlap</tt> that   determines if and how much overlap there should be between the matrix   partitions on the various MPI processes. The default settings are 1 for   the relaxation parameter, 0 for the diagonal augmentation and 0 for the   overlap.     Note that a parallel application of the SSOR preconditioner is actually a   block-Jacobi preconditioner with block size equal to the local matrix   size. Spoken more technically, this parallel operation is an [1.x.0] with an SSOR  [2.x.10] approximate solve [2.x.11]  as inner   solver, based on the outer parallel partitioning.    
*  [2.x.12]   
*  [2.x.13]   
* [0.x.34]*
     Standardized data struct to pipe additional flags to the     preconditioner. The parameter <tt>omega</tt> specifies the relaxation     parameter in the SSOR preconditioner. The parameter     <tt>min_diagonal</tt> can be used to make the application of the     preconditioner also possible when some diagonal elements are zero. In a     default application this would mean that we divide by zero, so by     setting the parameter <tt>min_diagonal</tt> to a small nonzero value     the SOR will work on a matrix that is not too far away from the one we     want to treat. Finally, <tt>overlap</tt> governs the overlap of the     partitions when the preconditioner runs in parallel, forming a so-     called additive Schwarz preconditioner.    
* [0.x.35]*
       Constructor. By default, set the damping parameter to one, we do not       modify the diagonal, and there is no overlap (i.e. in parallel, we       run a BlockJacobi preconditioner, where each block is inverted       approximately by an SSOR).      
* [0.x.36]*
       This specifies the (over-) relaxation parameter in the SSOR       preconditioner.      
* [0.x.37]*
       This specifies the minimum value the diagonal elements should have.       This might be necessary when the SSOR preconditioner is used on       matrices with zero diagonal elements. In that case, a straight-       forward application would not be possible since we divide by the       diagonal element.      
* [0.x.38]*
       This determines how large the overlap of the local matrix portions on       each processor in a parallel application should be.      
* [0.x.39]*
       Sets how many times the given operation should be applied during the       vmult() operation.      
* [0.x.40]*
     Take the sparse matrix the preconditioner object should be built of,     and additional flags (damping parameter, overlap in parallel     computations, etc.) if there are any.    
* [0.x.41]*
   A wrapper class for a (pointwise) SOR preconditioner for Trilinos   matrices. This preconditioner works both in serial and in parallel,   depending on the matrix it is based on.     The AdditionalData data structure allows to set preconditioner options.   For the SOR preconditioner, these options are the damping/relaxation   parameter <tt>omega</tt>, a <tt>min_diagonal</tt> argument that can be   used to make the preconditioner work even if the matrix contains some   zero elements on the diagonal, and a parameter <tt>overlap</tt> that   determines if and how much overlap there should be between the matrix   partitions on the various MPI processes. The default settings are 1 for   the relaxation parameter, 0 for the diagonal augmentation and 0 for the   overlap.     Note that a parallel application of the SOR preconditioner is actually a   block-Jacobi preconditioner with block size equal to the local matrix   size. Spoken more technically, this parallel operation is an [1.x.1] with an SOR  [2.x.14] approximate solve [2.x.15]  as inner   solver, based on the outer parallel partitioning.    
*  [2.x.16]   
*  [2.x.17]   
* [0.x.42]*
     Standardized data struct to pipe additional flags to the     preconditioner. The parameter <tt>omega</tt> specifies the relaxation     parameter in the SOR preconditioner. The parameter     <tt>min_diagonal</tt> can be used to make the application of the     preconditioner also possible when some diagonal elements are zero. In a     default application this would mean that we divide by zero, so by     setting the parameter <tt>min_diagonal</tt> to a small nonzero value     the SOR will work on a matrix that is not too far away from the one we     want to treat. Finally, <tt>overlap</tt> governs the overlap of the     partitions when the preconditioner runs in parallel, forming a so-     called additive Schwarz preconditioner.    
* [0.x.43]*
       Constructor. By default, set the damping parameter to one, we do not       modify the diagonal, and there is no overlap (i.e. in parallel, we       run a BlockJacobi preconditioner, where each block is inverted       approximately by an SOR.      
* [0.x.44]*
       This specifies the (over-) relaxation parameter in the SOR       preconditioner.      
* [0.x.45]*
       This specifies the minimum value the diagonal elements should have.       This might be necessary when the SOR preconditioner is used on       matrices with zero diagonal elements. In that case, a straight-       forward application would not be possible since we divide by the       diagonal element.      
* [0.x.46]*
       This determines how large the overlap of the local matrix portions on       each processor in a parallel application should be.      
* [0.x.47]*
       Sets how many times the given operation should be applied during the       vmult() operation.      
* [0.x.48]*
     Take the sparse matrix the preconditioner object should be built of,     and additional flags (damping parameter, overlap in parallel     computations etc.) if there are any.    
* [0.x.49]*
   A wrapper class for a block Jacobi preconditioner for Trilinos matrices.   As opposed to PreconditionSOR where each row is treated separately, this   scheme collects block of a given size and inverts a full matrix for all   these rows simultaneously. Trilinos allows to select several strategies   for selecting which rows form a block, including "linear" (i.e., divide   the local range of the matrix in slices of the block size), "greedy" or   "metis". Note that the term  [2.x.18] block Jacobi [2.x.19]  does not relate to   possible blocks in the MPI setting, but small blocks of dense matrices   extracted from the sparse matrix local to each processor.     The AdditionalData data structure allows to set preconditioner options.    
*  [2.x.20]   
*  [2.x.21]   
* [0.x.50]*
     Standardized data struct to pipe additional flags to the     preconditioner. The parameter <tt>block_size</tt> sets the size of     small blocks. It is recommended to choose this parameter not too large     (a few hundreds at most) since this implementation uses a dense matrix     for the block. The parameter <tt>block_creation_type</tt> allows to     pass the strategy for finding the blocks to Ifpack. The parameter     <tt>omega</tt> specifies the relaxation parameter in the SOR     preconditioner. The parameter <tt>min_diagonal</tt> can be used to make     the application of the preconditioner also possible when some diagonal     elements are zero. In a default application this would mean that we     divide by zero, so by setting the parameter <tt>min_diagonal</tt> to a     small nonzero value the SOR will work on a matrix that is not too far     away from the one we want to treat.    
* [0.x.51]*
       Constructor. By default, use a block size of 1, use linear       subdivision of the rows, set the damping parameter to one, and do not       modify the diagonal.      
* [0.x.52]*
       This specifies the size of blocks.      
* [0.x.53]*
       Strategy for creation of blocks passed on to Ifpack block relaxation       (variable 'partitioner: type') with this string as the given value.       Available types in Ifpack include "linear" (i.e., divide the local       range of the matrix in slices of the block size), "greedy" "metis".       For a full list, see the documentation of Ifpack.      
* [0.x.54]*
       This specifies the (over-) relaxation parameter in the Jacobi       preconditioner.      
* [0.x.55]*
       This specifies the minimum value the diagonal elements should have.       This might be necessary when the block Jacobi preconditioner is used       on matrices with zero diagonal elements. In that case, a straight-       forward application would not be possible since we divide by the       diagonal element.      
* [0.x.56]*
       Sets how many times the given operation should be applied during the       vmult() operation.      
* [0.x.57]*
     Take the sparse matrix the preconditioner object should be built of,     and additional flags (damping parameter, etc.) if there are any.    
* [0.x.58]*
   A wrapper class for a block SSOR preconditioner for Trilinos matrices. As   opposed to PreconditionSSOR where each row is treated separately (point-   wise), this scheme collects block of a given size and inverts a full   matrix for all these rows simultaneously. Trilinos allows to select   several strategies for selecting which rows form a block, including   "linear" (i.e., divide the local range of the matrix in slices of the   block size), "greedy" or "metis".     The AdditionalData data structure allows to set preconditioner options.     Note that a parallel application of this preconditioner is actually a   block-Jacobi preconditioner with (outer) block size equal to the local   matrix size. Spoken more technically, this parallel operation is an [1.x.2] with a block SSOR  [2.x.22] approximate solve [2.x.23]  as inner   solver, based on the outer parallel partitioning.    
*  [2.x.24]   
*  [2.x.25]   
* [0.x.59]*
     Standardized data struct to pipe additional flags to the     preconditioner. The parameter <tt>block_size</tt> sets the size of     small blocks. It is recommended to choose this parameter not too large     (a few hundreds at most) since this implementation uses a dense matrix     for the block. The parameter <tt>block_creation_type</tt> allows to     pass the strategy for finding the blocks to Ifpack. The parameter     <tt>omega</tt> specifies the relaxation parameter in the SSOR     preconditioner. The parameter <tt>min_diagonal</tt> can be used to make     the application of the preconditioner also possible when some diagonal     elements are zero. In a default application this would mean that we     divide by zero, so by setting the parameter <tt>min_diagonal</tt> to a     small nonzero value the SOR will work on a matrix that is not too far     away from the one we want to treat. Finally, <tt>overlap</tt> governs     the overlap of the partitions when the preconditioner runs in parallel,     forming a so-called additive Schwarz preconditioner.    
* [0.x.60]*
       Constructor. By default, use a block size of 1, use linear       subdivision of the rows, set the damping parameter to one, we do not       modify the diagonal, and there is no overlap (i.e. in parallel, we       run a BlockJacobi preconditioner, where each block is inverted       approximately by a block SOR).      
* [0.x.61]*
       This specifies the size of blocks.      
* [0.x.62]*
       Strategy for creation of blocks passed on to Ifpack block relaxation       (variable 'partitioner: type') with this string as the given value.       Available types in Ifpack include "linear" (i.e., divide the local       range of the matrix in slices of the block size), "greedy" "metis".       For a full list, see the documentation of Ifpack.      
* [0.x.63]*
       This specifies the (over-) relaxation parameter in the SOR       preconditioner.      
* [0.x.64]*
       This specifies the minimum value the diagonal elements should have.       This might be necessary when the SSOR preconditioner is used on       matrices with zero diagonal elements. In that case, a straight-       forward application would not be possible since we divide by the       diagonal element.      
* [0.x.65]*
       This determines how large the overlap of the local matrix portions on       each processor in a parallel application should be.      
* [0.x.66]*
       Sets how many times the given operation should be applied during the       vmult() operation.      
* [0.x.67]*
     Take the sparse matrix the preconditioner object should be built of,     and additional flags (damping parameter, overlap in parallel     computations, etc.) if there are any.    
* [0.x.68]*
   A wrapper class for a block SOR preconditioner for Trilinos matrices. As   opposed to PreconditionSOR where each row is treated separately, this   scheme collects block of a given size and inverts a full matrix for all   these rows simultaneously. Trilinos allows to select several strategies   for selecting which rows form a block, including "linear" (i.e., divide   the local range of the matrix in slices of the block size), "greedy" or   "metis".     The AdditionalData data structure allows to set preconditioner options.     Note that a parallel application of this preconditioner is actually a   block-Jacobi preconditioner with (outer) block size equal to the local   matrix size. Spoken more technically, this parallel operation is an [1.x.3] with a block SOR  [2.x.26] approximate solve [2.x.27]  as inner   solver, based on the outer parallel partitioning.    
*  [2.x.28]   
*  [2.x.29]   
* [0.x.69]*
     Standardized data struct to pipe additional flags to the     preconditioner. The parameter <tt>block_size</tt> sets the size of     small blocks. It is recommended to choose this parameter not too large     (a few hundreds at most) since this implementation uses a dense matrix     for the block. The parameter <tt>block_creation_type</tt> allows to     pass the strategy for finding the blocks to Ifpack. The parameter     <tt>omega</tt> specifies the relaxation parameter in the SOR     preconditioner. The parameter <tt>min_diagonal</tt> can be used to make     the application of the preconditioner also possible when some diagonal     elements are zero. In a default application this would mean that we     divide by zero, so by setting the parameter <tt>min_diagonal</tt> to a     small nonzero value the SOR will work on a matrix that is not too far     away from the one we want to treat. Finally, <tt>overlap</tt> governs     the overlap of the partitions when the preconditioner runs in parallel,     forming a so-called additive Schwarz preconditioner.    
* [0.x.70]*
       Constructor. By default, use a block size of 1, use linear       subdivision of the rows, set the damping parameter to one, we do not       modify the diagonal, and there is no overlap (i.e. in parallel, we       run a BlockJacobi preconditioner, where each block is inverted       approximately by a block SOR).      
* [0.x.71]*
       This specifies the size of blocks.      
* [0.x.72]*
       Strategy for creation of blocks passed on to Ifpack block relaxation       (variable 'partitioner: type') with this string as the given value.       Available types in Ifpack include "linear" (i.e., divide the local       range of the matrix in slices of the block size), "greedy" "metis".       For a full list, see the documentation of Ifpack.      
* [0.x.73]*
       This specifies the (over-) relaxation parameter in the SOR       preconditioner.      
* [0.x.74]*
       This specifies the minimum value the diagonal elements should have.       This might be necessary when the SOR preconditioner is used on       matrices with zero diagonal elements. In that case, a straight-       forward application would not be possible since we divide by the       diagonal element.      
* [0.x.75]*
       This determines how large the overlap of the local matrix portions on       each processor in a parallel application should be.      
* [0.x.76]*
       Sets how many times the given operation should be applied during the       vmult() operation.      
* [0.x.77]*
     Take the sparse matrix the preconditioner object should be built of,     and additional flags (damping parameter, overlap in parallel     computations etc.) if there are any.    
* [0.x.78]*
   A wrapper class for an incomplete Cholesky factorization (IC)   preconditioner for  [2.x.30]  symmetric Trilinos matrices. This preconditioner   works both in serial and in parallel, depending on the matrix it is based   on. In general, an incomplete factorization does not take all fill-in   elements that would appear in a full factorization (that is the basis for   a direct solve). Trilinos allows to set the amount of fill-in elements,   governed by the additional data argument <tt>ic_fill</tt>, so one can   gradually choose between a factorization on the sparse matrix structure   only (<tt>ic_fill=0</tt>) to a full factorization (<tt>ic_fill</tt> in   the range of 10 to 50, depending on the spatial dimension of the PDE   problem and the degree of the finite element basis functions; generally,   more required fill-in elements require this parameter to be set to a   higher integer value).     The AdditionalData data structure allows to set preconditioner options.   Besides the fill-in argument, these options are some options for   perturbations (see the documentation of the AdditionalData structure for   details), and a parameter <tt>overlap</tt> that determines if and how   much overlap there should be between the matrix partitions on the various   MPI processes.  The default settings are 0 for the additional fill-in, 0   for the absolute augmentation tolerance, 1 for the relative augmentation   tolerance, 0 for the overlap.     Note that a parallel application of the IC preconditioner is actually a   block-Jacobi preconditioner with block size equal to the local matrix   size. Spoken more technically, this parallel operation is an [1.x.4] with an IC  [2.x.31] approximate solve [2.x.32]  as inner solver,   based on the (outer) parallel partitioning.    
*  [2.x.33]   
*  [2.x.34]   
* [0.x.79]*
     Standardized data struct to pipe additional parameters to the     preconditioner. The Trilinos IC decomposition allows for some fill-in,     so it actually is a threshold incomplete Cholesky factorization. The     amount of fill-in, and hence, the amount of memory used by this     preconditioner, is controlled by the parameter <tt>ic_fill</tt>, which     specifies this as a double. When forming the preconditioner, for     certain problems bad conditioning (or just bad luck) can cause the     preconditioner to be very poorly conditioned. Hence it can help to add     diagonal perturbations to the original matrix and form the     preconditioner for this slightly better matrix. <tt>ic_atol</tt> is an     absolute perturbation that is added to the diagonal before forming the     prec, and <tt>ic_rtol</tt> is a scaling factor  [2.x.35] . The last     parameter specifies the overlap of the partitions when the     preconditioner runs in parallel.    
* [0.x.80]*
       Constructor. By default, set the drop tolerance to 0, the level of       extra fill-ins is set to be zero (just use the matrix structure, do       not generate any additional fill-in), the tolerance level are 0 and       1, respectively, and the overlap in case of a parallel execution is       zero. This overlap in a block-application of the IC in the parallel       case makes the preconditioner a so-called additive Schwarz       preconditioner.      
* [0.x.81]*
       This specifies the amount of additional fill-in elements besides the       sparse matrix structure. When <tt>ic_fill</tt> is large, this means       that many fill-ins will be added, so that the IC preconditioner comes       closer to a direct sparse Cholesky decomposition. Note, however, that       this will drastically increase the memory requirement, especially       when the preconditioner is used in 3D.      
* [0.x.82]*
       This specifies the amount of an absolute perturbation that will be       added to the diagonal of the matrix, which sometimes can help to get       better preconditioners.      
* [0.x.83]*
       This specifies the factor by which the diagonal of the matrix will be       scaled, which sometimes can help to get better preconditioners.      
* [0.x.84]*
       This determines how large the overlap of the local matrix portions on       each processor in a parallel application should be.      
* [0.x.85]*
     Initialize function. Takes the matrix the preconditioner should be     computed of, and additional flags if there are any.    
* [0.x.86]*
   A wrapper class for an incomplete LU factorization (ILU(k))   preconditioner for Trilinos matrices. This preconditioner works both in   serial and in parallel, depending on the matrix it is based on. In   general, an incomplete factorization does not take all fill-in elements   that would appear in a full factorization (that is the basis for a direct   solve). Trilinos allows to set the amount of fill-in elements, governed   by the additional data argument <tt>ilu_fill</tt>, so one can gradually   choose between a factorization on the sparse matrix structure only   (<tt>ilu_fill=0</tt>) to a full factorization (<tt>ilu_fill</tt> in the   range of 10 to 50, depending on the spatial dimension of the PDE problem   and the degree of the finite element basis functions; generally, more   required fill-in elements require this parameter to be set to a higher   integer value).     The AdditionalData data structure allows to set preconditioner options.   See the documentation of the AdditionalData structure for details.     Note that a parallel application of the ILU preconditioner is actually a   block-Jacobi preconditioner with block size equal to the local matrix   size. Spoken more technically, this parallel operation is an [1.x.5] with an ILU  [2.x.36] approximate solve [2.x.37]  as inner   solver, based on the (outer) parallel partitioning.    
*  [2.x.38]   
*  [2.x.39]   
* [0.x.87]*
     Standardized data struct to pipe additional parameters to the     preconditioner:      [2.x.40]           [2.x.41]   [2.x.42]  This specifies the amount of additional fill-in     elements besides the original sparse matrix structure. If  [2.x.43]  is  [2.x.44]      fill, the sparsity pattern of  [2.x.45]  is used for the storage of the     result of the Gaussian elimination. This is known as ILU( [2.x.46] ) in the     literature.  When  [2.x.47]  is large, the preconditioner comes closer to     a (direct) sparse LU decomposition. Note, however, that this will     drastically increase the memory requirement, especially when the     preconditioner is used in 3D.          [2.x.48]   [2.x.49]  and  [2.x.50]  These two parameters allow     perturbation of the diagonal of the matrix, which sometimes can help to     get better preconditioners especially in the case of bad conditioning.     Before factorization, the diagonal entry  [2.x.51]  is replaced by      [2.x.52] , where  [2.x.53]  is the     absolute threshold  [2.x.54]  and  [2.x.55]  is the relative     threshold  [2.x.56]  The default values ( [2.x.57] ,  [2.x.58] )     therefore use the original, unmodified diagonal entry. Suggested values     are in the order of  [2.x.59]  to  [2.x.60]  for  [2.x.61]  and 1.01 for      [2.x.62]           [2.x.63]   [2.x.64]  This determines how large the overlap of the local     matrix portions on each processor in a parallel application should be.     An overlap of 0 corresponds to a block diagonal decomposition on each     processor, an overlap of 1 will additionally include a row j if there     is a nonzero entry in column j in one of the own rows. Higher overlap     numbers work accordingly in a recursive fashion. Increasing  [2.x.65]      will increase communication and storage cost. According to the IFPACK     documentation, an overlap of 1 is often effective and values of more     than 3 are rarely needed.          [2.x.66]     
* [0.x.88]*
       Constructor with default values for all parameters.      
* [0.x.89]*
       Additional fill-in, see class documentation above.      
* [0.x.90]*
       The amount of perturbation to add to diagonal entries. See the class       documentation above for details.      
* [0.x.91]*
       Scaling actor for diagonal entries. See the class documentation above       for details.      
* [0.x.92]*
       Overlap between processors. See the class documentation for details.      
* [0.x.93]*
     Initialize function. Takes the matrix which is used to form the     preconditioner, and additional flags if there are any.    
* [0.x.94]*
   A wrapper class for a thresholded incomplete LU factorization (ILU-T)   preconditioner for Trilinos matrices. This preconditioner works both in   serial and in parallel, depending on the matrix it is based on. In   general, an incomplete factorization does not take all fill-in elements   that would appear in a full factorization (that is the basis for a direct   solve). For the ILU-T preconditioner, the parameter <tt>ilut_drop</tt>   lets the user specify which elements should be dropped (i.e., should not   be part of the incomplete decomposition). Trilinos calculates first the   complete factorization for one row, and then skips those elements that   are lower than the threshold. This is the main difference to the non-   thresholded ILU preconditioner, where the parameter <tt>ilut_fill</tt>   governs the incomplete factorization structure. This parameter is   available here as well, but provides only some extra information here.     The AdditionalData data structure allows to set preconditioner options.   Besides the fill-in arguments, these options are some options for   perturbations (see the documentation of the AdditionalData structure for   details), and a parameter <tt>overlap</tt> that determines if and how   much overlap there should be between the matrix partitions on the various   MPI processes. The default settings are 0 for the additional fill-in, 0   for the absolute augmentation tolerance, 1 for the relative augmentation   tolerance, 0 for the overlap.     Note that a parallel application of the ILU-T preconditioner is actually   a block-Jacobi preconditioner with block size equal to the local matrix   size. Spoken more technically, this parallel operation is an [1.x.6] with an ILU  [2.x.67] approximate solve [2.x.68]  as inner   solver, based on the (outer) parallel partitioning.    
*  [2.x.69]   
*  [2.x.70]   
* [0.x.95]*
     Standardized data struct to pipe additional parameters to the     preconditioner. The Trilinos ILU-T decomposition allows for some fill-     in, so it actually is a threshold incomplete LU factorization. The     amount of fill-in, and hence, the amount of memory used by this     preconditioner, is controlled by the parameters <tt>ilut_drop</tt> and     <tt>ilut_fill</tt>, which specifies a threshold about which values     should form the incomplete factorization and the level of additional     fill-in. When forming the preconditioner, for certain problems bad     conditioning (or just bad luck) can cause the preconditioner to be very     poorly conditioned. Hence it can help to add diagonal perturbations to     the original matrix and form the preconditioner for this slightly     better matrix. <tt>ilut_atol</tt> is an absolute perturbation that is     added to the diagonal before forming the prec, and <tt>ilu_rtol</tt> is     a scaling factor  [2.x.71] . The last parameter specifies the     overlap of the partitions when the preconditioner runs in parallel.    
* [0.x.96]*
       Constructor. By default, no element will be dropped, the level of       extra fill-ins is set to be zero (just use the matrix structure, do       not generate any additional fill-in except the one that results from       non-dropping large elements), the tolerance level are 0 and 1,       respectively, and the overlap in case of a parallel execution is       zero. This overlap in a block-application of the ILU in the parallel       case makes the preconditioner a so-called additive Schwarz       preconditioner.      
* [0.x.97]*
       This specifies the relative size of elements which should be dropped       when forming an incomplete LU decomposition with threshold.      
* [0.x.98]*
       This specifies the amount of additional fill-in elements besides the       sparse matrix structure. When <tt>ilu_fill</tt> is large, this means       that many fill-ins will be added, so that the ILU preconditioner       comes closer to a (direct) sparse LU decomposition. Note, however,       that this will drastically increase the memory requirement,       especially when the preconditioner is used in 3D.      
* [0.x.99]*
       This specifies the amount of an absolute perturbation that will be       added to the diagonal of the matrix, which sometimes can help to get       better preconditioners.      
* [0.x.100]*
       This specifies the factor by which the diagonal of the matrix will be       scaled, which sometimes can help to get better preconditioners.      
* [0.x.101]*
       This determines how large the overlap of the local matrix portions on       each processor in a parallel application should be.      
* [0.x.102]*
     Initialize function. Takes the matrix which is used to form the     preconditioner, and additional flags if there are any.    
* [0.x.103]*
   A wrapper class for a sparse direct LU decomposition on parallel blocks   for Trilinos matrices. When run in serial, this corresponds to a direct   solve on the matrix.     The AdditionalData data structure allows to set preconditioner options.     Note that a parallel application of the block direct solve preconditioner   is actually a block-Jacobi preconditioner with block size equal to the   local matrix size. Spoken more technically, this parallel operation is an   [1.x.7] with an  [2.x.72] exact solve [2.x.73]  as inner solver, based on   the (outer) parallel partitioning.    
*  [2.x.74]   
*  [2.x.75]   
* [0.x.104]*
     Standardized data struct to pipe additional parameters to the     preconditioner.    
* [0.x.105]*
       Constructor.      
* [0.x.106]*
       This determines how large the overlap of the local matrix portions on       each processor in a parallel application should be.      
* [0.x.107]*
     Initialize function. Takes the matrix which is used to form the     preconditioner, and additional flags if there are any.    
* [0.x.108]*
   A wrapper class for a Chebyshev preconditioner for Trilinos matrices.     The AdditionalData data structure allows to set preconditioner options.    
*  [2.x.76]   
*  [2.x.77]   
* [0.x.109]*
     Standardized data struct to pipe additional parameters to the     preconditioner.    
* [0.x.110]*
       Constructor.      
* [0.x.111]*
       This determines the degree of the Chebyshev polynomial. The degree of       the polynomial gives the number of matrix-vector products to be       performed for one application of the vmult() operation.      
* [0.x.112]*
       This sets the maximum eigenvalue of the matrix, which needs to be set       properly for appropriate performance of the Chebyshev preconditioner.      
* [0.x.113]*
       This sets the ratio between the maximum and the minimum eigenvalue.      
* [0.x.114]*
       This sets the minimum eigenvalue, which is an optional parameter only       used internally for checking whether we use an identity matrix.      
* [0.x.115]*
       This sets a threshold below which the diagonal element will not be       inverted in the Chebyshev algorithm.      
* [0.x.116]*
       When this flag is set to <tt>true</tt>, it enables the method       <tt>vmult(dst, src)</tt> to use non-zero data in the vector       <tt>dst</tt>, appending to it the Chebyshev corrections. This can be       useful in some situations (e.g. when used for high-frequency error       smoothing), but not the way the solver classes expect a       preconditioner to work (where one ignores the content in <tt>dst</tt>       for the preconditioner application). The user should really know what       they are doing when touching this flag.      
* [0.x.117]*
     Initialize function. Takes the matrix which is used to form the     preconditioner, and additional flags if there are any.    
* [0.x.118]*
   This class implements an algebraic multigrid (AMG) preconditioner based   on the Trilinos ML implementation, which is a black-box preconditioner   that works well for many PDE-based linear problems.  What this class does   is twofold.  When the initialize() function is invoked, a ML   preconditioner object is created based on the matrix that we want the   preconditioner to be based on. A call of the respective    [2.x.78]  function does call the respective operation in the   Trilinos package, where it is called  [2.x.79] . Use of   this class is explained in the  [2.x.80]  tutorial program.     Since the Trilinos objects we want to use are heavily dependent on Epetra   objects, we recommend using this class in conjunction with Trilinos   (Epetra) sparse matrices and vectors. There is support for use with   matrices of the  [2.x.81]  class and corresponding vectors,   too, but this requires generating a copy of the matrix, which is slower   and takes (much) more memory. When doing such a copy operation, we can   still profit from the fact that some of the entries in the preconditioner   matrix are zero and hence can be neglected.     The implementation is able to distinguish between matrices from elliptic   problems and convection dominated problems. We use the standard options   provided by Trilinos ML for elliptic problems, except that we use a   Chebyshev smoother instead of a symmetric Gauss-Seidel smoother.  For   most elliptic problems, Chebyshev provides a better damping of high   frequencies (in the algebraic sense) than Gauss-Seidel (SSOR), and is   faster (Chebyshev requires only some matrix-vector products, whereas SSOR   requires substitutions which are more expensive). Moreover, Chebyshev is   perfectly parallel in the sense that it does not degenerate when used on   many processors. SSOR, on the other hand, gets more Jacobi-like on many   processors.     For proper functionality of this class we recommend using Trilinos v9.0   and higher. Older versions may have problems with generating the coarse-   matrix structure when using matrices with many nonzero entries per row   (i.e., matrices stemming from higher order finite element   discretizations).    
*  [2.x.82]   
*  [2.x.83]   
* [0.x.119]*
     A data structure that is used to control details of how the algebraic     multigrid is set up. The flags detailed in here are then passed to the     Trilinos ML implementation. A structure of the current type are passed     to the constructor of PreconditionAMG.    
* [0.x.120]*
       Constructor. By default, we pretend to work on elliptic problems with       linear finite elements on a scalar equation.             Making use of the  [2.x.84]  function, the        [2.x.85]  vector can be initialized for a given field in the       following manner:            
* [1.x.8]
*       
* [0.x.121]*
       Fill in a  [2.x.86]  that can be used to initialize the       AMG preconditioner.             The  [2.x.87]  is used in conjunction with the  [2.x.88]  to       configure the null space settings for the preconditioner.       The  [2.x.89]  are initialized by this function, and       must remain in scope until  [2.x.90]  has been       called.            
*  [2.x.91]  The set parameters reflect the current settings in this       object, with various options being set both directly though the state       of the member variables (e.g. the "smoother: type") as well as       indirectly (e.g. the "aggregation: type"). If you wish to have       fine-grained control over the configuration of the AMG preconditioner,       then you can create the parameter list using this function (which       conveniently sets the null space of the operator), change the relevant       settings, and use the amended parameters list as an argument to        [2.x.92]  instead of the AdditionalData object       itself.             See the documentation for the       [1.x.9] for details on what options are available for       modification.            
*  [2.x.93]  Any user-defined parameters that are not in conflict with those       set by this data structure will be retained.      
* [0.x.122]*
       Fill in a parameter list that can be used to initialize the       AMG preconditioner.            
*  [2.x.94]  Any user-defined parameters that are not in conflict with those       set by this data structure will be retained.      
* [0.x.123]*
       Configure the null space setting in the  [2.x.95]  for       the input  [2.x.96]  based on the state of the  [2.x.97]        variable.      
* [0.x.124]*
       Configure the null space setting in the  [2.x.98]  for       the input  [2.x.99]  based on the state of the  [2.x.100]        variable.      
* [0.x.125]*
       Determines whether the AMG preconditioner should be optimized for       elliptic problems (ML option smoothed aggregation SA, using a       Chebyshev smoother) or for non-elliptic problems (ML option non-       symmetric smoothed aggregation NSSA, smoother is SSOR with       underrelaxation).      
* [0.x.126]*
       Determines whether the matrix that the preconditioner is built upon       is generated from linear or higher-order elements.      
* [0.x.127]*
       Defines how many multigrid cycles should be performed by the       preconditioner.      
* [0.x.128]*
       Defines whether a w-cycle should be used instead of the standard       setting of a v-cycle.      
* [0.x.129]*
       This threshold tells the AMG setup how the coarsening should be       performed. In the AMG used by ML, all points that strongly couple       with the tentative coarse-level point form one aggregate. The term        [2.x.101] strong coupling [2.x.102]  is controlled by the variable       <tt>aggregation_threshold</tt>, meaning that all elements that are       not smaller than <tt>aggregation_threshold</tt> times the diagonal       element do couple strongly.      
* [0.x.130]*
       Specifies the constant modes (near null space) of the matrix. This       parameter tells AMG whether we work on a scalar equation (where the       near null space only consists of ones, and default value is OK) or on       a vector-valued equation. For vector-valued equation problem with       <tt>n_component</tt>, the provided  [2.x.103]  should fulfill       the following requirements:        [2.x.104]         [2.x.105]   n_component.size() == <tt>n_component</tt>  [2.x.106]         [2.x.107]   n_component[*].size() == n_dof_local or n_component[*].size()       == n_dof_global  [2.x.108]         [2.x.109]   n_component[<tt>ic</tt>][<tt>id</tt>] ==       "<tt>id</tt> [2.x.110] th [2.x.111]  DoF is corresponding to component <tt>ic</tt>        [2.x.112]         [2.x.113]       
* [0.x.131]*
       Determines how many sweeps of the smoother should be performed. When       the flag <tt>elliptic</tt> is set to <tt>true</tt>, i.e., for       elliptic or almost elliptic problems, the polynomial degree of the       Chebyshev smoother is set to <tt>smoother_sweeps</tt>. The term       sweeps refers to the number of matrix-vector products performed in       the Chebyshev case. In the non-elliptic case,       <tt>smoother_sweeps</tt> sets the number of SSOR relaxation sweeps       for post-smoothing to be performed.      
* [0.x.132]*
       Determines the overlap in the SSOR/Chebyshev error smoother when run       in parallel.      
* [0.x.133]*
       If this flag is set to <tt>true</tt>, then internal information from       the ML preconditioner is printed to screen. This can be useful when       debugging the preconditioner.      
* [0.x.134]*
       Determines which smoother to use for the AMG cycle. Possibilities for       smoother_type are the following:        [2.x.114]         [2.x.115]   "Aztec"  [2.x.116]         [2.x.117]   "IFPACK"  [2.x.118]         [2.x.119]   "Jacobi"  [2.x.120]         [2.x.121]   "ML symmetric Gauss-Seidel"  [2.x.122]         [2.x.123]   "symmetric Gauss-Seidel"  [2.x.124]         [2.x.125]   "ML Gauss-Seidel"  [2.x.126]         [2.x.127]   "Gauss-Seidel"  [2.x.128]         [2.x.129]   "block Gauss-Seidel"  [2.x.130]         [2.x.131]   "symmetric block Gauss-Seidel"  [2.x.132]         [2.x.133]   "Chebyshev"  [2.x.134]         [2.x.135]   "MLS"  [2.x.136]         [2.x.137]   "Hiptmair"  [2.x.138]         [2.x.139]   "Amesos-KLU"  [2.x.140]         [2.x.141]   "Amesos-Superlu"  [2.x.142]         [2.x.143]   "Amesos-UMFPACK"  [2.x.144]         [2.x.145]   "Amesos-Superludist"  [2.x.146]         [2.x.147]   "Amesos-MUMPS"  [2.x.148]         [2.x.149]   "user-defined"  [2.x.150]         [2.x.151]   "SuperLU"  [2.x.152]         [2.x.153]   "IFPACK-Chebyshev"  [2.x.154]         [2.x.155]   "self"  [2.x.156]         [2.x.157]   "do-nothing"  [2.x.158]         [2.x.159]   "IC"  [2.x.160]         [2.x.161]   "ICT"  [2.x.162]         [2.x.163]   "ILU"  [2.x.164]         [2.x.165]   "ILUT"  [2.x.166]         [2.x.167]   "Block Chebyshev"  [2.x.168]         [2.x.169]   "IFPACK-Block Chebyshev"  [2.x.170]         [2.x.171]       
* [0.x.135]*
       Determines which solver to use on the coarsest level. The same       settings as for the smoother type are possible.      
* [0.x.136]*
     Destructor.    
* [0.x.137]*
     Let Trilinos compute a multilevel hierarchy for the solution of a     linear system with the given matrix. The function uses the matrix     format specified in  [2.x.172]     
* [0.x.138]*
     Let Trilinos compute a multilevel hierarchy for the solution of a     linear system with the given matrix. As opposed to the other initialize     function above, this function uses an abstract interface to an object     of type Epetra_RowMatrix which allows a user to pass quite general     objects to the ML preconditioner.         This initialization routine is useful in cases where the operator to be     preconditioned is not a  [2.x.173]  object but still     allows getting a copy of the entries in each of the locally owned matrix     rows (method ExtractMyRowCopy) and implements a matrix-vector product     (methods Multiply or Apply). An example are operators which provide     faster matrix-vector multiplications than possible with matrix entries     (matrix-free methods). These implementations can be beneficially     combined with Chebyshev smoothers that only perform matrix-vector     products. The interface class Epetra_RowMatrix is very flexible to     enable this kind of implementation.    
* [0.x.139]*
     Let Trilinos compute a multilevel hierarchy for the solution of a     linear system with the given matrix. The function uses the matrix     format specified in  [2.x.174]          This function is similar to the one above, but allows the user     to set all the options of the Trilinos ML preconditioner. In     order to find out about all the options for ML, we refer to the     [1.x.10]. In particular, users need to follow the     ML instructions in case a vector-valued problem ought to be     solved.    
* [0.x.140]*
     Let Trilinos compute a multilevel hierarchy for the solution of a     linear system with the given matrix. As opposed to the other initialize     function above, this function uses an abstract interface to an object     of type Epetra_RowMatrix which allows a user to pass quite general     objects to the ML preconditioner.    
* [0.x.141]*
     Let Trilinos compute a multilevel hierarchy for the solution of a     linear system with the given matrix. This function takes a deal.II     matrix and copies the content into a Trilinos matrix, so the function     can be considered rather inefficient.    
* [0.x.142]*
     This function can be used for a faster recalculation of the     preconditioner construction when the matrix entries underlying the     preconditioner have changed, but the matrix sparsity pattern has     remained the same. What this function does is taking the already     generated coarsening structure, computing the AMG prolongation and     restriction according to a smoothed aggregation strategy and then     building the whole multilevel hierarchy. This function can be     considerably faster than the initialize function, since the coarsening     pattern is usually the most difficult thing to do when setting up the     AMG ML preconditioner.    
* [0.x.143]*
     Destroys the preconditioner, leaving an object like just after having     called the constructor.    
* [0.x.144]*
     Prints an estimate of the memory consumption of this class.    
* [0.x.145]*
     A copy of the deal.II matrix into Trilinos format.    
* [0.x.146]*
   This class implements an algebraic multigrid (AMG) preconditioner based   on the Trilinos MueLu implementation, which is a black-box preconditioner   that works well for many PDE-based linear problems. The interface of   PreconditionerAMGMueLu is the same as the interface of PreconditionerAMG   except for the higher_order_elements parameter which does not exist in   PreconditionerAMGMueLu.    
*  [2.x.175]  You need to configure Trilinos with MueLU support for this   preconditioner to work.    
*  [2.x.176]  At the moment 64bit-indices are not supported.      [2.x.177]  This interface should not be considered as stable.    
*  [2.x.178]   
*  [2.x.179]   
* [0.x.147]*
     A data structure that is used to control details of how the algebraic     multigrid is set up. The flags detailed in here are then passed to the     Trilinos MueLu implementation. A structure of the current type are     passed to the constructor of PreconditionAMGMueLu.    
* [0.x.148]*
       Constructor. By default, we pretend to work on elliptic problems with       linear finite elements on a scalar equation.      
* [0.x.149]*
       Determines whether the AMG preconditioner should be optimized for       elliptic problems (MueLu option smoothed aggregation SA, using a       Chebyshev smoother) or for non-elliptic problems (MueLu option non-       symmetric smoothed aggregation NSSA, smoother is SSOR with       underrelaxation).      
* [0.x.150]*
       Defines how many multigrid cycles should be performed by the       preconditioner.      
* [0.x.151]*
       Defines whether a w-cycle should be used instead of the standard       setting of a v-cycle.      
* [0.x.152]*
       This threshold tells the AMG setup how the coarsening should be       performed. In the AMG used by MueLu, all points that strongly couple       with the tentative coarse-level point form one aggregate. The term        [2.x.180] strong coupling [2.x.181]  is controlled by the variable       <tt>aggregation_threshold</tt>, meaning that all elements that are       not smaller than <tt>aggregation_threshold</tt> times the diagonal       element do couple strongly.      
* [0.x.153]*
       Specifies the constant modes (near null space) of the matrix. This       parameter tells AMG whether we work on a scalar equation (where the       near null space only consists of ones) or on a vector-valued       equation.      
* [0.x.154]*
       Determines how many sweeps of the smoother should be performed. When       the flag <tt>elliptic</tt> is set to <tt>true</tt>, i.e., for       elliptic or almost elliptic problems, the polynomial degree of the       Chebyshev smoother is set to <tt>smoother_sweeps</tt>. The term       sweeps refers to the number of matrix-vector products performed in       the Chebyshev case. In the non-elliptic case,       <tt>smoother_sweeps</tt> sets the number of SSOR relaxation sweeps       for post-smoothing to be performed.      
* [0.x.155]*
       Determines the overlap in the SSOR/Chebyshev error smoother when run       in parallel.      
* [0.x.156]*
       If this flag is set to <tt>true</tt>, then internal information from       the ML preconditioner is printed to screen. This can be useful when       debugging the preconditioner.      
* [0.x.157]*
       Determines which smoother to use for the AMG cycle. Possibilities for       smoother_type are the following:        [2.x.182]         [2.x.183]   "Aztec"  [2.x.184]         [2.x.185]   "IFPACK"  [2.x.186]         [2.x.187]   "Jacobi"  [2.x.188]         [2.x.189]   "ML symmetric Gauss-Seidel"  [2.x.190]         [2.x.191]   "symmetric Gauss-Seidel"  [2.x.192]         [2.x.193]   "ML Gauss-Seidel"  [2.x.194]         [2.x.195]   "Gauss-Seidel"  [2.x.196]         [2.x.197]   "block Gauss-Seidel"  [2.x.198]         [2.x.199]   "symmetric block Gauss-Seidel"  [2.x.200]         [2.x.201]   "Chebyshev"  [2.x.202]         [2.x.203]   "MLS"  [2.x.204]         [2.x.205]   "Hiptmair"  [2.x.206]         [2.x.207]   "Amesos-KLU"  [2.x.208]         [2.x.209]   "Amesos-Superlu"  [2.x.210]         [2.x.211]   "Amesos-UMFPACK"  [2.x.212]         [2.x.213]   "Amesos-Superludist"  [2.x.214]         [2.x.215]   "Amesos-MUMPS"  [2.x.216]         [2.x.217]   "user-defined"  [2.x.218]         [2.x.219]   "SuperLU"  [2.x.220]         [2.x.221]   "IFPACK-Chebyshev"  [2.x.222]         [2.x.223]   "self"  [2.x.224]         [2.x.225]   "do-nothing"  [2.x.226]         [2.x.227]   "IC"  [2.x.228]         [2.x.229]   "ICT"  [2.x.230]         [2.x.231]   "ILU"  [2.x.232]         [2.x.233]   "ILUT"  [2.x.234]         [2.x.235]   "Block Chebyshev"  [2.x.236]         [2.x.237]   "IFPACK-Block Chebyshev"  [2.x.238]         [2.x.239]       
* [0.x.158]*
       Determines which solver to use on the coarsest level. The same       settings as for the smoother type are possible.      
* [0.x.159]*
     Constructor.    
* [0.x.160]*
     Destructor.    
* [0.x.161]*
     Let Trilinos compute a multilevel hierarchy for the solution of a     linear system with the given matrix. The function uses the matrix     format specified in  [2.x.240]     
* [0.x.162]*
     Let Trilinos compute a multilevel hierarchy for the solution of a     linear system with the given matrix. As opposed to the other initialize     function above, this function uses an object of type     Epetra_CrsMatrixCrs.    
* [0.x.163]*
     Let Trilinos compute a multilevel hierarchy for the solution of a     linear system with the given matrix. The function uses the matrix     format specified in  [2.x.241]          This function is similar to the one above, but allows the user     to set most of the options of the Trilinos ML     preconditioner. In order to find out about all the options for     ML, we refer to the [1.x.11]. Not all ML options have a corresponding     MueLu option.    
* [0.x.164]*
     Let Trilinos compute a multilevel hierarchy for the solution of a     linear system with the given matrix. As opposed to the other initialize     function above, this function uses an object of type Epetra_CrsMatrix.    
* [0.x.165]*
     Let Trilinos compute a multilevel hierarchy for the solution of a     linear system with the given matrix. This function takes a deal.ii     matrix and copies the content into a Trilinos matrix, so the function     can be considered rather inefficient.    
* [0.x.166]*
     Destroys the preconditioner, leaving an object like just after having     called the constructor.    
* [0.x.167]*
     Prints an estimate of the memory consumption of this class.    
* [0.x.168]*
     A copy of the deal.II matrix into Trilinos format.    
* [0.x.169]*
   A wrapper class for an identity preconditioner for Trilinos matrices.    
*  [2.x.242]   
*  [2.x.243]   
* [0.x.170]*
     This function is only present to provide the interface of a     preconditioner to be handed to a smoother.  This does nothing.    
* [0.x.171]*
     The matrix argument is ignored and here just for compatibility with more     complex preconditioners.    
*  [2.x.244]  This function must be called when this preconditioner is to be     wrapped in a LinearOperator without an exemplar materix.    
* [0.x.172]*
     Apply the preconditioner, i.e., dst = src.    
* [0.x.173]*
     Apply the transport conditioner, i.e., dst = src.    
* [0.x.174]*
     Apply the preconditioner on deal.II data structures instead of the ones     provided in the Trilinos wrapper class, i.e., dst = src.    
* [0.x.175]*
     Apply the transpose preconditioner on deal.II data structures instead     of the ones provided in the Trilinos wrapper class, i.e. dst = src.    
* [0.x.176]*
     Apply the preconditioner on deal.II parallel data structures instead of     the ones provided in the Trilinos wrapper class, i.e., dst = src.    
* [0.x.177]*
     Apply the transpose preconditioner on deal.II parallel data structures     instead of the ones provided in the Trilinos wrapper class, i.e., dst =     src.    
* [0.x.178]

include/deal.II-translator/lac/trilinos_solver_0.txt
[0.x.0]*
   Base class for solver classes using the Trilinos solvers. Since solvers   in Trilinos are selected based on flags passed to a generic solver   object, basically all the actual solver calls happen in this class, and   derived classes simply set the right flags to select one solver or   another, or to set certain parameters for individual solvers. For a   general discussion on the Trilinos solver package AztecOO, we refer to   the [1.x.0].     This solver class can also be used as a standalone class, where the   respective Krylov method is set via the flag <tt>solver_name</tt>. This   can be done at runtime (e.g., when parsing the solver from a   ParameterList) and is similar to the deal.II class SolverSelector.    
*  [2.x.0]   
* [0.x.1]*
     Enumeration object that is set in the constructor of the derived     classes and tells Trilinos which solver to use. This option can also be     set in the user program, so one might use this base class instead of     one of the specialized derived classes when the solver should be set at     runtime. Currently enabled options are:    
* [0.x.2]*
       Use the conjugate gradient (CG) algorithm.      
* [0.x.3]*
       Use the conjugate gradient squared (CGS) algorithm.      
* [0.x.4]*
       Use the generalized minimum residual (GMRES) algorithm.      
* [0.x.5]*
       Use the biconjugate gradient stabilized (BICGStab) algorithm.      
* [0.x.6]*
       Use the transpose-free quasi-minimal residual (TFQMR) method.      
* [0.x.7]*
     Standardized data struct to pipe additional data to the solver.    
* [0.x.8]*
       Set the additional data field to the desired output format and puts       the restart parameter in case the derived class is GMRES.             TODO: Find a better way for setting the GMRES restart parameter since       it is quite inelegant to set a specific option of one solver in the       base class for all solvers.      
* [0.x.9]*
       Enables/disables the output of solver details (residual in each       iterations etc.).      
* [0.x.10]*
       Restart parameter for GMRES solver.      
* [0.x.11]*
     Constructor. Takes the solver control object and creates the solver.    
* [0.x.12]*
     Second constructor. This constructor takes an enum object that     specifies the solver name and sets the appropriate Krylov method.    
* [0.x.13]*
     Destructor.    
* [0.x.14]*
     Solve the linear system <tt>Ax=b</tt>. Depending on the information     provided by derived classes and the object passed as a preconditioner,     one of the linear solvers and preconditioners of Trilinos is chosen.    
* [0.x.15]*
     Solve the linear system <tt>Ax=b</tt> where <tt>A</tt> is an operator.     This function can be used for matrix free computation. Depending on the     information provided by derived classes and the object passed as a     preconditioner, one of the linear solvers and preconditioners of     Trilinos is chosen.    
* [0.x.16]*
     Solve the linear system <tt>Ax=b</tt> where both <tt>A</tt> and its      [2.x.1]  are an operator.     This function can be used when both <tt>A</tt> and the  [2.x.2]      are LinearOperators derived from a TrilinosPayload.     Depending on the information provided by derived classes and the object     passed as a preconditioner, one of the linear solvers and preconditioners     of Trilinos is chosen.    
* [0.x.17]*
     Solve the linear system <tt>Ax=b</tt> where <tt>A</tt> is an operator,     and the vectors  [2.x.3]  and  [2.x.4]  are native Trilinos vector types.     This function can be used when <tt>A</tt> is a LinearOperators derived     from a TrilinosPayload.     Depending on the information provided by derived classes and the object     passed as a preconditioner, one of the linear solvers and preconditioners     of Trilinos is chosen.    
* [0.x.18]*
     Solve the linear system <tt>Ax=b</tt> where both <tt>A</tt> and its      [2.x.5]  are an operator, and the vectors  [2.x.6]  and  [2.x.7]  are     native Trilinos vector types.     This function can be used when both <tt>A</tt> and the  [2.x.8]      are LinearOperators derived from a TrilinosPayload.     Depending on the information provided by derived classes and the object     passed as a preconditioner, one of the linear solvers and preconditioners     of Trilinos is chosen.    
* [0.x.19]*
     Solve the linear system <tt>Ax=b</tt>. Depending on the information     provided by derived classes and the object passed as a preconditioner,     one of the linear solvers and preconditioners of Trilinos is chosen.     This class works with matrices according to the TrilinosWrappers     format, but can take deal.II vectors as argument. Since deal.II are     serial vectors (not distributed), this function does only what you     expect in case the matrix is locally owned. Otherwise, an exception     will be thrown.    
* [0.x.20]*
     Solve the linear system <tt>Ax=b</tt> where <tt>A</tt> is an operator.     This function can be used for matrix free computations. Depending on     the information provided by derived classes and the object passed as a     preconditioner, one of the linear solvers and preconditioners of     Trilinos is chosen. This class works with matrices according to the     TrilinosWrappers format, but can take deal.II vectors as argument.     Since deal.II are serial vectors (not distributed), this function does     only what you expect in case the matrix is locally owned. Otherwise, an     exception will be thrown.    
* [0.x.21]*
     Solve the linear system <tt>Ax=b</tt> for deal.II's parallel     distributed vectors. Depending on the information provided by derived     classes and the object passed as a preconditioner, one of the linear     solvers and preconditioners of Trilinos is chosen.    
* [0.x.22]*
     Solve the linear system <tt>Ax=b</tt> where <tt>A</tt> is an operator.     This function can be used for matrix free computation. Depending on the     information provided by derived classes and the object passed as a     preconditioner, one of the linear solvers and preconditioners of     Trilinos is chosen.    
* [0.x.23]*
     Access to object that controls convergence.    
* [0.x.24]*
     Exception    
* [0.x.25]*
     Reference to the object that controls convergence of the iterative     solver. In fact, for these Trilinos wrappers, Trilinos does so itself,     but we copy the data from this object before starting the solution     process, and copy the data back into it afterwards.    
* [0.x.26]*
     The solve function is used to set properly the Epetra_LinearProblem,     once it is done this function solves the linear problem.    
* [0.x.27]*
     A function that sets the preconditioner that the solver will apply    
* [0.x.28]*
     A structure that collects the Trilinos sparse matrix, the right hand     side vector and the solution vector, which is passed down to the     Trilinos solver.    
* [0.x.29]*
     A structure that contains a Trilinos object that can query the linear     solver and determine whether the convergence criterion have been met.    
* [0.x.30]*
     A structure that contains the Trilinos solver and preconditioner     objects.    
* [0.x.31]*
     Store a copy of the flags for this particular solver.    
* [0.x.32]*
   An implementation of the solver interface using the Trilinos CG solver.    
*  [2.x.9]   
* [0.x.33]*
     Standardized data struct to pipe additional data to the solver.    
* [0.x.34]*
       Set the additional data field to the desired output format.      
* [0.x.35]*
     Constructor. In contrast to deal.II's own solvers, there is no need to     give a vector memory object.         The last argument takes a structure with additional, solver dependent     flags for tuning.    
* [0.x.36]*
     Store a copy of the flags for this particular solver.    
* [0.x.37]*
   An implementation of the solver interface using the Trilinos CGS solver.    
*  [2.x.10]   
* [0.x.38]*
     Standardized data struct to pipe additional data to the solver.    
* [0.x.39]*
       Set the additional data field to the desired output format.      
* [0.x.40]*
     Constructor. In contrast to deal.II's own solvers, there is no need to     give a vector memory object.         The last argument takes a structure with additional, solver dependent     flags for tuning.    
* [0.x.41]*
     Store a copy of the flags for this particular solver.    
* [0.x.42]*
   An implementation of the solver interface using the Trilinos GMRES   solver.  
* [0.x.43]*
     Standardized data struct to pipe additional data to the solver.    
* [0.x.44]*
       Constructor. By default, set the number of temporary vectors to 30,       i.e. do a restart every 30 iterations.      
* [0.x.45]*
     Constructor. In contrast to deal.II's own solvers, there is no need to     give a vector memory object.         The last argument takes a structure with additional, solver dependent     flags for tuning.    
* [0.x.46]*
     Store a copy of the flags for this particular solver.    
* [0.x.47]*
   An implementation of the solver interface using the Trilinos BiCGStab   solver.    
*  [2.x.11]   
* [0.x.48]*
     Standardized data struct to pipe additional data to the solver.    
* [0.x.49]*
       Set the additional data field to the desired output format.      
* [0.x.50]*
     Constructor. In contrast to deal.II's own solvers, there is no need to     give a vector memory object.         The last argument takes a structure with additional, solver dependent     flags for tuning.    
* [0.x.51]*
     Store a copy of the flags for this particular solver.    
* [0.x.52]*
   An implementation of the solver interface using the Trilinos TFQMR   solver.    
*  [2.x.12]   
* [0.x.53]*
     Standardized data struct to pipe additional data to the solver.    
* [0.x.54]*
       Set the additional data field to the desired output format.      
* [0.x.55]*
     Constructor. In contrast to deal.II's own solvers, there is no need to     give a vector memory object.         The last argument takes a structure with additional, solver dependent     flags for tuning.    
* [0.x.56]*
     Store a copy of the flags for this particular solver.    
* [0.x.57]*
   An implementation of Trilinos direct solvers (using the Amesos package).   The data field  [2.x.13]  can be used to specify the   type of solver. It allows the use of built-in solvers Amesos_Klu as well   as third-party solvers Amesos_Superludist or Amesos_Mumps.     For instructions on how to install Trilinos for use with direct solvers   other than KLU, see the link to the Trilinos installation instructions   linked to from the deal.II ReadMe file.    
*  [2.x.14]   
* [0.x.58]*
     Standardized data struct to pipe additional data to the solver.    
* [0.x.59]*
       Set the additional data field to the desired output format.      
* [0.x.60]*
       Enables/disables the output of solver details (residual in each       iterations etc.).      
* [0.x.61]*
       Set the solver type (for third party solver support of Trilinos       Amesos package). Possibilities are:        [2.x.15]         [2.x.16]   "Amesos_Lapack"  [2.x.17]         [2.x.18]   "Amesos_Scalapack"  [2.x.19]         [2.x.20]   "Amesos_Klu"  [2.x.21]         [2.x.22]   "Amesos_Umfpack"  [2.x.23]         [2.x.24]   "Amesos_Pardiso"  [2.x.25]         [2.x.26]   "Amesos_Taucs"  [2.x.27]         [2.x.28]   "Amesos_Superlu"  [2.x.29]         [2.x.30]   "Amesos_Superludist"  [2.x.31]         [2.x.32]   "Amesos_Dscpack"  [2.x.33]         [2.x.34]   "Amesos_Mumps"  [2.x.35]         [2.x.36]        Note that the availability of these solvers in deal.II depends on       which solvers were set when configuring Trilinos.      
* [0.x.62]*
     Constructor. Takes the solver control object and creates the solver.    
* [0.x.63]*
     Destructor.    
* [0.x.64]*
     Initializes the direct solver for the matrix <tt>A</tt> and creates a     factorization for it with the package chosen from the additional     data structure. Note that there is no need for a preconditioner     here and solve() is not called.    
* [0.x.65]*
     Solve the linear system <tt>Ax=b</tt> based on the     package set in initialize(). Note the matrix is not refactorized during     this call.    
* [0.x.66]*
     Solve the linear system <tt>Ax=b</tt> based on the package set in     initialize() for deal.II's own parallel vectors. Note the matrix is not     refactorized during this call.    
* [0.x.67]*
     Solve the linear system <tt>Ax=b</tt>. Creates a factorization of the     matrix with the package chosen from the additional data structure and     performs the solve. Note that there is no need for a preconditioner     here.    
* [0.x.68]*
     Solve the linear system <tt>Ax=b</tt>. This class works with Trilinos     matrices, but takes deal.II serial vectors as argument. Since these     vectors are not distributed, this function does only what you expect in     case the matrix is serial (i.e., locally owned). Otherwise, an     exception will be thrown.    
* [0.x.69]*
     Solve the linear system <tt>Ax=b</tt> for deal.II's own parallel     vectors. Creates a factorization of the matrix with the package chosen     from the additional data structure and performs the solve. Note that     there is no need for a preconditioner here.    
* [0.x.70]*
     Access to object that controls convergence.    
* [0.x.71]*
     Exception    
* [0.x.72]*
     Actually performs the operations for solving the linear system,     including the factorization and forward and backward substitution.    
* [0.x.73]*
     Reference to the object that controls convergence of the iterative     solver. In fact, for these Trilinos wrappers, Trilinos does so itself,     but we copy the data from this object before starting the solution     process, and copy the data back into it afterwards.    
* [0.x.74]*
     A structure that collects the Trilinos sparse matrix, the right hand     side vector and the solution vector, which is passed down to the     Trilinos solver.    
* [0.x.75]*
     A structure that contains the Trilinos solver and preconditioner     objects.    
* [0.x.76]*
     Store a copy of the flags for this particular solver.    
* [0.x.77]

include/deal.II-translator/lac/trilinos_sparse_matrix_0.txt
[0.x.0]*
   Iterators for Trilinos matrices  
* [0.x.1]*
     Exception    
* [0.x.2]*
     Exception    
* [0.x.3]*
     Handling of indices for both constant and non constant Accessor objects         For a regular  [2.x.0]  we would use an accessor for the     sparsity pattern. For Trilinos matrices, this does not seem so simple,     therefore, we write a little base class here.    
* [0.x.4]*
       Declare the type for container size.      
* [0.x.5]*
       Constructor.      
* [0.x.6]*
       Row number of the element represented by this object.      
* [0.x.7]*
       Index in row of the element represented by this object.      
* [0.x.8]*
       Column number of the element represented by this object.      
* [0.x.9]*
       Pointer to the matrix object. This object should be handled as a       const pointer or non-const by the appropriate derived classes. In       order to be able to implement both, it is not const here, so handle       with care!      
* [0.x.10]*
       Current row number.      
* [0.x.11]*
       Current index in row.      
* [0.x.12]*
       Discard the old row caches (they may still be used by other       accessors) and generate new ones for the row pointed to presently by       this accessor.      
* [0.x.13]*
       Cache where we store the column indices of the present row. This is       necessary, since Trilinos makes access to the elements of its       matrices rather hard, and it is much more efficient to copy all       column entries of a row once when we enter it than repeatedly asking       Trilinos for individual ones. This also makes some sense since it is       likely that we will access them sequentially anyway.             In order to make copying of iterators/accessor of acceptable       performance, we keep a shared pointer to these entries so that more       than one accessor can access this data if necessary.      
* [0.x.14]*
       Cache for the values of this row.      
* [0.x.15]*
     General template for sparse matrix accessors. The first template     argument denotes the underlying numeric type, the second the constness     of the matrix.         The general template is not implemented, only the specializations for     the two possible values of the second template argument. Therefore, the     interface listed here only serves as a template provided since doxygen     does not link the specializations.    
* [0.x.16]*
       Value of this matrix entry.      
* [0.x.17]*
       Value of this matrix entry.      
* [0.x.18]*
     The specialization for a const Accessor.    
* [0.x.19]*
       Typedef for the type (including constness) of the matrix to be used       here.      
* [0.x.20]*
       Constructor. Since we use accessors only for read access, a const       matrix pointer is sufficient.      
* [0.x.21]*
       Copy constructor to get from a const or non-const accessor to a const       accessor.      
* [0.x.22]*
       Value of this matrix entry.      
* [0.x.23]*
     The specialization for a mutable Accessor.    
* [0.x.24]*
         Constructor.        
* [0.x.25]*
         Conversion operator to the data type of the matrix.        
* [0.x.26]*
         Set the element of the matrix we presently point to to  [2.x.1]         
* [0.x.27]*
         Add  [2.x.2]  to the element of the matrix we presently point to.        
* [0.x.28]*
         Subtract  [2.x.3]  from the element of the matrix we presently point to.        
* [0.x.29]*
         Multiply the element of the matrix we presently point to by  [2.x.4]         
* [0.x.30]*
         Divide the element of the matrix we presently point to by  [2.x.5]         
* [0.x.31]*
         Pointer to the accessor that denotes which element we presently         point to.        
* [0.x.32]*
       Typedef for the type (including constness) of the matrix to be used       here.      
* [0.x.33]*
       Constructor. Since we use accessors only for read access, a const       matrix pointer is sufficient.      
* [0.x.34]*
       Value of this matrix entry.      
* [0.x.35]*
     This class acts as an iterator walking over the elements of Trilinos     matrices. The implementation of this class is similar to the one for     PETSc matrices.         Note that Trilinos stores the elements within each row in ascending     order. This is opposed to the deal.II sparse matrix style where the     diagonal element (if it exists) is stored before all other values, and     the PETSc sparse matrices, where one can't guarantee a certain order of     the elements.        
*  [2.x.6]     
* [0.x.36]*
       Declare type for container size.      
* [0.x.37]*
       Typedef for the matrix type (including constness) we are to operate       on.      
* [0.x.38]*
       Constructor. Create an iterator into the matrix  [2.x.7]  for the       given row and the index within it.      
* [0.x.39]*
       Copy constructor with optional change of constness.      
* [0.x.40]*
       Prefix increment.      
* [0.x.41]*
       Postfix increment.      
* [0.x.42]*
       Dereferencing operator.      
* [0.x.43]*
       Dereferencing operator.      
* [0.x.44]*
       Comparison. True, if both iterators point to the same matrix       position.      
* [0.x.45]*
       Inverse of <tt>==</tt>.      
* [0.x.46]*
       Comparison operator. Result is true if either the first row number is       smaller or if the row numbers are equal and the first index is       smaller.      
* [0.x.47]*
       Comparison operator. The opposite of the previous operator      
* [0.x.48]*
       Exception      
* [0.x.49]*
       Store an object of the accessor class.      
* [0.x.50]*
   This class implements a wrapper to use the Trilinos distributed sparse   matrix class Epetra_FECrsMatrix. This is precisely the kind of matrix we   deal with all the time
* 
*  - we most likely get it from some assembly   process, where also entries not locally owned might need to be written   and hence need to be forwarded to the owner process.  This class is   designed to be used in a distributed memory architecture with an MPI   compiler on the bottom, but works equally well also for serial processes.   The only requirement for this class to work is that Trilinos has been   installed with the same compiler as is used for generating deal.II.     The interface of this class is modeled after the existing SparseMatrix   class in deal.II. It has almost the same member functions, and is often   exchangeable. However, since Trilinos only supports a single scalar type   (double), it is not templated, and only works with doubles.     Note that Trilinos only guarantees that operations do what you expect if   the functions  [2.x.8]  has been called after matrix assembly.   Therefore, you need to call  [2.x.9]  before you actually   use the matrix. This also calls  [2.x.10]  that compresses the   storage format for sparse matrices by discarding unused elements.   Trilinos allows to continue with assembling the matrix after calls to   these functions, though.     [1.x.0]     When writing into Trilinos matrices from several threads in shared   memory, several things must be kept in mind as there is no built-in locks   in this class to prevent data races. Simultaneous access to the same   matrix row at the same time can lead to data races and must be explicitly   avoided by the user. However, it is possible to access [1.x.1]   rows of the matrix from several threads simultaneously under the   following three conditions:    [2.x.11]     [2.x.12]  The matrix uses only one MPI process.    [2.x.13]  The matrix has been initialized with the reinit() method with a   DynamicSparsityPattern (that includes the set of locally relevant rows,   i.e., the rows that an assembly routine will possibly write into).    [2.x.14]  The matrix has been initialized from a    [2.x.15]  object that in turn has been   initialized with the reinit function specifying three index sets, one for   the rows, one for the columns and for the larger set of  [2.x.16]    writeable_rows, and the operation is an addition. At some point in the   future, Trilinos support might be complete enough such that initializing   from a  [2.x.17]  that has been filled by a   function similar to  [2.x.18]  always results in a   matrix that allows several processes to write into the same matrix row.   However, Trilinos until version at least 11.12 does not correctly support   this feature.    [2.x.19]      Note that all other reinit methods and constructors of    [2.x.20]  will result in a matrix that needs to   allocate off-processor entries on demand, which breaks thread-safety. Of   course, using the respective reinit method for the block Trilinos   sparsity pattern and block matrix also results in thread-safety.    
*  [2.x.21]   
*  [2.x.22]   
* [0.x.51]*
     Declare the type for container size.    
* [0.x.52]*
     Exception    
* [0.x.53]*
     A structure that describes some of the traits of this class in terms of     its run-time behavior. Some other classes (such as the block matrix     classes) that take one or other of the matrix classes as its template     parameters can tune their behavior based on the variables in this     class.    
* [0.x.54]*
       It is safe to elide additions of zeros to individual elements of this       matrix.      
* [0.x.55]*
     Declare an alias for the iterator class.    
* [0.x.56]*
     Declare an alias for the const iterator class.    
* [0.x.57]*
     Declare an alias in analogy to all the other container classes.    
* [0.x.58]*
      [2.x.23]  Constructors and initialization.    
* [0.x.59]*
     Default constructor. Generates an empty (zero-size) matrix.    
* [0.x.60]*
     Generate a matrix that is completely stored locally, having #m rows and     #n columns.         The number of columns entries per row is specified as the maximum     number of entries argument.    
* [0.x.61]*
     Generate a matrix that is completely stored locally, having #m rows and     #n columns.         The vector <tt>n_entries_per_row</tt> specifies the number of entries     in each row.    
* [0.x.62]*
     Generate a matrix from a Trilinos sparsity pattern object.    
* [0.x.63]*
     Move constructor. Create a new sparse matrix by stealing the internal     data.    
* [0.x.64]*
     Copy constructor is deleted.    
* [0.x.65]*
     operator= is deleted.    
* [0.x.66]*
     Destructor. Made virtual so that one can use pointers to this class.    
* [0.x.67]*
     This function initializes the Trilinos matrix with a deal.II sparsity     pattern, i.e. it makes the Trilinos Epetra matrix know the position of     nonzero entries according to the sparsity pattern. This function is     meant for use in serial programs, where there is no need to specify how     the matrix is going to be distributed among different processors. This     function works in %parallel, too, but it is recommended to manually     specify the %parallel partitioning of the matrix using an Epetra_Map.     When run in %parallel, it is currently necessary that each processor     holds the sparsity_pattern structure because each processor sets its     rows.         This is a collective operation that needs to be called on all     processors in order to avoid a dead lock.    
* [0.x.68]*
     This function reinitializes the Trilinos sparse matrix from a (possibly     distributed) Trilinos sparsity pattern.         This is a collective operation that needs to be called on all     processors in order to avoid a dead lock.         If you want to write to the matrix from several threads and use MPI,     you need to use this reinit method with a sparsity pattern that has     been created with explicitly stating writeable rows. In all other     cases, you cannot mix MPI with multithreaded writing into the matrix.    
* [0.x.69]*
     This function copies the layout of  [2.x.24]  to the calling     matrix. The values are not copied, but you can use copy_from() for     this.         This is a collective operation that needs to be called on all     processors in order to avoid a dead lock.    
* [0.x.70]*
     This function initializes the Trilinos matrix using the deal.II sparse     matrix and the entries stored therein. It uses a threshold to copy only     elements with modulus larger than the threshold (so zeros in the     deal.II matrix can be filtered away).         The optional parameter <tt>copy_values</tt> decides whether only the     sparsity structure of the input matrix should be used or the matrix     entries should be copied, too.         This is a collective operation that needs to be called on all     processors in order to avoid a deadlock.        
*  [2.x.25]  If a different sparsity pattern is given in the last argument     (i.e., one that differs from the one used in the sparse matrix given in     the first argument), then the resulting Trilinos matrix will have the     sparsity pattern so given. This of course also means that all entries     in the given matrix that are not part of this separate sparsity pattern     will in fact be dropped.    
* [0.x.71]*
     This reinit function takes as input a Trilinos Epetra_CrsMatrix and     copies its sparsity pattern. If so requested, even the content (values)     will be copied.    
* [0.x.72]*
      [2.x.26]  Constructors and initialization using an IndexSet description    
* [0.x.73]*
     Constructor using an IndexSet and an MPI communicator to describe the     %parallel partitioning. The parameter  [2.x.27]  sets the     number of nonzero entries in each row that will be allocated. Note that     this number does not need to be exact, and it is even allowed that the     actual matrix structure has more nonzero entries than specified in the     constructor. However it is still advantageous to provide good estimates     here since this will considerably increase the performance of the     matrix setup. However, there is no effect in the performance of matrix-     vector products, since Trilinos reorganizes the matrix memory prior to     use (in the compress() step).    
* [0.x.74]*
     Same as before, but now set the number of nonzeros in each matrix row     separately. Since we know the number of elements in the matrix exactly     in this case, we can already allocate the right amount of memory, which     makes the creation process including the insertion of nonzero elements     by the respective  [2.x.28]  call considerably faster.    
* [0.x.75]*
     This constructor is similar to the one above, but it now takes two     different IndexSet partitions for row and columns. This interface is     meant to be used for generating rectangular matrices, where the first     index set describes the %parallel partitioning of the degrees of     freedom associated with the matrix rows and the second one the     partitioning of the matrix columns. The second index set specifies the     partitioning of the vectors this matrix is to be multiplied with, not     the distribution of the elements that actually appear in the matrix.         The parameter  [2.x.29]  defines how much memory will be     allocated for each row. This number does not need to be accurate, as     the structure is reorganized in the compress() call.    
* [0.x.76]*
     This constructor is similar to the one above, but it now takes two     different Epetra maps for rows and columns. This interface is meant to     be used for generating rectangular matrices, where one map specifies     the %parallel distribution of degrees of freedom associated with matrix     rows and the second one specifies the %parallel distribution the dofs     associated with columns in the matrix. The second map also provides     information for the internal arrangement in matrix vector products     (i.e., the distribution of vector this matrix is to be multiplied     with), but is not used for the distribution of the columns &ndash;     rather, all column elements of a row are stored on the same processor     in any case. The vector <tt>n_entries_per_row</tt> specifies the number     of entries in each row of the newly generated matrix.    
* [0.x.77]*
     This function is initializes the Trilinos Epetra matrix according to     the specified sparsity_pattern, and also reassigns the matrix rows to     different processes according to a user-supplied index set and     %parallel communicator. In programs following the style of the tutorial     programs, this function (and the respective call for a rectangular     matrix) are the natural way to initialize the matrix size, its     distribution among the MPI processes (if run in %parallel) as well as     the location of non-zero elements. Trilinos stores the sparsity pattern     internally, so it won't be needed any more after this call, in contrast     to the deal.II own object. The optional argument  [2.x.30]  can     be used for reinitialization with a sparsity pattern that is not fully     constructed. This feature is only implemented for input sparsity     patterns of type DynamicSparsityPattern. If the flag is not set, each     processor just sets the elements in the sparsity pattern that belong to     its rows.         This is a collective operation that needs to be called on all     processors in order to avoid a dead lock.    
* [0.x.78]*
     This function is similar to the other initialization function above,     but now also reassigns the matrix rows and columns according to two     user-supplied index sets.  To be used for rectangular matrices. The     optional argument  [2.x.31]  can be used for reinitialization     with a sparsity pattern that is not fully constructed. This feature is     only implemented for input sparsity patterns of type     DynamicSparsityPattern.         This is a collective operation that needs to be called on all     processors in order to avoid a dead lock.    
* [0.x.79]*
     This function initializes the Trilinos matrix using the deal.II sparse     matrix and the entries stored therein. It uses a threshold to copy only     elements with modulus larger than the threshold (so zeros in the     deal.II matrix can be filtered away). In contrast to the other reinit     function with deal.II sparse matrix argument, this function takes a     %parallel partitioning specified by the user instead of internally     generating it.         The optional parameter <tt>copy_values</tt> decides whether only the     sparsity structure of the input matrix should be used or the matrix     entries should be copied, too.         This is a collective operation that needs to be called on all     processors in order to avoid a dead lock.    
* [0.x.80]*
     This function is similar to the other initialization function with     deal.II sparse matrix input above, but now takes index sets for both     the rows and the columns of the matrix. Chosen for rectangular     matrices.         The optional parameter <tt>copy_values</tt> decides whether only the     sparsity structure of the input matrix should be used or the matrix     entries should be copied, too.         This is a collective operation that needs to be called on all     processors in order to avoid a dead lock.    
* [0.x.81]*
      [2.x.32]  Information on the matrix    
* [0.x.82]*
     Return the number of rows in this matrix.    
* [0.x.83]*
     Return the number of columns in this matrix.    
* [0.x.84]*
     Return the local dimension of the matrix, i.e. the number of rows     stored on the present MPI process. For sequential matrices, this number     is the same as m(), but for %parallel matrices it may be smaller.         To figure out which elements exactly are stored locally, use     local_range().    
* [0.x.85]*
     Return a pair of indices indicating which rows of this matrix are     stored locally. The first number is the index of the first row stored,     the second the index of the one past the last one that is stored     locally. If this is a sequential matrix, then the result will be the     pair (0,m()), otherwise it will be a pair (i,i+n), where     <tt>n=local_size()</tt>.    
* [0.x.86]*
     Return whether  [2.x.33]  is in the local range or not, see also     local_range().    
* [0.x.87]*
     Return the total number of nonzero elements of this matrix (summed     over all MPI processes).    
* [0.x.88]*
     Number of entries in a specific row.    
* [0.x.89]*
     Return the state of the matrix, i.e., whether compress() needs to be     called after an operation requiring data exchange. A call to compress()     is also needed when the method set() has been called (even when working     in serial).    
* [0.x.90]*
     Determine an estimate for the memory consumption (in bytes) of this     object. Note that only the memory reserved on the current processor is     returned in case this is called in an MPI-based program.    
* [0.x.91]*
     Return the MPI communicator object in use with this matrix.    
* [0.x.92]*
      [2.x.34]  Modifying entries    
* [0.x.93]*
     This operator assigns a scalar to a matrix. Since this does usually not     make much sense (should we set all matrix entries to this value?  Only     the nonzero entries of the sparsity pattern?), this operation is only     allowed if the actual value to be assigned is zero. This operator only     exists to allow for the obvious notation <tt>matrix=0</tt>, which sets     all elements of the matrix to zero, but keeps the sparsity pattern     previously used.    
* [0.x.94]*
     Release all memory and return to a state just like after having called     the default constructor.         This is a collective operation that needs to be called on all     processors in order to avoid a dead lock.    
* [0.x.95]*
     This command does two things:      [2.x.35]       [2.x.36]  If the matrix was initialized without a sparsity pattern, elements     have been added manually using the set() command. When this process is     completed, a call to compress() reorganizes the internal data     structures (sparsity pattern) so that a fast access to data is possible     in matrix-vector products.      [2.x.37]  If the matrix structure has already been fixed (either by     initialization with a sparsity pattern or by calling compress() during     the setup phase), this command does the %parallel exchange of data.     This is necessary when we perform assembly on more than one (MPI)     process, because then some non-local row data will accumulate on nodes     that belong to the current's processor element, but are actually held     by another. This command is usually called after all elements have been     traversed.      [2.x.38]          In both cases, this function compresses the data structures and allows     the resulting matrix to be used in all other operations like matrix-     vector products. This is a collective operation, i.e., it needs to be     run on all processors when used in %parallel.         See      [2.x.39]  "Compressing distributed objects"     for more information.    
* [0.x.96]*
     Set the element ([1.x.2]) to  [2.x.40]          This function is able to insert new elements into the matrix as long as     compress() has not been called, so the sparsity pattern will be     extended. When compress() is called for the first time (or in case the     matrix is initialized from a sparsity pattern), no new elements can be     added and an insertion of elements at positions which have not been     initialized will throw an exception.         For the case that the matrix is constructed without a sparsity pattern     and new matrix entries are added on demand, please note the following     behavior imposed by the underlying Epetra_FECrsMatrix data structure:     If the same matrix entry is inserted more than once, the matrix entries     will be added upon calling compress() (since Epetra does not track     values to the same entry before the final compress() is called), even     if  [2.x.41]  is specified as argument to compress(). In     the case you cannot make sure that matrix entries are only set once,     initialize the matrix with a sparsity pattern to fix the matrix     structure before inserting elements.    
* [0.x.97]*
     Set all elements given in a FullMatrix<double> into the sparse matrix     locations given by <tt>indices</tt>. In other words, this function     writes the elements in <tt>full_matrix</tt> into the calling matrix,     using the local-to-global indexing specified by <tt>indices</tt> for     both the rows and the columns of the matrix. This function assumes a     quadratic sparse matrix and a quadratic full_matrix, the usual     situation in FE calculations.         This function is able to insert new elements into the matrix as long as     compress() has not been called, so the sparsity pattern will be     extended. After compress() has been called for the first time or the     matrix has been initialized from a sparsity pattern, extending the     sparsity pattern is no longer possible and an insertion of elements at     positions which have not been initialized will throw an exception.         The optional parameter <tt>elide_zero_values</tt> can be used to     specify whether zero values should be inserted anyway or they should be     filtered away. The default value is <tt>false</tt>, i.e., even zero     values are inserted/replaced.         For the case that the matrix is constructed without a sparsity pattern     and new matrix entries are added on demand, please note the following     behavior imposed by the underlying Epetra_FECrsMatrix data structure:     If the same matrix entry is inserted more than once, the matrix entries     will be added upon calling compress() (since Epetra does not track     values to the same entry before the final compress() is called), even     if  [2.x.42]  is specified as argument to compress(). In     the case you cannot make sure that matrix entries are only set once,     initialize the matrix with a sparsity pattern to fix the matrix     structure before inserting elements.    
* [0.x.98]*
     Same function as before, but now including the possibility to use     rectangular full_matrices and different local-to-global indexing on     rows and columns, respectively.    
* [0.x.99]*
     Set several elements in the specified row of the matrix with column     indices as given by <tt>col_indices</tt> to the respective value.         This function is able to insert new elements into the matrix as long as     compress() has not been called, so the sparsity pattern will be     extended. After compress() has been called for the first time or the     matrix has been initialized from a sparsity pattern, extending the     sparsity pattern is no longer possible and an insertion of elements at     positions which have not been initialized will throw an exception.         The optional parameter <tt>elide_zero_values</tt> can be used to     specify whether zero values should be inserted anyway or they should be     filtered away. The default value is <tt>false</tt>, i.e., even zero     values are inserted/replaced.         For the case that the matrix is constructed without a sparsity pattern     and new matrix entries are added on demand, please note the following     behavior imposed by the underlying Epetra_FECrsMatrix data structure:     If the same matrix entry is inserted more than once, the matrix entries     will be added upon calling compress() (since Epetra does not track     values to the same entry before the final compress() is called), even     if  [2.x.43]  is specified as argument to compress(). In     the case you cannot make sure that matrix entries are only set once,     initialize the matrix with a sparsity pattern to fix the matrix     structure before inserting elements.    
* [0.x.100]*
     Set several elements to values given by <tt>values</tt> in a given row     in columns given by col_indices into the sparse matrix.         This function is able to insert new elements into the matrix as long as     compress() has not been called, so the sparsity pattern will be     extended. After compress() has been called for the first time or the     matrix has been initialized from a sparsity pattern, extending the     sparsity pattern is no longer possible and an insertion of elements at     positions which have not been initialized will throw an exception.         The optional parameter <tt>elide_zero_values</tt> can be used to     specify whether zero values should be inserted anyway or they should be     filtered away. The default value is <tt>false</tt>, i.e., even zero     values are inserted/replaced.         For the case that the matrix is constructed without a sparsity pattern     and new matrix entries are added on demand, please note the following     behavior imposed by the underlying Epetra_FECrsMatrix data structure:     If the same matrix entry is inserted more than once, the matrix entries     will be added upon calling compress() (since Epetra does not track     values to the same entry before the final compress() is called), even     if  [2.x.44]  is specified as argument to compress(). In     the case you cannot make sure that matrix entries are only set once,     initialize the matrix with a sparsity pattern to fix the matrix     structure before inserting elements.    
* [0.x.101]*
     Add  [2.x.45]  to the element ([1.x.3]).         Just as the respective call in deal.II SparseMatrix<Number> class (but     in contrast to the situation for PETSc based matrices), this function     throws an exception if an entry does not exist in the sparsity pattern.     Moreover, if <tt>value</tt> is not a finite number an exception is     thrown.    
* [0.x.102]*
     Add all elements given in a FullMatrix<double> into sparse matrix     locations given by <tt>indices</tt>. In other words, this function adds     the elements in <tt>full_matrix</tt> to the respective entries in     calling matrix, using the local-to-global indexing specified by     <tt>indices</tt> for both the rows and the columns of the matrix. This     function assumes a quadratic sparse matrix and a quadratic full_matrix,     the usual situation in FE calculations.         Just as the respective call in deal.II SparseMatrix<Number> class (but     in contrast to the situation for PETSc based matrices), this function     throws an exception if an entry does not exist in the sparsity pattern.         The optional parameter <tt>elide_zero_values</tt> can be used to     specify whether zero values should be added anyway or these should be     filtered away and only non-zero data is added. The default value is     <tt>true</tt>, i.e., zero values won't be added into the matrix.    
* [0.x.103]*
     Same function as before, but now including the possibility to use     rectangular full_matrices and different local-to-global indexing on     rows and columns, respectively.    
* [0.x.104]*
     Set several elements in the specified row of the matrix with column     indices as given by <tt>col_indices</tt> to the respective value.         Just as the respective call in deal.II SparseMatrix<Number> class (but     in contrast to the situation for PETSc based matrices), this function     throws an exception if an entry does not exist in the sparsity pattern.         The optional parameter <tt>elide_zero_values</tt> can be used to     specify whether zero values should be added anyway or these should be     filtered away and only non-zero data is added. The default value is     <tt>true</tt>, i.e., zero values won't be added into the matrix.    
* [0.x.105]*
     Add an array of values given by <tt>values</tt> in the given global     matrix row at columns specified by col_indices in the sparse matrix.         Just as the respective call in deal.II SparseMatrix<Number> class (but     in contrast to the situation for PETSc based matrices), this function     throws an exception if an entry does not exist in the sparsity pattern.         The optional parameter <tt>elide_zero_values</tt> can be used to     specify whether zero values should be added anyway or these should be     filtered away and only non-zero data is added. The default value is     <tt>true</tt>, i.e., zero values won't be added into the matrix.    
* [0.x.106]*
     Multiply the entire matrix by a fixed factor.    
* [0.x.107]*
     Divide the entire matrix by a fixed factor.    
* [0.x.108]*
     Copy the given (Trilinos) matrix (sparsity pattern and entries).    
* [0.x.109]*
     Add <tt>matrix</tt> scaled by <tt>factor</tt> to this matrix, i.e. the     matrix <tt>factor*matrix</tt> is added to <tt>this</tt>. If the     sparsity pattern of the calling matrix does not contain all the     elements in the sparsity pattern of the input matrix, this function     will throw an exception.    
* [0.x.110]*
     Remove all elements from this <tt>row</tt> by setting them to zero. The     function does not modify the number of allocated nonzero entries, it     only sets the entries to zero.         This operation is used in eliminating constraints (e.g. due to hanging     nodes) and makes sure that we can write this modification to the matrix     without having to read entries (such as the locations of non-zero     elements) from it &mdash; without this operation, removing constraints     on %parallel matrices is a rather complicated procedure.         The second parameter can be used to set the diagonal entry of this row     to a value different from zero. The default is to set it to zero.        
*  [2.x.46]  If the matrix is stored in parallel across multiple processors     using MPI, this function only touches rows that are locally stored and     simply ignores all other row indices. Further, in the context of     parallel computations, you will get into trouble if you clear a row     while other processors still have pending writes or additions into the     same row. In other words, if another processor still wants to add     something to an element of a row and you call this function to zero out     the row, then the next time you call compress() may add the remote     value to the zero you just created. Consequently, you will want to call     compress() after you made the last modifications to a matrix and before     starting to clear rows.    
* [0.x.111]*
     Same as clear_row(), except that it works on a number of rows at once.         The second parameter can be used to set the diagonal entries of all     cleared rows to something different from zero. Note that all of these     diagonal entries get the same value
* 
*  -  if you want different values for     the diagonal entries, you have to set them by hand.        
*  [2.x.47]  If the matrix is stored in parallel across multiple processors     using MPI, this function only touches rows that are locally stored and     simply ignores all other row indices. Further, in the context of     parallel computations, you will get into trouble if you clear a row     while other processors still have pending writes or additions into the     same row. In other words, if another processor still wants to add     something to an element of a row and you call this function to zero out     the row, then the next time you call compress() may add the remote     value to the zero you just created. Consequently, you will want to call     compress() after you made the last modifications to a matrix and before     starting to clear rows.    
* [0.x.112]*
     Sets an internal flag so that all operations performed by the matrix,     i.e., multiplications, are done in transposed order. However, this does     not reshape the matrix to transposed form directly, so care should be     taken when using this flag.        
*  [2.x.48]  Calling this function any even number of times in succession will     return the object to its original state.    
* [0.x.113]*
      [2.x.49]  Entry Access    
* [0.x.114]*
     Return the value of the entry ([1.x.4]).  This may be an expensive     operation and you should always take care where to call this function.     As in the deal.II sparse matrix class, we throw an exception if the     respective entry doesn't exist in the sparsity pattern of this class,     which is requested from Trilinos. Moreover, an exception will be thrown     when the requested element is not saved on the calling process.    
* [0.x.115]*
     Return the value of the matrix entry ([1.x.5]). If this entry does     not exist in the sparsity pattern, then zero is returned. While this     may be convenient in some cases, note that it is simple to write     algorithms that are slow compared to an optimal solution, since the     sparsity of the matrix is not used.  On the other hand, if you want to     be sure the entry exists, you should use operator() instead.         The lack of error checking in this function can also yield surprising     results if you have a parallel matrix. In that case, just because you     get a zero result from this function does not mean that either the     entry does not exist in the sparsity pattern or that it does but has a     value of zero. Rather, it could also be that it simply isn't stored on     the current processor; in that case, it may be stored on a different     processor, and possibly so with a nonzero value.    
* [0.x.116]*
     Return the main diagonal element in the [1.x.6]th row. This function     throws an error if the matrix is not quadratic and it also throws an     error if [1.x.7] is not element of the local matrix.  See also the     comment in trilinos_sparse_matrix.cc.    
* [0.x.117]*
      [2.x.50]  Multiplications    
* [0.x.118]*
     Matrix-vector multiplication: let [1.x.8] with [1.x.9]     being this matrix.         Source and destination must not be the same vector.         This function can be called with several types of vector objects,     namely  [2.x.51]  can be      [2.x.52]       [2.x.53]   [2.x.54]       [2.x.55]   [2.x.56]       [2.x.57]   [2.x.58]       [2.x.59]  Vector<double>,      [2.x.60]   [2.x.61]       [2.x.62]          When using vectors of type  [2.x.63]  the vector      [2.x.64]  has to be initialized with the same IndexSet that was used for     the row indices of the matrix and the vector  [2.x.65]  has to be     initialized with the same IndexSet that was used for the column indices     of the matrix.         This function will be called when the underlying number type for the     matrix object and the one for the vector object are the same.     Despite looking complicated, the return type is just `void`.         In case of a serial vector, this function will only work when     running on one processor, since the matrix object is inherently     distributed. Otherwise, an exception will be thrown.    
* [0.x.119]*
     Same as the function above for the case that the underlying number type     for the matrix object and the one for the vector object do not coincide.     This case is not implemented. Calling it will result in a runtime error.     Despite looking complicated, the return type is just `void`.    
* [0.x.120]*
     Matrix-vector multiplication: let [1.x.10] with     [1.x.11] being this matrix. This function does the same as vmult() but     takes the transposed matrix.         Source and destination must not be the same vector.         This function can be called with several types of vector objects,     see the discussion about  [2.x.66]  in vmult().         This function will be called when the underlying number type for the     matrix object and the one for the vector object are the same.     Despite looking complicated, the return type is just `void`.    
* [0.x.121]*
     Same as the function above for the case that the underlying number type     for the matrix object and the one for the vector object do not coincide.     This case is not implemented. Calling it will result in a runtime error.     Despite looking complicated, the return type is just `void`.    
* [0.x.122]*
     Adding matrix-vector multiplication. Add [1.x.12] on [1.x.13]     with [1.x.14] being this matrix.         Source and destination must not be the same vector.         This function can be called with several types of vector objects,     see the discussion about  [2.x.67]  in vmult().    
* [0.x.123]*
     Adding matrix-vector multiplication. Add [1.x.15] to     [1.x.16] with [1.x.17] being this matrix. This function does the same     as vmult_add() but takes the transposed matrix.         Source and destination must not be the same vector.         This function can be called with several types of vector objects,     see the discussion about  [2.x.68]  in vmult().    
* [0.x.124]*
     Return the square of the norm of the vector  [2.x.69]  with respect to the     norm induced by this matrix, i.e.,  [2.x.70] . This is useful,     e.g. in the finite element context, where the  [2.x.71]  norm of a function     equals the matrix norm with respect to the mass matrix of the vector     representing the nodal values of the finite element function.         Obviously, the matrix needs to be quadratic for this operation.         The implementation of this function is not as efficient as the one in     the  [2.x.72]  class used in deal.II (i.e. the original one, not     the Trilinos wrapper class) since Trilinos doesn't support this     operation and needs a temporary vector.         The vector has to be initialized with the same IndexSet the matrix     was initialized with.         In case of a localized Vector, this function will only work when     running on one processor, since the matrix object is inherently     distributed. Otherwise, an exception will be thrown.    
* [0.x.125]*
     Compute the matrix scalar product  [2.x.73] .         The implementation of this function is not as efficient as the one in     the  [2.x.74]  class used in deal.II (i.e. the original one, not     the Trilinos wrapper class) since Trilinos doesn't support this     operation and needs a temporary vector.         The vector  [2.x.75]  has to be initialized with the same IndexSet that     was used for the row indices of the matrix and the vector  [2.x.76]  has     to be initialized with the same IndexSet that was used for the     column indices of the matrix.         In case of a localized Vector, this function will only work when     running on one processor, since the matrix object is inherently     distributed. Otherwise, an exception will be thrown.         This function is only implemented for square matrices.    
* [0.x.126]*
     Compute the residual of an equation [1.x.18], where the residual is     defined to be [1.x.19]. Write the residual into  [2.x.77]  The     [1.x.20] norm of the residual vector is returned.         Source [1.x.21] and destination [1.x.22] must not be the same vector.         The vectors  [2.x.78]  and  [2.x.79]  have to be initialized with the same     IndexSet that was used for the row indices of the matrix and the vector      [2.x.80]  has to be initialized with the same IndexSet that was used for the     column indices of the matrix.         In case of a localized Vector, this function will only work when     running on one processor, since the matrix object is inherently     distributed. Otherwise, an exception will be thrown.    
* [0.x.127]*
     Perform the matrix-matrix multiplication <tt>C = A B</tt>, or, if an     optional vector argument is given, <tt>C = A diag(V) B</tt>, where     <tt>diag(V)</tt> defines a diagonal matrix with the vector entries.         This function assumes that the calling matrix <tt>A</tt> and <tt>B</tt>     have compatible sizes. The size of <tt>C</tt> will be set within this     function.         The content as well as the sparsity pattern of the matrix C will be     changed by this function, so make sure that the sparsity pattern is not     used somewhere else in your program. This is an expensive operation, so     think twice before you use this function.    
* [0.x.128]*
     Perform the matrix-matrix multiplication with the transpose of     <tt>this</tt>, i.e., <tt>C = A<sup>T</sup> B</tt>, or, if an optional     vector argument is given, <tt>C = A<sup>T</sup> diag(V) B</tt>,     where <tt>diag(V)</tt> defines a diagonal matrix with the vector     entries.         This function assumes that the calling matrix <tt>A</tt> and <tt>B</tt>     have compatible sizes. The size of <tt>C</tt> will be set within this     function.         The content as well as the sparsity pattern of the matrix C will be     changed by this function, so make sure that the sparsity pattern is not     used somewhere else in your program. This is an expensive operation, so     think twice before you use this function.    
* [0.x.129]*
      [2.x.81]  Matrix norms    
* [0.x.130]*
     Return the [1.x.23]<sub>1</sub>-norm of the matrix, that is  [2.x.82] , (max. sum of columns).  This is the natural matrix norm that     is compatible to the l1-norm for vectors, i.e.   [2.x.83] .  (cf. Haemmerlin-Hoffmann: Numerische Mathematik)    
* [0.x.131]*
     Return the linfty-norm of the matrix, that is      [2.x.84] , (max. sum of rows).  This is the natural matrix norm that     is compatible to the linfty-norm of vectors, i.e.   [2.x.85] .  (cf. Haemmerlin-Hoffmann: Numerische     Mathematik)    
* [0.x.132]*
     Return the frobenius norm of the matrix, i.e. the square root of the     sum of squares of all entries in the matrix.    
* [0.x.133]*
      [2.x.86]  Access to underlying Trilinos data    
* [0.x.134]*
     Return a const reference to the underlying Trilinos Epetra_CrsMatrix     data.    
* [0.x.135]*
     Return a const reference to the underlying Trilinos Epetra_CrsGraph     data that stores the sparsity pattern of the matrix.    
* [0.x.136]*
      [2.x.87]  Partitioners    
* [0.x.137]*
     Return the partitioning of the domain space of this matrix, i.e., the     partitioning of the vectors this matrix has to be multiplied with.    
* [0.x.138]*
     Return the partitioning of the range space of this matrix, i.e., the     partitioning of the vectors that are result from matrix-vector     products.    
* [0.x.139]*
      [2.x.88]  Iterators    
* [0.x.140]*
     Return an iterator pointing to the first element of the matrix.         The elements accessed by iterators within each row are ordered in the     way in which Trilinos stores them, though the implementation guarantees     that all elements of one row are accessed before the elements of the     next row. If your algorithm relies on visiting elements within one row,     you will need to consult with the Trilinos documentation on the order     in which it stores data. It is, however, generally not a good and long-     term stable idea to rely on the order in which receive elements if you     iterate over them.         When you iterate over the elements of a parallel matrix, you will only     be able to access the locally owned rows. (You can access the other     rows as well, but they will look empty.) In that case, you probably     want to call the begin() function that takes the row as an argument to     limit the range of elements to loop over.    
* [0.x.141]*
     Like the function above, but for non-const matrices.    
* [0.x.142]*
     Return an iterator pointing the element past the last one of this     matrix.    
* [0.x.143]*
     Like the function above, but for non-const matrices.    
* [0.x.144]*
     Return an iterator pointing to the first element of row  [2.x.89]          Note that if the given row is empty, i.e. does not contain any nonzero     entries, then the iterator returned by this function equals     <tt>end(r)</tt>. The returned iterator may not be dereferenceable in     that case if neither row  [2.x.90]  nor any of the following rows contain any     nonzero entries.         The elements accessed by iterators within each row are ordered in the     way in which Trilinos stores them, though the implementation guarantees     that all elements of one row are accessed before the elements of the     next row. If your algorithm relies on visiting elements within one row,     you will need to consult with the Trilinos documentation on the order     in which it stores data. It is, however, generally not a good and long-     term stable idea to rely on the order in which receive elements if you     iterate over them.        
*  [2.x.91]  When you access the elements of a parallel matrix, you can only     access the elements of rows that are actually stored locally. (You can     access the other rows as well, but they will look empty.) Even then, if     another processor has since written into, or added to, an element of     the matrix that is stored on the current processor, then you will still     see the old value of this entry unless you have called compress()     between modifying the matrix element on the remote processor and     accessing it on the current processor. See the documentation of the     compress() function for more information.    
* [0.x.145]*
     Like the function above, but for non-const matrices.    
* [0.x.146]*
     Return an iterator pointing the element past the last one of row  [2.x.92]  ,     or past the end of the entire sparsity pattern if none of the rows     after  [2.x.93]  contain any entries at all.         Note that the end iterator is not necessarily dereferenceable. This is     in particular the case if it is the end iterator for the last row of a     matrix.    
* [0.x.147]*
     Like the function above, but for non-const matrices.    
* [0.x.148]*
      [2.x.94]  Input/Output    
* [0.x.149]*
     Abstract Trilinos object that helps view in ASCII other Trilinos     objects. Currently this function is not implemented.  TODO: Not     implemented.    
* [0.x.150]*
     Print the matrix to the given stream, using the format <tt>(line,col)     value</tt>, i.e. one nonzero entry of the matrix per line. The optional     flag outputs the sparsity pattern in Trilinos style, where the data is     sorted according to the processor number when printed to the stream, as     well as a summary of the matrix like the global size.    
* [0.x.151]*
      [2.x.95]  Exceptions    
* [0.x.152]*
     Exception    
* [0.x.153]*
     Exception    
* [0.x.154]*
     Exception    
* [0.x.155]*
     Exception    
* [0.x.156]*
     Exception    
* [0.x.157]*
     Exception    
* [0.x.158]*
     For some matrix storage formats, in particular for the PETSc     distributed blockmatrices, set and add operations on individual     elements can not be freely mixed. Rather, one has to synchronize     operations when one wants to switch from setting elements to adding to     elements.  BlockMatrixBase automatically synchronizes the access by     calling this helper function for each block.  This function ensures     that the matrix is in a state that allows adding elements; if it     previously already was in this state, the function does nothing.    
* [0.x.159]*
     Same as prepare_add() but prepare the matrix for setting elements if     the representation of elements in this class requires such an     operation.    
* [0.x.160]*
     Pointer to the user-supplied Epetra Trilinos mapping of the matrix     columns that assigns parts of the matrix to the individual processes.    
* [0.x.161]*
     A sparse matrix object in Trilinos to be used for finite element based     problems which allows for assembling into non-local elements.  The     actual type, a sparse matrix, is set in the constructor.    
* [0.x.162]*
     A sparse matrix object in Trilinos to be used for collecting the non-     local elements if the matrix was constructed from a Trilinos sparsity     pattern with the respective option.    
* [0.x.163]*
     An export object used to communicate the nonlocal matrix.    
* [0.x.164]*
     Trilinos doesn't allow to mix additions to matrix entries and     overwriting them (to make synchronization of %parallel computations     simpler). The way we do it is to, for each access operation, store     whether it is an insertion or an addition. If the previous one was of     different type, then we first have to flush the Trilinos buffers;     otherwise, we can simply go on. Luckily, Trilinos has an object for     this which does already all the %parallel communications in such a     case, so we simply use their model, which stores whether the last     operation was an addition or an insertion.    
* [0.x.165]*
     A boolean variable to hold information on whether the vector is     compressed or not.    
* [0.x.166]*
       This is an extension class to LinearOperators for Trilinos sparse       matrix and preconditioner types. It provides the interface to       performing basic operations (<tt>vmult</tt> and <tt>Tvmult</tt>)  on       Trilinos vector types. It fulfills the requirements necessary for       wrapping a Trilinos solver, which calls Epetra_Operator functions, as a       LinearOperator.            
*  [2.x.96]  The  [2.x.97]  or        [2.x.98]  that this payload wraps is passed by       reference to the <tt>vmult</tt> and <tt>Tvmult</tt> functions. This       object is not thread-safe when the transpose flag is set on it or the       Trilinos object to which it refers. See the docuemtation for the        [2.x.99]        function for further details.                  
*  [2.x.100]       
* [0.x.167]*
         Definition for the internally supported vector type.        
* [0.x.168]*
         Definition for the vector type for the domain space of the operator.        
* [0.x.169]*
         Definition for the vector type for the range space of the operator.        
* [0.x.170]*
          [2.x.101]  Constructors / destructor        
* [0.x.171]*
         Default constructor                
*  [2.x.102]  By design, the resulting object is inoperable since there is         insufficient information with which to construct the domain and         range maps.        
* [0.x.172]*
         Constructor for a sparse matrix based on an exemplary matrix        
* [0.x.173]*
         Constructor for a preconditioner based on an exemplary matrix        
* [0.x.174]*
         Constructor for a preconditioner based on an exemplary preconditioner        
* [0.x.175]*
         Default copy constructor        
* [0.x.176]*
         Composite copy constructor                 This is required for PackagedOperations as it sets up the domain and         range maps, and composite <tt>vmult</tt> and <tt>Tvmult</tt>         operations based on the combined operation of both operations        
* [0.x.177]*
         Destructor        
* [0.x.178]*
         Return a payload configured for identity operations        
* [0.x.179]*
         Return a payload configured for null operations        
* [0.x.180]*
         Return a payload configured for transpose operations        
* [0.x.181]*
         Return a payload configured for inverse operations                 Invoking this factory function will configure two additional         functions, namely <tt>inv_vmult</tt> and <tt>inv_Tvmult</tt>, both of         which wrap inverse operations. The <tt>vmult</tt> and <tt>Tvmult</tt>         operations retain the standard         definitions inherited from  [2.x.103]                 
*  [2.x.104]  This function is enabled only if the solver and preconditioner         derive from the respective TrilinosWrappers base classes.         The C++ compiler will therefore only consider this function if the         following criterion are satisfied:         1. the  [2.x.105]  derives from  [2.x.106]  and         2. the  [2.x.107]  derives from  [2.x.108]         
* [0.x.182]*
         Return a payload configured for inverse operations                 Invoking this factory function will configure two additional         functions, namely <tt>inv_vmult</tt> and <tt>inv_Tvmult</tt>, both of         which         are disabled because the  [2.x.109]  or  [2.x.110]  are not         compatible with Epetra_MultiVector.         The <tt>vmult</tt> and <tt>Tvmult</tt> operations retain the standard         definitions inherited from  [2.x.111]                 
*  [2.x.112]  The C++ compiler will only consider this function if the         following criterion are satisfied:         1. the  [2.x.113]  does not derive from  [2.x.114]  and         2. the  [2.x.115]  does not derive from          [2.x.116]         
* [0.x.183]*
          [2.x.117]  LinearOperator functionality        
* [0.x.184]*
         Return an IndexSet that defines the partitioning of the domain space         of this matrix, i.e., the partitioning of the vectors this matrix has         to be multiplied with / operate on.        
* [0.x.185]*
         Return an IndexSet that defines the partitioning of the range space         of this matrix, i.e., the partitioning of the vectors that result         from matrix-vector products.        
* [0.x.186]*
         Return the MPI communicator object in use with this Payload.        
* [0.x.187]*
         Sets an internal flag so that all operations performed by the matrix,         i.e., multiplications, are done in transposed order.        
*  [2.x.118]  This does not reshape the matrix to transposed form directly,         so care should be taken when using this flag.        
* [0.x.188]*
         The standard matrix-vector operation to be performed by the payload         when Apply is called.                
*  [2.x.119]  This is not called by a LinearOperator, but rather by Trilinos         functions that expect this to mimic the action of the LinearOperator.        
* [0.x.189]*
         The standard transpose matrix-vector operation to be performed by         the payload when Apply is called.                
*  [2.x.120]  This is not called by a LinearOperator, but rather by Trilinos         functions that expect this to mimic the action of the LinearOperator.        
* [0.x.190]*
         The inverse matrix-vector operation to be performed by the payload         when ApplyInverse is called.                
*  [2.x.121]  This is not called by a LinearOperator, but rather by Trilinos         functions that expect this to mimic the action of the         InverseOperator.        
* [0.x.191]*
         The inverse transpose matrix-vector operation to be performed by         the payload when ApplyInverse is called.                
*  [2.x.122]  This is not called by a LinearOperator, but rather by Trilinos         functions that expect this to mimic the action of the         InverseOperator.        
* [0.x.192]*
          [2.x.123]  Core Epetra_Operator functionality        
* [0.x.193]*
         Return the status of the transpose flag for this operator                 This overloads the same function from the Trilinos class         Epetra_Operator.        
* [0.x.194]*
         Sets an internal flag so that all operations performed by the matrix,         i.e., multiplications, are done in transposed order.                 This overloads the same function from the Trilinos class         Epetra_Operator.                
*  [2.x.124]  This does not reshape the matrix to transposed form directly,         so care should be taken when using this flag. When the flag is set to         true (either here or directly on the underlying Trilinos object         itself), this object is no longer thread-safe. In essence, it is not         possible ensure that the transposed state of the LinearOperator and         the underlying Trilinos object remain synchronized throughout all         operations that may occur on different threads simultaneously.        
* [0.x.195]*
         Apply the vmult operation on a vector  [2.x.125]  (of internally defined         type VectorType) and store the result in the vector  [2.x.126]                  This overloads the same function from the Trilinos class         Epetra_Operator.                
*  [2.x.127]  The intended operation depends on the status of the internal         transpose flag. If this flag is set to true, the result will be         the equivalent of performing a Tvmult operation.        
* [0.x.196]*
         Apply the vmult inverse operation on a vector  [2.x.128]  (of internally         defined type VectorType) and store the result in the vector  [2.x.129]                  In practise, this function is only called from a Trilinos solver if         the wrapped object is to act as a preconditioner.                 This overloads the same function from the Trilinos class         Epetra_Operator.                
*  [2.x.130]  This function will only be operable if the payload has been         initialized with an InverseOperator, or is a wrapper to a         preconditioner. If not, then using this function will lead to an         error being thrown.        
*  [2.x.131]  The intended operation depends on the status of the internal         transpose flag. If this flag is set to true, the result will be         the equivalent of performing a Tvmult operation.        
* [0.x.197]*
          [2.x.132]  Additional Epetra_Operator functionality        
* [0.x.198]*
         Return a label to describe this class.                 This overloads the same function from the Trilinos class         Epetra_Operator.        
* [0.x.199]*
         Return a reference to the underlying MPI communicator for         this object.                 This overloads the same function from the Trilinos class         Epetra_Operator.        
* [0.x.200]*
         Return the partitioning of the domain space of this matrix, i.e., the         partitioning of the vectors this matrix has to be multiplied with.                 This overloads the same function from the Trilinos class         Epetra_Operator.        
* [0.x.201]*
         Return the partitioning of the range space of this matrix, i.e., the         partitioning of the vectors that are result from matrix-vector         products.                 This overloads the same function from the Trilinos class         Epetra_Operator.        
* [0.x.202]*
         A flag recording whether the operator is to perform standard         matrix-vector multiplication, or the transpose operation.        
* [0.x.203]*
         Internal communication pattern in case the matrix needs to be copied         from deal.II format.        
* [0.x.204]*
         Epetra_Map that sets the partitioning of the domain space of         this operator.        
* [0.x.205]*
         Epetra_Map that sets the partitioning of the range space of         this operator.        
* [0.x.206]*
         Return a flag that describes whether this operator can return the         computation of the infinity norm. Since in general this is not the         case, this always returns a negetive result.                 This overloads the same function from the Trilinos class         Epetra_Operator.        
* [0.x.207]*
         Return the infinity norm of this operator.         Throws an error since, in general, we cannot compute this value.                 This overloads the same function from the Trilinos class         Epetra_Operator.        
* [0.x.208]*
       Return an operator that returns a payload configured to support the       addition of two LinearOperators      
* [0.x.209]*
       Return an operator that returns a payload configured to support the       multiplication of two LinearOperators      
* [0.x.210]

include/deal.II-translator/lac/trilinos_sparsity_pattern_0.txt
[0.x.0]*
     Accessor class for iterators into sparsity patterns. This class is also     the base class for both const and non-const accessor classes into     sparse matrices.         Note that this class only allows read access to elements, providing     their row and column number. It does not allow modifying the sparsity     pattern itself.        
*  [2.x.0]     
* [0.x.1]*
       Declare type for container size.      
* [0.x.2]*
       Constructor.      
* [0.x.3]*
       Row number of the element represented by this object.      
* [0.x.4]*
       Index in row of the element represented by this object.      
* [0.x.5]*
       Column number of the element represented by this object.      
* [0.x.6]*
       Exception      
* [0.x.7]*
       Exception      
* [0.x.8]*
       The matrix accessed.      
* [0.x.9]*
       Current row number.      
* [0.x.10]*
       Current index in row.      
* [0.x.11]*
       Cache where we store the column indices of the present row. This is       necessary, since Trilinos makes access to the elements of its       matrices rather hard, and it is much more efficient to copy all       column entries of a row once when we enter it than repeatedly asking       Trilinos for individual ones. This also makes some sense since it is       likely that we will access them sequentially anyway.             In order to make copying of iterators/accessor of acceptable       performance, we keep a shared pointer to these entries so that more       than one accessor can access this data if necessary.      
* [0.x.12]*
       Discard the old row caches (they may still be used by other       accessors) and generate new ones for the row pointed to presently by       this accessor.      
* [0.x.13]*
     Iterator class for sparsity patterns of type      [2.x.1]  Access to individual elements of the     sparsity pattern is handled by the Accessor class in this namespace.    
* [0.x.14]*
       Declare type for container size.      
* [0.x.15]*
       Constructor. Create an iterator into the matrix  [2.x.2]  for the       given row and the index within it.      
* [0.x.16]*
       Copy constructor.      
* [0.x.17]*
       Prefix increment.      
* [0.x.18]*
       Postfix increment.      
* [0.x.19]*
       Dereferencing operator.      
* [0.x.20]*
       Dereferencing operator.      
* [0.x.21]*
       Comparison. True, if both iterators point to the same matrix       position.      
* [0.x.22]*
       Inverse of <tt>==</tt>.      
* [0.x.23]*
       Comparison operator. Result is true if either the first row number is       smaller or if the row numbers are equal and the first index is       smaller.      
* [0.x.24]*
       Exception      
* [0.x.25]*
       Store an object of the accessor class.      
* [0.x.26]*
   This class implements a wrapper class to use the Trilinos distributed   sparsity pattern class Epetra_FECrsGraph. This class is designed to be   used for construction of %parallel Trilinos matrices. The functionality   of this class is modeled after the existing sparsity pattern classes,   with the difference that this class can work fully in %parallel according   to a partitioning of the sparsity pattern rows.     This class has many similarities to the  DynamicSparsityPattern, since it   can dynamically add elements to the pattern without any memory being   previously reserved for it. However, it also has a method    [2.x.3]  that finalizes the pattern and enables its   use with Trilinos sparse matrices.    
*  [2.x.4]   
*  [2.x.5]   
* [0.x.27]*
     Declare type for container size.    
* [0.x.28]*
     Declare an alias for the iterator class.    
* [0.x.29]*
      [2.x.6]  Basic constructors and initialization    
* [0.x.30]*
     Default constructor. Generates an empty (zero-size) sparsity pattern.    
* [0.x.31]*
     Generate a sparsity pattern that is completely stored locally, having      [2.x.7]  rows and  [2.x.8]  columns. The resulting matrix will be completely     stored locally, too.         It is possible to specify the number of columns entries per row using     the optional  [2.x.9]  argument. However, this value does     not need to be accurate or even given at all, since one does usually     not have this kind of information before building the sparsity pattern     (the usual case when the function  [2.x.10]  is     called). The entries are allocated dynamically in a similar manner as     for the deal.II DynamicSparsityPattern classes. However, a good     estimate will reduce the setup time of the sparsity pattern.    
* [0.x.32]*
     Generate a sparsity pattern that is completely stored locally, having      [2.x.11]  rows and  [2.x.12]  columns. The resulting matrix will be completely     stored locally, too.         The vector <tt>n_entries_per_row</tt> specifies the number of entries     in each row (an information usually not available, though).    
* [0.x.33]*
     Move constructor. Create a new sparse matrix by stealing the internal     data.    
* [0.x.34]*
     Copy constructor. Sets the calling sparsity pattern to be the same as     the input sparsity pattern.    
* [0.x.35]*
     Destructor. Made virtual so that one can use pointers to this class.    
* [0.x.36]*
     Initialize a sparsity pattern that is completely stored locally, having      [2.x.13]  rows and  [2.x.14]  columns. The resulting matrix will be completely     stored locally.         The number of columns entries per row is specified as the maximum     number of entries argument.  This does not need to be an accurate     number since the entries are allocated dynamically in a similar manner     as for the deal.II DynamicSparsityPattern classes, but a good estimate     will reduce the setup time of the sparsity pattern.    
* [0.x.37]*
     Initialize a sparsity pattern that is completely stored locally, having      [2.x.15]  rows and  [2.x.16]  columns. The resulting matrix will be completely     stored locally.         The vector <tt>n_entries_per_row</tt> specifies the number of entries     in each row.    
* [0.x.38]*
     Copy function. Sets the calling sparsity pattern to be the same as the     input sparsity pattern.    
* [0.x.39]*
     Copy function from one of the deal.II sparsity patterns. If used in     parallel, this function uses an ad-hoc partitioning of the rows and     columns.    
* [0.x.40]*
     Copy operator. This operation is only allowed for empty objects, to     avoid potentially very costly operations automatically synthesized by     the compiler. Use copy_from() instead if you know that you really want     to copy a sparsity pattern with non-trivial content.    
* [0.x.41]*
     Release all memory and return to a state just like after having called     the default constructor.         This is a collective operation that needs to be called on all     processors in order to avoid a dead lock.    
* [0.x.42]*
     In analogy to our own SparsityPattern class, this function compresses     the sparsity pattern and allows the resulting pattern to be used for     actually generating a (Trilinos-based) matrix. This function also     exchanges non-local data that might have accumulated during the     addition of new elements. This function must therefore be called once     the structure is fixed. This is a collective operation, i.e., it needs     to be run on all processors when used in parallel.    
* [0.x.43]*
      [2.x.17]  Constructors and initialization using an IndexSet description    
* [0.x.44]*
     Constructor for a square sparsity pattern using an IndexSet and an MPI     communicator for the description of the %parallel partitioning.     Moreover, the number of nonzero entries in the rows of the sparsity     pattern can be specified. Note that this number does not need to be     exact, and it is even allowed that the actual sparsity structure has     more nonzero entries than specified in the constructor. However it is     still advantageous to provide good estimates here since a good value     will avoid repeated allocation of memory, which considerably increases     the performance when creating the sparsity pattern.    
* [0.x.45]*
     Same as before, but now use the exact number of nonzeros in each m row.     Since we know the number of elements in the sparsity pattern exactly in     this case, we can already allocate the right amount of memory, which     makes the creation process by the respective  [2.x.18]      call considerably faster. However, this is a rather unusual situation,     since knowing the number of entries in each row is usually connected to     knowing the indices of nonzero entries, which the sparsity pattern is     designed to describe.    
* [0.x.46]*
     This constructor is similar to the one above, but it now takes two     different index sets to describe the %parallel partitioning of rows and     columns. This interface is meant to be used for generating rectangular     sparsity pattern. Note that there is no real parallelism along the     columns &ndash; the processor that owns a certain row always owns all     the column elements, no matter how far they might be spread out. The     second Epetra_Map is only used to specify the number of columns and for     internal arrangements when doing matrix-vector products with vectors     based on that column map.         The number of columns entries per row is specified as the maximum     number of entries argument.    
* [0.x.47]*
     This constructor is similar to the one above, but it now takes two     different index sets for rows and columns. This interface is meant to     be used for generating rectangular matrices, where one map specifies     the %parallel distribution of rows and the second one specifies the     distribution of degrees of freedom associated with matrix columns. This     second map is however not used for the distribution of the columns     themselves &ndash; rather, all column elements of a row are stored on     the same processor. The vector <tt>n_entries_per_row</tt> specifies the     number of entries in each row of the newly generated matrix.    
* [0.x.48]*
     This constructor constructs general sparsity patterns, possible non-     square ones. Constructing a sparsity pattern this way allows the user     to explicitly specify the rows into which we are going to add elements.     This set is required to be a superset of the first index set  [2.x.19]      row_parallel_partitioning that includes also rows that are owned by     another processor (ghost rows). Note that elements can only be added to     rows specified by  [2.x.20]          This method is beneficial when the rows to which a processor is going     to write can be determined before actually inserting elements into the     matrix. For the typical  [2.x.21]  class used     in deal.II, we know that a processor only will add row elements for     what we call the locally relevant dofs (see      [2.x.22]  The other constructors     methods use general Trilinos facilities that allow to add elements to     arbitrary rows (as done by all the other reinit functions). However,     this flexibility come at a cost, the most prominent being that adding     elements into the same matrix from multiple threads in shared memory is     not safe whenever MPI is used. For these settings, the current method     is the one to choose: It will store the off-processor data as an     additional sparsity pattern (that is then passed to the Trilinos matrix     via the reinit method) which can be organized in such a way that     thread-safety can be ensured (as long as the user makes sure to never     write into the same matrix row simultaneously, of course).    
* [0.x.49]*
     Reinitialization function for generating a square sparsity pattern     using an IndexSet and an MPI communicator for the description of the     %parallel partitioning and the number of nonzero entries in the rows of     the sparsity pattern. Note that this number does not need to be exact,     and it is even allowed that the actual sparsity structure has more     nonzero entries than specified in the constructor. However it is still     advantageous to provide good estimates here since this will     considerably increase the performance when creating the sparsity     pattern.         This function does not create any entries by itself, but provides the     correct data structures that can be used by the respective add()     function.    
* [0.x.50]*
     Same as before, but now use the exact number of nonzeros in each m row.     Since we know the number of elements in the sparsity pattern exactly in     this case, we can already allocate the right amount of memory, which     makes process of adding entries to the sparsity pattern considerably     faster. However, this is a rather unusual situation, since knowing the     number of entries in each row is usually connected to knowing the     indices of nonzero entries, which the sparsity pattern is designed to     describe.    
* [0.x.51]*
     This reinit function is similar to the one above, but it now takes two     different index sets for rows and columns. This interface is meant to     be used for generating rectangular sparsity pattern, where one index     set describes the %parallel partitioning of the dofs associated with     the sparsity pattern rows and the other one of the sparsity pattern     columns. Note that there is no real parallelism along the columns     &ndash; the processor that owns a certain row always owns all the     column elements, no matter how far they might be spread out. The second     IndexSet is only used to specify the number of columns and for internal     arrangements when doing matrix-vector products with vectors based on an     EpetraMap based on that IndexSet.         The number of columns entries per row is specified by the argument     <tt>n_entries_per_row</tt>.    
* [0.x.52]*
     This reinit function is used to specify general matrices, possibly non-     square ones. In addition to the arguments of the other reinit method     above, it allows the user to explicitly specify the rows into which we     are going to add elements. This set is a superset of the first index     set  [2.x.23]  that includes also rows that are owned     by another processor (ghost rows).         This method is beneficial when the rows to which a processor is going     to write can be determined before actually inserting elements into the     matrix. For the typical  [2.x.24]  class used     in deal.II, we know that a processor only will add row elements for     what we call the locally relevant dofs (see      [2.x.25]  Trilinos matrices allow to     add elements to arbitrary rows (as done by all the other reinit     functions) and this is what all the other reinit methods do, too.     However, this flexibility come at a cost, the most prominent being that     adding elements into the same matrix from multiple threads in shared     memory is not safe whenever MPI is used. For these settings, the     current method is the one to choose: It will store the off-processor     data as an additional sparsity pattern (that is then passed to the     Trilinos matrix via the reinit method) which can be organized in such a     way that thread-safety can be ensured (as long as the user makes sure     to never write into the same matrix row simultaneously, of course).    
* [0.x.53]*
     Same as before, but now using a vector <tt>n_entries_per_row</tt> for     specifying the number of entries in each row of the sparsity pattern.    
* [0.x.54]*
     Reinit function. Takes one of the deal.II sparsity patterns and the     %parallel partitioning of the rows and columns specified by two index     sets and a %parallel communicator for initializing the current Trilinos     sparsity pattern. The optional argument  [2.x.26]  can be used     for reinitialization with a sparsity pattern that is not fully     constructed. This feature is only implemented for input sparsity     patterns of type DynamicSparsityPattern.    
* [0.x.55]*
     Reinit function. Takes one of the deal.II sparsity patterns and a     %parallel partitioning of the rows and columns for initializing the     current Trilinos sparsity pattern. The optional argument  [2.x.27]      exchange_data can be used for reinitialization with a sparsity pattern     that is not fully constructed. This feature is only implemented for     input sparsity patterns of type DynamicSparsityPattern.    
* [0.x.56]*
      [2.x.28]  Information on the sparsity pattern    
* [0.x.57]*
     Return the state of the sparsity pattern, i.e., whether compress()     needs to be called after an operation requiring data exchange.    
* [0.x.58]*
     Return the maximum number of entries per row on the current processor.    
* [0.x.59]*
     Return the number of rows in this sparsity pattern.    
* [0.x.60]*
     Return the number of columns in this sparsity pattern.    
* [0.x.61]*
     Return the local dimension of the sparsity pattern, i.e. the number of     rows stored on the present MPI process. In the sequential case, this     number is the same as n_rows(), but for parallel matrices it may be     smaller.         To figure out which elements exactly are stored locally, use     local_range().    
* [0.x.62]*
     Return a pair of indices indicating which rows of this sparsity pattern     are stored locally. The first number is the index of the first row     stored, the second the index of the one past the last one that is     stored locally. If this is a sequential matrix, then the result will be     the pair (0,n_rows()), otherwise it will be a pair (i,i+n), where     <tt>n=local_size()</tt>.    
* [0.x.63]*
     Return whether  [2.x.29]  is in the local range or not, see also     local_range().    
* [0.x.64]*
     Return the number of nonzero elements of this sparsity pattern.    
* [0.x.65]*
     Return the number of entries in the given row.         In a parallel context, the row in question may of course not be     stored on the current processor, and in that case it is not     possible to query the number of entries in it. In that case,     the returned value is `static_cast<size_type>(-1)`.    
* [0.x.66]*
     Compute the bandwidth of the matrix represented by this structure. The     bandwidth is the maximum of  [2.x.30]  for which the index pair  [2.x.31]      represents a nonzero entry of the matrix. Consequently, the maximum     bandwidth a  [2.x.32]  matrix can have is  [2.x.33] .    
* [0.x.67]*
     Return whether the object is empty. It is empty if no memory is     allocated, which is the same as when both dimensions are zero.    
* [0.x.68]*
     Return whether the index ([1.x.0]) exists in the sparsity pattern     (i.e., it may be non-zero) or not.    
* [0.x.69]*
     Return whether a given  [2.x.34]  is stored in the current object     on this process.    
* [0.x.70]*
     Determine an estimate for the memory consumption (in bytes) of this     object. Currently not implemented for this class.    
* [0.x.71]*
      [2.x.35]  Adding entries    
* [0.x.72]*
     Add the element ([1.x.1]) to the sparsity pattern.    
* [0.x.73]*
     Add several elements in one row to the sparsity pattern.    
* [0.x.74]*
      [2.x.36]  Access of underlying Trilinos data    
* [0.x.75]*
     Return a const reference to the underlying Trilinos Epetra_CrsGraph     data that stores the sparsity pattern.    
* [0.x.76]*
     Return a const reference to the underlying Trilinos Epetra_Map that     sets the parallel partitioning of the domain space of this sparsity     pattern, i.e., the partitioning of the vectors matrices based on this     sparsity pattern are multiplied with.    
* [0.x.77]*
     Return a const reference to the underlying Trilinos Epetra_Map that     sets the partitioning of the range space of this sparsity pattern,     i.e., the partitioning of the vectors that are result from matrix-     vector products.    
* [0.x.78]*
     Return the MPI communicator object in use with this matrix.    
* [0.x.79]*
      [2.x.37]  Partitioners    
* [0.x.80]*
     Return the partitioning of the domain space of this pattern, i.e., the     partitioning of the vectors a matrix based on this sparsity pattern has     to be multiplied with.    
* [0.x.81]*
     Return the partitioning of the range space of this pattern, i.e., the     partitioning of the vectors that are the result from matrix-vector     products from a matrix based on this pattern.    
* [0.x.82]*
      [2.x.38]  Iterators    
* [0.x.83]*
     Iterator starting at the first entry.    
* [0.x.84]*
     Final iterator.    
* [0.x.85]*
     Iterator starting at the first entry of row  [2.x.39]          Note that if the given row is empty, i.e. does not contain any nonzero     entries, then the iterator returned by this function equals     <tt>end(r)</tt>. Note also that the iterator may not be dereferenceable     in that case.    
* [0.x.86]*
     Final iterator of row <tt>r</tt>. It points to the first element past     the end of line  [2.x.40]  or past the end of the entire sparsity pattern.         Note that the end iterator is not necessarily dereferenceable. This is     in particular the case if it is the end iterator for the last row of a     matrix.    
* [0.x.87]*
      [2.x.41]  Input/Output    
* [0.x.88]*
     Abstract Trilinos object that helps view in ASCII other Trilinos     objects. Currently this function is not implemented.  TODO: Not     implemented.    
* [0.x.89]*
     Print (the locally owned part of) the sparsity pattern to the given     stream, using the format <tt>(line,col)</tt>. The optional flag outputs     the sparsity pattern in Trilinos style, where even the according     processor number is printed to the stream, as well as a summary before     actually writing the entries.    
* [0.x.90]*
     Print the sparsity of the matrix in a format that <tt>gnuplot</tt>     understands and which can be used to plot the sparsity pattern in a     graphical way. The format consists of pairs <tt>i j</tt> of nonzero     elements, each representing one entry of this matrix, one per line of     the output file. Indices are counted from zero on, as usual. Since     sparsity patterns are printed in the same way as matrices are     displayed, we print the negative of the column index, which means that     the <tt>(0,0)</tt> element is in the top left rather than in the bottom     left corner.         Print the sparsity pattern in gnuplot by setting the data style to dots     or points and use the <tt>plot</tt> command.    
* [0.x.91]*
      [2.x.42]  Exceptions      [2.x.43]     
* [0.x.92]*
     Exception    
* [0.x.93]*
     Exception    
* [0.x.94]*
     Exception    
* [0.x.95]*
     Exception    
* [0.x.96]*
     Exception    
* [0.x.97]*
     Pointer to the user-supplied Epetra Trilinos mapping of the matrix     columns that assigns parts of the matrix to the individual processes.    
* [0.x.98]*
     A sparsity pattern object in Trilinos to be used for finite element     based problems which allows for adding non-local elements to the     pattern.    
* [0.x.99]*
     A sparsity pattern object for the non-local part of the sparsity     pattern that is going to be sent to the owning processor. Only used     when the particular constructor or reinit method with writable_rows     argument is set    
* [0.x.100]

include/deal.II-translator/lac/trilinos_tpetra_communication_pattern_0.txt
[0.x.0]*
     This class implements a wrapper to  [2.x.0]  and  [2.x.1]     
* [0.x.1]*
       Reinitialize the communication pattern. The first argument  [2.x.2]        vector_space_vector_index_set is the index set associated to a       VectorSpaceVector object. The second argument  [2.x.3]        read_write_vector_index_set is the index set associated to a       ReadWriteVector object.      
* [0.x.2]*
       Reinitialize the object.      
* [0.x.3]*
       Return the underlying MPI communicator.      
* [0.x.4]*
       Return the underlying  [2.x.4]  object.      
* [0.x.5]*
       Return the underlying  [2.x.5]  object.      
* [0.x.6]*
       Shared pointer to the MPI communicator used.      
* [0.x.7]*
       Shared pointer to the  [2.x.6]  object used.      
* [0.x.8]*
       Shared pointer to the  [2.x.7]  object used.      
* [0.x.9]

include/deal.II-translator/lac/trilinos_tpetra_vector_0.txt
[0.x.0]*
   A namespace for classes that provide wrappers for Trilinos' Tpetra vectors.     This namespace provides wrappers for the  [2.x.0]  class from the   Tpetra package (https://trilinos.github.io/tpetra.html) that is part of   Trilinos.  
* [0.x.1]*
     This class implements a wrapper to the Trilinos distributed vector     class  [2.x.1]  This class is derived from the      [2.x.2]  class and requires Trilinos to be     compiled with MPI support.         Tpetra uses Kokkos for thread-parallelism and chooses the execution and     memory space automatically depending on Kokkos configuration. The     priority is ranked from highest to lowest:
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  -  [2.x.3] 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  -  [2.x.4] 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  -  [2.x.5] 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  -  [2.x.6]          In case Kokkos was configured with CUDA support, this class stores the     values in unified virtual memory space and performs its action on the     GPU. In particular, there is no need for manually synchronizing memory     between host and device.        
*  [2.x.7]     
*  [2.x.8]     
* [0.x.2]*
       Constructor. Create a vector of dimension zero.      
* [0.x.3]*
       Copy constructor. Sets the dimension and the partitioning to that of       the given vector and copies all elements.      
* [0.x.4]*
       This constructor takes an IndexSet that defines how to distribute the       individual components among the MPI processors. Since it also       includes information about the size of the vector, this is all we       need to generate a %parallel vector.      
* [0.x.5]*
       Reinit functionality. This function destroys the old vector content       and generates a new one based on the input partitioning. The flag       <tt>omit_zeroing_entries</tt> determines whether the vector should be       filled with zero (false) or left untouched (true).      
* [0.x.6]*
       Change the dimension to that of the vector  [2.x.9]  The elements of  [2.x.10]  are not       copied.      
* [0.x.7]*
       Copy function. This function takes a Vector and copies all the       elements. The Vector will have the same parallel distribution as  [2.x.11]        V.      
* [0.x.8]*
       Sets all elements of the vector to the scalar  [2.x.12]  This operation is       only allowed if  [2.x.13]  is equal to zero.      
* [0.x.9]*
       Imports all the elements present in the vector's IndexSet from the       input vector  [2.x.14]   [2.x.15]   [2.x.16]  is used to decide if       the elements in  [2.x.17]  should be added to the current vector or replace the       current elements. The last parameter can be used if the same       communication pattern is used multiple times. This can be used to       improve performance.      
* [0.x.10]*
       Multiply the entire vector by a fixed factor.      
* [0.x.11]*
       Divide the entire vector by a fixed factor.      
* [0.x.12]*
       Add the vector  [2.x.18]  to the present one.      
* [0.x.13]*
       Subtract the vector  [2.x.19]  from the present one.      
* [0.x.14]*
       Return the scalar product of two vectors. The vectors need to have the       same layout.      
* [0.x.15]*
       Add  [2.x.20]  to all components. Note that  [2.x.21]  a scalar not a vector.      
* [0.x.16]*
       Simple addition of a multiple of a vector, i.e. <tt>*this +=       a*V</tt>. The vectors need to have the same layout.      
* [0.x.17]*
       Multiple addition of multiple of a vector, i.e. <tt>*this> +=       a*V+b*W</tt>. The vectors need to have the same layout.      
* [0.x.18]*
       Scaling and simple addition of a multiple of a vector, i.e. <tt>*this       = s*(*this)+a*V</tt>.      
* [0.x.19]*
       Scale each element of this vector by the corresponding element in the       argument. This function is mostly meant to simulate multiplication       (and immediate re-assignment) by a diagonal scaling matrix. The       vectors need to have the same layout.      
* [0.x.20]*
       Assignment <tt>*this = a*V</tt>.      
* [0.x.21]*
       Return whether the vector contains only elements with value zero.      
* [0.x.22]*
       Return the mean value of the element of this vector.      
* [0.x.23]*
       Return the l<sub>1</sub> norm of the vector (i.e., the sum of the       absolute values of all entries among all processors).      
* [0.x.24]*
       Return the l<sub>2</sub> norm of the vector (i.e., the square root of       the sum of the square of all entries among all processors).      
* [0.x.25]*
       Return the maximum norm of the vector (i.e., the maximum absolute value       among all entries and among all processors).      
* [0.x.26]*
       Performs a combined operation of a vector addition and a subsequent       inner product, returning the value of the inner product. In other       words, the result of this function is the same as if the user called      
* [1.x.0]
*              The reason this function exists is that this operation involves less       memory transfer than calling the two functions separately. This       method only needs to load three vectors,  [2.x.22]   [2.x.23]   [2.x.24]  whereas       calling separate methods means to load the calling vector  [2.x.25]        twice. Since most vector operations are memory transfer limited, this       reduces the time by 25\% (or 50\% if  [2.x.26]  equals  [2.x.27]              The vectors need to have the same layout.             For complex-valued vectors, the scalar product in the second step is       implemented as        [2.x.28] .      
* [0.x.27]*
       This function always returns false and is present only for backward       compatibility.      
* [0.x.28]*
       Return the global size of the vector, equal to the sum of the number of       locally owned indices among all processors.      
* [0.x.29]*
       Return the local size of the vector, i.e., the number of indices       owned locally.      
* [0.x.30]*
       Return the MPI communicator object in use with this object.      
* [0.x.31]*
       Return an index set that describes which elements of this vector are       owned by the current processor. As a consequence, the index sets       returned on different processors if this is a distributed vector will       form disjoint sets that add up to the complete index set. Obviously, if       a vector is created on only one processor, then the result would       satisfy      
* [1.x.1]
*       
* [0.x.32]*
       Return a const reference to the underlying Trilinos        [2.x.29]  class.      
* [0.x.33]*
       Return a (modifiable) reference to the underlying Trilinos        [2.x.30]  class.      
* [0.x.34]*
       Prints the vector to the output stream  [2.x.31]       
* [0.x.35]*
       Return the memory consumption of this class in bytes.      
* [0.x.36]*
       The vectors have different partitioning, i.e. their IndexSet objects       don't represent the same indices.      
* [0.x.37]*
       Attempt to perform an operation between two incompatible vector types.            
*  [2.x.32]       
* [0.x.38]*
       Exception thrown by an error in Trilinos.            
*  [2.x.33]       
* [0.x.39]*
       Create the CommunicationPattern for the communication between the       IndexSet  [2.x.34]  and the current vector based       on the communicator  [2.x.35]       
* [0.x.40]*
       Pointer to the actual Tpetra vector object.      
* [0.x.41]*
       IndexSet of the elements of the last imported vector.      
* [0.x.42]*
       CommunicationPattern for the communication between the       source_stored_elements IndexSet and the current vector.      
* [0.x.43]*
 Declare  [2.x.36]  as distributed vector.

* 
* [0.x.44]

include/deal.II-translator/lac/trilinos_tpetra_vector.templates_0.txt
[0.x.0]

include/deal.II-translator/lac/trilinos_vector_0.txt
[0.x.0]*
  [2.x.0]  TrilinosWrappers  [2.x.1] 

* 
* [0.x.1]*
 A namespace in which wrapper classes for Trilinos objects reside.
* 

* 
*  [2.x.2] 

* 
* [0.x.2]*
    [2.x.3]  internal  
* [0.x.3]*
   A namespace for internal implementation details of the TrilinosWrapper   members.    
*  [2.x.4]   
* [0.x.4]*
     Declare type for container size.    
* [0.x.5]*
     This class implements a wrapper for accessing the Trilinos vector in     the same way as we access deal.II objects: it is initialized with a     vector and an element within it, and has a conversion operator to     extract the scalar value of this element. It also has a variety of     assignment operator for writing to this one element.        
*  [2.x.5]     
* [0.x.6]*
       Constructor. It is made private so as to only allow the actual vector       class to create it.      
* [0.x.7]*
       Copy constructor.      
* [0.x.8]*
       This looks like a copy operator, but does something different than       usual. In particular, it does not copy the member variables of this       reference. Rather, it handles the situation where we have two vectors        [2.x.6]  and  [2.x.7]  and assign elements like in <tt>v(i)=w(i)</tt>. Here,       both left and right hand side of the assignment have data type       VectorReference, but what we really mean is to assign the vector       elements represented by the two references. This operator implements       this operation. Note also that this allows us to make the assignment       operator const.      
* [0.x.9]*
       Same as above but for non-const reference objects.      
* [0.x.10]*
       Set the referenced element of the vector to <tt>s</tt>.      
* [0.x.11]*
       Add <tt>s</tt> to the referenced element of the vector->      
* [0.x.12]*
       Subtract <tt>s</tt> from the referenced element of the vector->      
* [0.x.13]*
       Multiply the referenced element of the vector by <tt>s</tt>.      
* [0.x.14]*
       Divide the referenced element of the vector by <tt>s</tt>.      
* [0.x.15]*
       Convert the reference to an actual value, i.e. return the value of       the referenced element of the vector.      
* [0.x.16]*
       Exception      
* [0.x.17]*
       Point to the vector we are referencing.      
* [0.x.18]*
       Index of the referenced element of the vector.      
* [0.x.19]*
    [2.x.8]   
* [0.x.20]*
   Namespace for Trilinos vector classes that work in parallel over MPI.    
*  [2.x.9]   
* [0.x.21]*
     This class implements a wrapper to use the Trilinos distributed vector     class Epetra_FEVector, the (parallel) partitioning of which     is governed by an Epetra_Map.     The Epetra_FEVector is precisely the kind of vector     we deal with all the time
* 
*  - we probably get it from some assembly     process, where also entries not locally owned might need to written and     hence need to be forwarded to the owner.         The interface of this class is modeled after the existing Vector class in     deal.II. It has almost the same member functions, and is often     exchangeable. However, since Trilinos only supports a single scalar type     (double), it is not templated, and only works with that type.         Note that Trilinos only guarantees that operations do what you expect     if the function  [2.x.10]  has been called after vector assembly     in order to distribute the data. This is necessary since some processes     might have accumulated data of elements that are not owned by     themselves, but must be sent to the owning process. In order to avoid     using the wrong data, you need to call  [2.x.11]  before you     actually use the vectors.         [1.x.0]         The parallel functionality of Trilinos is built on top of the Message     Passing Interface (MPI). MPI's communication model is built on     collective communications: if one process wants something from another,     that other process has to be willing to accept this communication. A     process cannot query data from another process by calling a remote     function, without that other process expecting such a transaction. The     consequence is that most of the operations in the base class of this     class have to be called collectively. For example, if you want to     compute the l2 norm of a parallel vector,  [2.x.12]  all processes across     which this vector is shared have to call the  [2.x.13]  function. If     you don't do this, but instead only call the  [2.x.14]  function on one     process, then the following happens: This one process will call one of     the collective MPI functions and wait for all the other processes to     join in on this. Since the other processes don't call this function,     you will either get a time-out on the first process, or, worse, by the     time the next a call to a Trilinos function generates an MPI message on     the other processes, you will get a cryptic message that only a subset     of processes attempted a communication. These bugs can be very hard to     figure out, unless you are well-acquainted with the communication model     of MPI, and know which functions may generate MPI messages.         One particular case, where an MPI message may be generated unexpectedly     is discussed below.             [1.x.1]         Trilinos does of course allow read access to individual elements of a     vector, but in the distributed case only to elements that are stored     locally. We implement this through calls like <tt>d=vec(i)</tt>.     However, if you access an element outside the locally stored range, an     exception is generated.         In contrast to read access, Trilinos (and the respective deal.II     wrapper classes) allow to write (or add) to individual elements of     vectors, even if they are stored on a different process. You can do     this by writing into or adding to elements using the syntax     <tt>vec(i)=d</tt> or <tt>vec(i)+=d</tt>, or similar operations. There     is one catch, however, that may lead to very confusing error messages:     Trilinos requires application programs to call the compress() function     when they switch from performing a set of operations that add to     elements, to performing a set of operations that write to elements. The     reasoning is that all processes might accumulate addition operations to     elements, even if multiple processes write to the same elements. By the     time we call compress() the next time, all these additions are     executed. However, if one process adds to an element, and another     overwrites to it, the order of execution would yield non-deterministic     behavior if we don't make sure that a synchronization with compress()     happens in between.         In order to make sure these calls to compress() happen at the     appropriate time, the deal.II wrappers keep a state variable that store     which is the presently allowed operation: additions or writes. If it     encounters an operation of the opposite kind, it calls compress() and     flips the state. This can sometimes lead to very confusing behavior, in     code that may for example look like this:        
* [1.x.2]
*          This code can run into trouble: by the time we see the first addition     operation, we need to flush the overwrite buffers for the vector, and     the deal.II library will do so by calling compress(). However, it will     only do so for all processes that actually do an addition
* 
*  -  if the     condition is never true for one of the processes, then this one will     not get to the actual compress() call, whereas all the other ones do.     This gets us into trouble, since all the other processes hang in the     call to flush the write buffers, while the one other process advances     to the call to compute the l2 norm. At this time, you will get an error     that some operation was attempted by only a subset of processes. This     behavior may seem surprising, unless you know that write/addition     operations on single elements may trigger this behavior.         The problem described here may be avoided by placing additional calls     to compress(), or making sure that all processes do the same type of     operations at the same time, for example by placing zero additions if     necessary.             [1.x.3]         Parallel vectors come in two kinds: without and with ghost elements.     Vectors without ghost elements uniquely partition the vector elements     between processors: each vector entry has exactly one processor that     owns it. For such vectors, you can read those elements that the     processor you are currently on owns, and you can write into any element     whether you own it or not: if you don't own it, the value written or     added to a vector element will be shipped to the processor that owns     this vector element the next time you call compress(), as described     above.         What we call a 'ghosted' vector (see      [2.x.15]  "vectors with ghost elements"     ) is simply a view of the parallel vector where the element     distributions overlap. The 'ghosted' Trilinos vector in itself has no     idea of which entries are ghosted and which are locally owned. In fact,     a ghosted vector may not even store all of the elements a non- ghosted     vector would store on the current processor.  Consequently, for     Trilinos vectors, there is no notion of an 'owner' of vector elements     in the way we have it in the non-ghost case view.         This explains why we do not allow writing into ghosted vectors on the     Trilinos side: Who would be responsible for taking care of the     duplicated entries, given that there is not such information as locally     owned indices? In other words, since a processor doesn't know which     other processors own an element, who would it send a value to if one     were to write to it? The only possibility would be to send this     information to [1.x.4] other processors, but that is clearly not     practical. Thus, we only allow reading from ghosted vectors, which     however we do very often.         So how do you fill a ghosted vector if you can't write to it? This only     happens through the assignment with a non-ghosted vector. It can go     both ways (non-ghosted is assigned to a ghosted vector, and a ghosted     vector is assigned to a non-ghosted one; the latter one typically only     requires taking out the locally owned part as most often ghosted     vectors store a superset of elements of non-ghosted ones). In general,     you send data around with that operation and it all depends on the     different views of the two vectors. Trilinos also allows you to get     subvectors out of a big vector that way.             [1.x.5]         When writing into Trilinos vectors from several threads in shared     memory, several things must be kept in mind as there is no built-in     locks in this class to prevent data races. Simultaneous access to the     same vector entry at the same time results in data races and must be     explicitly avoided by the user. However, it is possible to access     [1.x.6] entries of the vector from several threads     simultaneously when only one MPI process is present or the vector has     been constructed with an additional index set for ghost entries in     write mode.        
*  [2.x.16]     
*  [2.x.17]              2008, 2009, 2017    
* [0.x.22]*
       Declare some of the standard types used in all containers. These types       parallel those in the <tt>C</tt> standard libraries       <tt>vector<...></tt> class.      
* [0.x.23]*
        [2.x.18]  1: Basic Object-handling      
* [0.x.24]*
       Default constructor that generates an empty (zero size) vector. The       function <tt>reinit()</tt> will have to give the vector the correct       size and distribution among processes in case of an MPI run.      
* [0.x.25]*
       Copy constructor using the given vector.      
* [0.x.26]*
       This constructor takes an IndexSet that defines how to distribute the       individual components among the MPI processors. Since it also       includes information about the size of the vector, this is all we       need to generate a %parallel vector.             Depending on whether the  [2.x.19]  argument uniquely       subdivides elements among processors or not, the resulting vector may       or may not have ghost elements. See the general documentation of this       class for more information.             In case the provided IndexSet forms an overlapping partitioning,       it is not clear which elements are owned by which process and       locally_owned_elements() will return an IndexSet of size zero.              [2.x.20]         [2.x.21]  "vectors with ghost elements"      
* [0.x.27]*
       Creates a ghosted parallel vector.             Depending on whether the  [2.x.22]  argument uniquely subdivides       elements among processors or not, the resulting vector may or may not       have ghost elements. See the general documentation of this class for       more information.              [2.x.23]         [2.x.24]  "vectors with ghost elements"      
* [0.x.28]*
       Copy constructor from the TrilinosWrappers vector class. Since a       vector of this class does not necessarily need to be distributed       among processes, the user needs to supply us with an IndexSet and an       MPI communicator that set the partitioning details.             Depending on whether the  [2.x.25]  argument uniquely       subdivides elements among processors or not, the resulting vector may       or may not have ghost elements. See the general documentation of this       class for more information.              [2.x.26]         [2.x.27]  "vectors with ghost elements"      
* [0.x.29]*
       Copy-constructor from deal.II vectors. Sets the dimension to that of       the given vector, and copies all the elements.             Depending on whether the  [2.x.28]  argument uniquely       subdivides elements among processors or not, the resulting vector may       or may not have ghost elements. See the general documentation of this       class for more information.              [2.x.29]         [2.x.30]  "vectors with ghost elements"      
* [0.x.30]*
       Move constructor. Creates a new vector by stealing the internal data       of the vector  [2.x.31]       
* [0.x.31]*
       Destructor.      
* [0.x.32]*
       Release all memory and return to a state just like after having called       the default constructor.      
* [0.x.33]*
       Reinit functionality. This function sets the calling vector to the       dimension and the parallel distribution of the input vector, but does       not copy the elements in <tt>v</tt>. If <tt>omit_zeroing_entries</tt>       is not <tt>true</tt>, the elements in the vector are initialized with       zero. If it is set to <tt>true</tt>, the vector entries are in an       unspecified state and the user has to set all elements. In the       current implementation, this method does not touch the vector entries       in case the vector layout is unchanged from before, otherwise entries       are set to zero.  Note that this behavior might change between       releases without notification.             This function has a third argument, <tt>allow_different_maps</tt>,       that allows for an exchange of data between two equal-sized vectors       (but being distributed differently among the processors). A trivial       application of this function is to generate a replication of a whole       vector on each machine, when the calling vector is built with a map       consisting of all indices on each process, and <tt>v</tt>       is a distributed vector. In this case, the variable       <tt>omit_zeroing_entries</tt> needs to be set to <tt>false</tt>,       since it does not make sense to exchange data between differently       parallelized vectors without touching the elements.      
* [0.x.34]*
       Reinit functionality. This function destroys the old vector content       and generates a new one based on the input partitioning.  The flag       <tt>omit_zeroing_entries</tt> determines whether the vector should be       filled with zero (false). If the flag is set to <tt>true</tt>, the       vector entries are in an unspecified state and the user has to set       all elements. In the current implementation, this method still sets       the entries to zero, but this might change between releases without       notification.             Depending on whether the  [2.x.32]  argument uniquely       subdivides elements among processors or not, the resulting vector may       or may not have ghost elements. See the general documentation of this       class for more information.             In case  [2.x.33]  is overlapping, it is not clear which       process should own which elements. Hence, locally_owned_elements()       returns an empty IndexSet in this case.              [2.x.34]         [2.x.35]  "vectors with ghost elements"      
* [0.x.35]*
       Reinit functionality. This function destroys the old vector content       and generates a new one based on the input partitioning. In addition       to just specifying one index set as in all the other methods above,       this method allows to supply an additional set of ghost entries.       There are two different versions of a vector that can be created. If       the flag  [2.x.36]  is set to  [2.x.37]  the vector only       allows read access to the joint set of  [2.x.38]  and        [2.x.39]  The effect of the reinit method is then equivalent       to calling the other reinit method with an index set containing both       the locally owned entries and the ghost entries.             If the flag  [2.x.40]  is set to true, this creates an       alternative storage scheme for ghost elements that allows multiple       threads to write into the vector (for the other reinit methods, only       one thread is allowed to write into the ghost entries at a time).             Depending on whether the  [2.x.41]  argument uniquely       subdivides elements among processors or not, the resulting vector may       or may not have ghost elements. See the general documentation of this       class for more information.              [2.x.42]         [2.x.43]  "vectors with ghost elements"      
* [0.x.36]*
       Create vector by merging components from a block vector.      
* [0.x.37]*
       Compress the underlying representation of the Trilinos object, i.e.       flush the buffers of the vector object if it has any. This function is       necessary after writing into a vector element-by-element and before       anything else can be done on it.             The (defaulted) argument can be used to specify the compress mode       ( [2.x.44] ) in case the vector has not       been written to since the last time this function was called. The       argument is ignored if the vector has been added or written to since       the last time compress() was called.             See        [2.x.45]  "Compressing distributed objects"       for more information.      
* [0.x.38]*
       Set all components of the vector to the given number  [2.x.46]  Simply       pass this down to the base class, but we still need to declare this       function to make the example given in the discussion about making the       constructor explicit work.       the constructor explicit work.             Since the semantics of assigning a scalar to a vector are not       immediately clear, this operator can only be used if you want       to set the entire vector to zero. This allows the intuitive notation       <tt>v=0</tt>.      
* [0.x.39]*
       Copy the given vector. Resize the present vector if necessary. In       this case, also the Epetra_Map that designs the parallel partitioning       is taken from the input vector.      
* [0.x.40]*
       Move the given vector. This operator replaces the present vector with        [2.x.47]  by efficiently swapping the internal data structures.      
* [0.x.41]*
       Another copy function. This one takes a deal.II vector and copies it       into a TrilinosWrapper vector. Note that since we do not provide any       Epetra_map that tells about the partitioning of the vector among the       MPI processes, the size of the TrilinosWrapper vector has to be the       same as the size of the input vector.      
* [0.x.42]*
       This reinit function is meant to be used for parallel calculations       where some non-local data has to be used. The typical situation where       one needs this function is the call of the        [2.x.48]  function (or of some derivatives)       in parallel. Since it is usually faster to retrieve the data in       advance, this function can be called before the assembly forks out to       the different processors. What this function does is the following:       It takes the information in the columns of the given matrix and looks       which data couples between the different processors. That data is       then queried from the input vector. Note that you should not write to       the resulting vector any more, since the some data can be stored       several times on different processors, leading to unpredictable       results. In particular, such a vector cannot be used for matrix-       vector products as for example done during the solution of linear       systems.      
* [0.x.43]*
       Imports all the elements present in the vector's IndexSet from the       input vector  [2.x.49]   [2.x.50]   [2.x.51]  is used to decide if       the elements in  [2.x.52]  should be added to the current vector or replace the       current elements.      
* [0.x.44]*
       Test for equality. This function assumes that the present vector and       the one to compare with have the same size already, since comparing       vectors of different sizes makes not much sense anyway.      
* [0.x.45]*
       Test for inequality. This function assumes that the present vector and       the one to compare with have the same size already, since comparing       vectors of different sizes makes not much sense anyway.      
* [0.x.46]*
       Return the global dimension of the vector.      
* [0.x.47]*
       Return the local dimension of the vector, i.e. the number of elements       stored on the present MPI process. For sequential vectors, this number       is the same as size(), but for parallel vectors it may be smaller.             To figure out which elements exactly are stored locally, use       local_range().             If the vector contains ghost elements, they are included in this       number.              [2.x.53]  This function is deprecated.      
* [0.x.48]*
       Return the local size of the vector, i.e., the number of indices       owned locally.      
* [0.x.49]*
       Return a pair of indices indicating which elements of this vector are       stored locally. The first number is the index of the first element       stored, the second the index of the one past the last one that is       stored locally. If this is a sequential vector, then the result will be       the pair  [2.x.54] , otherwise it will be a pair        [2.x.55]  and        [2.x.56]  is the first element of the vector stored on this       processor, corresponding to the half open interval  [2.x.57]             
*  [2.x.58]  The description above is true most of the time, but not always.       In particular, Trilinos vectors need not store contiguous ranges of       elements such as  [2.x.59] . Rather, it can store vectors where the       elements are distributed in an arbitrary way across all processors and       each processor simply stores a particular subset, not necessarily       contiguous. In this case, this function clearly makes no sense since it       could, at best, return a range that includes all elements that are       stored locally. Thus, the function only succeeds if the locally stored       range is indeed contiguous. It will trigger an assertion if the local       portion of the vector is not contiguous.      
* [0.x.50]*
       Return whether  [2.x.60]  is in the local range or not, see also       local_range().            
*  [2.x.61]  The same limitation for the applicability of this function       applies as listed in the documentation of local_range().      
* [0.x.51]*
       Return an index set that describes which elements of this vector are       owned by the current processor. Note that this index set does not       include elements this vector may store locally as ghost elements but       that are in fact owned by another processor. As a consequence, the       index sets returned on different processors if this is a distributed       vector will form disjoint sets that add up to the complete index set.       Obviously, if a vector is created on only one processor, then the       result would satisfy      
* [1.x.7]
*       
* [0.x.52]*
       Return if the vector contains ghost elements. This answer is true if       there are ghost elements on at least one process.              [2.x.62]         [2.x.63]  "vectors with ghost elements"      
* [0.x.53]*
       This function only exists for compatibility with the  [2.x.64]         [2.x.65]  class and does nothing: this class       implements ghost value updates in a different way that is a better fit       with the underlying Trilinos vector object.      
* [0.x.54]*
       Return the scalar (inner) product of two vectors. The vectors must have       the same size.      
* [0.x.55]*
       Return the square of the  [2.x.66] -norm.      
* [0.x.56]*
       Mean value of the elements of this vector.      
* [0.x.57]*
       Compute the minimal value of the elements of this vector.      
* [0.x.58]*
       Compute the maximal value of the elements of this vector.      
* [0.x.59]*
        [2.x.67] -norm of the vector.  The sum of the absolute values.      
* [0.x.60]*
        [2.x.68] -norm of the vector.  The square root of the sum of the squares of       the elements.      
* [0.x.61]*
        [2.x.69] -norm of the vector. The [1.x.8]th root of the sum of the       [1.x.9]th powers of the absolute values of the elements.      
* [0.x.62]*
       Maximum absolute value of the elements.      
* [0.x.63]*
       Performs a combined operation of a vector addition and a subsequent       inner product, returning the value of the inner product. In other       words, the result of this function is the same as if the user called      
* [1.x.10]
*              The reason this function exists is for compatibility with deal.II's own       vector classes which can implement this functionality with less memory       transfer. However, for Trilinos vectors such a combined operation is       not natively supported and thus the cost is completely equivalent as       calling the two methods separately.             For complex-valued vectors, the scalar product in the second step is       implemented as        [2.x.70] .      
* [0.x.64]*
       Return whether the vector contains only elements with value zero. This       is a collective operation. This function is expensive, because       potentially all elements have to be checked.      
* [0.x.65]*
       Return  [2.x.71]  if the vector has no negative entries, i.e. all entries       are zero or positive. This function is used, for example, to check       whether refinement indicators are really all positive (or zero).      
* [0.x.66]*
        [2.x.72]  2: Data-Access      
* [0.x.67]*
       Provide access to a given element, both read and write.             When using a vector distributed with MPI, this operation only makes       sense for elements that are actually present on the calling processor.       Otherwise, an exception is thrown.      
* [0.x.68]*
       Provide read-only access to an element.             When using a vector distributed with MPI, this operation only makes       sense for elements that are actually present on the calling processor.       Otherwise, an exception is thrown.      
* [0.x.69]*
       Provide access to a given element, both read and write.             Exactly the same as operator().      
* [0.x.70]*
       Provide read-only access to an element.             Exactly the same as operator().      
* [0.x.71]*
       Instead of getting individual elements of a vector via operator(),       this function allows getting a whole set of elements at once. The       indices of the elements to be read are stated in the first argument,       the corresponding values are returned in the second.             If the current vector is called  [2.x.73]  then this function is the equivalent       to the code      
* [1.x.11]
*               [2.x.74]  The sizes of the  [2.x.75]  and  [2.x.76]  arrays must be identical.      
* [0.x.72]*
       Instead of getting individual elements of a vector via operator(),       this function allows getting a whole set of elements at once. In       contrast to the previous function, this function obtains the       indices of the elements by dereferencing all elements of the iterator       range provided by the first two arguments, and puts the vector       values into memory locations obtained by dereferencing a range       of iterators starting at the location pointed to by the third       argument.             If the current vector is called  [2.x.77]  then this function is the equivalent       to the code      
* [1.x.12]
*               [2.x.78]  It must be possible to write into as many memory locations         starting at  [2.x.79]  as there are iterators between          [2.x.80]  and  [2.x.81]       
* [0.x.73]*
       Make the Vector class a bit like the <tt>vector<></tt> class of the C++       standard library by returning iterators to the start and end of the       locally owned elements of this vector. The ordering of local elements       corresponds to the one given by the global indices in case the vector       is constructed from an IndexSet or other methods in deal.II (note that       an Epetra_Map can contain elements in arbitrary orders, though).             It holds that end()
* 
*  - begin() == local_size().      
* [0.x.74]*
       Return constant iterator to the start of the locally owned elements of       the vector.      
* [0.x.75]*
       Return an iterator pointing to the element past the end of the array of       locally owned entries.      
* [0.x.76]*
       Return a constant iterator pointing to the element past the end of the       array of the locally owned entries.      
* [0.x.77]*
        [2.x.82]  3: Modification of vectors      
* [0.x.78]*
       A collective set operation: instead of setting individual elements of a       vector, this function allows to set a whole set of elements at once.       The indices of the elements to be set are stated in the first argument,       the corresponding values in the second.      
* [0.x.79]*
       This is a second collective set operation. As a difference, this       function takes a deal.II vector of values.      
* [0.x.80]*
       This collective set operation is of lower level and can handle anything       else &mdash; the only thing you have to provide is an address where all       the indices are stored and the number of elements to be set.      
* [0.x.81]*
       A collective add operation: This function adds a whole set of values       stored in  [2.x.83]  to the vector components specified by  [2.x.84]       
* [0.x.82]*
       This is a second collective add operation. As a difference, this       function takes a deal.II vector of values.      
* [0.x.83]*
       Take an address where <tt>n_elements</tt> are stored contiguously and       add them into the vector. Handles all cases which are not covered by       the other two <tt>add()</tt> functions above.      
* [0.x.84]*
       Multiply the entire vector by a fixed factor.      
* [0.x.85]*
       Divide the entire vector by a fixed factor.      
* [0.x.86]*
       Add the given vector to the present one.      
* [0.x.87]*
       Subtract the given vector from the present one.      
* [0.x.88]*
       Addition of  [2.x.85]  to all components. Note that  [2.x.86]  is a scalar and not       a vector.      
* [0.x.89]*
       Simple vector addition, equal to the <tt>operator+=</tt>.             Though, if the second argument <tt>allow_different_maps</tt> is set,       then it is possible to add data from a vector that uses a different       map, i.e., a vector whose elements are split across processors       differently. This may include vectors with ghost elements, for example.       In general, however, adding vectors with a different element-to-       processor map requires communicating data among processors and,       consequently, is a slower operation than when using vectors using the       same map.      
* [0.x.90]*
       Simple addition of a multiple of a vector, i.e. <tt>*this += a*V</tt>.      
* [0.x.91]*
       Multiple addition of scaled vectors, i.e. <tt>*this += a*V + b*W</tt>.      
* [0.x.92]*
       Scaling and simple vector addition, i.e.  <tt>*this = s*(*this) +       V</tt>.      
* [0.x.93]*
       Scaling and simple addition, i.e.  <tt>*this = s*(*this) + a*V</tt>.      
* [0.x.94]*
       Scale each element of this vector by the corresponding element in the       argument. This function is mostly meant to simulate multiplication (and       immediate re-assignment) by a diagonal scaling matrix.      
* [0.x.95]*
       Assignment <tt>*this = a*V</tt>.      
* [0.x.96]*
        [2.x.87]  4: Mixed stuff      
* [0.x.97]*
       Return a const reference to the underlying Trilinos Epetra_MultiVector       class.      
* [0.x.98]*
       Return a (modifiable) reference to the underlying Trilinos       Epetra_FEVector class.      
* [0.x.99]*
       Return a const reference to the underlying Trilinos Epetra_BlockMap       that sets the parallel partitioning of the vector.      
* [0.x.100]*
       Print to a stream.  [2.x.88]  denotes the desired precision with       which values shall be printed,  [2.x.89]  whether scientific       notation shall be used. If  [2.x.90]  is  [2.x.91]  then the vector is       printed in a line, while if  [2.x.92]  then the elements are printed on a       separate line each.      
* [0.x.101]*
       Swap the contents of this vector and the other vector  [2.x.93]  One could       do this operation with a temporary variable and copying over the data       elements, but this function is significantly more efficient since it       only swaps the pointers to the data of the two vectors and therefore       does not need to allocate temporary storage and move data around. Note       that the vectors need to be of the same size and base on the same map.             This function is analogous to the  [2.x.94]  function of all C++       standard containers. Also, there is a global function       <tt>swap(u,v)</tt> that simply calls <tt>u.swap(v)</tt>, again in       analogy to standard functions.      
* [0.x.102]*
       Estimate for the memory consumption in bytes.      
* [0.x.103]*
       Return a reference to the MPI communicator object in use with this       object.      
* [0.x.104]*
       Exception      
* [0.x.105]*
       Exception      
* [0.x.106]*
       Exception      
* [0.x.107]*
       Trilinos doesn't allow to mix additions to matrix entries and       overwriting them (to make synchronization of parallel computations       simpler). The way we do it is to, for each access operation, store       whether it is an insertion or an addition. If the previous one was of       different type, then we first have to flush the Trilinos buffers;       otherwise, we can simply go on.  Luckily, Trilinos has an object for       this which does already all the parallel communications in such a case,       so we simply use their model, which stores whether the last operation       was an addition or an insertion.      
* [0.x.108]*
       A boolean variable to hold information on whether the vector is       compressed or not.      
* [0.x.109]*
       Whether this vector has ghost elements. This is true on all processors       even if only one of them has any ghost elements.      
* [0.x.110]*
       Pointer to the actual Epetra vector object. This may represent a vector       that is in fact distributed among multiple processors. The object       requires an existing Epetra_Map for storing data when setting it up.      
* [0.x.111]*
       A vector object in Trilinos to be used for collecting the non-local       elements if the vector was constructed with an additional IndexSet       describing ghost elements.      
* [0.x.112]*
       An IndexSet storing the indices this vector owns exclusively.      
* [0.x.113]*
     Global function  [2.x.95]  which overloads the default implementation of     the C++ standard library which uses a temporary object. The function     simply exchanges the data of the two vectors.          [2.x.96]   [2.x.97]     
* [0.x.114]*
     A helper class used internally in linear_operator.h. Specialization for      [2.x.98]     
* [0.x.115]*
 Declare  [2.x.99]  as distributed vector.

* 
* [0.x.116]

include/deal.II-translator/lac/utilities_0.txt
[0.x.0]*
   A collection of linear-algebra utilities.  
* [0.x.1]*
     Return the elements of a continuous Givens rotation matrix and     the norm of the input vector.         That is for a given     pair  [2.x.0]  and  [2.x.1]  return  [2.x.2]  ,  [2.x.3]  and  [2.x.4]  such that     [1.x.0]        
*  [2.x.5]  The function is implemented for real valued numbers only.    
* [0.x.2]*
     Return the elements of a hyperbolic rotation matrix.         That is for a given     pair  [2.x.6]  and  [2.x.7]  return  [2.x.8]  ,  [2.x.9]  and  [2.x.10]  such that     [1.x.1]         Real valued solution only exists if  [2.x.11] , the function will     throw an error otherwise.        
*  [2.x.12]  The function is implemented for real valued numbers only.    
* [0.x.3]*
     Estimate an upper bound for the largest eigenvalue of  [2.x.13]  by a  [2.x.14] 
* 
*  - tep     Lanczos process starting from the initial vector  [2.x.15]  Typical     values of  [2.x.16]  are below 10. This estimator computes a k-step Lanczos     decomposition  [2.x.17]  where  [2.x.18]  contains k Lanczos     basis,  [2.x.19] ,  [2.x.20]  is the tridiagonal Lanczos matrix,  [2.x.21]  is     a residual vector  [2.x.22] , and  [2.x.23]  is the k-th canonical basis of      [2.x.24] . The returned value is  [2.x.25] .     If  [2.x.26]  is not  [2.x.27] , the eigenvalues of  [2.x.28]  will be written there.          [2.x.29]  is used to allocate memory for temporary vectors.     OperatorType has to provide  [2.x.30]  operation with     VectorType.         This function implements the algorithm from    
* [1.x.2]
*         
*  [2.x.31]  This function uses Lapack routines to compute the largest     eigenvalue of  [2.x.32] .        
*  [2.x.33]  This function provides an alternate estimate to that obtained from     several steps of SolverCG with      [2.x.34]     
* [0.x.4]*
     Apply Chebyshev polynomial of the operator  [2.x.35]  to  [2.x.36]  For a     non-defective operator  [2.x.37]  with a complete set of eigenpairs      [2.x.38] , the action of a polynomial filter  [2.x.39]  is     given by  [2.x.40] , where  [2.x.41] . Thus by appropriately choosing the polynomial filter, one can     alter the eigenmodes contained in  [2.x.42] .         This function uses Chebyshev polynomials of first kind. Below is an     example of polynomial  [2.x.43]  of degree  [2.x.44]  normalized to unity at      [2.x.45] .  [2.x.46]      By introducing a linear mapping  [2.x.47]  from  [2.x.48]  to      [2.x.49] , we can dump the corresponding modes in  [2.x.50]  The higher     the polynomial degree  [2.x.51] , the more rapid it grows outside of the      [2.x.52] . In order to avoid numerical overflow, we normalize     polynomial filter to unity at  [2.x.53]  Thus, the filtered operator     is  [2.x.54] .         The action of the Chebyshev filter only requires     evaluation of  [2.x.55]  of  [2.x.56]  and is based on the     recursion equation for Chebyshev polynomial of degree  [2.x.57] :      [2.x.58]  with  [2.x.59]  and  [2.x.60] .          [2.x.61]  is used to allocate memory for temporary objects.         This function implements the algorithm (with a minor fix of sign of      [2.x.62] ) from    
* [1.x.3]
*         
*  [2.x.63]  If  [2.x.64]  is equal to      [2.x.65] , no normalization     will be performed.    
* [0.x.5]

include/deal.II-translator/lac/vector_0.txt
[0.x.0]!  [2.x.0]  Vectors [2.x.1] 

* 
* [0.x.1]*
 A class that represents a vector of numerical elements. As for the other classes, in the  [2.x.2]  group, this class has a substantial number of member functions. These include:
* 

* 
* 
*  - functions that initialize the vector or change its size;
* 

* 
* 
*  - functions that compute properties of the vector, such as a variety of   norms;
* 

* 
* 
*  - functions that allow reading from or writing to individual elements of the   vector;
* 

* 
* 
*  - functions that implement algebraic operations for vectors, such as   addition of vectors; and
* 

* 
* 
*  - functions that allow inputting and outputting the data stored by vectors.
*  In contrast to the C++ standard library class  [2.x.3]  this class intends to implement not simply an array that allows access to its elements, but indeed a vector that is a member of the mathematical concept of a "vector space" suitable for numerical computations.
* 

* 
*  [2.x.4]  Instantiations for this template are provided for <tt> [2.x.5]   [2.x.6]   [2.x.7]   [2.x.8]  others can be generated in application programs (see the section on  [2.x.9]  in the manual).

* 
* [0.x.2]*
   Declare standard types used in all containers. These types parallel those   in the <tt>C++</tt> standard libraries <tt>vector<...></tt> class.  
* [0.x.3]*
   Declare a type that has holds real-valued numbers with the same precision   as the template argument to this class. If the template argument of this   class is a real data type, then real_type equals the template argument.   If the template argument is a  [2.x.10]  type then real_type equals the   type underlying the complex numbers.     This alias is used to represent the return type of norms.  
* [0.x.4]*
    [2.x.11]  Basic object handling  
* [0.x.5]*
   Constructor. Create a vector of dimension zero.  
* [0.x.6]*
   Copy constructor. Sets the dimension to that of the given vector, and   copies all elements.     We would like to make this constructor explicit, but standard containers   insist on using it implicitly.      [2.x.12]   
* [0.x.7]*
   Move constructor. Creates a new vector by stealing the internal data of   the vector  [2.x.13]   
* [0.x.8]*
   Copy constructor taking a vector of another data type.     This constructor will fail to compile if   there is no conversion path from  [2.x.14]  to  [2.x.15]  You may   lose accuracy when copying to a vector with data elements with   less accuracy.  
* [0.x.9]*
   Copy constructor taking an object of type  [2.x.16]  This   constructor can be used to initialize a vector using a brace-enclosed   list of numbers, such as in the following example:  
* [1.x.0]
*    This creates a vector of size 3, whose (double precision) elements have   values 1.0, 2.0, and 3.0.     This constructor will fail to compile if   there is no conversion path from  [2.x.17]  to  [2.x.18]  You may   lose accuracy when copying to a vector with data elements with   less accuracy.  
* [0.x.10]*
   Another copy constructor: copy the values from a PETSc vector class. This   copy constructor is only available if PETSc was detected during   configuration time.     Note that due to the communication model used in MPI, this operation can   only succeed if all processes do it at the same time when  [2.x.19]    is a distributed vector: It is not possible for only one process to   obtain a copy of a parallel vector while the other jobs do something   else.  
* [0.x.11]*
   Another copy constructor: copy the values from a Trilinos wrapper vector.   This copy constructor is only available if Trilinos was detected during   configuration time.    
*  [2.x.20]  Due to the communication model used in MPI, this operation can   only succeed if all processes that have knowledge of  [2.x.21]    (i.e. those given by  [2.x.22] ) do it at   the same time. This means that unless you use a split MPI communicator   then it is not normally possible for only one or a subset of processes   to obtain a copy of a parallel vector while the other jobs do something   else. In other words, calling this function is a 'collective operation'   that needs to be executed by all MPI processes that jointly share  [2.x.23]   
* [0.x.12]*
   Constructor. Set dimension to  [2.x.24]  and initialize all elements with zero.     The constructor is made explicit to avoid accidents like this:   <tt>v=0;</tt>. Presumably, the user wants to set every element of the   vector to zero, but instead, what happens is this call:   <tt>v=Vector [2.x.25]  i.e. the vector is replaced by one of   length zero.  
* [0.x.13]*
   Initialize the vector with a given range of values pointed to by the   iterators. This function is there in analogy to the  [2.x.26]  class.  
* [0.x.14]*
   Destructor, deallocates memory. Made virtual to allow for derived classes   to behave properly.  
* [0.x.15]*
   This function does nothing but exists for compatibility with the parallel   vector classes.     For the parallel vector wrapper class, this function compresses the   underlying representation of the vector, i.e. flushes the buffers of the   vector object if it has any. This function is necessary after writing   into a vector element-by-element and before anything else can be done on   it.     However, for the implementation of this class, it is immaterial and thus   an empty function.  
* [0.x.16]*
   Change the dimension of the vector to  [2.x.27]  The reserved memory for this   vector remains unchanged if possible, to make things faster; this may   waste some memory, so keep this in mind.  However, if <tt>N==0</tt> all   memory is freed, i.e. if you want to resize the vector and release the   memory not needed, you have to first call <tt>reinit(0)</tt> and then   <tt>reinit(N)</tt>. This cited behavior is analogous to that of the   standard library containers.     If  [2.x.28]  is false, the vector is filled by zeros.   Otherwise, the elements are left an unspecified state.     This function is virtual in order to allow for derived classes to handle   memory separately.  
* [0.x.17]*
   Same as above, but will preserve the values of vector upon resizing.   If we new size is bigger, we will have   [1.x.1]   whereas if the desired size is smaller, then   [1.x.2]  
* [0.x.18]*
   Apply [1.x.3]    [2.x.29]  (a triplet of cosine, sine and radius, see    [2.x.30]    to the vector in the plane spanned by the  [2.x.31]  and  [2.x.32]  unit vectors.  
* [0.x.19]*
   Change the dimension to that of the vector  [2.x.33]  The same applies as for   the other  [2.x.34]  function.     The elements of  [2.x.35]  are not copied, i.e.  this function is the same as   calling <tt>reinit (V.size(), omit_zeroing_entries)</tt>.  
* [0.x.20]*
   Swap the contents of this vector and the other vector  [2.x.36]  One could do   this operation with a temporary variable and copying over the data   elements, but this function is significantly more efficient since it only   swaps the pointers to the data of the two vectors and therefore does not   need to allocate temporary storage and move data around.     This function is analogous to the  [2.x.37]  function of all C++   standard containers. Also, there is a global function <tt>swap(u,v)</tt>   that simply calls <tt>u.swap(v)</tt>, again in analogy to standard   functions.     This function is virtual in order to allow for derived classes to handle   memory separately.  
* [0.x.21]*
   Set all components of the vector to the given number  [2.x.38]      Since the semantics of assigning a scalar to a vector are not immediately   clear, this operator should really only be used if you want to set the   entire vector to zero. This allows the intuitive notation <tt>v=0</tt>.   Assigning other values is deprecated and may be disallowed in the future.      [2.x.39]   
* [0.x.22]*
   Copy the given vector. Resize the present vector if necessary.      [2.x.40]   
* [0.x.23]*
   Move the given vector. This operator replaces the present vector with   the internal data of the vector  [2.x.41]  and resets  [2.x.42]  to the state it would   have after being newly default-constructed.  
* [0.x.24]*
   Copy the given vector. Resize the present vector if necessary.      [2.x.43]   
* [0.x.25]*
   Copy operator for assigning a block vector to a regular vector.  
* [0.x.26]*
   Another copy operator: copy the values from a PETSc wrapper vector   class. This operator is only available if PETSc was detected during   configuration time.     Note that due to the communication model used in MPI, this operation can   only succeed if all processes do it at the same time when  [2.x.44]    is a distributed vector: It is not possible for only one process to   obtain a copy of a parallel vector while the other jobs do something   else.  
* [0.x.27]*
   Another copy operator: copy the values from a (sequential or parallel,   depending on the underlying compiler) Trilinos wrapper vector class. This   operator is only available if Trilinos was detected during configuration   time.    
*  [2.x.45]  Due to the communication model used in MPI, this operation can   only succeed if all processes that have knowledge of  [2.x.46]    (i.e. those given by  [2.x.47] ) do it at   the same time. This means that unless you use a split MPI communicator   then it is not normally possible for only one or a subset of processes   to obtain a copy of a parallel vector while the other jobs do something   else. In other words, calling this function is a 'collective operation'   that needs to be executed by all MPI processes that jointly share  [2.x.48]   
* [0.x.28]*
   Test for equality. This function assumes that the present vector and the   one to compare with have the same size already, since comparing vectors   of different sizes makes not much sense anyway.  
* [0.x.29]*
   Test for inequality. This function assumes that the present vector and   the one to compare with have the same size already, since comparing   vectors of different sizes makes not much sense anyway.  
* [0.x.30]*
    [2.x.49]  Scalar products, norms and related operations  
* [0.x.31]*
   Return the scalar product of two vectors.  The return type is the   underlying type of  [2.x.50]  vector, so the return type and the accuracy   with which it the result is computed depend on the order of the arguments   of this vector.     For complex vectors, the scalar product is implemented as    [2.x.51] .      [2.x.52]  The algorithm uses pairwise summation   with the same order of summation in every run, which gives fully   repeatable results from one run to another.  
* [0.x.32]*
   Return the square of the  [2.x.53] -norm.      [2.x.54]  The algorithm uses pairwise summation   with the same order of summation in every run, which gives fully   repeatable results from one run to another.  
* [0.x.33]*
   Mean value of the elements of this vector.      [2.x.55]  The algorithm uses pairwise summation   with the same order of summation in every run, which gives fully   repeatable results from one run to another.  
* [0.x.34]*
    [2.x.56] -norm of the vector. The sum of the absolute values.      [2.x.57]  The algorithm uses pairwise summation   with the same order of summation in every run, which gives fully   repeatable results from one run to another.  
* [0.x.35]*
    [2.x.58] -norm of the vector. The square root of the sum of the squares of   the elements.      [2.x.59]  The algorithm uses pairwise summation   with the same order of summation in every run, which gives fully   repeatable results from one run to another.  
* [0.x.36]*
    [2.x.60] -norm of the vector. The pth root of the sum of the pth powers of   the absolute values of the elements.      [2.x.61]  The algorithm uses pairwise summation   with the same order of summation in every run, which gives fully   repeatable results from one run to another.  
* [0.x.37]*
   Maximum absolute value of the elements.  
* [0.x.38]*
   Performs a combined operation of a vector addition and a subsequent inner   product, returning the value of the inner product. In other words, the   result of this function is the same as if the user called  
* [1.x.4]
*      The reason this function exists is that this operation involves less   memory transfer than calling the two functions separately. This method   only needs to load three vectors,  [2.x.62]   [2.x.63]   [2.x.64]  whereas calling   separate methods means to load the calling vector  [2.x.65]  twice. Since   most vector operations are memory transfer limited, this reduces the time   by 25\% (or 50\% if  [2.x.66]  equals  [2.x.67]      For complex-valued vectors, the scalar product in the second step is   implemented as    [2.x.68] .      [2.x.69]  The algorithm uses pairwise summation   with the same order of summation in every run, which gives fully   repeatable results from one run to another.  
* [0.x.39]*
    [2.x.70]  Data access  
* [0.x.40]*
   Return a pointer to the underlying data buffer.  
* [0.x.41]*
   Return a const pointer to the underlying data buffer.  
* [0.x.42]*
   Make the  [2.x.71]  class a bit like the <tt>vector<></tt> class of the   C++ standard library by returning iterators to the start and end of the   elements of this vector.  
* [0.x.43]*
   Return constant iterator to the start of the vectors.  
* [0.x.44]*
   Return an iterator pointing to the element past the end of the array.  
* [0.x.45]*
   Return a constant iterator pointing to the element past the end of the   array.  
* [0.x.46]*
   Access the value of the  [2.x.72]  component.  
* [0.x.47]*
   Access the  [2.x.73]  component as a writeable reference.  
* [0.x.48]*
   Access the value of the  [2.x.74]  component.     Exactly the same as operator().  
* [0.x.49]*
   Access the  [2.x.75]  component as a writeable reference.     Exactly the same as operator().  
* [0.x.50]*
   Instead of getting individual elements of a vector via operator(),   this function allows getting a whole set of elements at once. The   indices of the elements to be read are stated in the first argument, the   corresponding values are returned in the second.     If the current vector is called  [2.x.76]  then this function is the equivalent   to the code  
* [1.x.5]
*       [2.x.77]  The sizes of the  [2.x.78]  and  [2.x.79]  arrays must be identical.  
* [0.x.51]*
   Instead of getting individual elements of a vector via operator(),   this function allows getting a whole set of elements at once. In   contrast to the previous function, this function obtains the   indices of the elements by dereferencing all elements of the iterator   range provided by the first two arguments, and puts the vector   values into memory locations obtained by dereferencing a range   of iterators starting at the location pointed to by the third   argument.     If the current vector is called  [2.x.80]  then this function is the equivalent   to the code  
* [1.x.6]
*       [2.x.81]  It must be possible to write into as many memory locations     starting at  [2.x.82]  as there are iterators between      [2.x.83]  and  [2.x.84]   
* [0.x.52]*
    [2.x.85]  Modification of vectors  
* [0.x.53]*
   Add the given vector to the present one.      [2.x.86]   
* [0.x.54]*
   Subtract the given vector from the present one.      [2.x.87]   
* [0.x.55]*
   A collective add operation: This function adds a whole set of values   stored in  [2.x.88]  to the vector components specified by  [2.x.89]   
* [0.x.56]*
   This is a second collective add operation. As a difference, this function   takes a deal.II vector of values.  
* [0.x.57]*
   Take an address where <tt>n_elements</tt> are stored contiguously and add   them into the vector. Handles all cases which are not covered by the   other two <tt>add()</tt> functions above.  
* [0.x.58]*
   Addition of  [2.x.90]  to all components. Note that  [2.x.91]  is a scalar and not a   vector.      [2.x.92]   
* [0.x.59]*
   Multiple addition of scaled vectors, i.e. <tt>*this += a*V+b*W</tt>.      [2.x.93]   
* [0.x.60]*
   Simple addition of a multiple of a vector, i.e. <tt>*this += a*V</tt>.      [2.x.94]   
* [0.x.61]*
   Scaling and simple vector addition, i.e.  <tt>*this = s*(*this)+V</tt>.      [2.x.95]   
* [0.x.62]*
   Scaling and simple addition, i.e.  <tt>*this = s*(*this)+a*V</tt>.      [2.x.96]   
* [0.x.63]*
   Scale each element of the vector by a constant value.      [2.x.97]   
* [0.x.64]*
   Scale each element of the vector by the inverse of the given value.      [2.x.98]   
* [0.x.65]*
   Scale each element of this vector by the corresponding element in the   argument. This function is mostly meant to simulate multiplication (and   immediate re-assignment) by a diagonal scaling matrix.      [2.x.99]   
* [0.x.66]*
   Scale each element of this vector by the corresponding element in the   argument. This function is mostly meant to simulate multiplication (and   immediate re-assignment) by a diagonal scaling matrix.  
* [0.x.67]*
   Assignment <tt>*this = a*u</tt>.      [2.x.100]   
* [0.x.68]*
   Assignment <tt>*this = a*u</tt>.  
* [0.x.69]*
   This function does nothing but exists for compatibility with the  [2.x.101]    parallel vector classes (e.g.,  [2.x.102]  class).  
* [0.x.70]*
    [2.x.103]  Input and output  
* [0.x.71]*
   Print to a stream.  [2.x.104]  denotes the desired precision with which   values shall be printed,  [2.x.105]  whether scientific notation shall   be used. If  [2.x.106]  is  [2.x.107]  then the vector is printed in a line,   while if  [2.x.108]  then the elements are printed on a separate line each.  
* [0.x.72]*
   Write the vector en bloc to a file. This is done in a binary mode, so the   output is neither readable by humans nor (probably) by other computers   using a different operating system or number format.  
* [0.x.73]*
   Read a vector en block from a file. This is done using the inverse   operations to the above function, so it is reasonably fast because the   bitstream is not interpreted.     The vector is resized if necessary.     A primitive form of error checking is performed which will recognize the   bluntest attempts to interpret some data as a vector stored bitwise to a   file, but not more.  
* [0.x.74]*
   Write the data of this object to a stream for the purpose of   serialization using the [BOOST serialization   library](https://www.boost.org/doc/libs/1_74_0/libs/serialization/doc/index.html).  
* [0.x.75]*
   Read the data of this object from a stream for the purpose of   serialization using the [BOOST serialization   library](https://www.boost.org/doc/libs/1_74_0/libs/serialization/doc/index.html).  
* [0.x.76]*
   Write and read the data of this object from a stream for the purpose   of serialization using the [BOOST serialization   library](https://www.boost.org/doc/libs/1_74_0/libs/serialization/doc/index.html).  
* [0.x.77]*
    [2.x.109]   
* [0.x.78]*
    [2.x.110]  Information about the object  
* [0.x.79]*
   Return true if the given global index is in the local range of this   processor.  Since this is not a distributed vector the method always   returns true.  
* [0.x.80]*
   Return an index set that describes which elements of this vector are   owned by the current processor. Note that this index set does not include   elements this vector may store locally as ghost elements but that are in   fact owned by another processor. As a consequence, the index sets   returned on different processors if this is a distributed vector will   form disjoint sets that add up to the complete index set. Obviously, if a   vector is created on only one processor, then the result would satisfy  
* [1.x.7]
*      Since the current data type does not support parallel data storage across   different processors, the returned index set is the complete index set.  
* [0.x.81]*
   Return dimension of the vector.  
* [0.x.82]*
   Return local dimension of the vector. Since this vector does not support   distributed data this is always the same value as size().    
*  [2.x.111]  This function exists for compatibility with    [2.x.112]   
* [0.x.83]*
   Return whether the vector contains only elements with value zero. This   function is mainly for internal consistency checks and should seldom be   used when not in debug mode since it uses quite some time.  
* [0.x.84]*
   Return  [2.x.113]  if the vector has no negative entries, i.e. all entries   are zero or positive. This function is used, for example, to check   whether refinement indicators are really all positive (or zero).     The function obviously only makes sense if the template argument of this   class is a real type. If it is a complex type, then an exception is   thrown.  
* [0.x.85]*
   Determine an estimate for the memory consumption (in bytes) of this   object.  
* [0.x.86]*
   This function exists for compatibility with the  [2.x.114]    parallel vector classes (e.g.,  [2.x.115]  class).   Always returns false since this implementation is serial.  
* [0.x.87]*
   Array of elements owned by this vector.  
* [0.x.88]*
   Convenience function used at the end of initialization or   reinitialization. Resets (if necessary) the loop partitioner to the   correct state, based on its current state and the length of the vector.  
* [0.x.89]*
   Actual implementation of the reinit functions.  
* [0.x.90]*
   For parallel loops with TBB, this member variable stores the affinity   information of loops.  
* [0.x.91]!  [2.x.116]  Vectors [2.x.117] 

* 
* [0.x.92]*
 Global function  [2.x.118]  which overloads the default implementation of the C++ standard library which uses a temporary object. The function simply exchanges the data of the two vectors.
*   [2.x.119]  Vector

* 
* [0.x.93]*
 Output operator writing a vector to a stream. This operator outputs the elements of the vector one by one, with a space between entries. Each entry is formatted according to the flags set on the output stream.
*   [2.x.120]  Vector

* 
* [0.x.94]*
 Declare  [2.x.121]  Number > as serial vector.
*   [2.x.122]  Vector

* 
* [0.x.95]

include/deal.II-translator/lac/vector_element_access_0.txt
[0.x.0]

include/deal.II-translator/lac/vector_memory_0.txt
[0.x.0]*
 Memory management base class for vectors. This is an abstract base class used, among other places, by all iterative methods to allocate space for auxiliary vectors.
*  The purpose of this class is as follows: in iterative solvers and other places, one needs to allocate temporary storage for vectors, for example for auxiliary vectors. One could allocate and release them anew every time, but this may be expensive in some situations if it has to happen very frequently. A common case for this is when an iterative method is used to invert a matrix in each iteration of an outer solver, such as when inverting a matrix block for a Schur complement solver. ( [2.x.0]  does this, for example, but instead just keeps a vector around permanently for temporary storage.)
*  In such situations, allocating and deallocating vectors anew in each call to the inner solver is expensive and leads to memory fragmentation. The present class allows to avoid this by offering an interface that other classes can use to allocate and deallocate vectors. Different derived classes then implement different strategies to provide temporary storage vectors to using classes.
*  For example, the PrimitiveVectorMemory class simply allocates and deallocates vectors via the operating system facilities (i.e., using  [2.x.1]  and  [2.x.2]  each time it is asked for a vector. It is an appropriate implementation to use for iterative solvers that are called only once, or very infrequently.
*  On the other hand, the GrowingVectorMemory class never returns memory space to the operating system memory management subsystem during its lifetime; it only marks them as unused and allows them to be reused next time a vector is requested.
* 

*  [1.x.0]
*  Classes derived from this base class return pointers to new vectors via the  [2.x.3]  function, and re-claim the vector when it is returned via  [2.x.4]  These two functions therefore play a similar role as  [2.x.5]  and  [2.x.6]  This includes the usual drawbacks: It is simple to forget to call  [2.x.7]  at the end of a function that uses this facility, or to forget it in an  [2.x.8]  branch of the function where one has an early  [2.x.9]  from the function. In both cases, this results in a memory leak: a correct piece of code has to call  [2.x.10]  for all allocated vectors at [1.x.1] possible exit points. This includes places where a function is left because an exception is thrown further down in the call stack and not explicitly handled here.
*  In other words, vectors allocated via  [2.x.11]  have the same issue as raw pointers allocated via  [2.x.12]  It is easy to write code that has memory leaks. In the case of raw pointers, the common solution is to use the  [2.x.13]  class instead (see http://en.cppreference.com/w/cpp/memory/unique_ptr). In the case of the current class, the  [2.x.14]  class is the solution: it is a class that for all practical purposes looks like a pointer, but upon destruction also returns the vector back to the VectorMemory object from which it got it. Since destruction of the  [2.x.15]  class happens whenever it goes out of scope (whether because the function explicitly returns, or because control flow leaves it due to an exception), a memory leak cannot happen: the vector the  [2.x.16]  object points to is [1.x.2] returned.

* 
* [0.x.1]*
   Virtual destructor. This destructor is declared  [2.x.17]  to allow   destroying objects of derived type through pointers to this base   class.  
* [0.x.2]*
   Return a pointer to a new vector. The number of elements or their   subdivision into blocks (if applicable) is unspecified and users of this   function should reset vectors to their proper size. The same holds for   the contents of vectors: they are unspecified. In other words,   the place that calls this function will need to resize or reinitialize   it appropriately.      [2.x.18]  Just like using  [2.x.19]      explicitly in code invites bugs where memory is leaked (either     because the corresponding  [2.x.20]  is forgotten     altogether, or because of exception safety issues), using the     alloc() and free() functions explicitly invites writing code     that accidentally leaks memory. You should consider using     the  [2.x.21]  class instead, which provides the     same kind of service that  [2.x.22]  provides     for arbitrary memory allocated on the heap.  
* [0.x.3]*
   Return a vector and indicate that it is not going to be used any further   by the place that called alloc() to get a pointer to it.      [2.x.23]  Just like using  [2.x.24]      explicitly in code invites bugs where memory is leaked (either     because the corresponding  [2.x.25]  is forgotten     altogether, or because of exception safety issues), using the     alloc() and free() functions explicitly invites writing code     that accidentally leaks memory. You should consider using     the  [2.x.26]  class instead, which provides the     same kind of service that  [2.x.27]  provides     for arbitrary memory allocated on the heap.  
* [0.x.4]*
    [2.x.28]  Exceptions    [2.x.29]   
* [0.x.5]*
   Vector was not allocated from this memory pool.  
* [0.x.6]*
   A class that looks like a pointer for all practical purposes and that   upon construction time allocates a vector from a VectorMemory object   (or an object of a class derived from VectorMemory) that is passed   to the constructor of this class. The destructor then automatically   returns the vector's ownership to the same VectorMemory object.     Pointers of this type are therefore safe in the sense that they   automatically call  [2.x.30]  when they are destroyed, whether   that happens at the end of a code block or because local variables are   destroyed during exception unwinding. These kinds of object thus relieve   the user from using vector management functions explicitly.     In many senses, this class acts like  [2.x.31]  in that   it is the unique owner of a chunk of memory that it frees upon destruction.   The main differences to  [2.x.32]  are (i) that it   allocates memory from a memory pool upon construction, and (ii) that the   memory is not destroyed using `operator delete` but returned to the   VectorMemory pool.  
* [0.x.7]*
     Default constructor. This constructor corresponds to a  [2.x.33]      object that does not own a vector. It can, however, later be     assigned another Pointer object via move assignment in which case     it will steal the vector owned by the other object     (as  [2.x.34]  does).    
* [0.x.8]*
     Move constructor: this creates a new Pointer by stealing the internal     data owned by  [2.x.35]     
* [0.x.9]*
     Move operator: this releases the vector owned by the current Pointer     and then steals the internal data owned by  [2.x.36]     
* [0.x.10]*
     Constructor. This constructor automatically allocates a vector from     the given vector memory object  [2.x.37]     
* [0.x.11]*
     Destructor, automatically releasing the vector from the memory pool.    
* [0.x.12]*
 Simple memory management. See the documentation of the base class for a description of its purpose.
*  This class allocates and deletes vectors as needed from the global heap, i.e. performs no specially adapted actions for memory management.

* 
* [0.x.13]*
   Return a pointer to a new vector. The number of elements or their   subdivision into blocks (if applicable) is unspecified and users of this   function should reset vectors to their proper size. The same holds for   the contents of vectors: they are unspecified. In other words,   the place that calls this function will need to resize or reinitialize   it appropriately.     For the present class, calling this function will allocate a new vector   on the heap and returning a pointer to it. Later calling free() then   returns the memory to the global heap managed by the operating system.      [2.x.38]  Just like using  [2.x.39]      explicitly in code invites bugs where memory is leaked (either     because the corresponding  [2.x.40]  is forgotten     altogether, or because of exception safety issues), using the     alloc() and free() functions explicitly invites writing code     that accidentally leaks memory. You should consider using     the  [2.x.41]  class instead, which provides the     same kind of service that  [2.x.42]  provides     for arbitrary memory allocated on the heap.  
* [0.x.14]*
   Return a vector and indicate that it is not going to be used any further   by the instance that called alloc() to get a pointer to it.     For the present class, this means that the vector is returned to the   global heap.      [2.x.43]  Just like using  [2.x.44]      explicitly in code invites bugs where memory is leaked (either     because the corresponding  [2.x.45]  is forgotten     altogether, or because of exception safety issues), using the     alloc() and free() functions explicitly invites writing code     that accidentally leaks memory. You should consider using     the  [2.x.46]  class instead, which provides the     same kind of service that  [2.x.47]  provides     for arbitrary memory allocated on the heap.  
* [0.x.15]*
 A pool based memory management class. See the documentation of the base class for a description of its purpose.
*  Each time a vector is requested from this class, it checks if it has one available and returns its address, or allocates a new one on the heap. If a vector is returned from its user, through the  [2.x.48]  member function, it doesn't return the allocated memory to the operating system memory subsystem, but keeps it around unused for later use if  [2.x.49]  is called again. The class therefore avoid the overhead of repeatedly allocating memory on the heap if temporary vectors are required and released frequently; on the other hand, it doesn't release once-allocated memory at the earliest possible time and may therefore lead to an increased overall memory consumption.
*  All GrowingVectorMemory objects of the same vector type use the same memory pool. (In other words: The pool of vectors from which this class draws is [1.x.3], rather than a regular member variable of the current class that is destroyed at the time that the surrounding GrowingVectorMemory object is destroyed.) Therefore, functions can create such a GrowingVectorMemory object whenever needed without the performance penalty of creating a new memory pool every time. A drawback of this policy is that vectors once allocated are only released at the end of the program run.

* 
* [0.x.16]*
   Declare type for container size.  
* [0.x.17]*
   Constructor.  The argument allows to preallocate a certain number of   vectors. The default is not to do this.  
* [0.x.18]*
   Destructor. The destructor also checks that all vectors that have been   allocated through the current object have all been released again.   However, as discussed in the class documentation, this does not imply   that their memory is returned to the operating system.  
* [0.x.19]*
   Return a pointer to a new vector. The number of elements or their   subdivision into blocks (if applicable) is unspecified and users of this   function should reset vectors to their proper size. The same holds for   the contents of vectors: they are unspecified. In other words,   the place that calls this function will need to resize or reinitialize   it appropriately.      [2.x.50]  Just like using  [2.x.51]      explicitly in code invites bugs where memory is leaked (either     because the corresponding  [2.x.52]  is forgotten     altogether, or because of exception safety issues), using the     alloc() and free() functions explicitly invites writing code     that accidentally leaks memory. You should consider using     the  [2.x.53]  class instead, which provides the     same kind of service that  [2.x.54]  provides     for arbitrary memory allocated on the heap.  
* [0.x.20]*
   Return a vector and indicate that it is not going to be used any further   by the instance that called alloc() to get a pointer to it.     For the present class, this means retaining the vector for later reuse by   the alloc() method.      [2.x.55]  Just like using  [2.x.56]      explicitly in code invites bugs where memory is leaked (either     because the corresponding  [2.x.57]  is forgotten     altogether, or because of exception safety issues), using the     alloc() and free() functions explicitly invites writing code     that accidentally leaks memory. You should consider using     the  [2.x.58]  class instead, which provides the     same kind of service that  [2.x.59]  provides     for arbitrary memory allocated on the heap.  
* [0.x.21]*
   Release all vectors that are not currently in use.  
* [0.x.22]*
   Memory consumed by this class and all currently allocated vectors.  
* [0.x.23]*
   A type that describes this entries of an array that represents   the vectors stored by this object. The first component of the pair   is be a flag telling whether the vector is used, the second   a pointer to the vector itself.  
* [0.x.24]*
   The class providing the actual storage for the memory pool.     This is where the actual storage for GrowingVectorMemory is provided.   Only one of these pools is used for each vector type, thus allocating all   vectors from the same storage.  
* [0.x.25]*
     Standard constructor creating an empty pool    
* [0.x.26]*
     Destructor.    
* [0.x.27]*
     Create data vector; does nothing after first initialization    
* [0.x.28]*
     Pointer to the storage object    
* [0.x.29]*
   Return an array of allocated vectors.  
* [0.x.30]*
   Overall number of allocations. Only used for bookkeeping and to generate   output at the end of an object's lifetime.  
* [0.x.31]*
   Number of vectors currently allocated in this object; used for detecting   memory leaks.  
* [0.x.32]*
   A flag controlling the logging of statistics by the destructor.  
* [0.x.33]*
   Mutex to synchronize access to internal data of this object from multiple   threads.  
* [0.x.34]

include/deal.II-translator/lac/vector_memory.templates_0.txt
[0.x.0]

include/deal.II-translator/lac/vector_operation_0.txt
[0.x.0]!  [2.x.0]  Vectors [2.x.1] 

* 
* [0.x.1]*
 This enum keeps track of the current operation in parallel linear algebra objects like Vectors and Matrices.
*  It is used in the various compress() functions. They also exist in serial codes for compatibility and are empty there.
*  See  [2.x.2]  "Compressing distributed objects" for more information.

* 
* [0.x.2]*
     The current operation is unknown.    
* [0.x.3]*
     The current operation is an insertion.    
* [0.x.4]*
     The current operation is an addition.    
* [0.x.5]*
     The current operation is a minimization.    
* [0.x.6]*
     The current operation is a maximization.    
* [0.x.7]

include/deal.II-translator/lac/vector_operations_internal_0.txt
[0.x.0]*
     This struct takes the loop range from the tbb parallel for loop and     translates it to the actual ranges of the for loop within the vector. It     encodes the grain size but might choose larger values of chunks than the     minimum grain size. The minimum grain size given to tbb is then simple     1. For affinity reasons, the layout in this loop must be kept in sync     with the respective class for reductions further down.    
* [0.x.1]*
     The minimum number of chunks (each of size 32) to divide the range     [first,last) into two (second part of the if branch in     accumulate_recursive).    
* [0.x.2]*
     This struct takes the loop range from the tbb parallel for loop and     translates it to the actual ranges of the reduction loop inside the     vector. It encodes the grain size but might choose larger values of     chunks than the minimum grain size. The minimum grain size given to tbb     is 1. For affinity reasons, the layout in this loop must be kept in sync     with the respective class for plain for loops further up.         Due to this construction, TBB usually only sees a loop of length     4*num_threads with grain size 1. The actual ranges inside the vector are     computed outside of TBB because otherwise TBB would split the ranges in     some unpredictable position which destroys exact bitwise     reproducibility. An important part of this is that inside      [2.x.0]  the recursive calls to accumulate are done     sequentially on one item a time (even though we could directly run it on     the whole range given through the  [2.x.1]  times the chunk size
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - but that would be unpredictable). Thus, the values we cannot control     are the positions in the array that gets filled
* 
*  - but up to that point     the algorithm TBB sees is just a parallel for and nothing unpredictable     can happen.         To sum up: Once the number of threads and the vector size are fixed, we     have an exact layout of how the calls into the recursive function will     happen. Inside the recursive function, we again only depend on the     length. Finally, the concurrent threads write into different positions in     a result vector in a thread-safe way and the addition in the short array     is again serial.    
* [0.x.3]*
       An operator used by TBB to work on a given  [2.x.2]  of chunks       [range.begin(), range.end()).      
* [0.x.4]*
     This is the general caller for parallel reduction operations that work in     parallel.    
* [0.x.5]

include/deal.II-translator/lac/vector_space_vector_0.txt
[0.x.0]!  [2.x.0]  Vectors   [2.x.1]   
* [0.x.1]*
   VectorSpaceVector is an abstract class that is used to define the   interface that vector classes need to implement when they want to   implement global operations. This class is complementary of   ReadWriteVector which allows the access of individual elements but does   not allow global operations.  
* [0.x.2]*
     Change the dimension to that of the vector V. The elements of V are not     copied.    
* [0.x.3]*
     Sets all elements of the vector to the scalar  [2.x.2]  This operation is     only allowed if  [2.x.3]  is equal to zero.    
* [0.x.4]*
     Multiply the entire vector by a fixed factor.    
* [0.x.5]*
     Divide the entire vector by a fixed factor.    
* [0.x.6]*
     Add the vector  [2.x.4]  to the present one.    
* [0.x.7]*
     Subtract the vector  [2.x.5]  from the present one.    
* [0.x.8]*
     Import all the elements present in the vector's IndexSet from the input     vector  [2.x.6]   [2.x.7]   [2.x.8]  is used to decide if     the elements in  [2.x.9]  should be added to the current vector or replace the     current elements. The last parameter can be used if the same     communication pattern is used multiple times. This can be used to improve     performance.    
* [0.x.9]*
     Return the scalar product of two vectors.    
* [0.x.10]*
     Add  [2.x.10]  to all components. Note that  [2.x.11]  is a scalar not a vector.    
* [0.x.11]*
     Simple addition of a multiple of a vector, i.e. <tt>*this += a*V</tt>.    
* [0.x.12]*
     Multiple addition of scaled vectors, i.e. <tt>*this += a*V+b*W</tt>.    
* [0.x.13]*
     Scaling and simple addition of a multiple of a vector, i.e. <tt>*this =     s*(*this)+a*V</tt>.    
* [0.x.14]*
     Scale each element of this vector by the corresponding element in the     argument. This function is mostly meant to simulate multiplication (and     immediate re-assignment) by a diagonal scaling matrix.    
* [0.x.15]*
     Assignment <tt>*this = a*V</tt>.    
* [0.x.16]*
     Return whether the vector contains only elements with value zero.    
* [0.x.17]*
     Return the mean value of all the entries of this vector.    
* [0.x.18]*
     Return the l<sub>1</sub> norm of the vector (i.e., the sum of the     absolute values of all entries among all processors).    
* [0.x.19]*
     Return the l<sub>2</sub> norm of the vector (i.e., the square root of     the sum of the square of all entries among all processors).    
* [0.x.20]*
     Return the maximum norm of the vector (i.e., the maximum absolute value     among all entries and among all processors).    
* [0.x.21]*
     Perform a combined operation of a vector addition and a subsequent     inner product, returning the value of the inner product. In other     words, the result of this function is the same as if the user called    
* [1.x.0]
*          The reason this function exists is that this operation involves less     memory transfer than calling the two functions separately. This method     only needs to load three vectors,  [2.x.12]   [2.x.13]   [2.x.14]  whereas calling     separate methods means to load the calling vector  [2.x.15]  twice. Since     most vector operations are memory transfer limited, this reduces the     time by 25\% (or 50\% if  [2.x.16]  equals  [2.x.17]          For complex-valued vectors, the scalar product in the second step is     implemented as      [2.x.18] .    
* [0.x.22]*
     This function does nothing and only exists for backward compatibility.    
* [0.x.23]*
     Return the global size of the vector, equal to the sum of the number of     locally owned indices among all processors.    
* [0.x.24]*
     Return an index set that describes which elements of this vector are     owned by the current processor. As a consequence, the index sets     returned on different processors if this is a distributed vector will     form disjoint sets that add up to the complete index set. Obviously, if     a vector is created on only one processor, then the result would     satisfy    
* [1.x.1]
*     
* [0.x.25]*
     Print the vector to the output stream  [2.x.19]     
* [0.x.26]*
     Return the memory consumption of this class in bytes.    
* [0.x.27]*
     Destructor. Declared as virtual so that inheriting classes (which may     manage their own memory) are destroyed correctly.    
* [0.x.28]*
   Shift all entries of the vector by a constant factor so that the mean   value of the vector becomes zero.  
* [0.x.29]

include/deal.II-translator/lac/vector.templates_0.txt
[0.x.0]

include/deal.II-translator/lac/vector_type_traits_0.txt
[0.x.0]*
 Type trait for a serial vector, i.e. a vector class for which storage is not supported to be distributed over processes.
*  The specialization

* 
* [1.x.0]
*  for a serial vector type, respectively,

* 
* [1.x.1]
*  for a vector type with support of distributed storage, must be done in a header file of a vector declaration.

* 
* [0.x.1]

