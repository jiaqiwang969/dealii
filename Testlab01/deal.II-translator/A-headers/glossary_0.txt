[0.x.0]*
  [2.x.0] 
*  This glossary explains a few terms that are frequently used in the documentation of classes of deal.II. The glossary often only gives a microscopic view of a particular concept; if you struggle with the bigger picture, it may therefore also be worth to consult the global overview of classes on the  [2.x.1]  page.
*   [2.x.2] 
*  <dt class="glossary"> [2.x.3]  GlossActive [1.x.0]</dt>  [2.x.4] A cell, face, or edge is defined as [1.x.1] if it is not refined any further, i.e., if it does not have children. Once a cell, face, or edge becomes a parent it is no longer active. Unless working with a multigrid algorithm, active cells are the only ones carrying degrees of freedom.  [2.x.5] 
* 

* 
*  <dt class="glossary"> [2.x.6]  GlossArtificialCell [1.x.2]</dt>  [2.x.7]  If a mesh is distributed across multiple MPI processes using the  [2.x.8]  class, each processor stores only the cells it owns, one layer of adjacent cells that are owned by other processors (called  [2.x.9]  "ghost cells"), all coarse level cells, and all cells that are necessary to maintain the invariant that adjacent cells must differ by at most one refinement level. The cells stored on each process that are not owned by this process and that are not ghost cells are called "artificial cells", and for these cells the predicate  [2.x.10]  returns true. Artificial cells are guaranteed to exist in the globally distributed mesh but they may be further refined on other processors. See the  [2.x.11]  "Distributed Computing paper" for more information.
*  The concept of artificial cells has no meaning for triangulations that store the entire mesh on each processor, i.e. the  [2.x.12]  class.  [2.x.13] 
* 

*  <dt class="glossary"> [2.x.14]  GlossBlockLA [1.x.3]</dt>
*   [2.x.15] It is often convenient to treat a matrix or vector as a collection of individual blocks. For example, in  [2.x.16]  (and other tutorial programs), we want to consider the global linear system  [2.x.17]  in the form [1.x.4]
*  where  [2.x.18]  are the values of velocity and pressure degrees of freedom, respectively,  [2.x.19]  is the mass matrix on the velocity space,  [2.x.20]  corresponds to the negative divergence operator, and  [2.x.21]  is its transpose and corresponds to the negative gradient.
*  Using such a decomposition into blocks, one can then define preconditioners that are based on the individual operators that are present in a system of equations (for example the Schur complement, in the case of  [2.x.22] ), rather than the entire matrix. In essence, blocks are used to reflect the structure of a PDE system in linear algebra, in particular allowing for modular solvers for problems with multiple solution components. On the other hand, the matrix and right hand side vector can also treated as a unit, which is convenient for example during assembly of the linear system when one may not want to make a distinction between the individual components, or for an outer Krylov space solver that doesn't care about the block structure (e.g. if only the preconditioner needs the block structure).
*  Splitting matrices and vectors into blocks is supported by the BlockSparseMatrix, BlockVector, and related classes. See the overview of the various linear algebra classes in the  [2.x.23]  module. The objects present two interfaces: one that makes the object look like a matrix or vector with global indexing operations, and one that makes the object look like a collection of sub-blocks that can be individually addressed. Depending on context, one may wish to use one or the other interface.
*  Typically, one defines the sub-structure of a matrix or vector by grouping the degrees of freedom that make up groups of physical quantities (for example all velocities) into individual blocks of the linear system. This is defined in more detail below in the glossary entry on  [2.x.24]  "Block (finite element)".  [2.x.25] 
* 

*  <dt class="glossary"> [2.x.26]  GlossBlock [1.x.5]</dt>  [2.x.27]  [1.x.6] Blocks are a generalization of  [2.x.28]  "components" in that they group together one or more components of a vector-valued finite element that one would like to consider jointly. One often wants to do this to define operators that correspond to the structure of a (part of a) differential operator acting on the vector-valued solution, such as the Schur complement solver in  [2.x.29] , or the block solvers and preconditioners of  [2.x.30] .
*  For the purpose of a discretization, blocks are the better concept to use since it is not always possible to address individual components of a solution. This is, in particular, the case for non- [2.x.31]  GlossPrimitive "primitive" elements. Take for instance the solution of the mixed Laplacian system with the FE_RaviartThomas element (see  [2.x.32] ). There, the first <tt>dim</tt> components are the directional velocities. Since the shape functions are linear combinations of those, these <tt>dim</tt> components constitute only a single block. On the other hand, the pressure variable is scalar and would form a the second block, but in the <tt>dim+1</tt>st component.
*  The minimal size of each block is dictated by the underlying finite element (a block consists of a single component for scalar elements, but in the case of the FE_RaviartThomas, for example, a block consists of <tt>dim</tt> components). However, several such minimal blocks can be grouped together into user defined blocks at will, and in accordance with the application. For instance, for the [1.x.7]<sub>2</sub><sup>[1.x.8]</sup>-[1.x.9]<sub>1</sub> (Taylor-Hood) Stokes element, there are [1.x.10]+1 components each of which could in principle form its own block. But we are typically more interested in having only two blocks, one of which consists of all the velocity vector components (i.e. this block would have [1.x.11] components) and the other having only the single pressure component.
*  [1.x.12] deal.II has a number of different finite element classes, all of which are derived from the FiniteElement base class (see the  [2.x.33]  "module on finite element classes"). With one exception, whether they are scalar or vector valued, they all define a single block: all vector components the finite element defines through its  [2.x.34]  function form a single block, i.e.  [2.x.35]  returns one.
*  The exception is the FESystem class that takes multiple simpler elements and connects them into more complicated ones. Consequently, it can have more than one block. A FESystem has as many blocks as it has base elements times their multiplicity (see the constructors of FESystem to understand this statement). In other words, it does not care how many blocks each base element has, and consequently you can produce a Stokes element that has only two blocks by creating the object

* 
* [1.x.13]
*  On the other hand, we could have produced a similar object with dim+1 blocks using

* 
* [1.x.14]
*  With the exception of the number of blocks, the two objects are the same for all practical purposes, however.
*  [1.x.15] While we have defined blocks above in terms of the vector components of a vector-valued solution function (or, equivalently, in terms of the vector-valued finite element space), every shape function of a finite element is part of one block or another. Consequently, we can partition all degrees of freedom defined on a DoFHandler into individual blocks. Since by default the DoFHandler class enumerates degrees of freedom in a more or less random way, you will first want to call the  [2.x.36]  function to make sure that all degrees of freedom that correspond to a single block are enumerated consecutively.
*  If you do this, you naturally partition matrices and vectors into blocks as well (see  [2.x.37]  "block (linear algebra)).  In most cases, when you subdivide a matrix or vector into blocks, you do so by creating one block for each block defined by the finite element (i.e. in most practical cases the FESystem object). However, this needs not be so: the  [2.x.38]  function allows to group several vector components or finite element blocks into the same logical block (see, for example, the  [2.x.39]  " [2.x.40] " or  [2.x.41]  tutorial programs, as opposed to  [2.x.42] ). As a consequence, using this feature, we can achieve the same result, i.e. subdividing matrices into  [2.x.43]  blocks and vectors into 2 blocks, for the second way of creating a Stokes element outlined above using an extra argument as we would have using the first way of creating the Stokes element with two blocks right away.
*  More information on this topic can be found in the documentation of FESystem, the  [2.x.44]  module and the tutorial programs referenced therein.
*  [1.x.16] Many functions allow you to restrict their operation to certain vector components or blocks. For example, this is the case for the functions that interpolate boundary values: one may want to only interpolate the boundary values for the velocity block of a finite element field but not the pressure block. The way to do this is by passing a BlockMask argument to such functions, see the  [2.x.45]  "block mask entry of this glossary".  [2.x.46] 
* 

*  <dt class="glossary"> [2.x.47]  GlossBlockMask [1.x.17]</dt>
*   [2.x.48]  In much the same way as one can think of elements as being composed of physical vector components (see  [2.x.49] ) or logical blocks (see  [2.x.50] ), there is frequently a need to select a set of such blocks for operations that are not intended to be run on [1.x.18] blocks of a finite element space. Selecting which blocks to work on happens using the BlockMask class.
*  Block masks work in much the same way as component masks, including the fact that the BlockMask class has similar semantics to the ComponentMask class. See  [2.x.51]  "the glossary entry on component masks" for more information.
* 

* 
*  [2.x.52]  While components and blocks provide two alternate but equally valid viewpoints on finite elements with multiple vector components, the fact is that throughout the library there are far more places where you can pass a ComponentMask argument rather than a BlockMask argument. Fortunately, one can be converted into the other, using the syntax  [2.x.53]  is a variable of type BlockMask. In other words, if you have a block mask but need to call a function that only accepts a component mask, this syntax can be used to obtain the necessary component mask.
*  [1.x.19] Block masks are typically created by asking the finite element to generate a block mask from certain selected vector components using code such as this where we create a mask that only denotes the velocity components of a Stokes element (see  [2.x.54] ):

* 
* [1.x.20]
*  The result is a block mask that, in 1d as well as 2d and 3d, would have values  [2.x.55] . Similarly, using

* 
* [1.x.21]
*  would result in a mask  [2.x.56]  in any dimension.
*  Note, however, that if we had defined the finite element in the following way:

* 
* [1.x.22]
*  then the code

* 
* [1.x.23]
*  would yield a block mask that in 2d has elements  [2.x.57]  because the element has  [2.x.58]  components and equally many blocks. See the discussion on what a block represents exactly in the  [2.x.59]  "block entry of this glossary".  [2.x.60] 
* 

*  <dt class="glossary"> [2.x.61]  GlossBoundaryForm [1.x.24]</dt>
*   [2.x.62] For a dim-dimensional triangulation in dim-dimensional space, the boundary form is a vector defined on faces. It is the vector product of the image of coordinate vectors on the surface of the unit cell. It is a vector normal to the surface, pointing outwards and having the length of the surface element.
*  A more general definition would be that (at least up to the length of this vector) it is exactly that vector that is necessary when considering integration by parts, i.e. equalities of the form  [2.x.63] . Using this definition then also explains what this vector should be in the case of domains (and corresponding triangulations) of dimension  [2.x.64]  that are embedded in a space  [2.x.65] : in that case, the boundary form is still a vector defined on the faces of the triangulation; it is orthogonal to all tangent directions of the boundary and within the tangent plane of the domain. Note that this is compatible with case  [2.x.66]  since there the tangent plane is the entire space  [2.x.67] .
*  In either case, the length of the vector equals the determinant of the transformation of reference face to the face of the current cell.  [2.x.68] 
* 

*  <dt class="glossary"> [2.x.69]  GlossBoundaryIndicator [1.x.25]</dt>
*   [2.x.70]  In a Triangulation object, every part of the boundary may be associated with a unique number (of type  [2.x.71]  that is used to determine what kinds of boundary conditions are to be applied to a particular part of a boundary. The boundary is composed of the faces of the cells and, in 3d, the edges of these faces.
*  By default, all boundary indicators of a mesh are zero, unless you are reading from a mesh file that specifically sets them to something different, or unless you use one of the mesh generation functions in namespace GridGenerator that have a  [2.x.72]  "colorize" option. A typical piece of code that sets the boundary indicator on part of the boundary to something else would look like this, here setting the boundary indicator to 42 for all faces located at  [2.x.73] :

* 
* [1.x.26]
*  This calls functions  [2.x.74]  In 3d, it may also be appropriate to call  [2.x.75]  instead on each of the selected faces. To query the boundary indicator of a particular face or edge, use  [2.x.76] 
*  Many of the functions in namespaces DoFTools and VectorTools take arguments that specify which part of the boundary to work on, and they specifically refer to boundary_ids. Examples are  [2.x.77]   [2.x.78]   [2.x.79]  and  [2.x.80]   [2.x.81] 
* 

* 
*  [2.x.82]  Boundary indicators are inherited from mother faces and edges to their children upon mesh refinement. Some more information about boundary indicators is also presented in a section of the documentation of the Triangulation class.
* 

* 
*  [2.x.83]  For parallel triangulations of type  [2.x.84]  it is not enough to set boundary indicators only once at the beginning. See the long discussion on this topic in the class documentation of  [2.x.85]  .  [2.x.86] 
* 

*  <dt class="glossary"> [2.x.87]  GlossCoarseMesh [1.x.27]</dt>  [2.x.88]    A "coarse mesh" in deal.II is a triangulation object that consists only   of cells that are not refined, i.e., a mesh in which no cell is a child   of another cell. This is generally how triangulations are first   constructed in deal.II, for example using (most of) the functions in   namespace GridGenerator, the functions in class GridIn, or directly   using the function  [2.x.89]  One can of   course do computations on such meshes, but most of the time (see, for   example, almost any of the tutorial programs) one first refines the   coarse mesh globally (using  [2.x.90]    or adaptively (in that case first computing a refinement   criterion, then one of the functions in namespace GridRefinement,   and finally calling    [2.x.91]  The mesh is   then no longer a "coarse mesh", but a "refined mesh".
*    In some contexts, we also use the phrase "the coarse mesh of a   triangulation", and by that mean that set of cells that the triangulation   started out with, i.e., from which all the currently    [2.x.92]  "active cells" of the triangulation have been obtained   by mesh refinement. (Some of the coarse mesh cells may of course also   be active if they have never been refined.)
*    Triangulation objects store cells in [1.x.28]: in   particular, all cells of a coarse mesh are on level zero. Their   children (if we executed  [2.x.93]  on a   coarse mesh) would then be at level one, etc. The coarse mesh of a   triangulation (in the sense of the previous paragraph) then   consists of exactly the level-zero cells of a triangulation. (Whether   they are active (i.e., have no children) or have been refined is not   important for this definition.)
*    Most of the triangulation classes in deal.II store the entire coarse   mesh along with at least some of the refined cells. (Both the    [2.x.94]  and  [2.x.95]  classes   actually store [1.x.29] cells of the entire mesh, whereas some   other classes such as  [2.x.96]  only   store [1.x.30] of the  [2.x.97]  "active cells" on   each process in a parallel computation.) In those cases,   one can query the triangulation for all coarse mesh   cells. Other triangulation classes (e.g.,    [2.x.98]  only store a part   of the coarse mesh. See also    [2.x.99]  "the concept of coarse cell ids"   for that case.  [2.x.100] 
* 

*  <dt class="glossary"> [2.x.101]  GlossCoarseCellId [1.x.31]</dt>  [2.x.102]    Most of the triangulation classes in deal.II, notably    [2.x.103]   [2.x.104]  and    [2.x.105]  store the entire    [2.x.106]  "coarse mesh"   of a triangulation on each process of a parallel computation. On the   other hand, this is not the case for other classes, notably for    [2.x.107]  which is designed for cases   where even the coarse mesh is too large to be stored on each process   and needs to be partitioned.
*    In those cases, it is often necessary in algorithms to reference a coarse   mesh cell uniquely. Because the triangulation object on the current   process does not actually store the entire coarse mesh, one needs to have   a globally unique identifier for each coarse mesh cell that is independent   of the index within level zero of the triangulation stored locally. This   globally unique ID is called the "coarse cell ID". It can be accessed via   the function call  
* [1.x.32]
*    where `triangulation` is the triangulation to which the iterator   `coarse_cell` pointing to a cell at level zero belongs. Here,   `coarse_cell->index()` returns the index of that cell within its   refinement level (see  [2.x.108]  This is a number   between zero and the number of coarse mesh cells stored on the   current process in a parallel computation; it uniquely identifies   a cell on that parallel process, but different parallel processes may   use that index for different cells located at different coordinates.
*    For those classes that store all coarse mesh cells on each process,   the  [2.x.109]  simply   returns a permutation of the possible argument values. In the   simplest cases, such as for a sequential or a parallel shared   triangulation, the function will in fact simply return the   value of the argument. For others, such as    [2.x.110]  the ordering of   coarse cell IDs is not the same as the ordering of coarse   cell indices. Finally, for classes such as    [2.x.111]  the function returns   the globally unique ID, which is from a larger set of possible   indices than the indices of the coarse cells actually stored on   the current process.  [2.x.112] 
* 

*  <dt class="glossary"> [2.x.113]  GlossColorization [1.x.33]</dt>  [2.x.114]  [2.x.115] Colorization [2.x.116]  is the process of marking certain parts of a Triangulation with different labels. The use of the word  [2.x.117] color [2.x.118]  comes from cartography, where countries on a map are made visually distinct from each other by assigning them different colors. Using the same term  [2.x.119] coloring [2.x.120]  is common in mathematics, even though we assign integers and not hues to different regions. deal.II refers to two processes as coloring:
*   [2.x.121]     [2.x.122]  Most of the functions in the GridGenerator namespace take an optional   argument  [2.x.123] . This argument controls whether or not the   different parts of the boundary will be assigned different    [2.x.124]  "boundary indicators".   Some functions also assign different    [2.x.125]  "material indicators"   as well. [2.x.126]     [2.x.127]  The function  [2.x.128]  computes a   decomposition of a Triangulation (more exactly, a range of iterators). No   two adjacent cells are given the same color. [2.x.129]   [2.x.130]   [2.x.131] 
* 

*  <dt class="glossary"> [2.x.132]  GlossComponent [1.x.34]</dt>
*   [2.x.133]  When considering systems of equations in which the solution is not just a single scalar function, we say that we have a [1.x.35] with a [1.x.36]. For example, the vector solution in the elasticity equation considered in  [2.x.134]  is  [2.x.135]  consisting of the displacements in each of the three coordinate directions. The solution then has three elements. Similarly, the 3d Stokes equation considered in  [2.x.136]  has four elements:  [2.x.137] . We call the elements of the vector-valued solution [1.x.37] in deal.II. To be well-posed, for the solution to have  [2.x.138]  components, there need to be  [2.x.139]  partial differential equations to describe them. This concept is discussed in great detail in the  [2.x.140]  module.
*  In finite element programs, one frequently wants to address individual elements (components) of this vector-valued solution, or sets of components. For example, we do this extensively in  [2.x.141] , and a lot of documentation is also provided in the module on  [2.x.142]  "Handling vector valued problems". If you are thinking only in terms of the partial differential equation (not in terms of its discretization), then the concept of [1.x.38] is the natural one.
*  On the other hand, when talking about finite elements and degrees of freedom, [1.x.39] are not always the correct concept because components are not always individually addressable. In particular, this is the case for  [2.x.143]  "non-primitive finite elements". Similarly, one may not always [1.x.40] to address individual components but rather sets of components &mdash; e.g. all velocity components together, and separate from the pressure in the Stokes system, without further splitting the velocities into their individual components. In either case, the correct concept to think in is that of a  [2.x.144]  "block".  Since each component, if individually addressable, is also a block, thinking in terms of blocks is most frequently the better strategy.
*  For a given finite element, the number of components can be queried using the  [2.x.145]  function, and you can find out which vector components are nonzero for a given finite element shape function using  [2.x.146]  The values and gradients of individual components of a shape function (if the element is primitive) can be queried using the  [2.x.147]  and  [2.x.148]  functions on the reference cell. The  [2.x.149]  and  [2.x.150]  functions do the same on a real cell. See also the documentation of the FiniteElement and FEValues classes.
*  [1.x.41] Many functions allow you to restrict their operation to certain vector components or blocks. For example, this is the case for the functions that interpolate boundary values: one may want to only interpolate the boundary values for the velocity components of a finite element field but not the pressure component. The way to do this is by passing a ComponentMask argument to such functions, see the  [2.x.151]  "component mask entry of this glossary".  [2.x.152] 
* 

*  <dt class="glossary"> [2.x.153]  GlossComponentMask [1.x.42]</dt>
*   [2.x.154]  When using vector-valued elements (see  [2.x.155] ) to solve systems of equations, one frequently wants to restrict some operations to only certain solution variables. For example, when solving the Stokes equations, one may wish to only interpolate boundary values for the velocity components but not the pressure. In deal.II, this is typically done by passing functions a [1.x.43]. Component masks are always specified as a ComponentMask object which one can think of as an array with as many entries as the finite element has components (e.g., in the Stokes case, there are  [2.x.156]  components) and where each entry is either true or false. In the example where we would like to interpolate boundary values only for the velocity components of the Stokes system, this component mask would then be  [2.x.157]  in 3d to indicate that no boundary values shall be set for the pressure variable (the last of the  [2.x.158]  vector components of the solution.
*  There are many functions that take such component masks, for example  [2.x.159]   [2.x.160]   [2.x.161]  etc. In some cases, there are multiple functions with these names but only some of them have a component mask argument.
*  [1.x.44] Many of the functions that take a component mask object that has been default constructed to indicate [1.x.45], i.e., as if the vector had the correct length and was filled with only  [2.x.162]  values. The reason is that default initialized objects can be constructed in place using the code snippet  [2.x.163]  and can thus be used as a default argument in function signatures.
*  In other words, ComponentMask objects can be in one of two states: They can have been initialized by a vector of booleans with a nonzero length; in that case, they represent a mask of a particular length where some elements may be true and others may be false. Or, the ComponentMask may have been default initialized (using the default constructor) in which case it represents an array of indefinite length (i.e., a length appropriate to the circumstances) in which [1.x.46] is true.
*  [1.x.47] Component masks are typically created by asking the finite element to generate a component mask from certain selected components using code such as this where we create a mask that only denotes the velocity components of a Stokes element (see  [2.x.164] ):

* 
* [1.x.48]
*  The result is a component mask that, in 2d, would have values  [2.x.165] . Similarly, using

* 
* [1.x.49]
*  would result in a mask  [2.x.166]  in 2d. Of course, in 3d, the result would be  [2.x.167] .
* 

* 
*  [2.x.168]  Just as one can think of composed elements as being made up of  [2.x.169]  "components" or  [2.x.170]  "blocks", there are component masks (represented by the ComponentMask class) and  [2.x.171]  "block masks" (represented by the BlockMask class). The FiniteElement class has functions that convert between the two kinds of objects.
* 

* 
*  [2.x.172]  Not all component masks actually make sense. For example, if you have a FE_RaviartThomas object in 2d, then it doesn't make any sense to have a component mask of the form  [2.x.173]  because you try to select individual vector components of a finite element where each shape function has both  [2.x.174]  and  [2.x.175]  velocities. In essence, while you can of course create such a component mask, there is nothing you can do with it.  [2.x.176] 
* 

* 
*  <dt class="glossary"> [2.x.177]  GlossCompress [1.x.50]</dt>
*   [2.x.178]  For %parallel computations, deal.II uses the vector and matrix classes defined in the PETScWrappers and TrilinosWrappers namespaces. When running programs in %parallel using MPI, these classes only store a certain number of rows or elements on the current processor, whereas the rest of the vector or matrix is stored on the other processors that belong to our MPI universe. This presents a certain problem when you assemble linear systems: we add elements to the matrix and right hand side vectors that may or may not be stored locally. Sometimes, we may also want to just [1.x.51] an element, not add to it.
*  Both PETSc and Trilinos allow adding to or setting elements that are not locally stored. In that case, they write the value that we want to store or add into a cache, and we need to call one of the functions  [2.x.179]   [2.x.180]   [2.x.181]  or  [2.x.182]  which will then ship the values in the cache to the MPI process that owns the element to which it is supposed to be added or written to. Due to the MPI model that only allows to initiate communication from the sender side (i.e. in particular, it is not a remote procedure call), these functions are collective, i.e. they need to be called by all processors.
*  There is one snag, however: both PETSc and Trilinos need to know whether the operation that these  [2.x.183]  functions invoke applies to adding elements or setting them.  In some cases, not all processors may be adding elements, for example if a processor does not own any cells when using a very  [2.x.184]  "coarse (initial) mesh". For this reason, compress() takes an argument of type VectorOperation, which can be either ::%add, or ::%insert. This argument is required for vectors and matrices starting with the 7.3 release.
*  In short, you need to call compress() in the following cases (and only in those cases, though calling compress() in other cases just costs some performance):
*  1. At the end of your assembly loop on matrices and vectors. This needs to be done if you write entries directly or if you use  [2.x.185]  Use  [2.x.186] 
*  2. When you are done setting individual elements in a matrix/vector before any other operations are done (adding to elements, other operations like scaling, solving, reading, etc.). Use  [2.x.187] 
*  3. Like in 2., but for adding values to individual elements. Use  [2.x.188] 
*  All other operations like scaling or adding vectors, assignments, calls into deal.II (VectorTools, AffineConstraints, ...) or solvers do not require calls to compress().  [2.x.189] 
* 

* 
*  [2.x.190]  Compressing is an operation that only applies to vectors whose elements are uniquely owned by one and only one processor in a parallel MPI universe. It does not apply to  [2.x.191]  "vectors with ghost elements".
* 

*  <dt class="glossary"> [2.x.192]  GlossConcept [1.x.52]</dt>
*   [2.x.193]  There are several places in deal.II where we require that a type in a template match a certain interface or behave in a certain way: such constraints are called  [2.x.194] concepts [2.x.195]  in C++. See the discussion in  [2.x.196]  for more information and a list of concepts in deal.II.  [2.x.197] 
* 

*  <dt class="glossary"> [2.x.198]  GlossDimension [1.x.53]</dt>
*   [2.x.199]  Many classes and functions in deal.II have two template parameters,  [2.x.200]  and  [2.x.201]  An example is the basic Triangulation class:

* 
* [1.x.54]
*  In all of these contexts where you see `dim` and `spacedim` referenced, these arguments have the following meaning:
*   [2.x.202]     [2.x.203]   [2.x.204]  denotes the dimensionality of the mesh. For example, a mesh   that consists of line segments is one-dimensional and consequently   corresponds to `dim==1`. A mesh consisting of quadrilaterals then has   `dim==2` and a mesh of hexahedra has `dim==3`. [2.x.205] 
*     [2.x.206]   [2.x.207]  denotes the dimensionality of the space in which such a   mesh lives. Generally, one-dimensional meshes live in a one-dimensional   space, and similarly for two-dimensional and three-dimensional meshes   that subdivide two- and three-dimensional domains. Consequently, the  [2.x.208]    spacedim template argument has a default equal to  [2.x.209]  But this need   not be the case: For example, we may want to solve an equation for   sediment transport on the surface of the Earth. In this case, the domain   is the two-dimensional surface of the Earth (`dim==2`) that lives in a   three-dimensional coordinate system (`spacedim==3`). [2.x.210]   [2.x.211] 
*  More generally, deal.II can be used to solve partial differential equations on [1.x.55] that are embedded in higher dimensional space. In other words, these two template arguments need to satisfy `dim <= spacedim`, though in many applications one simply has `dim == spacedim`.
*  Following the convention in geometry, we say that the "codimension" is defined as `spacedim-dim`. In other words, a triangulation consisting of quadrilaterals whose coordinates are three-dimensional (for which we would then use a `Triangulation<2,3>` object) has "codimension one".
*  Examples of uses where these two arguments are not the same are shown in  [2.x.212] ,  [2.x.213] ,  [2.x.214] .  [2.x.215] 
* 

*  <dt class="glossary"> [2.x.216]  GlossDoF [1.x.56]</dt>
*   [2.x.217]  The term "degree of freedom" (often abbreviated as "DoF") is commonly used in the finite element community to indicate two slightly different, but related things. The first is that we'd like to represent the finite element solution as a linear combination of shape functions, in the form  [2.x.218] . Here,  [2.x.219]  is a vector of expansion coefficients. Because we don't know their values yet (we will compute them as the solution of a linear or nonlinear system), they are called "unknowns" or "degrees of freedom". The second meaning of the term can be explained as follows: A mathematical description of finite element problem is often to say that we are looking for a finite dimensional function  [2.x.220]  that satisfies some set of equations (e.g.  [2.x.221]  for all test functions  [2.x.222] ). In other words, all we say here that the solution needs to lie in some space  [2.x.223] . However, to actually solve this problem on a computer we need to choose a basis of this space; this is the set of shape functions  [2.x.224]  we have used above in the expansion of  [2.x.225]  with coefficients  [2.x.226] . There are of course many bases of the space  [2.x.227] , but we will specifically choose the one that is described by the finite element functions that are traditionally defined locally on the cells of the mesh. Describing "degrees of freedom" in this context requires us to simply [1.x.57] the basis functions of the space  [2.x.228] . For  [2.x.229]  elements this means simply enumerating the vertices of the mesh in some way, but for higher elements one also has to enumerate the shape functions that are associated with edges, faces, or cell interiors of the mesh. The class that provides this enumeration of the basis functions of  [2.x.230]  is called DoFHandler.  The process of enumerating degrees of freedom is referred to as "distributing DoFs" in deal.II.  [2.x.231] 
*  <dt class="glossary"> [2.x.232]  GlossDirectionFlag [1.x.58]</dt>
*   [2.x.233] The [1.x.59] is used in triangulations embedded in a higher dimensional space to denote the orientation of cells and make the manifold oriented. It is accessed using  [2.x.234]  and set by the Triangulation class upon creation of a triangulation. You can change all direction flags of a triangulation using the  [2.x.235]  function.
*  The flag is necessary to make cases like this work: assume we have a one-dimensional mesh embedded in a two-dimensional space,
*     [2.x.236] 
*  In one dimensional meshes in one dimensional space, we can always make sure that the location of the left vertex of a cell has a smaller value than the location of the right vertex. However, if we embed a mesh in a higher dimensional space, we can no longer do this. For example, the cells in the mesh above may be described by the following vertex sets: <code>(0,1), (1,2), (3,2), (4,3), (4,5)</code>. (As a side remark, note that here we have vertices
* 
*  -  e.g. vertex 2
* 
*  -  that are the right end points of more than one cell.) If we define the normal to each cell as that unit vector that is right perpendicular to the vector that connects the first to the second vertex of the line, then we would end up with the following picture:
*     [2.x.237] 
*  In other words, this one-dimensional manifold is not oriented. We could in principle revert the order of vertices when creating such a mesh (though there are good reasons not to do so, for example because this mesh may have resulted from extracting the surface mesh of a two dimensional mesh, and we want to preserve the order of vertices of each line segment because they currently match the order of vertices of the faces of the 2d cells). An alternative strategy, chosen in deal.II, is to simply associate with each cell whether the normal should be the left or right normal to the cell. (The default is right normals.) In the example above, the flags for the five cells would be <code>true, true, false, false, true</code>. Multiplying the right normal with plus or minus one, depending on the value of the flag on each cell, yields a set of normal vectors that orient the manifold.
*  Similar issues happen with two-dimensional meshes in three space dimensions. We note that it would not be possible to find consistent direction flags if the two-dimensional manifold is not orientable; such manifolds are not currently supported by deal.II.  [2.x.238] 
* 

*  <dt class="glossary"> [2.x.239]  GlossDistorted [1.x.60]</dt>
*   [2.x.240] A [1.x.61] is a cell for which the mapping from the reference cell to real cell has a Jacobian whose determinant is non-positive somewhere in the cell. Typically, we only check the sign of this determinant at the vertices of the cell. The function  [2.x.241]  computes these determinants at the vertices.
*  By way of example, if all of the determinants are of roughly equal value and on the order of  [2.x.242]  then the cell is well-shaped. For example, a square cell or face has determinants equal to  [2.x.243]  whereas a strongly sheared parallelogram has a determinant much smaller. Similarly, a cell with very unequal edge lengths will have widely varying determinants. Conversely, a pinched cell in which the location of two or more vertices is collapsed to a single point has a zero determinant at this location. Finally, an inverted or twisted cell in which the location of two vertices is out of order will have negative determinants.
*  The following two images show a well-formed, a pinched, and a twisted cell for both 2d and 3d:
*   [2.x.244] 
*   [2.x.245] 
*  Distorted cells can appear in two different ways: The original  [2.x.246]  "coarse mesh" can already contain such cells, or they can be created as the result of moving or distorting a mesh by a relatively large amount.
*  If the appropriate flag is given upon creation of a triangulation, the function  [2.x.247]  which is called by the various functions in GridGenerator and GridIn (but can also be called from user code, see  [2.x.248]  and the example at the end of  [2.x.249] ), will signal the creation of coarse meshes with distorted cells by throwing an exception of type  [2.x.250]  There are legitimate cases for creating meshes with distorted cells (in particular collapsed/pinched cells) if you don't intend to assemble anything on these cells. For example, consider a case where one would like to simulate the behavior of an elastic material with a fluid-filled crack such as an oil reservoir. If the pressure becomes too large, the crack is closed
* 
*  -  and the cells that discretize the crack volume are collapsed to zero volume. As long as you don't integrate over these cells to simulate the behavior of the fluid (of which there isn't any if the crack has zero volume), such meshes are perfectly legitimate. As a consequence,  [2.x.251]  does not simply abort the program, but throws an exception that contains a list of cells that are distorted; this exception can be caught and, if you believe that you can ignore this condition, you can react by doing nothing with the caught exception.
*  The function  [2.x.252]  can, in some cases, fix distorted cells on refined meshes by moving around the vertices of a distorted child cell that has an undistorted parent.
*  Note that the Triangulation class does not test for the presence of distorted cells by default, since the determination whether a cell is distorted or not is not a cheap operation. If you want a Triangulation object to test for distortion of cells, you need to specify this upon creation of the object by passing the appropriate flag.  [2.x.253] 
* 

*  <dt class="glossary"> [2.x.254]  distributed_paper                           [1.x.62]</dt>
*   [2.x.255] The "distributed computing paper" is a paper by W. Bangerth, C. Burstedde, T. Heister and M. Kronbichler titled "Algorithms and Data Structures for Massively Parallel Generic Finite Element Codes" that describes the implementation of %parallel distributed computing in deal.II, i.e. computations where not only the linear system is split onto different machines as in, for example,  [2.x.256] , but also the Triangulation and DoFHandler objects. In essence, it is a guide to the  [2.x.257]  namespace and the techniques used in  [2.x.258] .
*  The full reference for the paper is as follows:

* 
* [1.x.63]
* 
*  For massively %parallel computations, deal.II builds on the [1.x.64] library. If you use this functionality, please also cite the p4est paper listed at their website.  [2.x.259] 
* 

*  <dt class="glossary"> [2.x.260]  GlossFaceOrientation [1.x.65]</dt>  [2.x.261] In a triangulation, the normal vector to a face can be deduced from the face orientation by applying the right hand side rule (x,y
* 
-> normal).  We note, that in the standard orientation of faces in 2d, faces 0 and 2 have normals that point into the cell, and faces 1 and 3 have normals pointing outward. In 3d, faces 0, 2, and 4 have normals that point into the cell, while the normals of faces 1, 3, and 5 point outward. This information, again, can be queried from  [2.x.262] 
*  However, it turns out that a significant number of 3d meshes cannot satisfy this convention. This is due to the fact that the face convention for one cell already implies something for the neighbor, since they share a common face and fixing it for the first cell also fixes the normal vectors of the opposite faces of both cells. It is easy to construct cases of loops of cells for which this leads to cases where we cannot find orientations for all faces that are consistent with this convention.
*  For this reason, above convention is only what we call the  [2.x.263] standard orientation [2.x.264] . deal.II actually allows faces in 3d to have either the standard direction, or its opposite, in which case the lines that make up a cell would have reverted orders, and the normal vector would have the opposite direction. You can ask a cell whether a given face has standard orientation by calling <tt>cell->face_orientation(face_no)</tt>: if the result is  [2.x.265]  then the face has standard orientation, otherwise its normal vector is pointing the other direction. There are not very many places in application programs where you need this information actually, but a few places in the library make use of this. Note that in 2d, the result is always  [2.x.266]  However, while every face in 2d is always in standard orientation, you can sometimes specify something to assume that this is not so; an example is the function  [2.x.267] 
*  There are two other flags that describe the orientation of a face: face_flip and face_rotation. Some documentation for these exists in the GeometryInfo class. An example of their use in user code is given in the  [2.x.268]  function.  [2.x.269] 
* 

*  <dt class="glossary"> [2.x.270]  GlossGeneralizedSupport [1.x.66]</dt>  [2.x.271] "Generalized support points" are, as the name suggests, a generalization of  [2.x.272]  "support points". The latter are used to describe that a finite element simply [1.x.67] values at individual points (the "support points"). If we call these points  [2.x.273]  (where the hat indicates that these points are defined on the reference cell  [2.x.274] ), then one typically defines shape functions  [2.x.275]  in such a way that the [1.x.68]  [2.x.276]  simply evaluate the function at the support point, i.e., that  [2.x.277] , and the basis is chosen so that  [2.x.278]  where  [2.x.279]  is the Kronecker delta function. This leads to the common  [2.x.280]  "Lagrange elements".
*  (In the vector valued case, the only other piece of information besides the support points  [2.x.281]  that one needs to provide is the [1.x.69]  [2.x.282]  the  [2.x.283] th node functional corresponds, so that  [2.x.284] .)
*  On the other hand, there are other kinds of elements that are not defined this way. For example, for the lowest order Raviart-Thomas element (see the FE_RaviartThomas class), the node functional evaluates not individual components of a vector-valued finite element function with  [2.x.285]  components, but the [1.x.70] of this vector:  [2.x.286] , where the  [2.x.287]  are the normal vectors to the face of the cell on which  [2.x.288]  is located. In other words, the node functional is a [1.x.71] of the components of  [2.x.289]  when evaluated at  [2.x.290] . Similar things happen for the BDM, ABF, and Nedelec elements (see the FE_BDM, FE_ABF, FE_Nedelec classes).
*  In these cases, the element does not have [1.x.72] because it is not purely interpolatory; however, some kind of interpolation is still involved when defining shape functions as the node functionals still require point evaluations at special points  [2.x.291] . In these cases, we call the points [1.x.73].
*  Finally, there are elements that still do not fit into this scheme. For example, some hierarchical basis functions (see, for example the FE_Q_Hierarchical element) are defined so that the node functionals are [1.x.74] of finite element functions,  [2.x.292]  in 2d, and similarly for 3d, where the  [2.x.293]  are the order of the moment described by shape function  [2.x.294] . Some other elements use moments over edges or faces. In all of these cases, node functionals are not defined through interpolation at all, and these elements then have neither support points, nor generalized support points.  [2.x.295] 
* 

*  <dt class="glossary"> [2.x.296]  geometry_paper [1.x.75]</dt>  [2.x.297] The "geometry paper" is a paper by L. Heltai, W. Bangerth, M. Kronbichler, and A. Mola, titled "Using exact geometry information in finite element computations", that describes how deal.II describes the geometry of domains. In particular, it discusses the algorithmic foundations on which the Manifold class is based, and what kind of information it needs to provide for mesh refinement, the computation of normal vectors, and the many other places where geometry enters into finite element computations.
*  The paper is currently available on arXiv at https://arxiv.org/abs/1910.09824 . The full reference for this paper is as follows:

* 
* [1.x.76]
*   [2.x.298] 
* 

*  <dt class="glossary"> [2.x.299]  GlossGhostCell [1.x.77]</dt>  [2.x.300]  If a mesh is distributed across multiple MPI processes using the  [2.x.301]  class, each processor stores only the cells it owns, one layer of adjacent cells that are owned by other processors, all  [2.x.302]  "coarse level cells", and all cells that are necessary to maintain the invariant that adjacent cells must differ by at most one refinement level. The cells stored on each process that are not owned by this process but that are adjacent to the ones owned by this process are called "ghost cells", and for these cells the predicate  [2.x.303]  returns true. Ghost cells are guaranteed to exist in the globally distributed mesh, i.e. these cells are actually owned by another process and are not further refined there. See the  [2.x.304]  "Distributed Computing paper" for more information.
*  The layer of ghost cells consists of all cells that are face, edge, or vertex neighbors of any locally owned cell and that are not locally owned themselves. In other word, the ghost cells completely enclose the subdomain of locally owned cells (with the exception of the boundary of the domain, of course).
*  The concept of ghost cells has no meaning for triangulations that store the entire mesh on each processor, i.e. the Triangulation and the  [2.x.305]  classes.  [2.x.306] 
* 

*  <dt class="glossary"> [2.x.307]  GlossGhostedVector [1.x.78]</dt>  [2.x.308]  In parallel computations, vectors come in two general kinds: without and with ghost elements. Vectors without ghost elements uniquely partition the vector elements between processors: each vector entry has exactly one processor that owns it, and this is the only processor that stores the value of this entry. In other words, if processor zero stores elements 0...49 of a vector and processor one stores elements 50...99, then processor one is out of luck accessing element 42 of this vector: it is not stored here and the value can not be assessed. This will result in an assertion.
*  On the other hand, there are many situations where one needs to know vector elements that aren't locally owned, for example to evaluate the solution on a locally owned cell (see  [2.x.309] ) for which one of the degrees of freedom is at an interface to a cell that we do not own locally (which, in this case must then be a  [2.x.310]  "ghost cell") and for which the neighboring cell may be the owner
* 
*  -  in other words, the degree of freedom is not a  [2.x.311]  "locally owned" but instead only a  [2.x.312]  "locally active DoFs". The values of such degrees of freedom are typically stored on the machine that owns the degree of freedom and, consequently, would not be accessible on the current machine.
*  Because one often needs these values anyway, there is a second kind of vector, often called "ghosted vector". Ghosted vectors store some elements on each processor for which that processor is not the owner. For such vectors, you can read those elements that the processor you are currently on stores but you cannot write into them because to make this work would require propagating the new value to all other processors that have a copy of this value (the list of such processors may be something which the current processor does not know and has no way of finding out efficiently). Since you cannot write into ghosted vectors, the only way to initialize such a vector is by assignment from a non-ghosted vector. This implies having to import those elements we locally want to store from other processors.
*  The way ghosted vectors are actually stored is different between the various implementations of parallel vectors. For PETSc (and the corresponding  [2.x.313]  class), ghosted vectors store the same elements as non-ghosted ones would, plus some additional elements that are owned by other processors. In other words, for each element there is a clear owner among all of the processors and those elements that the current processor stores but does not own (i.e., the "ghost elements") are simply mirror images of a primary value somewhere else
* 
*  -  thus, the name "ghost". This is also the case for the  [2.x.314]  class.
*  On the other hand, in Trilinos (and consequently in  [2.x.315]  a ghosted vector is simply a view of the parallel vector where the element distributions overlap. The 'ghosted' Trilinos vector in itself has no idea of which entries are ghosted and which are locally owned. In fact, a ghosted vector may not even store all of the elements a non-ghosted vector would store on the current processor. Consequently, for Trilinos vectors, there is no notion of an 'owner' of vector elements in the way we have it in the the non-ghost case view (or in the PETSc case) and the name "ghost element" may be misleading since in this view, every element we have available locally may or may not be stored somewhere else as well, but even if it is, the local element is not a mirror value of a primary location as there is no owner of each element.
* 

* 
*  [2.x.316]  The  [2.x.317]  documentation module provides a brief overview of where the different kinds of vectors are typically used.  [2.x.318] 
* 

*  <dt class="glossary"> [2.x.319]  hp_paper [1.x.79]</dt>  [2.x.320] The "hp-paper" is a paper by W. Bangerth and O. Kayser-Herold, titled "Data Structures and Requirements for hp Finite Element Software", that describes many of the algorithms and data structures used in the implementation of the hp-framework of deal.II. In particular, it summarizes many of the tricky points that have to be considered for %hp-finite elements using continuous elements.
*  The full reference for this paper is as follows:

* 
* [1.x.80]
*  It is available from [1.x.81], also see [1.x.82] for details.
*  The numerical examples shown in that paper are generated with a slightly modified version of  [2.x.321] . The main difference to that tutorial program is that various operations in the program were timed for the paper to compare different options and show that  [2.x.322]  methods are really not all that expensive.  [2.x.323] 
* 

*  <dt class="glossary"> [2.x.324]  GlossInterpolation [1.x.83]</dt>  [2.x.325] The purpose of interpolation with finite elements is computing a vector of coefficients representing a finite element function, such that the  [2.x.326]  "node values" of the original function and the finite element function coincide. Therefore, the interpolation process consists of evaluating all  [2.x.327]  "node functionals" [1.x.84] for the given function [1.x.85] and store the result as entry [1.x.86] in the coefficient vector.  [2.x.328] 
* 

*  <dt class="glossary"> [2.x.329]  GlossLagrange [1.x.87]</dt>  [2.x.330] Finite elements based on Lagrangian interpolation at  [2.x.331]  "support points".  [2.x.332] 
* 

*  <dt class="glossary"> [2.x.333]  GlossLocallyOwnedCell [1.x.88]</dt>  [2.x.334] This concept identifies a subset of all cells when using distributed meshes, see the  [2.x.335]  module. In such meshes, each cell is owned by exactly one processor. The locally owned ones are those owned by the current processor.
*  Each processor in a parallel computation has a triangulation covering the entire domain that consists of cells that are locally owned, of  [2.x.336]  "ghost cells" and of  [2.x.337]  "artificial cells".  [2.x.338] 
* 

*  <dt class="glossary"> [2.x.339]  GlossLocallyOwnedDof [1.x.89]</dt>  [2.x.340] This concept identifies a subset of all degrees of freedom when using distributed meshes, see the  [2.x.341]  module.  Locally owned degrees of freedom live on locally owned cells. Since degrees of freedom are owned by only one processor, degrees of freedom on interfaces between cells owned by different processors may be owned by one or the other, so not all degrees of freedom on a locally owned cell are also locally owned degrees of freedom.
*  Locally owned DoFs are a subset of the  [2.x.342]  "locally active DoFs".  [2.x.343] 
* 

*  <dt class="glossary"> [2.x.344]  GlossLocallyActiveDof [1.x.90]</dt>  [2.x.345] This concept identifies a subset of all degrees of freedom when using distributed meshes, see the  [2.x.346]  module.  Locally active degrees of freedom are those that live on locally owned cells. Degrees of freedom on interfaces between cells owned by different processors therefore belong to the set of locally active degrees of freedom for more than one processor.
*  Locally active DoFs are a superset of the  [2.x.347]  "locally owned DoFs" and a subset of the  [2.x.348]  "locally relevant DoFs".  [2.x.349] 
* 

*  <dt class="glossary"> [2.x.350]  GlossLocallyRelevantDof [1.x.91]</dt>  [2.x.351] This concept identifies a subset of all degrees of freedom when using distributed meshes, see the  [2.x.352]  module.  Locally relevant degrees of freedom are those that live on locally owned or ghost cells. Consequently, they may be owned by different processors.
*  Locally relevant DoFs are a superset of the  [2.x.353]  "locally active DoFs."  [2.x.354] 
* 

*  <dt class="glossary"> [2.x.355]  GlossManifoldIndicator [1.x.92]</dt>
*   [2.x.356]  Every object that makes up a Triangulation (cells, faces, edges, etc.), is associated with a unique number (of type  [2.x.357]  that is used to identify which manifold object is responsible to generate new points when the mesh is refined.
*  By default, all manifold indicators of a mesh are set to  [2.x.358]  A typical piece of code that sets the manifold indicator on a object to something else would look like this, here setting the manifold indicator to 42 for all cells whose center has an  [2.x.359]  component less than zero:
* 

* 
* [1.x.93]
* 
*  Here we call the function  [2.x.360]  It may also be appropriate to call  [2.x.361]  instead, to set recursively the manifold id on each face (and edge, if in 3d). To query the manifold indicator of a particular object edge, use  [2.x.362] 
*  The code above only sets the manifold indicators of a particular part of the Triangulation, but it does not by itself change the way the Triangulation class treats this object for the purposes of mesh refinement. For this, you need to call  [2.x.363]  to associate a manifold object with a particular manifold indicator. This allows the Triangulation objects to use a different method of finding new points on cells, faces or edges to be refined; the default is to use a FlatManifold object for all faces and edges.
* 

* 
*  [2.x.364]  Manifold indicators are inherited from parents to their children upon mesh refinement. Some more information about manifold indicators is also presented in a section of the documentation of the Triangulation class as well as in the  [2.x.365]  "Manifold documentation module". Manifold indicators are used in  [2.x.366]  and  [2.x.367] .  [2.x.368] 
*   [2.x.369]   [2.x.370]  "The module on Manifolds"
* 

*  <dt class="glossary"> [2.x.371]  GlossMaterialId [1.x.94]</dt>  [2.x.372] Each cell of a triangulation has associated with it a property called "material id". It is commonly used in problems with heterogeneous coefficients to identify which part of the domain a cell is in and, consequently, which value the coefficient should have on this particular cell. In practice, the material id of a cell is typically used to identify which cells belong to a particular part of the domain, e.g., when you have different materials (steel, concrete, wood) that are all part of the same domain. One would then usually query the material id associated with a cell during assembly of the bilinear form, and use it to determine (e.g., by table lookup, or a sequence of if-else statements) what the correct material coefficients would be for that cell.
*  This material_id may be set upon construction of a triangulation (through the CellData data structure), or later through use of cell iterators. For a typical use of this functionality, see the  [2.x.373]  tutorial program. The functions of the GridGenerator namespace typically set the material ID of all cells to zero. When reading a triangulation through the GridIn class, different input file formats have different conventions, but typically either explicitly specify the material id, or if they don't, then GridIn simply sets them to zero. Because the material of a cell is intended to pertain to a particular region of the domain, material ids are inherited by child cells from their parent upon mesh refinement.
*  The material id is set and queried using the  [2.x.374]   [2.x.375]  and  [2.x.376]  functions.  [2.x.377] 
* 

*  <dt class="glossary"> [2.x.378]  GlossMPICommunicator [1.x.95]</dt>  [2.x.379]  In the language of the Message Passing Interface (MPI), a communicator can be thought of as a mail system that allows sending messages to other members of the mail system. Within each communicator, each  [2.x.380]  "process" has a  [2.x.381]  "rank" (the equivalent of a house number) that allows to identify senders and receivers of messages. It is not possible to send messages via a communicator to receivers that are not part of this communicator/mail service.
*  When starting a parallel program via a command line call such as

* 
* [1.x.96]
*  (or the equivalent used in the batch submission system used on your cluster) the MPI system starts 32 copies of the  [2.x.382]  executable. Each of these has access to the  [2.x.383]  communicator that then consists of all 32 processors, each with its own rank. A subset of processes within this MPI universe can later agree to create other communicators that allow communication between only a subset of processes.  [2.x.384] 
* 

*  <dt class="glossary"> [2.x.385]  GlossMPIProcess [1.x.97]</dt>  [2.x.386]  When running parallel jobs on distributed memory machines, one almost always uses MPI. There, a command line call such as

* 
* [1.x.98]
*  (or the equivalent used in the batch submission system used on your cluster) starts 32 copies of the  [2.x.387]  executable. Some of these may actually run on the same machine, but in general they will be running on different machines that do not have direct access to each other's memory space.
*  In the language of the Message Passing Interface (MPI), each of these copies of the same executable running on (possibly different) machines are called [1.x.99]. The collection of all processes running in parallel is called the "MPI Universe" and is identified by the  [2.x.388]  "MPI communicator"  [2.x.389] .
*  Each process has immediate access only to the objects in its own memory space. A process can not read from or write into the memory of other processes. As a consequence, the only way by which processes can communicate is by sending each other messages. That said (and as explained in the introduction to  [2.x.390] ), one typically calls higher level MPI functions in which all processes that are part of a communicator participate. An example would be computing the sum over a set of integers where each process provides one term of the sum.  [2.x.391] 
* 

*  <dt class="glossary"> [2.x.392]  GlossMPIRank [1.x.100]</dt>  [2.x.393]  In the language of the Message Passing Interface (MPI), the [1.x.101] of an  [2.x.394]  "MPI process" is the number this process carries within the set  [2.x.395]  of all processes currently running as one parallel job. More correctly, it is the number within an  [2.x.396]  "MPI communicator" that groups together a subset of all processes with one parallel job (where  [2.x.397]  simply denotes the [1.x.102] set of processes).
*  Within each communicator, each process has a unique rank, distinct from the all other processes' ranks, that allows identifying one recipient or sender in MPI communication calls. Each process, running on one processor, can inquire about its own rank within a communicator by calling  [2.x.398]  The total number of processes participating in a communicator (i.e., the [1.x.103] of the communicator) can be obtained by calling  [2.x.399]   [2.x.400] 
* 

*  <dt class="glossary"> [2.x.401]  mg_paper [1.x.104]</dt>  [2.x.402] The "multigrid paper" is a paper by B. Janssen and G. Kanschat, titled "Adaptive Multilevel Methods with Local Smoothing for H1- and Hcurl-Conforming High Order Finite Element Methods", that describes many of the algorithms and data structures used in the implementation of the multigrid framework of deal.II. It underlies the implementation of the classes that are used in  [2.x.403]  for multigrid methods.
*  The full reference for this paper is as follows:

* 
* [1.x.105]
*  See [1.x.106] for the paper and [1.x.107] for more details.  [2.x.404] 
* 

*  <dt class="glossary"> [2.x.405]  GlossNodes [1.x.108]</dt>
*   [2.x.406] It is customary to define a finite element as a triple  [2.x.407]  where
* 

* 
* 
*  -  [2.x.408]  is the cell, where in deal.II this is always a line segment,   quadrilateral, or hexahedron;
* 

* 
* 
*  -  [2.x.409]  is a finite-dimensional space, e.g., a polynomial space mapped   from the  [2.x.410]  "reference cell" to  [2.x.411] ;
* 

* 
* 
*  -  [2.x.412]  is a set of "node functionals", i.e., functionals    [2.x.413] . The dimension of  [2.x.414]  must be equal to the number of node functionals. With this definition, we can define a basis of the local function space, i.e., a set of "shape functions"  [2.x.415] , by requiring that  [2.x.416] , where  [2.x.417]  is the Kronecker delta.
*  This definition of what a finite element is has several advantages, concerning analysis as well as implementation. For the analysis, it means that conformity with certain spaces  [2.x.418]  e.g. continuity, is up to the node functionals. In deal.II, it helps simplifying the implementation of more complex elements like FE_RaviartThomas considerably.
*  Examples for node functionals are values in  [2.x.419]  "support points" and moments with respect to Legendre polynomials. Examples:
*   [2.x.420] 
*  The construction of finite elements as outlined above allows writing code that describes a finite element simply by providing a polynomial space (without having to give it any particular basis
* 
*  -  whatever is convenient is entirely sufficient) and the nodal functionals. This is used, for example in the  [2.x.421]  function.  [2.x.422] 
* 

*  <dt class="glossary"> [2.x.423]  GlossParallelScaling [1.x.113]</dt>  [2.x.424] When we say that a parallel program "scales", what we mean is that the program does not become unduly slow (or takes unduly much memory) if we make the problem it solves larger, and that run time and memory consumption decrease proportionally if we keep the problem size the same but increase the number of processors (or cores) that work on it.
*  More specifically, think of a problem whose size is given by a number  [2.x.425]  (which could be the number of cells, the number of unknowns, or some other indicative quantity such as the number of CPU cycles necessary to solve it) and for which you have  [2.x.426]  processors available for solution. In an ideal world, the program would then require a run time of  [2.x.427] , and this would imply that we could reduce the run time to any desired value by just providing more processors. Likewise, for a program to be scalable, its overall memory consumption needs to be  [2.x.428]  and on each involved process needs to be  [2.x.429] , again implying that we can fit any problem into the fixed amount of memory computers attach to each processor, by just providing sufficiently many processors.
*  For practical assessments of scalability, we often distinguish between "strong" and "weak" scalability. These assess asymptotic statements such as  [2.x.430]  run time in the limits  [2.x.431]  and/or  [2.x.432] . Specifically, when we say that a program is "strongly scalable", we mean that if we have a problem of fixed size  [2.x.433] , then we can reduce the run time and memory consumption (on every processor) inversely proportional to  [2.x.434]  by just throwing more processors at the problem. In particular, strong scalability implies that if we provide twice as many processors, then run time and memory consumption on every process will be reduced by a factor of two. In other words, we can solve the [1.x.114] faster and faster by providing more and more processors.
*  Conversely, "weak scalability" means that if we increase the problem size  [2.x.435]  by a fixed factor, and increase the number of processors  [2.x.436]  available to solve the problem by the same factor, then the overall run time (and the memory consumption on every processor) remains the same. In other words, we can solve [1.x.115] within the same amount of wallclock time by providing more and more processors.
*  No program is truly scalable in this theoretical sense. Rather, all programs cease to scale once either  [2.x.437]  or  [2.x.438]  grows larger than certain limits. We therefore often say things such as "the program scales up to 4,000 cores", or "the program scales up to 100,000,000 unknowns". There are a number of reasons why programs cannot scale without limit; these can all be illustrated by just looking at the (relatively simple)  [2.x.439]  tutorial program:
* 

* 
* 
*  - Sequential sections: Many programs have sections of code that   either cannot or are not parallelized, i.e., where one processor has to do   a certain, fixed amount of work that does not decrease just because   there are a total of  [2.x.440]  processors around. In  [2.x.441] , this is   the case when generating graphical output: one processor creates   the graphical output for the entire problem, i.e., it needs to do    [2.x.442]  work. That means that this function has a run time   of  [2.x.443] , regardless of  [2.x.444] , and consequently the overall   program will not be able to achieve  [2.x.445]  run time but   have a run time that can be described as  [2.x.446]  where   the first term comes from scalable operations such as assembling   the linear system, and the latter from generating graphical   output on process 0. If  [2.x.447]  is sufficiently small, then the   program might look like it scales strongly for small numbers of   processors, but eventually strong scalability will cease. In   addition, the program can not scale weakly either because   increasing the size  [2.x.448]  of the problem while increasing the   number of processors  [2.x.449]  at the same rate does not keep the   run time of this one function constant.
* 

* 
* 
*  - Duplicated data structures: In  [2.x.450] , each processor stores the entire   mesh. That is, each processor has to store a data structure of size    [2.x.451] , regardless of  [2.x.452] . Eventually, if we make the problem   size large enough, this will overflow each processor's memory space   even if we increase the number of processors. It is thus clear that such   a replicated data structure prevents a program from scaling weakly.   But it also prevents it from scaling strongly because in order to   create an object of size  [2.x.453] , one has to at the very   least write into  [2.x.454]  memory locations, costing    [2.x.455]  in CPU time. Consequently, there is a component of the   overall algorithm that does not behave as  [2.x.456]  if we   provide more and more processors.
* 

* 
* 
*  - Communication: If, to pick just one example, you want to compute   the  [2.x.457]  norm of a vector of which all MPI processes store a few   entries, then every process needs to compute the sum of squares of   its own entries (which will require  [2.x.458]  time, and   consequently scale perfectly), but then every process needs to   send their partial sum to one process that adds them all up and takes   the square root. In the very best case, sending a message that   contains a single number takes a constant amount of time,   regardless of the overall number of processes. Thus, again, every   program that does communication cannot scale strongly because   there are parts of the program whose CPU time requirements do   not decrease with the number of processors  [2.x.459]  you allocate for   a fixed size  [2.x.460] . In reality, the situation is actually even   worse: the more processes are participating in a communication   step, the longer it will generally take, for example because   the one process that has to add everyone's contributions has   to add everything up, requiring  [2.x.461]  time. In other words,   CPU time [1.x.116] with the number of processes, therefore   not only preventing a program from scaling strongly, but also from   scaling weakly. (In reality, MPI libraries do not implement  [2.x.462]    norms by sending every message to one process that then adds everything   up; rather, they do pairwise reductions on a tree that doesn't   grow the run time as  [2.x.463]  but as  [2.x.464] ,   at the expense of more messages sent around. Be that as it may,   the fundamental point is that as you add more processors, the   run time will grow with  [2.x.465]  regardless of the way the operation   is actually implemented, and it can therefore not scale.)
*  These, and other reasons that prevent programs from scaling perfectly can be summarized in [1.x.117][1.x.118] that states that if a fraction  [2.x.466]  of a program's overall work  [2.x.467]  can be parallelized, i.e., it can be run in  [2.x.468]  time, and a fraction  [2.x.469]  of the program's work can not be parallelized (i.e., it consists either of work that only one process can do, such as generating graphical output in  [2.x.470] ; or that every process has to execute in a replicated way, such as sending a message with a local contribution to a dedicated process for accumulation), then the overall run time of the program will be

* 
* [1.x.119]
*  Consequently, the "speedup" you get, i.e., the factor by which your programs run faster on  [2.x.471]  processors compared to running the program on a single process (assuming this is possible) would be

* 
* [1.x.120]
*  If  [2.x.472] , which it is for all practically existing programs, then  [2.x.473]  as  [2.x.474] , implying that there is a point where it does not pay off in any significant way any more to throw more processors at the problem.
*  In practice, what matters is [1.x.121] or [1.x.122] or [1.x.123] a program scales. For deal.II, experience shows that on most clusters with a reasonable fast network, one can solve problems up to a few billion unknowns, up to a few thousand processors, and down to somewhere between 40,000 and 100,000 unknowns per process. The last number is the most relevant: if you have a problem with, say,  [2.x.475]  unknowns, then it makes sense to solve it on 1000-2500 processors since the number of degrees of freedom each process handles remains at more than 40,000. Consequently, there is enough work every process has to do so that the  [2.x.476]  time for communication does not dominate. But it doesn't make sense to solve such a problem with 10,000 or 100,000 processors, since each of these processor's local problem becomes so small that they spend most of their time waiting for communication, rather than doing work on their part of the work.  [2.x.477] 
*  <dt class="glossary"> [2.x.478]  GlossPeriodicConstraints [1.x.124]</dt>  [2.x.479] Periodic boundary condition are often used when only part of the physical relevant domain is modeled. One assumes that the solution simply continues periodically with respect to the boundaries that are considered periodic. In deal.II, support for this is through  [2.x.480]  and  [2.x.481]  As soon as a  [2.x.482]  is used also  [2.x.483]  has to be called to make sure that all the processes know about relevant parts of the triangulation on both sides of the periodic boundary. A typical process for distributed triangulations would be:
* 

* 
* 
*  - Create a mesh
* 

* 
* 
*  - Gather the periodic faces using  [2.x.484]  (Triangulation)
* 

* 
* 
*  - Add the periodicity information to the mesh using  [2.x.485] 
* 

* 
* 
*  - Gather the periodic faces using  [2.x.486]  (DoFHandler)
* 

* 
* 
*  - Add periodicity constraints using  [2.x.487] 
*  An example for this can be found in  [2.x.488] .  [2.x.489] 
* 

*  <dt class="glossary"> [2.x.490]  GlossPrimitive [1.x.125]</dt>  [2.x.491] A finite element (described by its shape functions) is primitive if there is a unique relation from shape function number to vector  [2.x.492]  GlossComponent "component". What this means is that each shape function of a vector-valued element has exactly one nonzero component if an element is primitive. This includes, in particular, all scalar elements as well as vector-valued elements assembled via the FESystem class from other primitive (for example scalar) elements as shown in  [2.x.493] ,  [2.x.494] ,  [2.x.495]  and several others. On the other hand, the FE_RaviartThomas class used in  [2.x.496]  and  [2.x.497] , or the FE_Nedelec class provide non-primitive finite elements because there, each vector-value shape function may have several non-zero components.  [2.x.498] 
* 

*  <dt class="glossary"> [2.x.499]  GlossReferenceCell [1.x.126]</dt>  [2.x.500] The hypercube [0,1]<sup>dim</sup>, on which all parametric finite element shape functions are defined. Many properties of the reference cell are described by the GeometryInfo class.  [2.x.501] 
* 

*  <dt class="glossary"> [2.x.502]  GlossSerialization [1.x.127]</dt>
*   [2.x.503] The term "serialization" refers to the process of writing the state of an object to a stream and later retrieve it again. A typical use case is to save the state of a program to disk for possible later resurrection, often in the context of checkpoint/restart strategies for long running computations or on computers that aren't very reliable (e.g. on very large clusters where individual nodes occasionally fail and then bring down an entire MPI job). In either case, one wants to occasionally save the state of the program so that, upon failure, one can restart it at that point rather than having to run it again from the beginning.
*  deal.II implements serialization facilities by implementing the necessary interfaces for the [1.x.128] library. See there for examples on how to save and restore objects.  [2.x.504] 
* 

*  <dt class="glossary"> [2.x.505]  GlossShape [1.x.129]</dt>  [2.x.506] The restriction of the finite element basis functions to a single grid cell.  [2.x.507] 
* 

*  <dt class="glossary"> [2.x.508]  GlossSubdomainId [1.x.130]</dt>  [2.x.509] Each cell of a triangulation has associated with it a property called the "subdomain id" that can be queried using a call like  [2.x.510]  and that can be set for example by using  [2.x.511] . (These calls resolve to  [2.x.512]  and  [2.x.513]  respectively.) While in principle this property can be used in any way application programs deem useful (it is simply an integer associated with each cell that can indicate whatever you want), at least for programs that run in %parallel it usually denotes the  [2.x.514]  "MPI rank" of the processor that "owns" this cell.
*  For programs that are parallelized based on MPI but where each processor stores the entire triangulation (as in, for example,  [2.x.515]  and  [2.x.516] , but not in  [2.x.517] ), subdomain ids are assigned to cells by partitioning a mesh, and each MPI process then only works on those cells it "owns", i.e., that belong to a subdomain the processor owns (traditionally, this is the case for the subdomain id whose numerical value coincides with the rank of the MPI process within the MPI communicator). Partitioning is typically done using the  [2.x.518]  function, but any other method can also be used to do this. (Alternatively, the  [2.x.519]  class can partition the mesh automatically using a similar approach.)
*  On the other hand, for programs that are parallelized using MPI but where meshes are held distributed across several processors using the  [2.x.520]  class, the subdomain id of cells is tied to the processor that owns the cell. In other words, querying the subdomain id of a cell tells you if the cell is owned by the current processor (i.e. if <code>cell- [2.x.521]  ==  [2.x.522]  or by another processor. In the %parallel distributed case, subdomain ids are only assigned to cells that the current processor owns as well as the immediately adjacent  [2.x.523]  "ghost cells". Cells further away are held on each processor to ensure that every MPI process has access to the full  [2.x.524]  "coarse grid" as well as to ensure the invariant that neighboring cells differ by at most one refinement level. These cells are called "artificial" (see  [2.x.525]  "here") and have the special subdomain id value  [2.x.526] 
*  In addition to regular subdomain ids, there is a second, closely related set of flags that are associated with each cell: "level subdomain ids." These exist not only for active cells but, in fact, for every cell in a mesh hierarchy. Their meaning is entirely analogous to the regular subdomain ids, but they are read and written by the  [2.x.527]  and  [2.x.528]  functions.  [2.x.529] 
* 

*  <dt class="glossary"> [2.x.530]  GlossSupport [1.x.131]</dt>  [2.x.531] Support points are by definition those points  [2.x.532] , such that for the shape functions  [2.x.533]  holds  [2.x.534] . Therefore, a finite element interpolation can be defined uniquely by the values in the support points.
*  Lagrangian elements fill the vector accessed by  [2.x.535]  such that the function  [2.x.536]  returns <tt>true</tt>. Naturally, these support points are on the  [2.x.537]  "reference cell".  Then, FEValues can be used (in conjunction with a Mapping) to access support points on the actual grid cells.
* 

* 
*  [2.x.538]  The concept of  [2.x.539]  "support points" is restricted to the finite element families based on Lagrange interpolation. For a more general concept, see  [2.x.540]  "generalized support points".  [2.x.541] 
* 

*  <dt class="glossary"> [2.x.542]  GlossTargetComponent [1.x.132]</dt>  [2.x.543]  When vectors and matrices are grouped into blocks by component, it is often desirable to collect several of the original components into a single one. This could be for instance, grouping the velocities of a Stokes system into a single block.  [2.x.544] 
* 

*  <dt class="glossary"> [2.x.545]  GlossUnitCell [1.x.133]</dt>  [2.x.546] See  [2.x.547]  "Reference cell".  [2.x.548] 
* 

*  <dt class="glossary"> [2.x.549]  GlossUnitSupport [1.x.134]</dt>  [2.x.550] These are the  [2.x.551]  "support points" on the reference cell, defined in FiniteElement. For example, the usual Q1 element in 1d has support points  at <tt>x=0</tt> and <tt>x=1</tt> (and similarly, in higher dimensions at the vertices of the unit square or cube). On the other hand, higher order Lagrangian elements have unit support points also in the interior of the unit line, square, or cube.  [2.x.552] 
* 

*  <dt class="glossary"> [2.x.553]  GlossUserFlags [1.x.135]</dt>  [2.x.554]    A triangulation offers one bit per line, quad, etc for user flags.   This field can be   accessed as all other data using iterators, using the syntax  
* [1.x.136]
*    Typically, this user flag is   used if an algorithm walks over all cells and needs information whether   another cell, e.g. a neighbor, has already been processed. Similarly,   it can be used to flag faces, quads or lines at the boundary for which   some operation has already been performed. The latter is often useful   since a loop such as  
* [1.x.137]
*    encounters some boundary lines more than once. Consequently, one would   set the user flag of the line in the body of the loop, and only enter the   body if the user flag had not previously been set. There are a number of   additional functions that can be accessed through the iterator interface;   see the TriaAccessor class for more information. Note that there are no   user flags that can be associated with vertices; however, since vertices   are numbered consecutively, this can easily be emulated in user code   using a vector of bools.
*    There are two functions,  [2.x.555]  and    [2.x.556]  which   write and read these flags to and from a stream or a vector of bools. Unlike    [2.x.557]  and  [2.x.558]    these two functions store   and read the flags of all used lines, quads, etc, i.e., not only of the   active ones.
*    If you want to store more specific user flags, you can use the functions    [2.x.559]  and  [2.x.560]    and the similarly for quads, etc.
*    As for the refinement and coarsening flags, there exist two versions of these   functions, one which reads/writes from a stream and one which does so from   a <tt>vector [2.x.561]  The latter is used to store flags temporarily, while the   first is used to store them in a file.
*    It is good practice to clear the user flags using the    [2.x.562]  function before usage, since it is   often necessary to use the flags in more than one function. If the flags may   be in use at the time a function that needs them is called, then this function   should save and restore the flags as described above.
*   
*  [2.x.563]  If more information than just a single boolean flag needs to be stored   with a cell, line, or face, then see about  [2.x.564]  "user data".  [2.x.565] 
* 

*  <dt class="glossary"> [2.x.566]  GlossUserData [1.x.138]</dt>  [2.x.567]    Just like the  [2.x.568]  "user flags", the Triangulation class offers a   field for each line, quad and hex in which to store more descriptive data than just   a single boolean flag. This is called "user data" and the data that can be stored   in it is either a single unsigned integer or a void pointer. Both are typically   used to index into a bigger array that contains more detailed data an application   wants to attach to a mesh entity.
*    User data is stored and retrieved in the following manner:  
* [1.x.139]
*    Similarly, there are functions  [2.x.569]  to set a pointer, and    [2.x.570]  and  [2.x.571]  to retrieve the index   and pointer. To clear all user indices or pointers, use  [2.x.572]    As with flags, there are functions that allow to save and restore user data,   either for all entities of the mesh hierarchy or for lines, quads or hexes   separately. There are a number of additional functions that can be accessed   through the iterator interface; see the TriaAccessor class for more information.
*   
*  [2.x.573]  User pointers and user indices are stored in the same   place. In order to avoid unwanted conversions, Triangulation   checks which one of them is in use and does not allow access to   the other one, until  [2.x.574]  has been called.
*   
*  [2.x.575]  The usual warning about the missing type safety of  [2.x.576]  pointers are   obviously in place here; responsibility for correctness of types etc   lies entirely with the user of the pointer.  [2.x.577] 
* 

*  <dt class="glossary"> [2.x.578]  workstream_paper [1.x.140]</dt>  [2.x.579] The "WorkStream paper" is a paper by B. Turcksin, M. Kronbichler and W. Bangerth   that discusses the design and implementation of WorkStream. WorkStream is, at its   core, a design pattern, i.e., something that is used over and over in finite element   codes and that can, consequently, be implemented generically. In particular, the   paper lays out the motivation for this pattern and then proposes different ways   of implementing it. It also compares the performance of different implementations.
*  The full reference for this paper is as follows:

* 
* [1.x.141]
*  It is available from [1.x.142], also see [1.x.143] for details.  [2.x.580] 
* 

*  <dt class="glossary"> [2.x.581]  GlossZOrder [1.x.144]</dt>  [2.x.582]   The "Z order" of cells describes an order in which cells are traversed.
*   By default, if you write a loop over all cells in deal.II, the cells  will be traversed in an order where coarser cells (i.e., cells that were  obtained from   [2.x.583]  "coarse mesh" cells with fewer refinement steps) come  before cells that are finer (i.e., cells that were obtained with more refinement  steps). Within each refinement level, cells are traversed in an order  that has something to do with the order in which they were created;  in essence, however, this order is best of thought of as "unspecified":  you will visit each cell on a given refinement level exactly once, in  some order, but you should not make any assumptions about this order.
*   Because the order in which cells are created factors into the order  of cells, it can happen that the order in which you traverse cells is  different for two identical meshes. For example, think of a 1d (coarse)  mesh with two cells: If you first refine the first of these cells and then  the other, then you will traverse the four cells on refinement level 1  in a different order than if you had first refined the second coarse  cell and then the first coarse cell.
*   This order is entirely practical for almost all applications because  in most cases, it does not actually matter in which order one traverses  cells. Furthermore, it allows using data structures that lead to  particularly low cache miss frequencies and are therefore efficient  for high performance computing applications.
*   On the other hand, there are cases where one would want to traverse  cells in a particular, specified and reproducible order that only  depends on the mesh itself, not its creation history or any other  seemingly arbitrary design decisions. The "Z order" is one way  to achieve this goal.
*   To explain the concept of the Z order, consider the following sequence  of meshes (with each cell numbered using the "level.index" notation,  where "level" is the number of refinements necessary to get from a   [2.x.584]  "coarse mesh" cell to a particular cell, and "index" the index of this  cell within a particular refinement level):
*    [2.x.585]    [2.x.586]    [2.x.587]    [2.x.588] 
*   Note how the cells on level 2 are ordered in the order in which they  were created. (Which is not always the case: if cells had been removed  in between, then newly created cells would have filled in the holes  so created.)
*   The "natural" order in which deal.II traverses cells would then be  0.0
* 
-> 1.0
* 
-> 1.1
* 
-> 1.2
* 
-> 1.3
* 
-> 2.0
* 
-> 2.1
* 
-> 2.2
* 
-> 2.3
* 
-> 2.4
* 
->  2.5
* 
-> 2.6
* 
-> 2.7. (If you want to traverse only over the   [2.x.589]  "active cells", then omit all cells from this  list that have children.)  This can be thought of as the "lexicographic"  order on the pairs of numbers "level.index", but because the index  within each level is not well defined, this is not a particularly useful  notion. Alternatively, one can also think of it as one possible breadth-first  traversal of the tree that corresponds to this mesh and that represents  the parent-child relationship between cells:
*    [2.x.590] 
*   On the other hand, the Z order corresponds to a particular  depth-first traversal of the tree. Namely: start with a cell, and if it  has children then iterate over these cell's children; this rule is  recursively applied as long as a child has children.
*   For the given mesh above, this yields the following order: 0.0
* 
-> 1.0
* 
-> 2.4
* 

* 
* 

* 
* 
-> 2.5
* 
-> 2.6
* 
-> 2.7
* 
-> 1.1
* 
-> 1.2
* 
-> 1.3
* 
-> 1.4
* 
-> 2.0
* 
-> 2.1
* 
-> 2.2
* 
-> 2.3.  (Again, if you only care about active cells, then remove 0.0, 1.0, and 1.3  from this list.) Because the order of children of a cell is well defined  (as opposed to the order of cells within each level), this "hierarchical"  traversal makes sense and is, in particular, independent of the history  of a triangulation.
*   In practice, it is easily implemented using a recursive function: 
* [1.x.145]
*   This function is then called as follows: 
* [1.x.146]
* 
*   Finally, as an explanation of the term "Z" order: if you draw a line through  all cells in the order in which they appear in this hierarchical fashion,  then it will look like a left-right inverted Z on each refined cell. Indeed,  the curve so defined can be thought of a space-filling curve and is also  sometimes called "Morton ordering", see  https://en.wikipedia.org/wiki/Z-order_curve .  [2.x.591] 
* 

* 
*   [2.x.592] 

* 
* [0.x.1]