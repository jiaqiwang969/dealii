[0.x.0]*
       An internal namespace used for  [2.x.0]        and for  [2.x.1]       
* [0.x.1]*
         Specialization of  [2.x.2]  for setting up the         Dictionary even if there are ranges in the IndexSet space not owned         by any processes.                
*  [2.x.3]  Only for internal usage.        
* [0.x.2]*
           Constructor.          
* [0.x.3]*
           Implementation of            [2.x.4]           
* [0.x.4]*
           Implementation of            [2.x.5]           
* [0.x.5]*
           Implementation of            [2.x.6]           
* [0.x.6]*
         Dictionary class with basic partitioning in terms of a single         interval of fixed size known to all MPI ranks for two-stage index         lookup.        
* [0.x.7]*
           The minimum grain size for the intervals.                     We choose to limit the smallest size an interval for the           two-stage lookup can have with the following two conflicting           goals in mind: On the one hand, we do not want intervals in the           dictionary to become too short. For uneven distributions of           unknowns (some ranks with several thousands of unknowns, others           with none), the lookup DoFs
* 
-> dictionary then involves sending           from one MPI rank to many other MPI ranks holding dictionary           intervals, leading to an exceedingly high number of messages some           ranks have to send. Also, fewer longer intervals are generally           more efficient to look up. On the other hand, a range size too           large leads to opposite effect of many messages that come into a           particular dictionary owner in the lookup DoFs
* 
->           dictionary. With the current setting, we get at most 64 messages           coming to a single MPI rank in case there is 1 dof per MPI rank,           which is reasonably low. At the same time, uneven distributions           up to factors of 4096 can be handled with at most 64 messages as           well.          
* [0.x.8]*
           A vector with as many entries as there are dofs in the dictionary           of the current process, and each entry containing the rank of the           owner of that dof in the IndexSet `owned_indices`. This is           queried in the index lookup, so we keep an expanded list.          
* [0.x.9]*
           A sorted vector containing the MPI ranks appearing in           `actually_owning_ranks`.          
* [0.x.10]*
           The number of unknowns in the dictionary for on each MPI rank           used for the index space splitting. For simplicity of index           lookup without additional communication, this number is the same           on all MPI ranks.          
* [0.x.11]*
           The local range of the global index space that is represented in           the dictionary, computed from `dofs_per_process`, the current           MPI rank, and range_minimum_grain_size.          
* [0.x.12]*
           The actual size, computed as the minimum of dofs_per_process and           the possible end of the index space. Equivalent to           `local_range.second
* 
*  - local_range.first`.          
* [0.x.13]*
           The global size of the index space.          
* [0.x.14]*
           The number of ranks the `owned_indices` IndexSet is distributed           among.          
* [0.x.15]*
           A stride to distribute the work more evenly over MPI ranks in           case the grain size forces us to have fewer ranges than we have           processors.          
* [0.x.16]*
           Set up the dictionary by computing the partitioning from the           global size and sending the rank information on locally owned           ranges to the owner of the dictionary part.          
* [0.x.17]*
           Translate a global dof index to the MPI rank in the dictionary           using `dofs_per_process`. We multiply by `stride_small_size` to           ensure a balance over the MPI ranks due to the grain size.          
* [0.x.18]*
           Given an MPI rank id of an arbitrary processor, return the index           offset where the local range of that processor begins.          
* [0.x.19]*
           Given the rank in the owned indices from `actually_owning_ranks`,           this returns the index of the rank in the           `actually_owning_rank_list`.          
* [0.x.20]*
           Compute the partition from the global size of the index space and           the number of ranks.          
* [0.x.21]*
         Specialization of  [2.x.7]  for the context of          [2.x.8]  and          [2.x.9]  with additional         payload.        
* [0.x.22]*
           Constructor.          
* [0.x.23]*
           The index space which describes the locally owned space.          
* [0.x.24]*
           The indices which are "ghosts" on a given rank and should be           looked up in terms of their owner rank from owned_indices.          
* [0.x.25]*
           The underlying MPI communicator.          
* [0.x.26]*
           The present MPI rank.          
* [0.x.27]*
           The total number of ranks participating in the MPI communicator           `comm`.          
* [0.x.28]*
           Controls whether the origin of ghost owner should also be           stored. If true, it will be added into `requesters` and can be           queried by `get_requesters()`.          
* [0.x.29]*
           The result of the index owner computation: To each index           contained in `indices_to_look_up`, this vector contains the MPI           rank of the owner in `owned_indices`.          
* [0.x.30]*
           Keeps track of the origin of the requests. The layout of the data           structure is as follows: The outermost vector has as many entries           as  [2.x.10]  and represents the           information we should send back to the owners from the present           dictionary entry. The second vector then collects a list of MPI           ranks that have requested data, using the rank in the first pair           entry and a list of index ranges as the second entry.          
* [0.x.31]*
           The dictionary handling the requests.          
* [0.x.32]*
           Array to collect the indices to look up, sorted by the rank in           the dictionary.          
* [0.x.33]*
           The field where the indices for incoming data from the process           are stored.          
* [0.x.34]*
           Implementation of            [2.x.11]            adding the owner of a particular index in request_buffer (and           keeping track of who requested a particular index in case that           information is also desired).          
* [0.x.35]*
           Implementation of            [2.x.12]           
* [0.x.36]*
           Implementation of            [2.x.13]           
* [0.x.37]*
           Implementation of            [2.x.14]           
* [0.x.38]*
           Implementation of            [2.x.15]           
* [0.x.39]*
           Resolve the origin of the requests by sending the information           accumulated in terms of the dictionary owners during the run of           the consensus algorithm back to the owner in the original           IndexSet. This requires some point-to-point communication.                      [2.x.16]  Map of processors and associated ranges of indices that                   are requested from the current rank          
* [0.x.40]*
           Stores the index request in the `requesters` field. We first find           out the owner of the index that was requested (using the guess in           `owner_index`, as we typically might look up on the same rank           several times in a row, which avoids the binary search in            [2.x.17]  Once we know the rank of the           owner, we the vector entry with the rank of the request. Here, we           utilize the fact that requests are processed rank-by-rank, so we           can simply look at the end of the vector if there is already some           data stored or not. Finally, we build ranges, again using that           the index list is sorted and we therefore only need to append at           the end.          
* [0.x.41]