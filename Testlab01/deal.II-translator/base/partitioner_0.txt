[0.x.0]*
     This class defines a model for the partitioning of a vector (or, in     fact, any linear data structure) among processors using MPI.         The partitioner stores the global vector size and the locally owned     range as a half-open interval [ [2.x.0]   [2.x.1]  on each process.     Furthermore, it includes a structure for the point-to-point communication     patterns. It allows the inclusion of ghost indices (i.e. indices that a     current processor needs to have access to, but are owned by another     process) through an IndexSet. In addition, it also stores the other     processors' ghost indices belonging to the current processor (see     import_targets()), which are the indices where other processors might     require information from. In a sense, these import indices form the dual     of the ghost indices. This information is gathered once when constructing     the partitioner, which obviates subsequent global communication steps     when exchanging data.         The figure below gives an example of index space  [2.x.2]  being split     into four parts that are each owned by one MPI process:      [2.x.3]      The first row (above the thick black line) shows which process owns which     elements. Below it, the next four lines indicate which elements of     the overall array each processor wants to know about
* 
*  -  this is     generally a superset of the locally owned elements, with the difference     being what are called "ghost elements".         To understand the remaining pieces of the figure (and this class),     remember that in MPI, you can't just ask another process for data.     (That's not quite true: There are mechanisms in newer MPI standards     for this, but as a general rule it's true.) Rather, if you need     information, you need to send another process a message, the other     process needs to expect the message and respond as appropriate with     a message of its own. In practice, it is therefore easier and faster     if each process willalready* know what it will be asked and, at     the appropriate time, just send that data. The remaining lines of     information set up this kind of scheme.         To this end, note that process 0 will want to know about five     elements it does not own itself: those with indices 20, 21 (owned     by process 1); and 40, 41, 43 (owned by process 2). Similar information     can be obtained from the following lines. To satisfy this need for     knowledge, it would therefore be quite useful if process 1 stored     that, at the appropriate time, it will have to send elements 20, 21     to process 0. Similarly, if you go through lines 2-4 below the thick     black line, you will see that process 0 should know that it will need     to send elements 1, 2, 13, 18, 19 to process 1; 18, 19 to process 2;     and elements 1, 2, 13 to process 3. These are called "import indices"     because other processes will want to import them. Instead of storing     these indices as a set, it is often useful to use half-open index     sets instead, and so the import indices listed above form the following     collection of sets: `[1,3)`, `[13,14)`, `[18,20)`, `[18,20)`,     `[1,3)`, `[13,14)`. This is how the import indices are shown     in the figure above. We now only have to know which of these     half-open sets are to be sent to whom. This is done in the line     above it where we list the "import targets" (i.e., the target     processes for an import operations): Process 1 will receive 5     elements (which are comprised of the first three half-open     target index sets), process 2 will receive 2 indices     (the fourth half-open interval), and process 3 will receive     3 indices (the remaining two half-open intervals). This information     is encoded as the pairs `{1,5}`, `{2,2}`, `{3,3}` as the import     targets. Similar considerations can be made for what processes     1, 2, and 3 will have to send out.         Finally, when receiving information, it is useful to know how     many indices each process will receive from whom since then one     can already pre-allocate buffers of the right size. This is listed     in the last line under "ghost targets": Process 0 will receive     two elements from process 1 (namely those with indices 20, 21),     and three from process 2 (namely those with indices 40, 41, 43).     This is encoded as pairs `{1,2}` and `{2,3}`. Again, similar     considerations can be made for what processes 1, 2, and 3     should expect, and what is then shown in their respective columns.         The main purpose of this class is to set up these data structures     knowing only which process owns which elements, and for which     additional ghost elements each process needs knowledge.             [1.x.0]         The partitioner includes a mechanism for converting global to local and     local to global indices. Internally, this class stores vector elements     using the convention as follows: The local range is associated with     local indices [0, locally_owned_size()), and ghost indices are stored     consecutively in [locally_owned_size(), locally_owned_size() +     n_ghost_indices()). The ghost indices are sorted according to their     global index.             [1.x.1]         This class also handles the ghost data exchange for partitioned     arrays of objects
* 
*  -  i.e., if a separate class stores the     locally owned elements on each process, then this class     facilitates the importation of those elements that are locally     needed as ghosts but stored elsewhere. An example of where this class     is used is the  [2.x.4]  class.         The data exchange happens through four functions:      [2.x.5]       [2.x.6]  export_to_ghosted_array_start() is used for initiating an export     operation that sends data from the locally owned data field, passed as     an array, to the ghost data arrays of other processes (according to the     ghost indices stored in the present class).     This call starts non-blocking MPI send communication     routines, but does not wait for the routines to finish. Thus, the user     may not write into the respective positions of the underlying arrays as     the data might still be needed by MPI.      [2.x.7]  export_to_ghosted_array_finish() finalizes the MPI data exchange     started in export_to_ghosted_array_start() and signals that the data in     the arrays may be used for further processing or modified as     appropriate.      [2.x.8]  import_from_ghosted_array_start() is used for initiating an import     operation that sends data from a ghost data field, passed as an array,     to the locally owned array according to the ghost indices stored in the     present class. A  [2.x.9]  flag can be passed to decide     on how to combine the data in the ghost field with the data at the     owner, since both relate to the same data entry. In assembly, this is     usually an add-to operation where contributions from all processes to     a locally owned element need to be added up. This call starts     non-blocking MPI communication routines, but does not wait for the     routines to finish. Thus, the user may not write into the respective     positions of the underlying arrays as the data might still be needed by     MPI.      [2.x.10]  import_from_ghosted_array_finish() finalizes the MPI data exchange     started in import_from_ghosted_array_start() and signals that the data     in the arrays may be used for further processing or modified as     appropriate.      [2.x.11]          The MPI communication routines are point-to-point communication patterns.             [1.x.2]         This partitioner class operates on a fixed set of ghost indices and     must always be compatible with the ghost indices inside the array whose     partitioning it represents. In some cases,     one only wants to send around some of the ghost indices present in     a vector, but without creating a copy of the vector with a     suitable index set
* 
*  - think e.g. of local time stepping where different     regions of a vector might be exchanged at different stages of a time     step slice. This class supports that case by the following model: A     vector is first created with the full ghosted index set. Then, a second     Partitioner instance is created that sets ghost indices with a tighter     index set as ghosts, but specifying the larger index set as the second     argument to the set_ghost_indices() call. When data is exchanged, the     export_to_ghosted_array_start() and import_from_ghosted_array_start()     detect this case and only send the selected indices, taken from the     full array of ghost entries.    
* [0.x.1]*
       Default constructor.      
* [0.x.2]*
       Constructor with size argument. Creates an MPI_COMM_SELF structure       where there is no real parallel layout.      
* [0.x.3]*
       Constructor that takes the number of locally-owned degrees of freedom        [2.x.12]  and the number of ghost degrees of freedom  [2.x.13]              The local index range is translated to global indices in an ascending       and one-to-one fashion, i.e., the indices of process  [2.x.14]  sit exactly       between the indices of the processes  [2.x.15]  and  [2.x.16] , respectively.            
*  [2.x.17]  Setting the  [2.x.18]  variable to an appropriate value         provides memory space for the ghost data in a vector's memory         allocation as and allows access to it via local_element(). However,         the associated global indices must be handled externally in this         case.      
* [0.x.4]*
       Constructor with index set arguments. This constructor creates a       distributed layout based on a given communicators, an IndexSet       describing the locally owned range and another one for describing       ghost indices that are owned by other processors, but that we need to       have read or write access to.      
* [0.x.5]*
       Constructor with one index set argument. This constructor creates a       distributed layout based on a given communicator, and an IndexSet       describing the locally owned range. It allows to set the ghost       indices at a later time. Apart from this, it is similar to the other       constructor with two index sets.      
* [0.x.6]*
       Reinitialize the communication pattern. The first argument       `vector_space_vector_index_set` is the index set associated to a       VectorSpaceVector object. The second argument       `read_write_vector_index_set` is the index set associated to a       ReadWriteVector object.      
* [0.x.7]*
       Set the locally owned indices. Used in the constructor.      
* [0.x.8]*
       Set the ghost indices after the constructor has been       called.             The optional parameter  [2.x.19]  allows defining an       indirect addressing into a larger set of ghost indices. This setup is       useful if a distributed vector is based on that larger ghost index       set but only a tighter subset should be communicated according to        [2.x.20]       
* [0.x.9]*
       Return the global size.      
* [0.x.10]*
       Return the number of locally owned indices,       i.e., local_range().second minus local_range().first.       The returned numbers need to add up to the total number of indices when       summed over all processes              [2.x.21]  Use the more clearly named function locally_owned_size()       instead.      
* [0.x.11]*
       Return the number of locally owned indices,       i.e., local_range().second minus local_range().first.       The returned numbers need to add up to the total number of indices when       summed over all processes      
* [0.x.12]*
       Return an IndexSet representation of the local range. This class       only supports contiguous local ranges, so the IndexSet actually only       consists of one single range of data, and is equivalent to the result       of local_range().      
* [0.x.13]*
       Return the local range. The returned pair consists of the index of       the first element and the index of the element one past the last       locally owned one.      
* [0.x.14]*
       Return true if the given global index is in the local range of this       processor.      
* [0.x.15]*
       Return the local index corresponding to the given global index. If       the given global index is neither locally owned nor a ghost, an       exception is thrown.             Note that the returned local index for locally owned indices will be       between 0 and locally_owned_size()
* 
*  - 1, and the local index for       ghosts is between locally_owned_size() and locally_owned_size() +       n_ghost_indices()
* 
*  - 1.      
* [0.x.16]*
       Return the global index corresponding to the given local index.             Note that the local index for locally owned indices must be between 0       and locally_owned_size()
* 
*  - 1, and the local index for ghosts must be       between locally_owned_size() and locally_owned_size() +       n_ghost_indices()
* 
*  - 1.      
* [0.x.17]*
       Return whether the given global index is a ghost index on the       present processor. Returns false for indices that are owned locally       and for indices not present at all.      
* [0.x.18]*
       Return an IndexSet representation of all ghost indices.      
* [0.x.19]*
       Return the number of ghost indices. Same as       ghost_indices().n_elements(), but cached for simpler access.      
* [0.x.20]*
       In case the partitioner was built to define ghost indices as a subset       of indices in a larger set of ghosts, this function returns the       numbering in terms of ranges within that set. Similar structure as in       an IndexSet, but tailored to be iterated over.             In case the partitioner did not take a second set of ghost indices       into account, this subset is simply defined as the half-open interval        [2.x.22] .      
* [0.x.21]*
       Return a list of processors (first entry) and the number of ghost       degrees of freedom owned by that processor (second entry). The sum of       the latter over all processors equals n_ghost_indices().      
* [0.x.22]*
       Return a vector of ranges of local indices that we are importing during       compress(), i.e., others' ghosts that belong to the local range.       Similar structure as in an IndexSet, but tailored to be iterated over,       and some indices may be duplicated. The returned pairs consists of the       index of the first element and the index of the element one past the       last one in a range.      
* [0.x.23]*
       Number of import indices, i.e., indices that are ghosts on other       processors and we will receive data from.      
* [0.x.24]*
       Return a list of processors (first entry) and the number of degrees       of freedom imported from it during compress() operation (second entry)       for all the processors that data is obtained from, i.e., locally owned       indices that are ghosts on other processors.            
*  [2.x.23]  The returned vector only contains those processor id's for which       the second entry is non-zero.      
* [0.x.25]*
       Check whether the given partitioner is compatible with the current       partitioner. Two partitioners are compatible if       they have the same local sizes and the same ghost indices. They do not       necessarily need to correspond to the same data that is stored based       on these partioner objects. This is a local operation       only, i.e., if only some processors decide that the partitioning is       not compatible, only these processors will return  [2.x.24]  whereas       the other processors will return  [2.x.25]       
* [0.x.26]*
       Check whether the given partitioner is compatible with the       current partitioner. Two partitioners are compatible if       they have the same local size and the same ghost indices. They do not       necessarily need to correspond to the same data that is stored based       on these partioner objects. As opposed to       is_compatible(), this method checks for compatibility among all       processors and the method only returns  [2.x.26]  if the partitioner is       the same on all processors. In other words, it does a global       "and" operation over the results returned by is_compatible() on all       involved processes.             This method performs global communication, so make sure to use it       only in a context where all processors call it the same number of       times.      
* [0.x.27]*
       Return the MPI ID of the calling processor. Cached to have simple       access.      
* [0.x.28]*
       Return the total number of MPI processor participating in the given       partitioner. Cached to have simple access.      
* [0.x.29]*
       Return the MPI communicator underlying the partitioner object.      
* [0.x.30]*
       Return whether ghost indices have been explicitly added as a  [2.x.27]        ghost_indices argument. Only true if a reinit() call or constructor       provided that argument.      
* [0.x.31]*
       Start the exportation of the data in a locally owned array to the       range described by the ghost indices of this class.              [2.x.28]  communication_channel Sets an offset to the MPI_Isend and       MPI_Irecv calls that avoids interference with other ongoing       export_to_ghosted_array_start() calls on different entries. Typically       handled within the blocks of a block vector. Any value less than 200       is a valid value.              [2.x.29]  locally_owned_array The array of data from which the data is       extracted and sent to the ghost entries on a remote processor.              [2.x.30]  temporary_storage A temporary storage array of length       n_import_indices() that is used to hold the packed data from the  [2.x.31]        locally_owned_array to be sent. Note that this array must not be       touched until the respective export_to_ghosted_array_finish() call       has been made because the model uses non-blocking communication.              [2.x.32]  ghost_array The array that will receive the exported data,       i.e., the entries that a remote processor sent to the calling       process. Its size must either be n_ghost_indices() or equal the       number of ghost indices in the larger index set that was given as       second argument to set_ghost_indices(). In case only selected indices       are sent, no guarantee is made regarding the entries that do not get       set. Some of them might be used to organize the transfer and later       reset to zero, so make sure you do not use them in computations.              [2.x.33]  requests The list of MPI requests for the ongoing non-blocking       communication that will be finalized in the       export_to_ghosted_array_finish() call.             This functionality is used in        [2.x.34]       
* [0.x.32]*
       Finish the exportation of the data in a locally owned array to the       range described by the ghost indices of this class.              [2.x.35]  ghost_array The array that will receive the exported data       started in the  [2.x.36]  This must be the       same array as passed to that function, otherwise the behavior is       undefined.              [2.x.37]  requests The list of MPI requests for the ongoing non-blocking       communication that were started in the       export_to_ghosted_array_start() call. This must be the same array as       passed to that function, otherwise MPI will likely throw an error.             This functionality is used in        [2.x.38]       
* [0.x.33]*
       Start importing the data on an array indexed by the ghost indices of       this class that is later accumulated into a locally owned array with       import_from_ghosted_array_finish().              [2.x.39]  vector_operation Defines how the data sent to the owner should       be combined with the existing entries, e.g., added into.              [2.x.40]  communication_channel Sets an offset to the MPI_Isend and       MPI_Irecv calls that avoids interference with other ongoing       import_from_ghosted_array_start() calls on different       entries. Typically handled within the blocks of a block vector.       Any value less than 200 is a valid value.              [2.x.41]  ghost_array The array of ghost data that is sent to a remote       owner of the respective index in a vector. Its size must either be       n_ghost_indices() or equal the number of ghost indices in the larger       index set that was given as second argument to       set_ghost_indices(). This or the subsequent       import_from_ghosted_array_finish() function, the order is       implementation-dependent, will set all data entries behind  [2.x.42]        ghost_array to zero.              [2.x.43]  temporary_storage A temporary storage array of length       n_import_indices() that is used to hold the packed data from MPI       communication that will later be written into the locally owned       array. Note that this array must not be touched until the respective       import_from_ghosted_array_finish() call has been made because the       model uses non-blocking communication.              [2.x.44]  requests The list of MPI requests for the ongoing non-blocking       communication that will be finalized in the       export_to_ghosted_array_finish() call.             This functionality is used in        [2.x.45]       
* [0.x.34]*
       Finish importing the data from an array indexed by the ghost       indices of this class into a specified locally owned array, combining       the results according to the given input  [2.x.46]               [2.x.47]  vector_operation Defines how the data sent to the owner should       be combined with the existing entries, e.g., added into.              [2.x.48]  temporary_storage The same array given to the       import_from_ghosted_array_start() call that contains the packed data       from MPI communication. In thus function, it is combined at the       corresponding entries described by the ghost relations according to        [2.x.49]               [2.x.50]  ghost_array The array of ghost data that is sent to a remote       owner of the respective index in a vector. Its size must either be       n_ghost_indices() or equal the number of ghost indices in the larger       index set that was given as second argument to       set_ghost_indices(). This function will set all data entries behind        [2.x.51]  to zero for the implementation-dependent cases when it       was not already done in the import_from_ghosted_array_start() call.              [2.x.52]  locally_owned_storage The array of data where the resulting data       sent by remote processes to the calling process will be accumulated       into.              [2.x.53]  requests The list of MPI requests for the ongoing non-blocking       communication that have been initiated in the       import_to_ghosted_array_finish() call. This must be the same array as       passed to that function, otherwise MPI will likely throw an error.             This functionality is used in        [2.x.54]       
* [0.x.35]*
       Compute the memory consumption of this structure.      
* [0.x.36]*
       Exception      
* [0.x.37]*
       Exception      
* [0.x.38]*
       Initialize import_indices_plain_dev from import_indices_data. This       function is only used when using CUDA-aware MPI.      
* [0.x.39]*
       The global size of the vector over all processors      
* [0.x.40]*
       The range of the vector that is stored locally.      
* [0.x.41]*
       The range of the vector that is stored locally. Extracted from       locally_owned_range for performance reasons.      
* [0.x.42]*
       The set of indices to which we need to have read access but that are       not locally owned      
* [0.x.43]*
       A variable caching the number of ghost indices. It would be expensive       to use  [2.x.55]  to compute this.      
* [0.x.44]*
       An array that contains information which processors my ghost indices       belong to and how many those indices are      
* [0.x.45]*
       The set of (local) indices that we are importing during compress(),       i.e., others' ghosts that belong to the local range. Similar       structure as in an IndexSet, but tailored to be iterated over, and       some indices may be duplicates.      
* [0.x.46]*
       The set of (local) indices that we are importing during compress(),       i.e., others' ghosts that belong to the local range. The data stored is       the same than in import_indices_data but the data is expanded in plain       arrays. This variable is only used when using CUDA-aware MPI.      
* [0.x.47]*
       A variable caching the number of ghost indices. It would be expensive       to compute it by iterating over the import indices and accumulate them.      
* [0.x.48]*
       The set of processors and length of data field which send us their       ghost data      
* [0.x.49]*
       An array that caches the number of chunks in the import indices per MPI       rank. The length is import_indices_data.size()+1.      
* [0.x.50]*
       A variable caching the number of ghost indices in a larger set of       indices given by the optional argument to set_ghost_indices().      
* [0.x.51]*
       An array that caches the number of chunks in the import indices per MPI       rank. The length is ghost_indices_subset_data.size()+1.      
* [0.x.52]*
       The set of indices that appear for an IndexSet that is a subset of a       larger set. Similar structure as in an IndexSet within all ghost       indices, but tailored to be iterated over.      
* [0.x.53]*
       The ID of the current processor in the MPI network      
* [0.x.54]*
       The total number of processors active in the problem      
* [0.x.55]*
       The MPI communicator involved in the problem      
* [0.x.56]*
       A variable storing whether the ghost indices have been explicitly set.      
* [0.x.57]