[0.x.0]*
     Helper struct to tell us if we can use SIMD instructions for the given      [2.x.0]  type.    
* [0.x.1]*
     Convert a function object of type F into an object that can be applied     to all elements of a range of synchronous iterators.    
* [0.x.2]*
       Constructor. Take and package the given function object.      
* [0.x.3]*
       The stored function object.      
* [0.x.4]*
       Apply F to a set of iterators with two elements.      
* [0.x.5]*
       Apply F to a set of iterators with three elements.      
* [0.x.6]*
       Apply F to a set of iterators with three elements.      
* [0.x.7]*
     Take a function object and create a Body object from it. We do this in     this helper function since alternatively we would have to specify the     actual data type of F
* 
*  -  which for function objects is often     extraordinarily complicated.    
* [0.x.8]*
     Encapsulate  [2.x.1]     
* [0.x.9]*
     Encapsulate  [2.x.2]  when an affinite_partitioner is provided.    
* [0.x.10]*
   An algorithm that performs the action <code>*out++ =   predicate(*in++)</code> where the  [2.x.3]  iterator ranges over   the given input range.     This algorithm does pretty much what  [2.x.4]  does. The difference   is that the function can run in parallel when deal.II is configured to   use multiple threads.     If running in parallel, the iterator range is split into several chunks   that are each packaged up as a task and given to the Threading Building   Blocks scheduler to work on as compute resources are available. The   function returns once all chunks have been worked on. The last argument   denotes the minimum number of elements of the iterator range per task;   the number must be large enough to amortize the startup cost of new   tasks, and small enough to ensure that tasks can be reasonably load   balanced.     For a discussion of the kind of problems to which this function is   applicable, see the    [2.x.5]  "Parallel computing with multiple processors"   module.  
* [0.x.11]*
   An algorithm that performs the action <code>*out++ = predicate(*in1++,  in2++)</code> where the  [2.x.6]  iterator ranges over the given   input range, using the parallel for operator of tbb.     This algorithm does pretty much what  [2.x.7]  does. The difference   is that the function can run in parallel when deal.II is configured to   use multiple threads.     If running in parallel, the iterator range is split into several chunks   that are each packaged up as a task and given to the Threading Building   Blocks scheduler to work on as compute resources are available. The   function returns once all chunks have been worked on. The last argument   denotes the minimum number of elements of the iterator range per task;   the number must be large enough to amortize the startup cost of new   tasks, and small enough to ensure that tasks can be reasonably load   balanced.     For a discussion of the kind of problems to which this function is   applicable, see the    [2.x.8]  "Parallel computing with multiple processors"   module.  
* [0.x.12]*
   An algorithm that performs the action <code>*out++ = predicate(*in1++,  in2++,in3++)</code> where the  [2.x.9]  iterator ranges over   the given input range.     This algorithm does pretty much what  [2.x.10]  does. The difference   is that the function can run in parallel when deal.II is configured to   use multiple threads.     If running in parallel, the iterator range is split into several chunks   that are each packaged up as a task and given to the Threading Building   Blocks scheduler to work on as compute resources are available. The   function returns once all chunks have been worked on. The last argument   denotes the minimum number of elements of the iterator range per task;   the number must be large enough to amortize the startup cost of new   tasks, and small enough to ensure that tasks can be reasonably load   balanced.     For a discussion of the kind of problems to which this function is   applicable, see the    [2.x.11]  "Parallel computing with multiple processors"   module.  
* [0.x.13]*
     Take a range argument and call the given function with its begin and     end.    
* [0.x.14]*
   This function applies the given function argument  [2.x.12]  to all elements in   the range  [2.x.13]  and may do so in parallel. An example   of its use is given in  [2.x.14] .     However, in many cases it is not efficient to call a function on each   element, so this function calls the given function object on sub-ranges.   In other words: if the given range  [2.x.15]  is smaller   than grainsize or if multithreading is not enabled, then we call    [2.x.16] ; otherwise, we may execute, possibly in   %parallel, a sequence of calls  [2.x.17]  where    [2.x.18]  and the   collection of calls we do to  [2.x.19]  will happen on disjoint   subintervals that collectively cover the original interval    [2.x.20] .     Oftentimes, the called function will of course have to get additional   information, such as the object to work on for a given value of the   iterator argument. This can be achieved by [1.x.0] certain   arguments. For example, here is an implementation of a matrix-vector   multiplication  [2.x.21]  for a full matrix  [2.x.22]  and vectors  [2.x.23] :  
* [1.x.1]
*      Note how we use the lambda function to convert    [2.x.24]  from a function that takes 5 arguments   to one taking 2 by binding the remaining arguments. The resulting function   object requires only two arguments, `begin_row` and `end_row`, with all   other arguments fixed.     The code, if in single-thread mode, will call    [2.x.25]  on the entire range    [2.x.26]  exactly once. In multi-threaded mode, however, it   may be called multiple times on subranges of this interval, possibly   allowing more than one CPU core to take care of part of the work.     The  [2.x.27]  argument (50 in the example above) makes sure that   subranges do not become too small, to avoid spending more time on   scheduling subranges to CPU resources than on doing actual work.     For a discussion of the kind of problems to which this function is   applicable, see also the    [2.x.28]  "Parallel computing with multiple processors"   module.  
* [0.x.15]*
   This is a class specialized to for loops with a fixed range given by   unsigned integers. This is an abstract base class that an actual worker   function is derived from. There is a public function apply that issues a   for loop in parallel, subdividing the work onto available processor cores   whenever there is enough work to be done (i.e., the number of elements is   larger than grain_size). Inside the function, a virtual function   apply_to_subrange specifying a range of two integers <tt>[lower,   upper)</tt> is called which needs to be defined in a derived class.     The parallelization cases covered by this class are a subset of what is   possible with the function apply_to_subranges (which also covers the case   of more general iterators that might not be described by an integer   range). However, for simple integer ranges one might prefer this class,   like when there are many structurally similar loops, e.g., some simple   copy or arithmetic operations on an array of pointers. In that case,   apply_to_subranges will generate a lot of code (or rather, a lot of   symbols) because it passes the long names generated by  [2.x.29]  to the   templated parallel for functions in TBB. This can considerably increase   compile times and the size of the object code. Similarly, the incorrect   use of  [2.x.30]  often results in very cryptic error messages, which can   be avoided by this class (only a virtual function needs to be defined in   a derived class). Finally, the additional cost of a virtual function is   negligible in the context of parallel functions: It is much more   expensive to actually issue the work onto a thread, which in turn should   be much less than the actual work done in the for loop.  
* [0.x.16]*
     Destructor. Made virtual to ensure that derived classes also have     virtual destructors.    
* [0.x.17]*
     This function runs the for loop over the given range     <tt>[lower,upper)</tt>, possibly in parallel when end-begin is larger     than the minimum parallel grain size. This function is marked const     because it any operation that changes the data of a derived class will     inherently not be thread-safe when several threads work with the same     data simultaneously.    
* [0.x.18]*
     Virtual function for working on subrange to be defined in a derived     class.  This function is marked const because it any operation that     changes the data of a derived class will inherently not be thread-safe     when several threads work with the same data simultaneously.    
* [0.x.19]*
     A class that conforms to the Body requirements of the TBB     parallel_reduce function. The first template argument denotes the type     on which the reduction is to be done. The second denotes the type of     the function object that shall be called for each subrange.    
* [0.x.20]*
       A variable that will hold the result of the reduction.      
* [0.x.21]*
       Constructor. Take the function object to call on each sub-range as       well as the neutral element with respect to the reduction operation.             The second argument denotes a function object that will be used to       reduce the result of two computations into one number. An example if       we want to simply accumulate integer results would be        [2.x.31]       
* [0.x.22]*
       Splitting constructor. See the TBB book for more details about this.      
* [0.x.23]*
       Join operation: merge the results from computations on different sub-       intervals.      
* [0.x.24]*
       Execute the given function on the specified range.      
* [0.x.25]*
       The function object to call on every sub-range.      
* [0.x.26]*
       The neutral element with respect to the reduction operation. This is       needed when calling the splitting constructor since we have to re-set       the result variable in this case.      
* [0.x.27]*
       The function object to be used to reduce the result of two calls into       one number.      
* [0.x.28]*
   This function works a lot like the apply_to_subranges(), but it allows to   accumulate numerical results computed on each subrange into one number.   The type of this number is given by the ResultType template argument that   needs to be explicitly specified.     An example of use of this function is to compute the value of the   expression  [2.x.32]  for a square matrix  [2.x.33]  and a vector  [2.x.34] . The sum   over rows can be parallelized and the whole code might look like this:  
* [1.x.2]
*      Here,  [2.x.35]  is called on the entire   range  [2.x.36]  if this range is less than the minimum   grainsize (above chosen as 50) or if deal.II is configured to not use   multithreading. Otherwise, it may be called on subsets of the given   range, with results from the individual subranges accumulated internally.      [2.x.37]  If ResultType is a floating point type, then accumulation is not   an associative operation. In other words, if the given function object is   called three times on three subranges, returning values  [2.x.38] , then the   returned result of this function is  [2.x.39] . However, depending on how   the three sub-tasks are distributed on available CPU resources, the   result may also be  [2.x.40]  or any other permutation; because floating   point addition is not associative (as opposed, of course, to addition of   real %numbers), the result of invoking this function several times may   differ on the order of round-off.     For a discussion of the kind of problems to which this function is   applicable, see also the    [2.x.41]  "Parallel computing with multiple processors"   module.  
* [0.x.29]*
   A class that wraps a TBB affinity partitioner in a thread-safe way. In   Vector, we use a shared pointer to share an affinity partitioner   between different vectors of the same size for improving data (and   NUMA) locality. However, when an outer task does multiple vector   operations, the shared pointer could lead to race conditions. This   class only allows one instance to get a partitioner. The other objects   cannot use that object and need to create their own copy.  
* [0.x.30]*
       Constructor.      
* [0.x.31]*
       Destructor. Check that the object is not in use any more, i.e., all       loops have been completed.      
* [0.x.32]*
       Return an affinity partitioner. In case the partitioner owned by the       class is free, it is returned here. In case another thread has not       released it yet, a new object is created. To free the partitioner       again, return it by the release_one_partitioner() call.      
* [0.x.33]*
       After using the partitioner in a tbb loop through       acquire_one_partitioner(), this call makes the partitioner available       again.      
* [0.x.34]*
       The stored partitioner that can accumulate knowledge over several       runs of  [2.x.42]       
* [0.x.35]*
       A flag to indicate whether the partitioner has been acquired but not       released yet, i.e., it is in use somewhere else.      
* [0.x.36]*
       A mutex to guard the access to the in_use flag.      
* [0.x.37]*
     If we do computations on vectors in parallel (say, we add two vectors     to get a third, and we do the loop over all elements in parallel), then     this variable determines the minimum number of elements for which it is     profitable to split a range of elements any further to distribute to     different threads.         This variable is available as a global writable variable in order to     allow the testsuite to also test the parallel case. By default, it is     set to several thousand elements, which is a case that the testsuite     would not normally encounter. As a consequence, in the testsuite we set     it to one
* 
*  -  a value that's hugely unprofitable but definitely tests     parallel operations.    
* [0.x.38]*
     Like  [2.x.43]  but now     denoting the number of rows of a matrix that should be worked on as a     minimum.    
* [0.x.39]*
     This is the function actually called by TBB for the ParallelForInteger     class.    
* [0.x.40]