[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27]
* [1.x.28][1.x.29][1.x.30]
* 

* In this program, we will mainly consider two aspects: [2.x.2]    [2.x.3]  Verification of correctness of the program and generation of convergence  tables;   [2.x.4]  Non-homogeneous Neumann boundary conditions for the Helmholtz equation. [2.x.5] Besides these topics, again a variety of improvements and tricks will beshown.
* 

* [1.x.31][1.x.32]
* 

* There has probably never been anon-trivial finite element program that worked right from the start. It istherefore necessary to find ways to verify whether a computed solution iscorrect or not. Usually, this is done by choosing the set-up of a simulationin such a way that we know the exact continuous solution and evaluate the differencebetween continuous and computed discrete solution. If this differenceconverges to zero with the right order of convergence, this is already a goodindication of correctness, although there may be other sources of errorpersisting which have only a small contribution to the total error or are ofhigher order. In the context of finite element simulations, this techniqueof picking the solution by choosing appropriate right hand sides andboundary conditionsis often called the [1.x.33].
* In this example, we will not go into the theories of systematic softwareverification which is a very complicated problem. Rather we will demonstratethe tools which deal.II can offer in this respect. This is basically centeredaround the functionality of a single function,  [2.x.6] This function computes the difference between a given continuous function anda finite element field in various norms on each cell.Of course, like with any other integral, we can only evaluate these norms using quadrature formulas;the choice of the right quadrature formula is therefore crucial to theaccurate evaluation of the error. This holds in particular for the  [2.x.7] norm, where we evaluate the maximal deviation of numerical and exact solutiononly at the quadrature points; one should then not try to use a quadraturerule whose evaluation occurs only at points where[super-convergence](https://en.wikipedia.org/wiki/Superconvergence) might occur, such asthe Gauss points of the lowest-order Gauss quadrature formula for which theintegrals in the assembly of the matrix is correct (e.g., for linear elements,do not use the QGauss(2) quadrature formula). In fact, this is generally goodadvice also for the other norms: if your quadrature points are fortuitouslychosen at locations where the error happens to be particularly small due tosuperconvergence, the computed error will look like it is much smaller thanit really is and may even suggest a higher convergence order. Consequently,we will choose a different quadrature formula for the integration of theseerror norms than for the assembly of the linear system.
* The function  [2.x.8]  evaluates the desired norm on eachcell  [2.x.9]  of the triangulation and returns a vector which holds thesevalues for each cell. From the local values, we can then obtain the global error. Forexample, if the vector  [2.x.10]  with element  [2.x.11]  for all cells [2.x.12]  contains the local  [2.x.13]  norms  [2.x.14] , then[1.x.34]is the global  [2.x.15]  error  [2.x.16] .
* In the program, we will show how to evaluate and use these quantities, and wewill monitor their values under mesh refinement. Of course, we have to choosethe problem at hand such that we can explicitly state the solution and itsderivatives, but since we want to evaluate the correctness of the program,this is only reasonable. If we know that the program produces the correctsolution for one (or, if one wants to be really sure: many) specificallychosen right hand sides, we can be rather confident that it will also computethe correct solution for problems where we don't know the exact values.
* In addition to simply computing these quantities, we will show how to generatenicely formatted tables from the data generated by this program thatautomatically computes convergence rates etc. In addition, we will comparedifferent strategies for mesh refinement.
* 

* [1.x.35][1.x.36]
* 

* The second, totallyunrelated, subject of this example program is the use of non-homogeneousboundary conditions. These are included into the variational form usingboundary integrals which we have to evaluate numerically when assembling theright hand side vector.
* Before we go into programming, let's have a brief look at the mathematicalformulation. The equation that we want to solve here is the Helmholtz equation"with the nice sign":[1.x.37]on the square  [2.x.17]  with  [2.x.18] , augmented by Dirichlet boundary conditions[1.x.38]on some part  [2.x.19]  of the boundary  [2.x.20] , and Neumann conditions[1.x.39]on the rest  [2.x.21] .In our particular testcase, we will use  [2.x.22] .(We say that this equation has the "nice sign" because the operator [2.x.23]  with the identity  [2.x.24]  and  [2.x.25]  is a positive definiteoperator; the [1.x.40] is  [2.x.26]  and results from modelingtime-harmonic processes. The operator is not positivedefinite if  [2.x.27]  is large, and this leads to all sorts of issueswe need not discuss here. The operator may also not be invertible
* 
*  - i.e., the equation does not have a unique solution
* 
*  -  if  [2.x.28] happens to be one of the eigenvalues of  [2.x.29] .)
* Because we want to verify the convergence of our numerical solution  [2.x.30] ,we want a setup so that we know the exact solution  [2.x.31] . This is wherethe Method of Manufactured Solutions comes in. To this end, let uschoose a function[1.x.41]where the centers  [2.x.32]  of the exponentials are   [2.x.33] ,   [2.x.34] , and   [2.x.35] ,and the half width is set to  [2.x.36] . The method of manufacturedsolution then says: choose
* [1.x.42]
* With this particular choice, we infer that of course the solution of theoriginal problem happens to be  [2.x.37] . In other words, by choosingthe right hand sides of the equation and the boundary conditions in aparticular way, we have manufactured ourselves a problem to which weknow the solution. This allows us then to compute the error of ournumerical solution. In the code below, we represent  [2.x.38]  by the [2.x.39]  class, and other classes will be used todenote  [2.x.40]  and  [2.x.41] .
* Using the above definitions, we can state the weak formulation of theequation, which reads: find  [2.x.42]  suchthat[1.x.43]for all test functions  [2.x.43] . Theboundary term  [2.x.44]  has appeared by integration by parts andusing  [2.x.45]  on  [2.x.46]  and  [2.x.47]  on  [2.x.48] . The cellmatrices and vectors which we use to build the global matrices and right handside vectors in the discrete formulation therefore look like this:[1.x.44]
* Since the generation of the domain integrals has been shown in previousexamples several times, only the generation of the contour integral is ofinterest here. It basically works along the following lines: for domainintegrals we have the  [2.x.49]  class that provides values andgradients of the shape values, as well as Jacobian determinants and otherinformation and specified quadrature points in the cell; likewise, there is aclass  [2.x.50]  that performs these tasks for integrations onfaces of cells. One provides it with a quadrature formula for a manifold withdimension one less than the dimension of the domain is, and the cell and thenumber of its face on which we want to perform the integration. The class willthen compute the values, gradients, normal vectors, weights, etc. at thequadrature points on this face, which we can then use in the same way as forthe domain integrals. The details of how this is done are shown in thefollowing program.
* 

* [1.x.45][1.x.46]
* 

* Besides the mathematical topics outlined above, we also want to use thisprogram to illustrate one aspect of good programming practice, namely the useof namespaces. In programming the deal.II library, we have take great care notto use names for classes and global functions that are overly generic, say [2.x.51]  etc. Furthermore, we have put everything intonamespace  [2.x.52] . But when one writes application programs thataren't meant for others to use, one doesn't always pay this much attention. Ifyou follow the programming style of  [2.x.53]  through  [2.x.54] , these functionsthen end up in the global namespace where, unfortunately, a lot of other stuffalso lives (basically everything the C language provides, along witheverything you get from the operating system through header files). To makethings a bit worse, the designers of the C language were also not alwayscareful in avoiding generic names; for example, the symbols <code>j1,jn</code> are defined in C header files (they denote Bessel functions).
* To avoid the problems that result if names of different functions or variablescollide (often with confusing error messages), it is good practice to puteverything you do into a [1.x.47]. Followingthis style, we will open a namespace  [2.x.55]  at the top of theprogram, import the deal.II namespace into it, put everything that's specificto this program (with the exception of  [2.x.56] , which must be inthe global namespace) into it, and only close it at the bottom of the file. Inother words, the structure of the program is of the kind
* [1.x.48]
* We will follow this scheme throughout the remainder of the deal.II tutorial.
* 

*  [1.x.49] [1.x.50]
*   [1.x.51]  [1.x.52]
* 

* 
*  These first include files have all been treated in previous examples, so we won't explain what is in them again.
* 

* 
* [1.x.53]
* 
*  In this example, we will not use the numeration scheme which is used per default by the DoFHandler class, but will renumber them using the Cuthill-McKee algorithm. As has already been explained in  [2.x.57] , the necessary functions are declared in the following file:
* 

* 
* [1.x.54]
* 
*  Then we will show a little trick how we can make sure that objects are not deleted while they are still in use. For this purpose, deal.II has the SmartPointer helper class, which is declared in this file:
* 

* 
* [1.x.55]
* 
*  Next, we will want to use the function  [2.x.58]  mentioned in the introduction, and we are going to use a ConvergenceTable that collects all important data during a run and prints it at the end as a table. These comes from the following two files:
* 

* 
* [1.x.56]
* 
*  And finally, we need to use the FEFaceValues class, which is declared in the same file as the FEValues class:
* 

* 
* [1.x.57]
* 
*  The last step before we go on with the actual implementation is to open a namespace  [2.x.59]  into which we will put everything, as discussed at the end of the introduction, and to import the members of namespace  [2.x.60]  into it:
* 

* 
* [1.x.58]
* 
*   [1.x.59]  [1.x.60]
* 

* 
*  Before implementing the classes that actually solve something, we first declare and define some function classes that represent right hand side and solution classes. Since we want to compare the numerically obtained solution to the exact continuous one, we need a function object that represents the continuous solution. On the other hand, we need the right hand side function, and that one of course shares some characteristics with the solution. In order to reduce dependencies which arise if we have to change something in both classes at the same time, we move the common characteristics of both functions into a base class.   
*   The common characteristics for solution (as explained in the introduction, we choose a sum of three exponentials) and right hand side, are these: the number of exponentials, their centers, and their half width. We declare them in the following class. Since the number of exponentials is a compile-time constant we use a fixed-length  [2.x.61]  to store the center points:
* 

* 
* [1.x.61]
* 
*  The variables which denote the centers and the width of the exponentials have just been declared, now we still need to assign values to them. Here, we can show another small piece of template sorcery, namely how we can assign different values to these variables depending on the dimension. We will only use the 2d case in the program, but we show the 1d case for exposition of a useful technique.   
*   First we assign values to the centers for the 1d case, where we place the centers equidistantly at
* 
*  - /3, 0, and 1/3. The <code>template &lt;&gt;</code> header for this definition indicates an explicit specialization. This means, that the variable belongs to a template, but that instead of providing the compiler with a template from which it can specialize a concrete variable by substituting  [2.x.62]  with some concrete value, we provide a specialization ourselves, in this case for  [2.x.63] . If the compiler then sees a reference to this variable in a place where the template argument equals one, it knows that it doesn't have to generate the variable from a template by substituting  [2.x.64] , but can immediately use the following definition:
* 

* 
* [1.x.62]
* 
*  Likewise, we can provide an explicit specialization for  [2.x.65] . We place the centers for the 2d case as follows:
* 

* 
* [1.x.63]
* 
*  There remains to assign a value to the half-width of the exponentials. We would like to use the same value for all dimensions. In this case, we simply provide the compiler with a template from which it can generate a concrete instantiation by substituting  [2.x.66]  with a concrete value:
* 

* 
* [1.x.64]
* 
*  After declaring and defining the characteristics of solution and right hand side, we can declare the classes representing these two. They both represent continuous functions, so they are derived from the Function&lt;dim&gt; base class, and they also inherit the characteristics defined in the SolutionBase class.   
*   The actual classes are declared in the following. Note that in order to compute the error of the numerical solution against the continuous one in the L2 and H1 (semi-)norms, we have to provide value and gradient of the exact solution. This is more than we have done in previous examples, where all we provided was the value at one or a list of points. Fortunately, the Function class also has virtual functions for the gradient, so we can simply overload the respective virtual member functions in the Function base class. Note that the gradient of a function in  [2.x.67]  space dimensions is a vector of size  [2.x.68] , i.e. a tensor of rank 1 and dimension  [2.x.69] . As for so many other things, the library provides a suitable class for this. One new thing about this class is that it explicitly uses the Tensor objects, which previously appeared as intermediate terms in  [2.x.70]  and  [2.x.71] . A tensor is a generalization of scalars (rank zero tensors), vectors (rank one tensors), and matrices (rank two tensors), as well as higher dimensional objects. The Tensor class requires two template arguments: the tensor rank and tensor dimension. For example, here we use tensors of rank one (vectors) with dimension  [2.x.72]  entries.) While this is a bit less flexible than using Vector, the compiler can generate faster code when the length of the vector is known at compile time. Additionally, specifying a Tensor of rank one and dimension  [2.x.73]  guarantees that the tensor will have the right shape (since it is built into the type of the object itself), so the compiler can catch most size-related mistakes for us.
* 

* 
* [1.x.65]
* 
*  The actual definition of the values and gradients of the exact solution class is according to their mathematical definition and does not need much explanation.   
*   The only thing that is worth mentioning is that if we access elements of a base class that is template dependent (in this case the elements of SolutionBase&lt;dim&gt;), then the C++ language forces us to write  [2.x.74] , and similarly for other members of the base class. C++ does not require the  [2.x.75]  qualification if the base class is not template dependent. The reason why this is necessary is complicated; C++ books will explain under the phrase [1.x.66], and there is also a lengthy description in the deal.II FAQs.
* 

* 
* [1.x.67]
* 
*  Likewise, this is the computation of the gradient of the solution.  In order to accumulate the gradient from the contributions of the exponentials, we allocate an object  [2.x.76]  that denotes the mathematical quantity of a tensor of rank  [2.x.77]  and dimension  [2.x.78] . Its default constructor sets it to the vector containing only zeroes, so we need not explicitly care for its initialization.   
*   Note that we could as well have taken the type of the object to be Point&lt;dim&gt; instead of Tensor&lt;1,dim&gt;. Tensors of rank 1 and points are almost exchangeable, and have only very slightly different mathematical meanings. In fact, the Point&lt;dim&gt; class is derived from the Tensor&lt;1,dim&gt; class, which makes up for their mutual exchange ability. Their main difference is in what they logically mean: points are points in space, such as the location at which we want to evaluate a function (see the type of the first argument of this function for example). On the other hand, tensors of rank 1 share the same transformation properties, for example that they need to be rotated in a certain way when we change the coordinate system; however, they do not share the same connotation that points have and are only objects in a more abstract space than the one spanned by the coordinate directions. (In fact, gradients live in `reciprocal' space, since the dimension of their components is not that of a length, but of one over length).
* 

* 
* [1.x.68]
* 
*  For the gradient, note that its direction is along (x-x_i), so we add up multiples of this distance vector, where the factor is given by the exponentials.
* 

* 
* [1.x.69]
* 
*  Besides the function that represents the exact solution, we also need a function which we can use as right hand side when assembling the linear system of discretized equations. This is accomplished using the following class and the following definition of its function. Note that here we only need the value of the function, not its gradients or higher derivatives.
* 

* 
* [1.x.70]
* 
*  The value of the right hand side is given by the negative Laplacian of the solution plus the solution itself, since we wanted to solve Helmholtz's equation:
* 

* 
* [1.x.71]
* 
*  The first contribution is the Laplacian:
* 

* 
* [1.x.72]
* 
*  And the second is the solution itself:
* 

* 
* [1.x.73]
* 
*   [1.x.74]  [1.x.75]
* 

* 
*  Then we need the class that does all the work. Except for its name, its interface is mostly the same as in previous examples.   
*   One of the differences is that we will use this class in several modes: for different finite elements, as well as for adaptive and global refinement. The decision whether global or adaptive refinement shall be used is communicated to the constructor of this class through an enumeration type declared at the top of the class. The constructor then takes a finite element object and the refinement mode as arguments.   
*   The rest of the member functions are as before except for the  [2.x.79]  function: After the solution has been computed, we perform some analysis on it, such as computing the error in various norms. To enable some output, it requires the number of the refinement cycle, and consequently gets it as an argument.
* 

* 
* [1.x.76]
* 
*  Now for the data elements of this class. Among the variables that we have already used in previous examples, only the finite element object differs: The finite elements which the objects of this class operate on are passed to the constructor of this class. It has to store a pointer to the finite element for the member functions to use. Now, for the present class there is no big deal in that, but since we want to show techniques rather than solutions in these programs, we will here point out a problem that often occurs
* 
*  -  and of course the right solution as well.     
*   Consider the following situation that occurs in all the example programs: we have a triangulation object, and we have a finite element object, and we also have an object of type DoFHandler that uses both of the first two. These three objects all have a lifetime that is rather long compared to most other objects: they are basically set at the beginning of the program or an outer loop, and they are destroyed at the very end. The question is: can we guarantee that the two objects which the DoFHandler uses, live at least as long as they are in use? This means that the DoFHandler must have some kind of knowledge on the destruction of the other objects.     
*   We will show here how the library managed to find out that there are still active references to an object and the object is still alive from the point of view of a using object. Basically, the method is along the following line: all objects that are subject to such potentially dangerous pointers are derived from a class called Subscriptor. For example, the Triangulation, DoFHandler, and a base class of the FiniteElement class are derived from Subscriptor. This latter class does not offer much functionality, but it has a built-in counter which we can subscribe to, thus the name of the class. Whenever we initialize a pointer to that object, we can increase its use counter, and when we move away our pointer or do not need it any more, we decrease the counter again. This way, we can always check how many objects still use that object. Additionally, the class requires to know about a pointer that it can use to tell the subscribing object about its invalidation.     
*   If an object of a class that is derived from the Subscriptor class is destroyed, it also has to call the destructor of the Subscriptor class. In this destructor, we tell all the subscribing objects about the invalidation of the object using the stored pointers. The same happens when the object appears on the right hand side of a move expression, i.e., it will no longer contain valid content after the operation. The subscribing class is expected to check the value stored in its corresponding pointer before trying to access the object subscribed to.     
*   This is exactly what the SmartPointer class is doing. It basically acts just like a pointer, i.e. it can be dereferenced, can be assigned to and from other pointers, and so on. On top of that it uses the mechanism described above to find out if the pointer this class is representing is dangling when we try to dereference it. In that case an exception is thrown.     
*   In the present example program, we want to protect the finite element object from the situation that for some reason the finite element pointed to is destroyed while still in use. We therefore use a SmartPointer to the finite element object; since the finite element object is actually never changed in our computations, we pass a const FiniteElement&lt;dim&gt; as template argument to the SmartPointer class. Note that the pointer so declared is assigned at construction time of the solve object, and destroyed upon destruction, so the lock on the destruction of the finite element object extends throughout the lifetime of this HelmholtzProblem object.
* 

* 
* [1.x.77]
* 
*  The second to last variable stores the refinement mode passed to the constructor. Since it is only set in the constructor, we can declare this variable constant, to avoid that someone sets it involuntarily (e.g. in an `if'-statement where == was written as = by chance).
* 

* 
* [1.x.78]
* 
*  For each refinement level some data (like the number of cells, or the L2 error of the numerical solution) will be generated and later printed. The TableHandler can be used to collect all this data and to output it at the end of the run as a table in a simple text or in LaTeX format. Here we don't only use the TableHandler but we use the derived class ConvergenceTable that additionally evaluates rates of convergence:
* 

* 
* [1.x.79]
* 
*   [1.x.80]  [1.x.81]
* 

* 
*   [1.x.82]  [1.x.83]
* 

* 
*  In the constructor of this class, we only set the variables passed as arguments, and associate the DoF handler object with the triangulation (which is empty at present, however).
* 

* 
* [1.x.84]
* 
*   [1.x.85]  [1.x.86]
* 

* 
*  The following function sets up the degrees of freedom, sizes of matrices and vectors, etc. Most of its functionality has been showed in previous examples, the only difference being the renumbering step immediately after first distributing degrees of freedom.   
*   Renumbering the degrees of freedom is not overly difficult, as long as you use one of the algorithms included in the library. It requires only a single line of code. Some more information on this can be found in  [2.x.80] .   
*   Note, however, that when you renumber the degrees of freedom, you must do so immediately after distributing them, since such things as hanging nodes, the sparsity pattern etc. depend on the absolute numbers which are altered by renumbering.   
*   The reason why we introduce renumbering here is that it is a relatively cheap operation but often has a beneficial effect: While the CG iteration itself is independent of the actual ordering of degrees of freedom, we will use SSOR as a preconditioner. SSOR goes through all degrees of freedom and does some operations that depend on what happened before; the SSOR operation is therefore not independent of the numbering of degrees of freedom, and it is known that its performance improves by using renumbering techniques. A little experiment shows that indeed, for example, the number of CG iterations for the fifth refinement cycle of adaptive refinement with the Q1 program used here is 40 without, but 36 with renumbering. Similar savings can generally be observed for all the computations in this program.
* 

* 
* [1.x.87]
* 
*   [1.x.88]  [1.x.89]
* 

* 
*  Assembling the system of equations for the problem at hand is mostly as for the example programs before. However, some things have changed anyway, so we comment on this function fairly extensively.   
*   At the top of the function you will find the usual assortment of variable declarations. Compared to previous programs, of importance is only that we expect to solve problems also with bi-quadratic elements and therefore have to use sufficiently accurate quadrature formula. In addition, we need to compute integrals over faces, i.e.  [2.x.81]  dimensional objects. The declaration of a face quadrature formula is then straightforward:
* 

* 
* [1.x.90]
* 
*  Then we need objects which can evaluate the values, gradients, etc of the shape functions at the quadrature points. While it seems that it should be feasible to do it with one object for both domain and face integrals, there is a subtle difference since the weights in the domain integrals include the measure of the cell in the domain, while the face integral quadrature requires the measure of the face in a lower-dimensional manifold. Internally these two classes are rooted in a common base class which does most of the work and offers the same interface to both domain and interface integrals.     
*   For the domain integrals in the bilinear form for Helmholtz's equation, we need to compute the values and gradients, as well as the weights at the quadrature points. Furthermore, we need the quadrature points on the real cell (rather than on the unit cell) to evaluate the right hand side function. The object we use to get at this information is the FEValues class discussed previously.     
*   For the face integrals, we only need the values of the shape functions, as well as the weights. We also need the normal vectors and quadrature points on the real cell since we want to determine the Neumann values from the exact solution object (see below). The class that gives us this information is called FEFaceValues:
* 

* 
* [1.x.91]
* 
*  Then we need some objects already known from previous examples: An object denoting the right hand side function, its values at the quadrature points on a cell, the cell matrix and right hand side, and the indices of the degrees of freedom on a cell.     
*   Note that the operations we will do with the right hand side object are only querying data, never changing the object. We can therefore declare it  [2.x.82] :
* 

* 
* [1.x.92]
* 
*  Finally we define an object denoting the exact solution function. We will use it to compute the Neumann values at the boundary from it. Usually, one would of course do so using a separate object, in particular since the exact solution is generally unknown while the Neumann values are prescribed. We will, however, be a little bit lazy and use what we already have in information. Real-life programs would to go other ways here, of course.
* 

* 
* [1.x.93]
* 
*  Now for the main loop over all cells. This is mostly unchanged from previous examples, so we only comment on the things that have changed.
* 

* 
* [1.x.94]
* 
*  The first thing that has changed is the bilinear form. It now contains the additional term from the Helmholtz equation:
* 

* 
* [1.x.95]
* 
*  Then there is that second term on the right hand side, the contour integral. First we have to find out whether the intersection of the faces of this cell with the boundary part Gamma2 is nonzero. To this end, we loop over all faces and check whether its boundary indicator equals  [2.x.83] , which is the value that we have assigned to that portions of the boundary composing Gamma2 in the  [2.x.84]  function further below. (The default value of boundary indicators is  [2.x.85] , so faces can only have an indicator equal to  [2.x.86]  if we have explicitly set it.)
* 

* 
* [1.x.96]
* 
*  If we came into here, then we have found an external face belonging to Gamma2. Next, we have to compute the values of the shape functions and the other quantities which we will need for the computation of the contour integral. This is done using the  [2.x.87]  function which we already know from the FEValue class:
* 

* 
* [1.x.97]
* 
*  And we can then perform the integration by using a loop over all quadrature points.               
*   On each quadrature point, we first compute the value of the normal derivative. We do so using the gradient of the exact solution and the normal vector to the face at the present quadrature point obtained from the  [2.x.88]  object. This is then used to compute the additional contribution of this face to the right hand side:
* 

* 
* [1.x.98]
* 
*  Now that we have the contributions of the present cell, we can transfer it to the global matrix and right hand side vector, as in the examples before:
* 

* 
* [1.x.99]
* 
*  Likewise, elimination and treatment of boundary values has been shown previously.     
*   We note, however that now the boundary indicator for which we interpolate boundary values (denoted by the second parameter to  [2.x.89] ) does not represent the whole boundary any more. Rather, it is that portion of the boundary which we have not assigned another indicator (see below). The degrees of freedom at the boundary that do not belong to Gamma1 are therefore excluded from the interpolation of boundary values, just as we want.
* 

* 
* [1.x.100]
* 
*   [1.x.101]  [1.x.102]
* 

* 
*  Solving the system of equations is done in the same way as before:
* 

* 
* [1.x.103]
* 
*   [1.x.104]  [1.x.105]
* 

* 
*  Now for the function doing grid refinement. Depending on the refinement mode passed to the constructor, we do global or adaptive refinement.   
*   Global refinement is simple, so there is not much to comment on.  In case of adaptive refinement, we use the same functions and classes as in the previous example program. Note that one could treat Neumann boundaries differently than Dirichlet boundaries, and one should in fact do so here since we have Neumann boundary conditions on part of the boundaries, but since we don't have a function here that describes the Neumann values (we only construct these values from the exact solution when assembling the matrix), we omit this detail even though doing this in a strictly correct way would not be hard to add.   
*   At the end of the switch, we have a default case that looks slightly strange: an  [2.x.90]  condition. Since the  [2.x.91]  macro raises an error whenever the condition is false, this means that whenever we hit this statement the program will be aborted. This in intentional: Right now we have only implemented two refinement strategies (global and adaptive), but someone might want to add a third strategy (for example adaptivity with a different refinement criterion) and add a third member to the enumeration that determines the refinement mode. If it weren't for the default case of the switch statement, this function would simply run to its end without doing anything. This is most likely not what was intended. One of the defensive programming techniques that you will find all over the deal.II library is therefore to always have default cases that abort, to make sure that values not considered when listing the cases in the switch statement are eventually caught, and forcing programmers to add code to handle them. We will use this same technique in other places further down as well.
* 

* 
* [1.x.106]
* 
*   [1.x.107]  [1.x.108]
* 

* 
*  Finally we want to process the solution after it has been computed. For this, we integrate the error in various (semi-)norms, and we generate tables that will later be used to display the convergence against the continuous solution in a nice format.
* 

* 
* [1.x.109]
* 
*  Our first task is to compute error norms. In order to integrate the difference between computed numerical solution and the continuous solution (described by the Solution class defined at the top of this file), we first need a vector that will hold the norm of the error on each cell. Since accuracy with 16 digits is not so important for these quantities, we save some memory by using  [2.x.92]  instead of  [2.x.93]  values.     
*   The next step is to use a function from the library which computes the error in the L2 norm on each cell.  We have to pass it the DoF handler object, the vector holding the nodal values of the numerical solution, the continuous solution as a function object, the vector into which it shall place the norm of the error on each cell, a quadrature rule by which this norm shall be computed, and the type of norm to be used. Here, we use a Gauss formula with three points in each space direction, and compute the L2 norm.     
*   Finally, we want to get the global L2 norm. This can of course be obtained by summing the squares of the norms on each cell, and taking the square root of that value. This is equivalent to taking the l2 (lower case  [2.x.94] ) norm of the vector of norms on each cell:
* 

* 
* [1.x.110]
* 
*  By same procedure we get the H1 semi-norm. We re-use the  [2.x.95]  vector since it is no longer used after computing the  [2.x.96]  variable above. The global  [2.x.97]  semi-norm error is then computed by taking the sum of squares of the errors on each individual cell, and then the square root of it
* 
*  -  an operation that is conveniently performed by  [2.x.98] 
* 

* 
* [1.x.111]
* 
*  Finally, we compute the maximum norm. Of course, we can't actually compute the true maximum of the error overall* points in the domain, but only the maximum over a finite set of evaluation points that, for convenience, we will still call "quadrature points" and represent by an object of type Quadrature even though we do not actually perform any integration.     
*   There is then the question of what points precisely we want to evaluate at. It turns out that the result we get depends quite sensitively on the "quadrature" points being used. There is also the issue of superconvergence: Finite element solutions are, on some meshes and for polynomial degrees  [2.x.99] , particularly accurate at the node points as well as at Gauss-Lobatto points, much more accurate than at randomly chosen points. (See  [2.x.100]  and the discussion and references in Section 1.2 for more information on this.) In other words, if we are interested in finding the largest difference  [2.x.101] , then we ought to look at points  [2.x.102]  that are specifically not of this "special" kind of points and we should specifically not use `QGauss(fe->degree+1)` to define where we evaluate. Rather, we use a special quadrature rule that is obtained by iterating the trapezoidal rule by the degree of the finite element times two plus one in each space direction. Note that the constructor of the QIterated class takes a one-dimensional quadrature rule and a number that tells it how often it shall repeat this rule in each space direction.     
*   Using this special quadrature rule, we can then try to find the maximal error on each cell. Finally, we compute the global L infinity error from the L infinity errors on each cell with a call to  [2.x.103] 
* 

* 
* [1.x.112]
* 
*  After all these errors have been computed, we finally write some output. In addition, we add the important data to the TableHandler by specifying the key of the column and the value.  Note that it is not necessary to define column keys beforehand
* 
*  -  it is sufficient to just add values, and columns will be introduced into the table in the order values are added the first time.
* 

* 
* [1.x.113]
* 
*   [1.x.114]  [1.x.115]
* 

* 
*  As in previous example programs, the  [2.x.104]  function controls the flow of execution. The basic layout is as in previous examples: an outer loop over successively refined grids, and in this loop first problem setup, assembling the linear system, solution, and post-processing.   
*   The first task in the main loop is creation and refinement of grids. This is as in previous examples, with the only difference that we want to have part of the boundary marked as Neumann type, rather than Dirichlet.   
*   For this, we will use the following convention: Faces belonging to Gamma1 will have the boundary indicator  [2.x.105]  (which is the default, so we don't have to set it explicitly), and faces belonging to Gamma2 will use  [2.x.106]  as boundary indicator.  To set these values, we loop over all cells, then over all faces of a given cell, check whether it is part of the boundary that we want to denote by Gamma2, and if so set its boundary indicator to  [2.x.107] . For the present program, we consider the left and bottom boundaries as Gamma2. We determine whether a face is part of that boundary by asking whether the x or y coordinates (i.e. vector components 0 and 1) of the midpoint of a face equals
* 
*  - , up to some small wiggle room that we have to give since it is instable to compare floating point numbers that are subject to round off in intermediate computations.   
*   It is worth noting that we have to loop over all cells here, not only the active ones. The reason is that upon refinement, newly created faces inherit the boundary indicator of their parent face. If we now only set the boundary indicator for active faces, coarsen some cells and refine them later on, they will again have the boundary indicator of the parent cell which we have not modified, instead of the one we intended. Consequently, we have to change the boundary indicators of faces of all cells on Gamma2, whether they are active or not. Alternatively, we could of course have done this job on the coarsest mesh (i.e. before the first refinement step) and refined the mesh only after that.
* 

* 
* [1.x.116]
* 
*  The next steps are already known from previous examples. This is mostly the basic set-up of every finite element program:
* 

* 
* [1.x.117]
* 
*  The last step in this chain of function calls is usually the evaluation of the computed solution for the quantities one is interested in. This is done in the following function. Since the function generates output that indicates the number of the present refinement step, we pass this number as an argument.
* 

* 
* [1.x.118]
* 
*   [1.x.119]  [1.x.120]
* 

* 
*  After the last iteration we output the solution on the finest grid. This is done using the following sequence of statements which we have already discussed in previous examples. The first step is to generate a suitable filename (called  [2.x.108]  here, since we want to output data in VTK format; we add the prefix to distinguish the filename from that used for other output files further down below). Here, we augment the name by the mesh refinement algorithm, and as above we make sure that we abort the program if another refinement method is added and not handled by the following switch statement:
* 

* 
* [1.x.121]
* 
*  We augment the filename by a postfix denoting the finite element which we have used in the computation. To this end, the finite element base class stores the maximal polynomial degree of shape functions in each coordinate variable as a variable  [2.x.109] , and we use for the switch statement (note that the polynomial degree of bilinear shape functions is really 2, since they contain the term  [2.x.110] ; however, the polynomial degree in each coordinate variable is still only 1). We again use the same defensive programming technique to safeguard against the case that the polynomial degree has an unexpected value, using the  [2.x.111]  idiom in the default branch of the switch statement:
* 

* 
* [1.x.122]
* 
*  Once we have the base name for the output file, we add an extension appropriate for VTK output, open a file, and add the solution vector to the object that will do the actual output:
* 

* 
* [1.x.123]
* 
*  Now building the intermediate format as before is the next step. We introduce one more feature of deal.II here. The background is the following: in some of the runs of this function, we have used biquadratic finite elements. However, since almost all output formats only support bilinear data, the data is written only bilinear, and information is consequently lost.  Of course, we can't change the format in which graphic programs accept their inputs, but we can write the data differently such that we more closely resemble the information available in the quadratic approximation. We can, for example, write each cell as four sub-cells with bilinear data each, such that we have nine data points for each cell in the triangulation. The graphic programs will, of course, display this data still only bilinear, but at least we have given some more of the information we have.     
*   In order to allow writing more than one sub-cell per actual cell, the  [2.x.112]  function accepts a parameter (the default is  [2.x.113] , which is why you haven't seen this parameter in previous examples). This parameter denotes into how many sub-cells per space direction each cell shall be subdivided for output. For example, if you give  [2.x.114] , this leads to 4 cells in 2D and 8 cells in 3D. For quadratic elements, two sub-cells per space direction is obviously the right choice, so this is what we choose. In general, for elements of polynomial order  [2.x.115]  subdivisions, and the order of the elements is determined in the same way as above.     
*   With the intermediate format so generated, we can then actually write the graphical output:
* 

* 
* [1.x.124]
* 
*   [1.x.125]  [1.x.126]
* 

* 
*  After graphical output, we would also like to generate tables from the error computations we have done in  [2.x.116] . There, we have filled a table object with the number of cells for each refinement step as well as the errors in different norms.
* 

* 
*  For a nicer textual output of this data, one may want to set the precision with which the values will be written upon output. We use 3 digits for this, which is usually sufficient for error norms. By default, data is written in fixed point notation. However, for columns one would like to see in scientific notation another function call sets the  [2.x.117] , leading to floating point representation of numbers.
* 

* 
* [1.x.127]
* 
*  For the output of a table into a LaTeX file, the default captions of the columns are the keys given as argument to the  [2.x.118]  functions. To have TeX captions that differ from the default ones you can specify them by the following function calls. Note, that `\\' is reduced to `\' by the compiler such that the real TeX caption is, e.g., ` [2.x.119] -error'.
* 

* 
* [1.x.128]
* 
*  Finally, the default LaTeX format for each column of the table is `c' (centered). To specify a different (e.g. `right') one, the following function may be used:
* 

* 
* [1.x.129]
* 
*  After this, we can finally write the table to the standard output stream  [2.x.120]  (after one extra empty line, to make things look prettier). Note, that the output in text format is quite simple and that captions may not be printed directly above the specific columns.
* 

* 
* [1.x.130]
* 
*  The table can also be written into a LaTeX file.  The (nicely) formatted table can be viewed at after calling `latex filename' and e.g. `xdvi filename', where filename is the name of the file to which we will write output now. We construct the file name in the same way as before, but with a different prefix "error":
* 

* 
* [1.x.131]
* 
*   [1.x.132]  [1.x.133]
* 

* 
*  In case of global refinement, it might be of interest to also output the convergence rates. This may be done by the functionality the ConvergenceTable offers over the regular TableHandler. However, we do it only for global refinement, since for adaptive refinement the determination of something like an order of convergence is somewhat more involved. While we are at it, we also show a few other things that can be done with tables.
* 

* 
* [1.x.134]
* 
*  The first thing is that one can group individual columns together to form so-called super columns. Essentially, the columns remain the same, but the ones that were grouped together will get a caption running across all columns in a group. For example, let's merge the "cycle" and "cells" columns into a super column named "n cells":
* 

* 
* [1.x.135]
* 
*  Next, it isn't necessary to always output all columns, or in the order in which they were originally added during the run. Selecting and re-ordering the columns works as follows (note that this includes super columns):
* 

* 
* [1.x.136]
* 
*  For everything that happened to the ConvergenceTable until this point, it would have been sufficient to use a simple TableHandler. Indeed, the ConvergenceTable is derived from the TableHandler but it offers the additional functionality of automatically evaluating convergence rates. For example, here is how we can let the table compute reduction and convergence rates (convergence rates are the binary logarithm of the reduction rate):
* 

* 
* [1.x.137]
* 
*  Each of these function calls produces an additional column that is merged with the original column (in our example the `L2' and the `H1' column) to a supercolumn.
* 

* 
*  Finally, we want to write this convergence chart again, first to the screen and then, in LaTeX format, to disk. The filename is again constructed as above.
* 

* 
* [1.x.138]
* 
*  The final step before going to  [2.x.121]  is then to close the namespace  [2.x.122]  into which we have put everything we needed for this program:
* 

* 
* [1.x.139]
* 
*   [1.x.140]  [1.x.141]
* 

* 
*  The main function is mostly as before. The only difference is that we solve three times, once for Q1 and adaptive refinement, once for Q1 elements and global refinement, and once for Q2 elements and global refinement.
* 

* 
*  Since we instantiate several template classes below for two space dimensions, we make this more generic by declaring a constant at the beginning of the function denoting the number of space dimensions. If you want to run the program in 1d or 2d, you will then only have to change this one instance, rather than all uses below:
* 

* 
* [1.x.142]
* 
*  Now for the three calls to the main class. Each call is blocked into curly braces in order to destroy the respective objects (i.e. the finite element and the HelmholtzProblem object) at the end of the block and before we go to the next run. This avoids conflicts with variable names, and also makes sure that memory is released immediately after one of the three runs has finished, and not only at the end of the  [2.x.123]  block.
* 

* 
* [1.x.143]
* [1.x.144][1.x.145]
* 

* 
* The program generates two kinds of output. The first are the outputfiles  [2.x.124] , [2.x.125] , and [2.x.126] . We show the latter in a 3d viewhere:
* 

*  [2.x.127] 
* 

* 
* 

* Secondly, the program writes tables not only to disk, but also to thescreen while running. The output looks like the following (recall thatcolumns labeled as " [2.x.128] " actually show the  [2.x.129]  [1.x.146]normof the error, not the full  [2.x.130]  norm):
* 

* 
* [1.x.147]
* 
* 

* One can see the error reduction upon grid refinement, and for thecases where global refinement was performed, also the convergencerates can be seen. The linear and quadratic convergence rates of Q1and Q2 elements in the  [2.x.131]  semi-norm can clearly be seen, asare the quadratic and cubic rates in the  [2.x.132]  norm.
* 

* 
* 

* Finally, the program also generated LaTeX versions of the tables (not shownhere) that is written into a file in a way so that it could becopy-pasted into a LaTeX document.
* 

* [1.x.148][1.x.149]
* 

* What we showed above is how to determine the size of the error [2.x.133]  in a number of different norms. We did this primarilybecause we were interested in testing that our solutionsconverge*.But from an engineering perspective, the question is often morepractical: How fine do I have to make my mesh so that the error is"small enough"? In other words, if in the table above the  [2.x.134] semi-norm has been reduced to `4.121e-03`, is this good enough for meto sign the blueprint and declare that our numerical simulation showedthat the bridge is strong enough?
* In practice, we are rarely in this situation because I can nottypically compare the numerical solution  [2.x.135]  against the exactsolution  [2.x.136]  in situations that matter
* 
*  -  if I knew  [2.x.137] , I would nothave to compute  [2.x.138] . But even if I could, the question to ask ingeneral is then: `4.121e-03`what*? The solution will have physicalunits, say kg-times-meter-squared, and I'm integrating a function withunits square of the above over the domain, and then take the squareroot. So if the domain is two-dimensional, the units of [2.x.139]  are kg-times-meter-cubed. The question is then: Is [2.x.140]  kg-times-meter-cubed small? That depends on whatyou're trying to simulate: If you're an astronomer used to massesmeasured in solar masses and distances in light years, then yes, thisis a fantastically small number. But if you're doing atomic physics,then no: That's not small, and your error is most certainly notsufficiently small; you need a finer mesh.
* In other words, when we look at these sorts of numbers, we generallyneed to compare against a "scale". One way to do that is to not lookat theabsolute* error  [2.x.141]  in whatever norm, but at the
*relative* error  [2.x.142] . If this ratio is  [2.x.143] , thenyou know thaton average*, the difference between  [2.x.144]  and  [2.x.145]  is0.001 per cent
* 
*  -  probably small enough for engineering purposes.
* How do we compute  [2.x.146] ? We just need to do an integration loop overall cells, quadrature points on these cells, and then sum things upand take the square root at the end. But there is a simpler way oftenused: You can call
* [1.x.150]
* which computes  [2.x.147] . Alternatively, if you're particularlylazy and don't feel like creating the `zero_vector`, you could usethat if the mesh is not too coarse, then  [2.x.148] , andwe can compute  [2.x.149]  by calling
* [1.x.151]
* In both cases, one then only has to combine the vector of cellwisenorms into one global norm as we already do in the program, by calling
* [1.x.152]
* 
* 

* 
* [1.x.153][1.x.154]
* 

* [1.x.155][1.x.156]
* 

* Go ahead and run the program with higher order elements ( [2.x.150] ,  [2.x.151] , ...). Youwill notice that assertions in several parts of the code will trigger (forexample in the generation of the filename for the data output). You might have to address these,but it should not be very hard to get the program to work!
* [1.x.157][1.x.158]
* 

* Is Q1 or Q2 better? What about adaptive versus global refinement? A (somewhatunfair but typical) metric to compare them, is to look at the error as afunction of the number of unknowns.
* To see this, create a plot in log-log style with the number of unknowns on the [2.x.152]  axis and the  [2.x.153]  error on the  [2.x.154]  axis. You can add reference lines for [2.x.155]  and  [2.x.156]  and check that global and adaptive refinementfollow those. If one makes the (not completely unreasonable)assumption that with a good linear solver, the computational effort isproportional to the number of unknowns  [2.x.157] , then it is clear that anerror reduction of  [2.x.158]  is substantially better than areduction of the form  [2.x.159] : That is, that adaptiverefinement gives us the desired error level with less computationalwork than if we used global refinement. This is not a particularlysurprising conclusion, but it's worth checking these sorts ofassumptions in practice.
* Of course, a fairer comparison would be to plot runtime (switch to releasemode first!) instead of number of unknowns on the  [2.x.160]  axis. If youplotted run time against the number of unknowns by timing eachrefinement step (e.g., using the Timer class), you will notice thatthe linear solver is not perfect
* 
*  -  its run time grows faster thanproportional to the linear system size
* 
*  -  and picking a betterlinear solver might be appropriate for this kind of comparison.
* 

* [1.x.159][1.x.160] [2.x.161] 
* [0.x.1]