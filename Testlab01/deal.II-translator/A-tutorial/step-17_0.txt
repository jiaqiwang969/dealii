[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21]
* [1.x.22][1.x.23][1.x.24]
* 

* [1.x.25][1.x.26]
* 

* This program does not introduce any new mathematical ideas; in fact, all itdoes is to do the exact same computations that  [2.x.2] already does, but it does so in a different manner: instead of using deal.II'sown linear algebra classes, we build everything on top of classes deal.IIprovides that wrap around the linear algebra implementation of the [1.x.27] library. Andsince PETSc allows to distribute matrices and vectors across several computerswithin an MPI network, the resulting code will even be able to solve theproblem in %parallel. If you don't know what PETSc is, then this would be agood time to take a quick glimpse at their homepage.
* As a prerequisite of this program, you need to have PETSc installed, and ifyou want to run in %parallel on a cluster, you also need [1.x.28] to partition meshes. The installation of deal.IItogether with these two additional libraries is described in the [1.x.29] file.
* Now, for the details: as mentioned, the program does not compute anything new,so the use of finite element classes, etc., is exactly the same as before. Thedifference to previous programs is that we have replaced almost all uses ofclasses  [2.x.3]  by theirnear-equivalents  [2.x.4]  and [2.x.5]  that store data in a way so thatevery processor in the MPI network only storesa part of the matrix or vector. More specifically, each processor willonly store those rows of the matrix that correspond to a degree offreedom it "owns". For vectors, they either store only elements thatcorrespond to degrees of freedom the processor owns (this is what isnecessary for the right hand side), or also some additional elementsthat make sure that every processor has access the solution componentsthat live on the cells the processor owns (so-called [2.x.6]  "locally active DoFs") or also on neighboring cells(so-called  [2.x.7]  "locally relevant DoFs").
* The interface the classes from the PETScWrapper namespace provide is very similar to thatof the deal.II linear algebra classes, but instead of implementing thisfunctionality themselves, they simply pass on to their corresponding PETScfunctions. The wrappers are therefore only used to give PETSc a more modern,object oriented interface, and to make the use of PETSc and deal.II objects asinterchangeable as possible. The main point of using PETSc is that it can runin %parallel. We will make use of this by partitioning the domain into as manyblocks ("subdomains") as there are processes in the MPI network. At the sametime, PETSc also provides dummy MPI stubs, so you can run this program on asingle machine if PETSc was configured without MPI.
* 

* [1.x.30][1.x.31]
* 

* Developing software to run in %parallel via MPI requires a bit of a change inmindset because one typically has to split up all data structures so thatevery processor only stores a piece of the entire problem. As a consequence,you can't typically access all components of a solution vector on eachprocessor
* 
*  -  each processor may simply not have enough memory to hold theentire solution vector. Because data is split up or "distributed" acrossprocessors, we call the programming model used by MPI "distributed memorycomputing" (as opposed to "shared memory computing", which would meanthat multiple processors can all access all data within one memoryspace, for example whenever multiple cores in a single machine workon a common task). Some of the fundamentals of distributed memorycomputing are discussed in the [2.x.8]  "Parallel computing with multiple processors using distributed memory"documentation module, which is itself a sub-module of the [2.x.9]  "Parallel computing" module.
* In general, to be truly able to scale to large numbers of processors, oneneeds to split between the available processors [1.x.32] data structurewhose size scales with the size of the overall problem. (For a definitionof what it means for a program to "scale", see [2.x.10]  "this glossary entry".) This includes, forexample, the triangulation, the matrix, and all global vectors (solution, righthand side). If one doesn't split all of these objects, one of those will bereplicated on all processors and will eventually simply become too largeif the problem size (and the number of available processors) becomes large.(On the other hand, it is completely fine to keep objects with a size thatis independent of the overall problem size on every processor. For example,each copy of the executable will create its own finite element object, or thelocal matrix we use in the assembly.)
* In the current program (as well as in the related  [2.x.11] ), we will not goquite this far but present a gentler introduction to using MPI. Morespecifically, the only data structures we will parallelize are matrices andvectors. We do, however, not split up the Triangulation andDoFHandler classes: each process still has a complete copy ofthese objects, and all processes have exact copies of what the other processeshave. We will then simply have to mark, in each copy of the triangulationon each of the processors, which processor owns which cells. Thisprocess is called "partitioning" a mesh into  [2.x.12]  "subdomains".
* For larger problems, having to store the [1.x.33] mesh on every processorwill clearly yield a bottleneck. Splitting up the mesh is slightly, though notmuch more complicated (from a user perspective, though it is [1.x.34] morecomplicated under the hood) to achieve andwe will show how to do this in  [2.x.13]  and some other programs. There arenumerous occasions where, in the course of discussing how a function of thisprogram works, we will comment on the fact that it will not scale to largeproblems and why not. All of these issues will be addressed in  [2.x.14]  andin particular  [2.x.15] , which scales to very large numbers of processes.
* Philosophically, the way MPI operates is as follows. You typically run aprogram via
* [1.x.35]
* which means to run it on (say) 32 processors. (If you are on a cluster system,you typically need to [1.x.36] the program to run whenever 32 processorsbecome available; this will be described in the documentation of yourcluster. But under the hood, whenever those processors become available,the same call as above will generally be executed.) What this does is thatthe MPI system will start 32 [1.x.37] of the  [2.x.16] executable. (The MPI term for each of these running executables is that youhave 32  [2.x.17]  "MPI processes".)This may happen on different machines that can't even readfrom each others' memory spaces, or it may happen on the same machine, butthe end result is the same: each of these 32 copies will run with somememory allocated to it by the operating system, and it will not directlybe able to read the memory of the other 31 copies. In order to collaboratein a common task, these 32 copies then have to [1.x.38] witheach other. MPI, short for [1.x.39], makes thispossible by allowing programs to [1.x.40]. You can thinkof this as the mail service: you can put a letter to a specific addressinto the mail and it will be delivered. But that's the extent to whichyou can control things. If you want the receiver to do somethingwith the content of the letter, for example return to you data you wantfrom over there, then two things need to happen: (i) the receiver needsto actually go check whether there is anything in their mailbox, and (ii) ifthere is, react appropriately, for example by sending data back. If youwait for this return message but the original receiver was distractedand not paying attention, then you're out of luck: you'll simply have towait until your requested over there will be worked on. In some cases,bugs will lead the original receiver to never check your mail, and in thatcase you will wait forever
* 
*  -  this is called a [1.x.41].( [2.x.18] 
* In practice, one does not usually program at the level of sending andreceiving individual messages, but uses higher level operations. Forexample, in the program we will use function calls that take a numberfrom each processor, add them all up, and return the sum to allprocessors. Internally, this is implemented using individual messages,but to the user this is transparent. We call such operations [1.x.42]because [1.x.43] processors participate in them. Collectives allow usto write programs where not every copy of the executable is doing somethingcompletely different (this would be incredibly difficult to program) butwhere in essence all copies are doing the same thing (though on differentdata) for themselves, running through the same blocks of code; then theycommunicate data through collectives; and then go back to doing somethingfor themselves again running through the same blocks of data. This is thekey piece to being able to write programs, and it is the key componentto making sure that programs can run on any number of processors,since we do not have to write different code for each of the participatingprocessors.
* (This is not to say that programs are never written in ways wheredifferent processors run through different blocks of code in theircopy of the executable. Programs internally also often communicatein other ways than through collectives. But in practice, %parallel finiteelement codes almost always follow the scheme where every copyof the program runs through the same blocks of code at the same time,interspersed by phases where all processors communicate with each other.)
* In reality, even the level of calling MPI collective functions is toolow. Rather, the program below will not contain any directcalls to MPI at all, but only deal.II functions that hide thiscommunication from users of the deal.II. This has the advantage thatyou don't have to learn the details of MPI and its rather intricatefunction calls. That said, you do have to understand the generalphilosophy behind MPI as outlined above.
* 

* [1.x.44][1.x.45]
* 

* The techniques this program then demonstrates are:
* 
*  - How to use the PETSc wrapper classes; this will already be visible in the  declaration of the principal class of this program,  [2.x.19] .
* 
*  - How to partition the mesh into subdomains; this happens in the   [2.x.20]  function.
* 
*  - How to parallelize operations for jobs running on an MPI network; here, this  is something one has to pay attention to in a number of places, most  notably in the   [2.x.21]  function.
* 
*  - How to deal with vectors that store only a subset of vector entries  and for which we have to ensure that they store what we need on the  current processors. See for example the   [2.x.22]   functions.
* 
*  - How to deal with status output from programs that run on multiple  processors at the same time. This is done via the  [2.x.23]   variable in the program, initialized in the constructor.
* Since all this can only be demonstrated using actual code, let us go straight to thecode without much further ado.
* 

*  [1.x.46] [1.x.47]
*   [1.x.48]  [1.x.49]
* 

* 
*  First the usual assortment of header files we have already used in previous example programs:
* 

* 
* [1.x.50]
* 
*  And here come the things that we need particularly for this example program and that weren't in  [2.x.24] . First, we replace the standard output  [2.x.25]  which is used in parallel computations for generating output only on one of the MPI processes.
* 

* 
* [1.x.51]
* 
*  We are going to query the number of processes and the number of the present process by calling the respective functions in the  [2.x.26]  namespace.
* 

* 
* [1.x.52]
* 
*  Then, we are going to replace all linear algebra components that involve the (global) linear system by classes that wrap interfaces similar to our own linear algebra classes around what PETSc offers (PETSc is a library written in C, and deal.II comes with wrapper classes that provide the PETSc functionality with an interface that is similar to the interface we already had for our own linear algebra classes). In particular, we need vectors and matrices that are distributed across several  [2.x.27]  "processes" in MPI programs (and simply map to sequential, local vectors and matrices if there is only a single process, i.e., if you are running on only one machine, and without MPI support):
* 

* 
* [1.x.53]
* 
*  Then we also need interfaces for solvers and preconditioners that PETSc provides:
* 

* 
* [1.x.54]
* 
*  And in addition, we need some algorithms for partitioning our meshes so that they can be efficiently distributed across an MPI network. The partitioning algorithm is implemented in the  [2.x.28]  namespace, and we need an additional include file for a function in  [2.x.29]  that allows to sort the indices associated with degrees of freedom so that they are numbered according to the subdomain they are associated with:
* 

* 
* [1.x.55]
* 
*  And this is simply C++ again:
* 

* 
* [1.x.56]
* 
*  The last step is as in all previous programs:
* 

* 
* [1.x.57]
* 
*   [1.x.58]  [1.x.59]
* 

* 
*  The first real part of the program is the declaration of the main class.  As mentioned in the introduction, almost all of this has been copied verbatim from  [2.x.30] , so we only comment on the few differences between the two tutorials.  There is one (cosmetic) change in that we let  [2.x.31]  return a value, namely the number of iterations it took to converge, so that we can output this to the screen at the appropriate place.
* 

* 
* [1.x.60]
* 
*  The first change is that we have to declare a variable that indicates the  [2.x.32]  "MPI communicator" over which we are supposed to distribute our computations.
* 

* 
* [1.x.61]
* 
*  Then we have two variables that tell us where in the parallel world we are. The first of the following variables,  [2.x.33] , tells us how many MPI processes there exist in total, while the second one,  [2.x.34] , indicates which is the number of the present process within this space of processes (in MPI language, this corresponds to the  [2.x.35]  "rank" of the process). The latter will have a unique value for each process between zero and (less than)  [2.x.36] . If this program is run on a single machine without MPI support, then their values are  [2.x.37] , respectively.
* 

* 
* [1.x.62]
* 
*  Next up is a stream-like variable  [2.x.38] . It is, in essence, just something we use for convenience: in a parallel program, if each process outputs status information, then there quickly is a lot of clutter. Rather, we would want to only have one  [2.x.39]  "process" output everything once, for example the one with  [2.x.40]  "rank" zero. At the same time, it seems silly to prefix [1.x.63] place where we create output with an  [2.x.41]  condition.     
*   To make this simpler, the ConditionalOStream class does exactly this under the hood: it acts as if it were a stream, but only forwards to a real, underlying stream if a flag is set. By setting this condition to  [2.x.42]  (where  [2.x.43]  corresponds to the rank of an MPI process), we make sure that output is only generated from the first process and that we don't get the same lines of output over and over again, once per process. Thus, we can use  [2.x.44]  everywhere and in every process, but on all but one process nothing will ever happen to the information that is piped into the object via  [2.x.45] .
* 

* 
* [1.x.64]
* 
*  The remainder of the list of member variables is fundamentally the same as in  [2.x.46] . However, we change the declarations of matrix and vector types to use parallel PETSc objects instead. Note that we do not use a separate sparsity pattern, since PETSc manages this internally as part of its matrix data structures.
* 

* 
* [1.x.65]
* 
*   [1.x.66]  [1.x.67]
* 

* 
*  The following is taken from  [2.x.47]  without change:
* 

* 
* [1.x.68]
* 
*   [1.x.69]  [1.x.70]
* 

* 
*   [1.x.71]  [1.x.72]
* 

* 
*  The first step in the actual implementation is the constructor of the main class. Apart from initializing the same member variables that we already had in  [2.x.48] , we here initialize the MPI communicator variable we shall use with the global MPI communicator linking all processes together (in more complex applications, one could here use a communicator object that only links a subset of all processes), and call the  [2.x.49]  helper functions to determine the number of processes and where the present one fits into this picture. In addition, we make sure that output is only generated by the (globally) first process. We do so by passing the stream we want to output to  [2.x.50]  and a true/false flag as arguments where the latter is determined by testing whether the process currently executing the constructor call is the first in the MPI universe.
* 

* 
* [1.x.73]
* 
*   [1.x.74]  [1.x.75]
* 

* 
*  Next, the function in which we set up the various variables for the global linear system to be solved needs to be implemented.   
*   However, before we proceed with this, there is one thing to do for a parallel program: we need to determine which MPI process is responsible for each of the cells. Splitting cells among processes, commonly called "partitioning the mesh", is done by assigning a  [2.x.51]  "subdomain id" to each cell. We do so by calling into the METIS library that does this in a very efficient way, trying to minimize the number of nodes on the interfaces between subdomains. Rather than trying to call METIS directly, we do this by calling the  [2.x.52]  function that does this at a much higher level of programming.   
*  

* 
*  [2.x.53]  As mentioned in the introduction, we could avoid this manual partitioning step if we used the  [2.x.54]  class for the triangulation object instead (as we do in  [2.x.55] ). That class does, in essence, everything a regular triangulation does, but it then also automatically partitions the mesh after every mesh creation or refinement operation.   
*   Following partitioning, we need to enumerate all degrees of freedom as usual.  However, we would like to enumerate the degrees of freedom in a way so that all degrees of freedom associated with cells in subdomain zero (which resides on process zero) come before all DoFs associated with cells on subdomain one, before those on cells on process two, and so on. We need this since we have to split the global vectors for right hand side and solution, as well as the matrix into contiguous chunks of rows that live on each of the processors, and we will want to do this in a way that requires minimal communication. This particular enumeration can be obtained by re-ordering degrees of freedom indices using  [2.x.56]    
*   The final step of this initial setup is that we get ourselves an IndexSet that indicates the subset of the global number of unknowns this process is responsible for. (Note that a degree of freedom is not necessarily owned by the process that owns a cell just because the degree of freedom lives on this cell: some degrees of freedom live on interfaces between subdomains, and are consequently only owned by one of the processes adjacent to this interface.)   
*   Before we move on, let us recall a fact already discussed in the introduction: The triangulation we use here is replicated across all processes, and each process has a complete copy of the entire triangulation, with all cells. Partitioning only provides a way to identify which cells out of all each process "owns", but it knows everything about all of them. Likewise, the DoFHandler object knows everything about every cell, in particular the degrees of freedom that live on each cell, whether it is one that the current process owns or not. This can not scale to large problems because eventually just storing the entire mesh, and everything that is associated with it, on every process will become infeasible if the problem is large enough. On the other hand, if we split the triangulation into parts so that every process stores only those cells it "owns" but nothing else (or, at least a sufficiently small fraction of everything else), then we can solve large problems if only we throw a large enough number of MPI processes at them. This is what we are going to in  [2.x.57] , for example, using the  [2.x.58]  class.  On the other hand, most of the rest of what we demonstrate in the current program will actually continue to work whether we have the entire triangulation available, or only a piece of it.
* 

* 
* [1.x.76]
* 
*  We need to initialize the objects denoting hanging node constraints for the present grid. As with the triangulation and DoFHandler objects, we will simply store [1.x.77] constraints on each process; again, this will not scale, but we show in  [2.x.59]  how one can work around this by only storing on each MPI process the constraints for degrees of freedom that actually matter on this particular process.
* 

* 
* [1.x.78]
* 
*  Now we create the sparsity pattern for the system matrix. Note that we again compute and store all entries and not only the ones relevant to this process (see  [2.x.60]  or  [2.x.61]  for a more efficient way to handle this).
* 

* 
* [1.x.79]
* 
*  Now we determine the set of locally owned DoFs and use that to initialize parallel vectors and matrix. Since the matrix and vectors need to work in parallel, we have to pass them an MPI communication object, as well as information about the partitioning contained in the IndexSet  [2.x.62]   The IndexSet contains information about the global size (the [1.x.80] number of degrees of freedom) and also what subset of rows is to be stored locally.  Note that the system matrix needs that partitioning information for the rows and columns. For square matrices, as it is the case here, the columns should be partitioned in the same way as the rows, but in the case of rectangular matrices one has to partition the columns in the same way as vectors are partitioned with which the matrix is multiplied, while rows have to partitioned in the same way as destination vectors of matrix-vector multiplications:
* 

* 
* [1.x.81]
* 
*   [1.x.82]  [1.x.83]
* 

* 
*  We now assemble the matrix and right hand side of the problem. There are some things worth mentioning before we go into detail. First, we will be assembling the system in parallel, i.e., each process will be responsible for assembling on cells that belong to this particular process. Note that the degrees of freedom are split in a way such that all DoFs in the interior of cells and between cells belonging to the same subdomain belong to the process that  [2.x.63]  the cell. However, even then we sometimes need to assemble on a cell with a neighbor that belongs to a different process, and in these cases when we add up the local contributions into the global matrix or right hand side vector, we have to transfer these entries to the process that owns these elements. Fortunately, we don't have to do this by hand: PETSc does all this for us by caching these elements locally, and sending them to the other processes as necessary when we call the  [2.x.64]  functions on the matrix and vector at the end of this function.   
*   The second point is that once we have handed over matrix and vector contributions to PETSc, it is a) hard, and b) very inefficient to get them back for modifications. This is not only the fault of PETSc, it is also a consequence of the distributed nature of this program: if an entry resides on another processor, then it is necessarily expensive to get it. The consequence of this is that we should not try to first assemble the matrix and right hand side as if there were no hanging node constraints and boundary values, and then eliminate these in a second step (using, for example,  [2.x.65]  Rather, we should try to eliminate hanging node constraints before handing these entries over to PETSc. This is easy: instead of copying elements by hand into the global matrix (as we do in  [2.x.66] ), we use the  [2.x.67]  functions to take care of hanging nodes at the same time. We also already did this in  [2.x.68] . The second step, elimination of boundary nodes, could also be done this way by putting the boundary values into the same AffineConstraints object as hanging nodes (see the way it is done in  [2.x.69] , for example); however, it is not strictly necessary to do this here because eliminating boundary values can be done with only the data stored on each process itself, and consequently we use the approach used before in  [2.x.70] , i.e., via  [2.x.71]    
*   All of this said, here is the actual implementation starting with the general setup of helper variables.  (Note that we still use the deal.II full matrix and vector types for the local systems as these are small and need not be shared across processes.)
* 

* 
* [1.x.84]
* 
*  The next thing is the loop over all elements. Note that we do not have to do [1.x.85] the work on every process: our job here is only to assemble the system on cells that actually belong to this MPI process, all other cells will be taken care of by other processes. This is what the if-clause immediately after the for-loop takes care of: it queries the subdomain identifier of each cell, which is a number associated with each cell that tells us about the owner process. In more generality, the subdomain id is used to split a domain into several parts (we do this above, at the beginning of  [2.x.72] ), and which allows to identify which subdomain a cell is living on. In this application, we have each process handle exactly one subdomain, so we identify the terms  [2.x.73] .     
*   Apart from this, assembling the local system is relatively uneventful if you have understood how this is done in  [2.x.74] . As mentioned above, distributing local contributions into the global matrix and right hand sides also takes care of hanging node constraints in the same way as is done in  [2.x.75] .
* 

* 
* [1.x.86]
* 
*  The next step is to "compress" the vector and the system matrix. This means that each process sends the additions that were made to those entries of the matrix and vector that the process did not own itself to the process that owns them. After receiving these additions from other processes, each process then adds them to the values it already has. These additions are combining the integral contributions of shape functions living on several cells just as in a serial computation, with the difference that the cells are assigned to different processes.
* 

* 
* [1.x.87]
* 
*  The global matrix and right hand side vectors have now been formed. We still have to apply boundary values, in the same way as we did, for example, in  [2.x.76] ,  [2.x.77] , and a number of other programs.     
*   The last argument to the call to  [2.x.78]  below allows for some optimizations. It controls whether we should also delete entries (i.e., set them to zero) in the matrix columns corresponding to boundary nodes, or to keep them (and passing  [2.x.79]  means: yes, do eliminate the columns). If we do eliminate columns, then the resulting matrix will be symmetric again if it was before; if we don't, then it won't. The solution of the resulting system should be the same, though. The only reason why we may want to make the system symmetric again is that we would like to use the CG method, which only works with symmetric matrices. The reason why we may [1.x.88] want to make the matrix symmetric is because this would require us to write into column entries that actually reside on other processes, i.e., it involves communicating data. This is always expensive.     
*   Experience tells us that CG also works (and works almost as well) if we don't remove the columns associated with boundary nodes, which can be explained by the special structure of this particular non-symmetry. To avoid the expense of communication, we therefore do not eliminate the entries in the affected columns.
* 

* 
* [1.x.89]
* 
*   [1.x.90]  [1.x.91]
* 

* 
*  Having assembled the linear system, we next need to solve it. PETSc offers a variety of sequential and parallel solvers, for which we have written wrappers that have almost the same interface as is used for the deal.II solvers used in all previous example programs. The following code should therefore look rather familiar.   
*   At the top of the function, we set up a convergence monitor, and assign it the accuracy to which we would like to solve the linear system. Next, we create an actual solver object using PETSc's CG solver which also works with parallel (distributed) vectors and matrices. And finally a preconditioner; we choose to use a block Jacobi preconditioner which works by computing an incomplete LU decomposition on each diagonal block of the matrix.  (In other words, each MPI process computes an ILU from the rows it stores by throwing away columns that correspond to row indices not stored locally; this yields a square matrix block from which we can compute an ILU. That means that if you run the program with only one process, then you will use an ILU(0) as a preconditioner, while if it is run on many processes, then we will have a number of blocks on the diagonal and the preconditioner is the ILU(0) of each of these blocks. In the extreme case of one degree of freedom per processor, this preconditioner is then simply a Jacobi preconditioner since the diagonal matrix blocks consist of only a single entry. Such a preconditioner is relatively easy to compute because it does not require any kind of communication between processors, but it is in general not very efficient for large numbers of processors.)   
*   Following this kind of setup, we then solve the linear system:
* 

* 
* [1.x.92]
* 
*  The next step is to distribute hanging node constraints. This is a little tricky, since to fill in the value of a constrained node you need access to the values of the nodes to which it is constrained (for example, for a Q1 element in 2d, we need access to the two nodes on the big side of a hanging node face, to compute the value of the constrained node in the middle).     
*   The problem is that we have built our vectors (in  [2.x.80] ) in such a way that every process is responsible for storing only those elements of the solution vector that correspond to the degrees of freedom this process "owns". There are, however, cases where in order to compute the value of the vector entry for a constrained degree of freedom on one process, we need to access vector entries that are stored on other processes.  PETSc (and, for that matter, the MPI model on which it is built) does not allow to simply query vector entries stored on other processes, so what we do here is to get a copy of the "distributed" vector where we store all elements locally. This is simple, since the deal.II wrappers have a conversion constructor for the deal.II Vector class. (This conversion of course requires communication, but in essence every process only needs to send its data to every other process once in bulk, rather than having to respond to queries for individual elements):
* 

* 
* [1.x.93]
* 
*  Of course, as in previous discussions, it is clear that such a step cannot scale very far if you wanted to solve large problems on large numbers of processes, because every process now stores [1.x.94] of the solution vector. (We will show how to do this better in  [2.x.81] .)  On the other hand, distributing hanging node constraints is simple on this local copy, using the usual function  [2.x.82]  In particular, we can compute the values of [1.x.95] constrained degrees of freedom, whether the current process owns them or not:
* 

* 
* [1.x.96]
* 
*  Then transfer everything back into the global vector. The following operation copies those elements of the localized solution that we store locally in the distributed solution, and does not touch the other ones. Since we do the same operation on all processors, we end up with a distributed vector (i.e., a vector that on every process only stores the vector entries corresponding to degrees of freedom that are owned by this process) that has all the constrained nodes fixed.     
*   We end the function by returning the number of iterations it took to converge, to allow for some output.
* 

* 
* [1.x.97]
* 
*   [1.x.98]  [1.x.99]
* 

* 
*  Using some kind of refinement indicator, the mesh can be refined. The problem is basically the same as with distributing hanging node constraints: in order to compute the error indicator (even if we were just interested in the indicator on the cells the current process owns), we need access to more elements of the solution vector than just those the current processor stores. To make this happen, we do essentially what we did in  [2.x.83]  already, namely get a [1.x.100] copy of the solution vector onto every process, and use that to compute. This is in itself expensive as explained above and it is particular unnecessary since we had just created and then destroyed such a vector in  [2.x.84] , but efficiency is not the point of this program and so let us opt for a design in which every function is as self-contained as possible.   
*   Once we have such a "localized" vector that contains [1.x.101] elements of the solution vector, we can compute the indicators for the cells that belong to the present process. In fact, we could of course compute [1.x.102] refinement indicators since our Triangulation and DoFHandler objects store information about all cells, and since we have a complete copy of the solution vector. But in the interest in showing how to operate in %parallel, let us demonstrate how one would operate if one were to only compute [1.x.103] error indicators and then exchange the remaining ones with the other processes. (Ultimately, each process needs a complete set of refinement indicators because every process needs to refine their mesh, and needs to refine it in exactly the same way as all of the other processes.)   
*   So, to do all of this, we need to:
* 

* 
* 
*  - First, get a local copy of the distributed solution vector.
* 

* 
* 
*  - Second, create a vector to store the refinement indicators.
* 

* 
* 
*  - Third, let the KellyErrorEstimator compute refinement indicators for all cells belonging to the present subdomain/process. The last argument of the call indicates which subdomain we are interested in. The three arguments before it are various other default arguments that one usually does not need (and does not state values for, but rather uses the defaults), but which we have to state here explicitly since we want to modify the value of a following argument (i.e., the one indicating the subdomain).
* 

* 
* [1.x.104]
* 
*  Now all processes have computed error indicators for their own cells and stored them in the respective elements of the  [2.x.85]  vector. The elements of this vector for cells not owned by the present process are zero. However, since all processes have a copy of the entire triangulation and need to keep these copies in sync, they need the values of refinement indicators for all cells of the triangulation. Thus, we need to distribute our results. We do this by creating a distributed vector where each process has its share and sets the elements it has computed. Consequently, when you view this vector as one that lives across all processes, then every element of this vector has been set once. We can then assign this parallel vector to a local, non-parallel vector on each process, making [1.x.105] error indicators available on every process.     
*   So in the first step, we need to set up a parallel vector. For simplicity, every process will own a chunk with as many elements as this process owns cells, so that the first chunk of elements is stored with process zero, the next chunk with process one, and so on. It is important to remark, however, that these elements are not necessarily the ones we will write to. This is a consequence of the order in which cells are arranged, i.e., the order in which the elements of the vector correspond to cells is not ordered according to the subdomain these cells belong to. In other words, if on this process we compute indicators for cells of a certain subdomain, we may write the results to more or less random elements of the distributed vector; in particular, they may not necessarily lie within the chunk of vector we own on the present process. They will subsequently have to be copied into another process' memory space, an operation that PETSc does for us when we call the  [2.x.86]  function. This inefficiency could be avoided with some more code, but we refrain from it since it is not a major factor in the program's total runtime.     
*   So here is how we do it: count how many cells belong to this process, set up a distributed vector with that many elements to be stored locally, copy over the elements we computed locally, and finally compress the result. In fact, we really only copy the elements that are nonzero, so we may miss a few that we computed to zero, but this won't hurt since the original values of the vector are zero anyway.
* 

* 
* [1.x.106]
* 
*  So now we have this distributed vector that contains the refinement indicators for all cells. To use it, we need to obtain a local copy and then use it to mark cells for refinement or coarsening, and actually do the refinement and coarsening. It is important to recognize that [1.x.107] process does this to its own copy of the triangulation, and does it in exactly the same way.
* 

* 
* [1.x.108]
* 
*   [1.x.109]  [1.x.110]
* 

* 
*  The final function of significant interest is the one that creates graphical output. This works the same way as in  [2.x.87] , with two small differences. Before discussing these, let us state the general philosophy this function will work: we intend for all of the data to be generated on a single process, and subsequently written to a file. This is, as many other parts of this program already discussed, not something that will scale. Previously, we had argued that we will get into trouble with triangulations, DoFHandlers, and copies of the solution vector where every process has to store all of the data, and that there will come to be a point where each process simply doesn't have enough memory to store that much data. Here, the situation is different: it's not only the memory, but also the run time that's a problem. If one process is responsible for processing [1.x.111] of the data while all of the other processes do nothing, then this one function will eventually come to dominate the overall run time of the program.  In particular, the time this function takes is going to be proportional to the overall size of the problem (counted in the number of cells, or the number of degrees of freedom), independent of the number of processes we throw at it.   
*   Such situations need to be avoided, and we will show in  [2.x.88]  and  [2.x.89]  how to address this issue. For the current problem, the solution is to have each process generate output data only for its own local cells, and write them to separate files, one file per process. This is how  [2.x.90]  operates. Alternatively, one could simply leave everything in a set of independent files and let the visualization software read all of them (possibly also using multiple processors) and create a single visualization out of all of them; this is the path  [2.x.91] ,  [2.x.92] , and all other parallel programs developed later on take.   
*   More specifically for the current function, all processes call this function, but not all of them need to do the work associated with generating output. In fact, they shouldn't, since we would try to write to the same file multiple times at once. So we let only the first process do this, and all the other ones idle around during this time (or start their work for the next iteration, or simply yield their CPUs to other jobs that happen to run at the same time). The second thing is that we not only output the solution vector, but also a vector that indicates which subdomain each cell belongs to. This will make for some nice pictures of partitioned domains.   
*   To implement this, process zero needs a complete set of solution components in a local vector. Just as with the previous function, the efficient way to do this would be to re-use the vector already created in the  [2.x.93]  function, but to keep things more self-contained, we simply re-create one here from the distributed solution vector.   
*   An important thing to realize is that we do this localization operation on all processes, not only the one that actually needs the data. This can't be avoided, however, with the simplified communication model of MPI we use for vectors in this tutorial program: MPI does not have a way to query data on another process, both sides have to initiate a communication at the same time. So even though most of the processes do not need the localized solution, we have to place the statement converting the distributed into a localized vector so that all processes execute it.   
*   (Part of this work could in fact be avoided. What we do is send the local parts of all processes to all other processes. What we would really need to do is to initiate an operation on all processes where each process simply sends its local chunk of data to process zero, since this is the only one that actually needs it, i.e., we need something like a gather operation. PETSc can do this, but for simplicity's sake we don't attempt to make use of this here. We don't, since what we do is not very expensive in the grand scheme of things: it is one vector communication among all processes, which has to be compared to the number of communications we have to do when solving the linear system, setting up the block-ILU for the preconditioner, and other operations.)
* 

* 
* [1.x.112]
* 
*  This being done, process zero goes ahead with setting up the output file as in  [2.x.94] , and attaching the (localized) solution vector to the output object.
* 

* 
* [1.x.113]
* 
*  The only other thing we do here is that we also output one value per cell indicating which subdomain (i.e., MPI process) it belongs to. This requires some conversion work, since the data the library provides us with is not the one the output class expects, but this is not difficult. First, set up a vector of integers, one per cell, that is then filled by the subdomain id of each cell.         
*   The elements of this vector are then converted to a floating point vector in a second step, and this vector is added to the DataOut object, which then goes off creating output in VTK format:
* 

* 
* [1.x.114]
* 
*   [1.x.115]  [1.x.116]
* 

* 
*  Lastly, here is the driver function. It is almost completely unchanged from  [2.x.95] , with the exception that we replace  [2.x.96]  stream. Apart from this, the only other cosmetic change is that we output how many degrees of freedom there are per process, and how many iterations it took for the linear solver to converge:
* 

* 
* [1.x.117]
* 
*   [1.x.118]  [1.x.119]
* 

* 
*  The  [2.x.97]  works the same way as most of the main functions in the other example programs, i.e., it delegates work to the  [2.x.98]  function of a managing object, and only wraps everything into some code to catch exceptions:
* 

* 
* [1.x.120]
* 
*  Here is the only real difference: MPI and PETSc both require that we initialize these libraries at the beginning of the program, and un-initialize them at the end. The class MPI_InitFinalize takes care of all of that. The trailing argument `1` means that we do want to run each MPI process with a single thread, a prerequisite with the PETSc parallel linear algebra.
* 

* 
* [1.x.121]
* [1.x.122][1.x.123]
* 

* 
* If the program above is compiled and run on a single processormachine, it should generate results that are very similar to thosethat we already got with  [2.x.99] . However, it becomes more interestingif we run it on a multicore machine or a cluster of computers. Themost basic way to run MPI programs is using a command line like
* [1.x.124]
* to run the  [2.x.100]  executable with 32 processors.
* (If you work on a cluster, then there is typically a step in between where youneed to set up a job script and submit the script to a scheduler. The schedulerwill execute the script whenever it can allocate 32 unused processors for yourjob. How to write such jobscripts differs from cluster to cluster, and you should find the documentationof your cluster to see how to do this. On my system, I have to use the command [2.x.101]  with a whole host of options to run a job in parallel.)
* Whether directly or through a scheduler, if you run this program on 8processors, you should get output like the following:
* [1.x.125]
* (This run uses a few more refinement cycles than the code available inthe examples/ directory. The run also used a version of METIS from2004 that generated different partitionings; consequently,the numbers you get today are slightly different.)
* As can be seen, we can easily get to almost four million unknowns. In fact, thecode's runtime with 8 processes was less than 7 minutes up to (and including)cycle 14, and 14 minutes including the second to last step. (These are numbersrelevant to when the code was initially written, in 2004.) I lost the timinginformation for the last step, though, but you get the idea. All this is afterrelease mode has been enabled by running  [2.x.102] , andwith the generation of graphical output switched off for the reasons stated inthe program comments above.( [2.x.103] The biggest 2d computations I did had roughly 7.1million unknowns, and were done on 32 processes. It took about 40 minutes.Not surprisingly, the limiting factor for how far one can go is how much memoryone has, since every process has to hold the entire mesh and DoFHandler objects,although matrices and vectors are split up. For the 7.1M computation, the memoryconsumption was about 600 bytes per unknown, which is not bad, but one has toconsider that this is for every unknown, whether we store the matrix and vectorentries locally or not.
* 

* 
* Here is some output generated in the 12th cycle of the program, i.e. with roughly300,000 unknowns:
*  [2.x.104] 
* As one would hope for, the x- (left) and y-displacements (right) shown hereclosely match what we already saw in  [2.x.105] . As shownthere and in  [2.x.106] , we could as well have produced avector plot of the displacement field, rather than plotting it as twoseparate scalar fields. What may be more interesting,though, is to look at the mesh and partition at this step:
*  [2.x.107] 
* Again, the mesh (left) shows the same refinement pattern as seenpreviously. The right panel shows the partitioning of the domain across the 8processes, each indicated by a different color. The picture shows that thesubdomains are smaller where mesh cells are small, a fact that needs to beexpected given that the partitioning algorithm tries to equilibrate the numberof cells in each subdomain; this equilibration is also easily identified inthe output shown above, where the number of degrees per subdomain is roughlythe same.
* 

* 
* It is worth noting that if we ran the same program with a different number ofprocesses, that we would likely get slightly different output: a differentmesh, different number of unknowns and iterations to convergence. The reasonfor this is that while the matrix and right hand side are the same independentof the number of processes used, the preconditioner is not: it performs anILU(0) on the chunk of the matrix of  [2.x.108] each processor separately [2.x.109] . Thus,it's effectiveness as a preconditioner diminishes as the number of processesincreases, which makes the number of iterations increase. Since a differentpreconditioner leads to slight changes in the computed solution, this willthen lead to slightly different mesh cells tagged for refinement, and largerdifferences in subsequent steps. The solution will always look very similar,though.
* 

* 
* Finally, here are some results for a 3d simulation. You can repeat these bychanging
* [1.x.126]
* to
* [1.x.127]
* in the main function. If you then run the program in parallel,you get something similar to this (this is for a job with 16 processes):
* [1.x.128]
* 
* 

* 
* The last step, going up to 1.5 million unknowns, takes about 55 minutes with16 processes on 8 dual-processor machines (of the kind available in 2003). Thegraphical output generated bythis job is rather large (cycle 5 already prints around 82 MB of data), sowe contend ourselves with showing output from cycle 4:
*  [2.x.110] 
* 

* 
* The left picture shows the partitioning of the cube into 16 processes, whereasthe right one shows the x-displacement along two cutplanes through the cube.
* 

* 
* [1.x.129][1.x.130][1.x.131]
* 

* The program keeps a complete copy of the Triangulation and DoFHandler objectson every processor. It also creates complete copies of the solution vector,and it creates output on only one processor. All of this is obviouslythe bottleneck as far as parallelization is concerned.
* Internally, within deal.II, parallelizing the datastructures used in hierarchic and unstructured triangulations is a hardproblem, and it took us a few more years to make this happen. The  [2.x.111] tutorial program and the  [2.x.112]  documentation module talk about howto do these steps and what it takes from an application perspective. Anobvious extension of the current program would be to use this functionality tocompletely distribute computations to many more processors than used here.
* 

* [1.x.132][1.x.133] [2.x.113] 
* [0.x.1]