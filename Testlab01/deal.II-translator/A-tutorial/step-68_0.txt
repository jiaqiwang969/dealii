[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28]
*  [2.x.2] 
* [1.x.29]
* [1.x.30][1.x.31]
* 

* [1.x.32][1.x.33]
* 

* Particles play an important part in numerical models for a large number of applications. Particles are routinely used as massless tracers to visualize the dynamic of a transient flow. They can also play an intrinsic role as part of a more complex finite element model, as is the case for the Particle-In-Cell (PIC) method  [2.x.3]  or they can even be used to simulate the motion of granular matter, as in the Discrete Element Method (DEM)  [2.x.4] . In the case of DEM, the resulting model is not related to the finite element method anymore, but just leads to a system of ordinary differential equation which describes the motion of the particles and the dynamic of their collisions. All of these models can be built using deal.II's particle handling capabilities.
* In the present step, we use particles as massless tracers to illustratethe dynamic of a vortical flow. Since the particles are massless tracers,the position of each particle  [2.x.5]  is described by thefollowing ordinary differential equation (ODE):[1.x.34]
* where  [2.x.6]  is the position of particle  [2.x.7]  and  [2.x.8]  the flow velocity at its position.In the present step, this ODE is solved using the explicit Euler method. The resulting scheme is:[1.x.35]
* where  [2.x.9]  and  [2.x.10]  are the positionof particle  [2.x.11]  at time  [2.x.12]  and  [2.x.13] , respectively and where  [2.x.14] is the time step. In the present step, the velocity at the location of particlesis obtained in two different fashions:
* 
*  - By evaluating the velocity function at the location of the particles;
* 
*  - By evaluating the velocity function on a background triangulation and, usinga  finite element support, interpolating at the position of the particle.
* The first approach is not practical, since the velocity profileis generally not known analytically. The second approach, based on interpolating a solutionat the position of the particles, mimics exactly what would be done in arealistic computational fluid dynamic simulation, and this follows the way we have also evaluatedthe finite element solution at particle locations in  [2.x.15] . In this step, we illustrate both strategies.
* We note that much greater accuracy could be obtained by using a fourthorder Runge-Kutta method or another appropriate scheme for the time integrationof the motion of the particles.  Implementing a more advanced time-integration schemewould be a straightforward extension of this step.
* [1.x.36][1.x.37]
* 

* In deal.II,  [2.x.16]  are very simple and flexible entities that can be usedto build PIC, DEM or any type of particle-based models. Particles have a locationin real space, a location in the reference space of the element in which theyare located and a unique ID. In the majority of cases, simulations that includeparticles require a significant number of them. Thus, it becomes interestingto handle all particles through an entity which agglomerates all particles.In deal.II, this is achieved through the use of the  [2.x.17]  class.
* By default, particles do not have a diameter,a mass or any other physical properties which we would generally expect of physical particles. However, througha ParticleHandler, particles have access to a  [2.x.18]  This PropertyPool isan array which can be used to store an arbitrary number of propertiesassociated with the particles. Consequently, users can build their ownparticle solver and attribute the desired properties to the particles (e.g., mass, charge,diameter, temperature, etc.). In the present tutorial, this is used tostore the value of the fluid velocity and the process id to which the particlesbelong.
* [1.x.38][1.x.39]
* 

* Although the present step is not computationally intensive, simulations thatinclude many particles can be computationally demanding and require parallelization.The present step showcases the distributed parallel capabilities of deal.II for particles.In general, there are three main challengesthat specifically arise in parallel distributed simulations that include particles:
* 
*  - Generating the particles on the distributed triangulation;
* 
*  - Exchanging the particles that leave local domains between the processors;
* 
*  - Load balancing the simulation so that every processor has a similar computational load.These challenges and their solution in deal.II have been discussed in more detail in [2.x.19] , but we will summarize them below.
* There are of course also questions on simply setting up a code that uses particles. These have largely already beenaddressed in  [2.x.20] . Some more advanced techniques will also be discussed in  [2.x.21] .
* [1.x.40][1.x.41]
* 

* Generating distributed particles in a scalable way is not straightforward sincethe processor to which they belong must first be identified before the cell inwhich they are located is found.  deal.II provides numerous capabilities togenerate particles through the  [2.x.22]  namespace.  Some of theseparticle generators create particles only on the locally owned subdomain. Forexample,  [2.x.23]  creates particlesat the same reference locations within each cell of the local subdomain and [2.x.24]  uses a globally defined probabilitydensity function to determine how many and where to generate particles locally.
* In other situations, such as the present step, particles must be generated atspecific locations on cells that may be owned only by a subset of the processors.In  most of these situations, the insertion of the particles is done for a verylimited number of time-steps and, consequently, does not constitute a largeportion of the computational cost. For these occasions, deal.II providesconvenient  [2.x.25]  that can globally insert the particles even ifthe particle is not located in a cell owned by the parallel process on which the call to create the particle is initiated. Thegenerators first locate on which subdomain the particles are situated, identifyin which cell they are located and exchange the necessary information amongthe processors to ensure that the particle is generated with the rightproperties. Consequently, this type of particle generation can be communicationintensive. The  [2.x.26]  and the [2.x.27]  generate particles using atriangulation and the points of an associated DoFHandler or quadraturerespectively. The triangulation that is used to generate the particles can bethe same triangulation that is used for the background mesh, in which case thesefunctions are very similar to the [2.x.28]  function described in theprevious paragraph. However, the triangulation used to generate particles canalso be different (non-matching) from the triangulation of the background grid,which is useful to generate particles in particular shapes (as in thisexample), or to transfer information between two different computational grids(as in  [2.x.29] ).  Furthermore, the  [2.x.30]  class provides the [2.x.31]  function which enables theglobal insertion of particles from a vector of arbitrary points and a globalvector of bounding boxes. In the present step, we use the [2.x.32]  function on a non-matching triangulation toinsert particles located at positions in the shape of a disk.
* [1.x.42][1.x.43]
* 

* As particles move around in parallel distributed computations they may leavethe locally owned subdomain and need to be transferred to their new ownerprocesses. This situation can arise in two very different ways: First, if theprevious owning process knows the new owner of the particles that were lost(for example because the particles moved from the locally owned cell of one processorinto an adjacent ghost cells of a distributedtriangulation) then the transfer can be handled efficiently as a point-to-pointcommunication between each process and the new owners. This transfer happensautomatically whenever particles are sorted into their new cells. Secondly,the previous owner may not know to which process the particle has moved. Inthis case the particle is discarded by default, as a global search for theowner can be expensive.  [2.x.33]  shows how such a discarded particle can stillbe collected, interpreted, and potentially reinserted by the user. In thepresent example we prevent the second case by imposing a CFL criterion on thetimestep to ensure particles will at most move into the ghost layer of thelocal process and can therefore be send to neighboring processes automatically.
* [1.x.44][1.x.45]
* 

* The last challenge that arises in parallel distributed computations usingparticles is to balance the computational load between work that is done on thegrid, for example solving the finite-element problem, and the work that is doneon the particles, for example advecting the particles or computing the forcesbetween particles or between particles and grid. By default, for example in [2.x.34] , deal.II distributes the background mesh as evenly as possible betweenthe available processes, that is it balances the number of cells on eachprocess. However, if some cells own many more particles than other cells, or ifthe particles of one cell are much more computationally expensive than theparticles in other cells, then this problem no longer scales efficiently (for adiscussion of what we consider "scalable" programs, see [2.x.35]  "this glossary entry"). Thus, we have to apply a form of"load balancing", which means we estimate the computational load that isassociated with each cell and its particles. Repartitioning the mesh thenaccounts for this combined computational load instead of the simplifiedassumption of the number of cells  [2.x.36] .
* In this section we only discussed the particle-specific challenges in distributedcomputation. Parallel challenges that particles share withfinite-element solutions (parallel output, data transfer during meshrefinement) can be addressed with the solutions found forfinite-element problems already discussed in other examples.
* [1.x.46][1.x.47]
* 

* In the present step, we use particles as massless tracers to illustratethe dynamics of a particular vortical flow: the Rayleigh--Kothe vortex. This flow patternis generally used as a complex test case for interface tracking methods(e.g., volume-of-fluid and level set approaches) sinceit leads to strong rotation and elongation of the fluid  [2.x.37] .
* The stream function  [2.x.38]  of this Rayleigh-Kothe vortex is defined as:
* [1.x.48]where  [2.x.39]  is half the period of the flow. The velocity profile in 2D ( [2.x.40] ) is :[1.x.49]
* 
* The velocity profile is illustrated in the following animation:
* [1.x.50]
* 
* It can be seen that this velocity reverses periodically due to the term [2.x.41]  and that material will end up at itsstarting position after every period of length  [2.x.42] . We will run this tutorialprogram for exactly one period and compare the final particle location to theinitial location to illustrate this flow property. This example uses the testcaseto produce two models that handle the particlesslightly differently. The first model prescribes the exact analytical velocitysolution as the velocity for each particle. Therefore in this model there is noerror in the assigned velocity to the particles, and any deviation of particlepositions from the analytical position at a given time results from the errorin solving the equation of motion for the particle inexactly, using a time stepping method. In the second model theanalytical velocity field is first interpolated to a finite-element vectorspace (to simulate the case that the velocity was obtained from solving afinite-element problem, in the same way as the ODE for each particle in  [2.x.43]  depends on a finite elementsolution). This finite-element "solution" is then evaluated atthe locations of the particles to solve their equation of motion. Thedifference between the two cases allows to assess whether the chosenfinite-element space is sufficiently accurate to advect the particles with theoptimal convergence rate of the chosen particle advection scheme, a questionthat is important in practice to determine the accuracy of the combinedalgorithm (see e.g.  [2.x.44] ).
* 

*  [1.x.51] [1.x.52]
*   [1.x.53]  [1.x.54]
* 

* 
*  

* 
* [1.x.55]
* 
*  From the following include file we import the ParticleHandler class that allows you to manage a collection of particles (objects of type  [2.x.45]  representing a collection of points with some attached properties (e.g., an id) floating on a  [2.x.46]  The methods and classes in the namespace Particles allows one to easily implement Particle-In-Cell methods and particle tracing on distributed triangulations:
* 

* 
* [1.x.56]
* 
*  We import the particles generator which allow us to insert the particles. In the present step, the particle are globally inserted using a non-matching hyper-shell triangulation:
* 

* 
* [1.x.57]
* 
*  Since the particles do not form a triangulation, they have their own specific DataOut class which will enable us to write them to commonly used parallel vtu format (or any number of other file formats):
* 

* 
* [1.x.58]
* 
*   [1.x.59]  [1.x.60]
* 

* 
*  Similarly to what is done in  [2.x.47] , we set up a class that holds all the parameters of our problem and derive it from the ParameterAcceptor class to simplify the management and creation of parameter files.   
*   The ParameterAcceptor paradigm requires all parameters to be writable by the ParameterAcceptor methods. In order to avoid bugs that would be very difficult to track down (such as writing things like `if (time = 0)` instead of `if(time == 0)`), we declare all the parameters in an external class, which is initialized before the actual `ParticleTracking` class, and pass it to the main class as a `const` reference.   
*   The constructor of the class is responsible for the connection between the members of this class and the corresponding entries in the ParameterHandler. Thanks to the use of the  [2.x.48]  method, this connection is trivial, but requires all members of this class to be writable.
* 

* 
* [1.x.61]
* 
*  This class consists largely of member variables that describe the details of the particle tracking simulation and its discretization. The following parameters are about where output should written to, the spatial discretization of the velocity (the default is  [2.x.49] ), the time step and the output frequency (how many time steps should elapse before we generate graphical output again):
* 

* 
* [1.x.62]
* 
*  We allow every grid to be refined independently. In this tutorial, no physics is resolved on the fluid grid, and its velocity is calculated analytically.
* 

* 
* [1.x.63]
* 
*  There remains the task of declaring what run-time parameters we can accept in input files. Since we have a very limited number of parameters, all parameters are declared in the same section.
* 

* 
* [1.x.64]
* 
*   [1.x.65]  [1.x.66]
* 

* 
*  The velocity profile is provided as a Function object. This function is hard-coded within the example.
* 

* 
* [1.x.67]
* 
*  The velocity profile for the Rayleigh-Kothe vertex is time-dependent. Consequently, the current time in the simulation (t) must be gathered from the Function object.
* 

* 
* [1.x.68]
* 
*   [1.x.69]  [1.x.70]
* 

* 
*  We are now ready to introduce the main class of our tutorial program.
* 

* 
* [1.x.71]
* 
*  This function is responsible for the initial generation of the particles on top of the background grid.
* 

* 
* [1.x.72]
* 
*  When the velocity profile is interpolated to the position of the particles, it must first be stored using degrees of freedom. Consequently, as is the case for other parallel case (e.g.  [2.x.50] ) we initialize the degrees of freedom on the background grid.
* 

* 
* [1.x.73]
* 
*  In one of the test cases, the function is mapped to the background grid and a finite element interpolation is used to calculate the velocity at the particle location. This function calculates the value of the function at the support point of the triangulation.
* 

* 
* [1.x.74]
* 
*  The next two functions are responsible for carrying out step of explicit Euler time integration for the cases where the velocity field is interpolated at the positions of the particles or calculated analytically, respectively.
* 

* 
* [1.x.75]
* 
*  The `cell_weight()` function indicates to the triangulation how much computational work is expected to happen on this cell, and consequently how the domain needs to be partitioned so that every MPI rank receives a roughly equal amount of work (potentially not an equal number of cells). While the function is called from the outside, it is connected to the corresponding signal from inside this class, therefore it can be `private`.
* 

* 
* [1.x.76]
* 
*  The following two functions are responsible for outputting the simulation results for the particles and for the velocity profile on the background mesh, respectively.
* 

* 
* [1.x.77]
* 
*  The private members of this class are similar to other parallel deal.II examples. The parameters are stored as a `const` member. It is important to note that we keep the `Vortex` class as a member since its time must be modified as the simulation proceeds.
* 

* 
*  

* 
* [1.x.78]
* 
*   [1.x.79]  [1.x.80]
* 

* 
*   [1.x.81]  [1.x.82]
* 

* 
*  The constructors and destructors are rather trivial. They are very similar to what is done in  [2.x.51] . We set the processors we want to work on to all machines available (`MPI_COMM_WORLD`) and initialize the  [2.x.52]  variable to only allow processor zero to output anything to the standard output.
* 

* 
*  

* 
* [1.x.83]
* 
*   [1.x.84]  [1.x.85]
* 

* 
*  This function is the key component that allow us to dynamically balance the computational load for this example. The function attributes a weight to every cell that represents the computational work on this cell. Here the majority of work is expected to happen on the particles, therefore the return value of this function (representing "work for this cell") is calculated based on the number of particles in the current cell. The function is connected to the cell_weight() signal inside the triangulation, and will be called once per cell, whenever the triangulation repartitions the domain between ranks (the connection is created inside the generate_particles() function of this class).
* 

* 
* [1.x.86]
* 
*  We do not assign any weight to cells we do not own (i.e., artificial or ghost cells)
* 

* 
* [1.x.87]
* 
*  This determines how important particle work is compared to cell work (by default every cell has a weight of 1000). We set the weight per particle much higher to indicate that the particle load is the only one that is important to distribute the cells in this example. The optimal value of this number depends on the application and can range from 0 (cheap particle operations, expensive cell operations) to much larger than 1000 (expensive particle operations, cheap cell operations, like presumed in this example).
* 

* 
* [1.x.88]
* 
*  This example does not use adaptive refinement, therefore every cell should have the status `CELL_PERSIST`. However this function can also be used to distribute load during refinement, therefore we consider refined or coarsened cells as well.
* 

* 
* [1.x.89]
* 
*   [1.x.90]  [1.x.91]
* 

* 
*  This function generates the tracer particles and the background triangulation on which these particles evolve.
* 

* 
* [1.x.92]
* 
*  We create a hyper cube triangulation which we globally refine. This triangulation covers the full trajectory of the particles.
* 

* 
* [1.x.93]
* 
*  In order to consider the particles when repartitioning the triangulation the algorithm needs to know three things:     
*   1. How much weight to assign to each cell (how many particles are in there); 2. How to pack the particles before shipping data around; 3. How to unpack the particles after repartitioning.     
*   We attach the correct functions to the signals inside  [2.x.53]  These signal will be called every time the repartition() function is called. These connections only need to be created once, so we might as well have set them up in the constructor of this class, but for the purpose of this example we want to group the particle related instructions.
* 

* 
* [1.x.94]
* 
*  This initializes the background triangulation where the particles are living and the number of properties of the particles.
* 

* 
* [1.x.95]
* 
*  We create a particle triangulation which is solely used to generate the points which will be used to insert the particles. This triangulation is a hyper shell which is offset from the center of the simulation domain. This will be used to generate a disk filled with particles which will allow an easy monitoring of the motion due to the vortex.
* 

* 
* [1.x.96]
* 
*  We generate the necessary bounding boxes for the particles generator. These bounding boxes are required to quickly identify in which process's subdomain the inserted particle lies, and which cell owns it.
* 

* 
* [1.x.97]
* 
*  We generate an empty vector of properties. We will attribute the properties to the particles once they are generated.
* 

* 
* [1.x.98]
* 
*  We generate the particles at the position of a single point quadrature. Consequently, one particle will be generated at the centroid of each cell.
* 

* 
* [1.x.99]
* 
*   [1.x.100]  [1.x.101]
* 

* 
*  This function sets up the background degrees of freedom used for the velocity interpolation and allocates the field vector where the entire solution of the velocity field is stored.
* 

* 
* [1.x.102]
* 
*  This function takes care of interpolating the vortex velocity field to the field vector. This is achieved rather easily by using the  [2.x.54]  function.
* 

* 
* [1.x.103]
* 
*   [1.x.104]  [1.x.105]
* 

* 
*  We integrate the particle trajectories using an analytically defined velocity field. This demonstrates a relatively trivial usage of the particles.
* 

* 
* [1.x.106]
* 
*  Looping over all particles in the domain using a particle iterator
* 

* 
* [1.x.107]
* 
*  We calculate the velocity of the particles using their current location.
* 

* 
* [1.x.108]
* 
*  This updates the position of the particles and sets the old position equal to the new position of the particle.
* 

* 
* [1.x.109]
* 
*  We store the processor id (a scalar) and the particle velocity (a vector) in the particle properties. In this example, this is done purely for visualization purposes.
* 

* 
* [1.x.110]
* 
*  In contrast to the previous function in this function we integrate the particle trajectories by interpolating the value of the velocity field at the degrees of freedom to the position of the particles.
* 

* 
* [1.x.111]
* 
*  We loop over all the local particles. Although this could be achieved directly by looping over all the cells, this would force us to loop over numerous cells which do not contain particles. Rather, we loop over all the particles, but, we get the reference of the cell in which the particle lies and then loop over all particles within that cell. This enables us to gather the values of the velocity out of the `velocity_field` vector once and use them for all particles that lie within the cell.
* 

* 
* [1.x.112]
* 
*  Next, compute the velocity at the particle locations by evaluating the finite element solution at the position of the particles. This is essentially an optimized version of the particle advection functionality in step 19, but instead of creating quadrature objects and FEValues objects for each cell, we do the evaluation by hand, which is somewhat more efficient and only matters for this tutorial, because the particle work is the dominant cost of the whole program.
* 

* 
* [1.x.113]
* 
*  Again, we store the particle velocity and the processor id in the particle properties for visualization purposes.
* 

* 
* [1.x.114]
* 
*   [1.x.115]  [1.x.116]
* 

* 
*  The next two functions take care of writing both the particles and the background mesh to vtu with a pvtu record. This ensures that the simulation results can be visualized when the simulation is launched in parallel.
* 

* 
* [1.x.117]
* 
*  Attach the solution data to data_out object
* 

* 
* [1.x.118]
* 
*   [1.x.119]  [1.x.120] This function orchestrates the entire simulation. It is very similar to the other time dependent tutorial programs
* 
*  -  take  [2.x.55]  or  [2.x.56]  as an example. Note that we use the DiscreteTime class to monitor the time, the time-step and the  [2.x.57] number. This function is relatively straightforward.
* 

* 
*  

* 
* [1.x.121]
* 
*  We set the initial property of the particles by doing an explicit Euler iteration with a time-step of 0 both in the case of the analytical and the interpolated approach.
* 

* 
* [1.x.122]
* 
*  The particles are advected by looping over time.
* 

* 
* [1.x.123]
* 
*  After the particles have been moved, it is necessary to identify in which cell they now reside. This is achieved by calling  [2.x.58] 
* 

* 
* [1.x.124]
* 
*   [1.x.125]  [1.x.126]
* 

* 
*  The remainder of the code, the `main()` function, is standard. We note that we run the particle tracking with the analytical velocity and the interpolated velocity and produce both results
* 

* 
* [1.x.127]
* [1.x.128][1.x.129]
* 

* The directory in which this program is run contains an example parameter file by default.If you do not specify a parameter file as an argument on the commandline, the program will try to read the file "parameters.prm" by default, andwill execute the code.
* On any number of cores, the simulation output will look like:
* [1.x.130]
* 
* We note that, by default, the simulation runs the particle tracking withan analytical velocity for 2000 iterations, then restarts from the beginning and runs the particle tracking withvelocity interpolation for the same duration. The results are written every10th iteration.
* [1.x.131][1.x.132]
* 

* The following animation displays the trajectory of the particles as theyare advected by the flow field. We see that after the complete duration of theflow, the particle go back to their initial configuration as is expected.
* [1.x.133]
* 
* [1.x.134][1.x.135]
* 

* The following animation shows the impact of dynamic load balancing. We clearlysee that the subdomains adapt themselves to balance the number of particles persubdomain. However, a perfect load balancing is not reached, in part due tothe coarseness of the background mesh.
* [1.x.136]
* 
* 

* [1.x.137][1.x.138]
* 

* This program highlights some of the main capabilities for handling particles in deal.II, notably theircapacity to be used in distributed parallel simulations. However, this step couldbe extended in numerous manners:
* 
*  - High-order time integration (for example using a Runge-Kutta 4 method) could beused to increase the accuracy or allow for larger time-step sizes with the same accuracy.
* 
*  - The full equation of motion (with inertia) could be solved for the particles. Inthis case the particles would need to have additional properties such as their mass,as in  [2.x.59] , and if one wanted to also consider interactions with the fluid, their diameter.
* 
*  - Coupling to a flow solver. This step could be straightforwardly coupled to any parallelprogram in which the Stokes ( [2.x.60] ,  [2.x.61] ) or the Navier-Stokes equations are solved (e.g.,  [2.x.62] ).
* 
*  - Computing the difference in final particle positions between the two modelswould allow to quantify the influence of the interpolation error on particle motion.
* 

* [1.x.139][1.x.140] [2.x.63] 
* [0.x.1]