[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] ,  [2.x.2] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28]
* [1.x.29]
*  [2.x.3] 
* [1.x.30][1.x.31][1.x.32]
* 

* [1.x.33][1.x.34]
* 

* The purpose of this tutorial is to create an efficient linear solver for theStokes equation and compare it to alternative approaches.  Here, we will useFGMRES with geometric multigrid as a preconditioner velocity block, and wewill show in the results section that this is a fundamentally better approachthan the linear solvers used in  [2.x.4]  (including the scheme described in"Possible Extensions").  Fundamentally, this is because only with multigrid itis possible to get  [2.x.5]  solve time, where  [2.x.6]  is the number of unknowns ofthe linear system. Using the Timer class, we collect some statistics tocompare setup times, solve times, and number of iterations. We also computeerrors to make sure that what we have implemented is correct.
* Let  [2.x.7] and  [2.x.8] . The Stokes equations read as follows in non-dimensionalized form:
* [1.x.35]
* 
* Note that we are using the deformation tensor instead of  [2.x.9]  (adetailed description of the difference between the two can be found in [2.x.10] , but in summary, the deformation tensor is more physical aswell as more expensive).
* [1.x.36][1.x.37]
* 

* The weak form ofthe discrete equations naturally leads to the following linear systemfor the nodal values of the velocity and pressure fields:[1.x.38]
* 
* Our goal is to compare several solution approaches.  While  [2.x.11] solves the linear system using a "Schur complement approach" in twoseparate steps, we instead attack theblock system at once using FMGRES with an efficientpreconditioner, in the spirit of the approach outlined in the "Results"section of  [2.x.12] . The idea is as follows: if we find a blockpreconditioner  [2.x.13]  such that the matrix
* [1.x.39]
* 
* is simple, then an iterative solver with that preconditioner willconverge in a few iterations. Notice that we are doing rightpreconditioning here.  Using the Schur complement  [2.x.14] ,we find that
* [1.x.40]
* 
* is a good choice. Let  [2.x.15]  be an approximation of  [2.x.16] and  [2.x.17]  of  [2.x.18] , we see[1.x.41]
* 
* Since  [2.x.19]  is aimed to be a preconditioner only, we shall usethe approximations on the right in the equation above.
* As discussed in  [2.x.20] ,  [2.x.21] , where  [2.x.22]  is the pressure mass matrix and is solved approximately by using CGwith ILU as a preconditioner, and  [2.x.23]  is obtained by one ofmultiple methods: solving a linear system with CG and ILU aspreconditioner, just using one application of an ILU, solving a linearsystem with CG and GMG (GeometricMultigrid as described in  [2.x.24] ) as a preconditioner, or just performing a single V-cycleof GMG.
* As a comparison, instead of FGMRES, we also use the direct solverUMFPACK on the whole system to compare our results with.  If you want to usea direct solver (like UMFPACK), the system needs to be invertible. To avoidthe one dimensional null space given by the constant pressures, we fix the first pressure unknown to zero. This is not necessary for the iterative solvers.
* 

* [1.x.42][1.x.43]
* 

* The test problem is a "Manufactured Solution" (see  [2.x.25]  fordetails), and we choose  [2.x.26]  and  [2.x.27] .We apply Dirichlet boundary conditions for the velocity on the wholeboundary of the domain  [2.x.28] .To enforce the boundary conditions we can just use our reference solution.
* If you look up in the deal.II manual what is needed to create a classderived from  [2.x.29] , you will find that thisclass has numerous  [2.x.30]  functions, including [2.x.31]   [2.x.32]   [2.x.33] etc., all of which can be overloaded.  Different parts of deal.IIwill require different ones of these particularfunctions. This can be confusing at first, but luckily the only thingyou actually have to implement is  [2.x.34]   The other virtualfunctions in the Function class have defaultimplementations inside that will call your implementation of  [2.x.35] by default.
* Notice that our reference solution fulfills  [2.x.36] . Inaddition, the pressure is chosen to have a mean value of zero.  Forthe "Method of Manufactured Solutions" of  [2.x.37] , we need to find  [2.x.38]  such that:
* [1.x.44]
* 
* Using the reference solution above, we obtain:
* [1.x.45]
* 
* [1.x.46][1.x.47]
* 

* Because we do not enforce the meanpressure to be zero for our numerical solution in the linear system,we need to post process the solution after solving. To do this we usethe  [2.x.39]  function to compute the mean valueof the pressure to subtract it from the pressure.
* 

* [1.x.48][1.x.49]
* 

* The way we implement geometric multigrid here only executes it on thevelocity variables (i.e., the  [2.x.40]  matrix described above) but not thepressure. One could implement this in different ways, including one inwhich one considers all coarse grid operations as acting on  [2.x.41]  block systems where we only consider the top leftblock. Alternatively, we can implement things by really onlyconsidering a linear system on the velocity part of the overall finiteelement discretization. The latter is the way we want to use here.
* To implement this, one would need to be able to ask questions such as"May I have just part of a DoFHandler?". This is not possible at thetime when this program was written, so in order to answer this requestfor our needs, we simply create a separate, second DoFHandler for just thevelocities. We then build linear systems for the multigridpreconditioner based on only this second DoFHandler, and simplytransfer the first block of (overall) vectors into correspondingvectors for the entire second DoFHandler. To make this work, we haveto assure that the [1.x.50] in which the (velocity) degrees of freedom areordered in the two DoFHandler objects is the same. This is in fact thecase by first distributing degrees of freedom on both, and then usingthe same sequence of DoFRenumbering operations on both.
* 

* [1.x.51][1.x.52]
* 

* The main difference between  [2.x.42]  and  [2.x.43]  is that we use blocksolvers instead of the Schur Complement approach used in [2.x.44] . Details of this approach can be found under the "Block Schurcomplement preconditioner" subsection of the "Possible Extensions"section of  [2.x.45] . For the preconditioner of the velocity block, weborrow a class from [1.x.53]called  [2.x.46]  that has the option to solve forthe inverse of  [2.x.47]  or just apply one preconditioner sweep for itinstead, which provides us with an expensive and cheap approach,respectively.
* 

*  [1.x.54] [1.x.55]
*   [1.x.56]  [1.x.57]
* 

* 
*  

* 
* [1.x.58]
* 
*  We need to include the following file to do timings:
* 

* 
* [1.x.59]
* 
*  This includes the files necessary for us to use geometric Multigrid
* 

* 
* [1.x.60]
* 
*  In order to make it easy to switch between the different solvers that are being used, we declare an enum that can be passed as an argument to the constructor of the main class.
* 

* 
* [1.x.61]
* 
*   [1.x.62]  [1.x.63]   
*   The class Solution is used to define the boundary conditions and to compute errors of the numerical solution. Note that we need to define the values and gradients in order to compute L2 and H1 errors. Here we decided to separate the implementations for 2d and 3d using template specialization.   
*   Note that the first dim components are the velocity components and the last is the pressure.
* 

* 
* [1.x.64]
* 
*  Note that for the gradient we need to return a Tensor<1,dim>
* 

* 
* [1.x.65]
* 
*  Implementation of  [2.x.48] . See the introduction for more information.
* 

* 
* [1.x.66]
* 
*   [1.x.67]  [1.x.68]
* 

* 
*  In the following, we will implement a preconditioner that expands on the ideas discussed in the Results section of  [2.x.49] . Specifically, we 1. use an upper block-triangular preconditioner because we want to use right preconditioning. 2. optionally allow using an inner solver for the velocity block instead of a single preconditioner application. 3. do not use InverseMatrix but explicitly call SolverCG. This approach is also used in the ASPECT code (see https://aspect.geodynamics.org) that solves the Stokes equations in the context of simulating convection in the earth mantle, and which has been used to solve problems on many thousands of processors.   
*   The bool flag  [2.x.50]  in the constructor allows us to either apply the preconditioner for the velocity block once or use an inner iterative solver for a more accurate approximation instead.   
*   Notice how we keep track of the sum of the inner iterations (preconditioner applications).
* 

* 
* [1.x.69]
* 
*  First solve with the approximation for S
* 

* 
* [1.x.70]
* 
*  Second, apply the top right block (B^T)
* 

* 
* [1.x.71]
* 
*  Finally, either solve with the top left block or just apply one preconditioner sweep
* 

* 
* [1.x.72]
* 
*   [1.x.73]  [1.x.74]   
*   This is the main class of the problem.
* 

* 
* [1.x.75]
* 
*  Finite element for the velocity only:
* 

* 
* [1.x.76]
* 
*  Finite element for the whole system:
* 

* 
* [1.x.77]
* 
*   [1.x.78]  [1.x.79]
* 

* 
*  This function sets up the DoFHandler, matrices, vectors, and Multigrid structures (if needed).
* 

* 
* [1.x.80]
* 
*  The main DoFHandler only needs active DoFs, so we are not calling distribute_mg_dofs() here
* 

* 
* [1.x.81]
* 
*  This block structure separates the dim velocity components from the pressure component (used for reordering). Note that we have 2 instead of dim+1 blocks like in  [2.x.51] , because our FESystem is nested and the dim velocity components appear as one block.
* 

* 
* [1.x.82]
* 
*  Velocities start at component 0:
* 

* 
* [1.x.83]
* 
*  ILU behaves better if we apply a reordering to reduce fillin. There is no advantage in doing this for the other solvers.
* 

* 
* [1.x.84]
* 
*  This ensures that all velocities DoFs are enumerated before the pressure unknowns. This allows us to use blocks for vectors and matrices and allows us to get the same DoF numbering for dof_handler and velocity_dof_handler.
* 

* 
* [1.x.85]
* 
*  This distributes the active dofs and multigrid dofs for the velocity space in a separate DoFHandler as described in the introduction.
* 

* 
* [1.x.86]
* 
*  The following block of code initializes the MGConstrainedDofs (using the boundary conditions for the velocity), and the sparsity patterns and matrices for each level. The resize() function of MGLevelObject<T> will destroy all existing contained objects.
* 

* 
* [1.x.87]
* 
*  The following makes use of a component mask for interpolation of the boundary values for the velocity only, which is further explained in the vector valued dealii  [2.x.52]  tutorial.
* 

* 
* [1.x.88]
* 
*  As discussed in the introduction, we need to fix one degree of freedom of the pressure variable to ensure solvability of the problem. We do this here by marking the first pressure dof, which has index n_u as a constrained dof.
* 

* 
* [1.x.89]
* 
*   [1.x.90]  [1.x.91]
* 

* 
*  In this function, the system matrix is assembled. We assemble the pressure mass matrix in the (1,1) block (if needed) and move it out of this location at the end of this function.
* 

* 
* [1.x.92]
* 
*  If true, we will assemble the pressure mass matrix in the (1,1) block:
* 

* 
* [1.x.93]
* 
*   [1.x.94]  [1.x.95]
* 

* 
*  Here, like in  [2.x.53] , we have a function that assembles the level and interface matrices necessary for the multigrid preconditioner.
* 

* 
* [1.x.96]
* 
*  This iterator goes over all cells (not just active)
* 

* 
* [1.x.97]
* 
*   [1.x.98]  [1.x.99]
* 

* 
*  This function sets up things differently based on if you want to use ILU or GMG as a preconditioner.  Both methods share the same solver (FGMRES) but require a different preconditioner to be initialized. Here we time not only the entire solve function, but we separately time the setup of the preconditioner as well as the solve itself.
* 

* 
* [1.x.100]
* 
*  Here we must make sure to solve for the residual with "good enough" accuracy
* 

* 
* [1.x.101]
* 
*  This is used to pass whether or not we want to solve for A inside the preconditioner.  One could change this to false to see if there is still convergence and if so does the program then run faster or slower
* 

* 
* [1.x.102]
* 
*  Transfer operators between levels
* 

* 
* [1.x.103]
* 
*  Setup coarse grid solver
* 

* 
* [1.x.104]
* 
*  Multigrid, when used as a preconditioner for CG, needs to be a symmetric operator, so the smoother must be symmetric
* 

* 
* [1.x.105]
* 
*  Now, we are ready to set up the V-cycle operator and the multilevel preconditioner.
* 

* 
* [1.x.106]
* 
*   [1.x.107]  [1.x.108]
* 

* 
*  This function computes the L2 and H1 errors of the solution. For this, we need to make sure the pressure has mean zero.
* 

* 
* [1.x.109]
* 
*  Compute the mean pressure  [2.x.54]  and then subtract it from each pressure coefficient. This will result in a pressure with mean value zero. Here we make use of the fact that the pressure is component  [2.x.55]  and that the finite element space is nodal.
* 

* 
* [1.x.110]
* 
*   [1.x.111]  [1.x.112]
* 

* 
*  This function generates graphical output like it is done in  [2.x.56] .
* 

* 
* [1.x.113]
* 
*   [1.x.114]  [1.x.115]
* 

* 
*  The last step in the Stokes class is, as usual, the function that generates the initial grid and calls the other functions in the respective order.
* 

* 
* [1.x.116]
* 
*   [1.x.117]  [1.x.118]
* 

* 
* [1.x.119]
* 
*  options for SolverType: UMFPACK FGMRES_ILU FGMRES_GMG
* 

* 
* [1.x.120]
* [1.x.121][1.x.122]
* 

* [1.x.123][1.x.124]
* 

* We first run the code and confirm that the finite element solution convergeswith the correct rates as predicted by the error analysis of mixed finiteelement problems. Given sufficiently smooth exact solutions  [2.x.57]  and  [2.x.58] ,the errors of the Taylor-Hood element  [2.x.59]  should be
* [1.x.125]
* see for example Ern/Guermond "Theory and Practice of Finite Elements", Section4.2.5 p195. This is indeed what we observe, using the  [2.x.60] element as an example (this is what is done in the code, but is easilychanged in  [2.x.61] ):
*  [2.x.62] 
* [1.x.126][1.x.127]
* 

* Let us compare the direct solver approach using UMFPACK to the twomethods in which we choose  [2.x.63]  and [2.x.64]  by solving linear systems with  [2.x.65]  usingCG. The preconditioner for CG is then either ILU or GMG.The following table summarizes solver iterations, timings, and virtualmemory (VM) peak usage:
*  [2.x.66] 
* As can be seen from the table:
* 1. UMFPACK uses large amounts of memory, especially in 3d. Also, UMFPACKtimings do not scale favorably with problem size.
* 2. Because we are using inner solvers for  [2.x.67]  and  [2.x.68] , ILU and GMG require thesame number of outer iterations.
* 3. The number of (inner) iterations for  [2.x.69]  increases for ILU with refinement, leadingto worse than linear scaling in solve time. In contrast, the number of inneriterations for  [2.x.70]  stays constant with GMG leading to nearly perfect scaling insolve time.
* 4. GMG needs slightly more memory than ILU to store the level and interfacematrices.
* [1.x.128][1.x.129]
* 

* [1.x.130][1.x.131]
* 

* Experiment with higher order stable FE pairs and check that you observe thecorrect convergence rates.
* [1.x.132][1.x.133]
* 

* The introduction also outlined another option to precondition theoverall system, namely one in which we do not choose  [2.x.71]  as in the table above, but in which [2.x.72]  is only a single preconditioner application withGMG or ILU, respectively.
* This is in fact implemented in the code: Currently, the boolean [2.x.73]  is set to  [2.x.74]  Theoption mentioned above is obtained by setting it to  [2.x.75] 
* What you will find is that the number of FGMRES iterations staysconstant under refinement if you use GMG this way. This means that theMultigrid is optimal and independent of  [2.x.76] .
* 

* [1.x.134][1.x.135] [2.x.77] 
* [0.x.1]