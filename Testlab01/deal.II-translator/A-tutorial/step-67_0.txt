[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] ,  [2.x.2] ,  [2.x.3] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28]
* 
*  [2.x.4] 
* [1.x.29]
* [1.x.30][1.x.31][1.x.32]
* 

* This tutorial program solves the Euler equations of fluid dynamics using anexplicit time integrator with the matrix-free framework applied to ahigh-order discontinuous Galerkin discretization in space. For details aboutthe Euler system and an alternative implicit approach, we also refer to the [2.x.5]  tutorial program. You might also want to look at  [2.x.6]  foran alternative approach to solving these equations.
* 

* [1.x.33][1.x.34]
* 

* The Euler equations are a conservation law, describing the motion of acompressible inviscid gas,[1.x.35]where the  [2.x.7]  components of the solution vector are  [2.x.8] . Here,  [2.x.9]  denotes the fluid density, [2.x.10]  the fluid velocity, and  [2.x.11]  theenergy density of the gas. The velocity is not directly solved for, but ratherthe variable  [2.x.12] , the linear momentum (since this is theconserved quantity).
* The Euler flux function, a  [2.x.13]  matrix, is defined as[1.x.36]with  [2.x.14]  the  [2.x.15]  identity matrix and  [2.x.16]  the outerproduct; its components denote the mass, momentum, and energy fluxes, respectively.The right hand side forcing is given by[1.x.37]where the vector  [2.x.17]  denotes the direction and magnitude ofgravity. It could, however, also denote any other external force per unit massthat is acting on the fluid. (Think, for example, of the electrostaticforces exerted by an external electric field on charged particles.)
* The three blocks of equations, the second involving  [2.x.18]  components, describethe conservation of mass, momentum, and energy. The pressure is not asolution variable but needs to be expressed through a "closure relationship"by the other variables; we here choose the relationship appropriatefor a gas with molecules composed of two atoms, which at moderatetemperatures is given by  [2.x.19]  with the constant  [2.x.20] .
* 

* [1.x.38][1.x.39]
* 

* For spatial discretization, we use a high-order discontinuous Galerkin (DG)discretization, using a solution expansion of the form[1.x.40]Here,  [2.x.21]  denotes the  [2.x.22] th basis function, writtenin vector form with separate shape functions for the different components andletting  [2.x.23]  go through the density, momentum, and energy variables,respectively. In this form, the space dependence is contained in the shapefunctions and the time dependence in the unknown coefficients  [2.x.24] . Asopposed to the continuous finite element method where some shape functionsspan across element boundaries, the shape functions are local to a singleelement in DG methods, with a discontinuity from one element to the next. Theconnection of the solution from one cell to its neighbors is insteadimposed by the numerical fluxesspecified below. This allows for some additional flexibility, for example tointroduce directionality in the numerical method by, e.g., upwinding.
* DG methods are popular methods for solving problems of transport characterbecause they combine low dispersion errors with controllable dissipation onbarely resolved scales. This makes them particularly attractive for simulationin the field of fluid dynamics where a wide range of active scales needs to berepresented and inadequately resolved features are prone to disturb theimportant well-resolved features. Furthermore, high-order DG methods arewell-suited for modern hardware with the right implementation. At the sametime, DG methods are no silver bullet. In particular when the solutiondevelops discontinuities (shocks), as is typical for the Euler equations insome flow regimes, high-order DG methods tend to oscillatory solutions, likeall high-order methods when not using flux- or slope-limiters. This is a consequence of [1.x.41]that states that any total variation limited (TVD) scheme that is linear (likea basic DG discretization) can at most be first-order accurate. Putdifferently, since DG methods aim for higher order accuracy, they cannot beTVD on solutions that develop shocks. Even though some communities claim thatthe numerical flux in DG methods can control dissipation, this is of limitedvalue unless [1.x.42] shocks in a problem align with cell boundaries. Anyshock that passes through the interior of cells will again produce oscillatorycomponents due to the high-order polynomials. In the finite element and DGcommunities, there exist a number of different approaches to deal with shocks,for example the introduction of artificial diffusion on troubled cells (usinga troubled-cell indicator based e.g. on a modal decomposition of thesolution), a switch to dissipative low-order finite volume methods on asubgrid, or the addition of some limiting procedures. Given the amplepossibilities in this context, combined with the considerable implementationeffort, we here refrain from the regime of the Euler equations with pronouncedshocks, and rather concentrate on the regime of subsonic flows with wave-likephenomena. For a method that works well with shocks (but is more expensive perunknown), we refer to the  [2.x.25]  tutorial program.
* For the derivation of the DG formulation, we multiply the Euler equations withtest functions  [2.x.26]  and integrate over an individual cell  [2.x.27] , whichgives[1.x.43]
* We then integrate the second term by parts, moving the divergencefrom the solution slot to the test function slot, and producing an integralover the element boundary:[1.x.44]In the surface integral, we have replaced the term  [2.x.28]  bythe term  [2.x.29] , the numerical flux. The role ofthe numerical flux is to connect the solution on neighboring elements andweakly impose continuity of the solution. This ensures that the globalcoupling of the PDE is reflected in the discretization, despite independentbasis functions on the cells. The connectivity to the neighbor is included bydefining the numerical flux as a function  [2.x.30]  of the solution from both sides of an interior face,  [2.x.31]  and  [2.x.32] . A basic property we require is that the numerical fluxneeds to be [1.x.45]. That is, we want all information (i.e.,mass, momentum, and energy) that leaves a cell overa face to enter the neighboring cell in its entirety and vice versa. This canbe expressed as  [2.x.33] , meaning that the numericalflux evaluates to the same result from either side. Combined with the factthat the numerical flux is multiplied by the unit outer normal vector on theface under consideration, which points in opposite direction from the twosides, we see that the conservation is fulfilled. An alternative point of viewof the numerical flux is as a single-valued intermediate state that links thesolution weakly from both sides.
* There is a large number of numerical flux functions available, also calledRiemann solvers. For the Euler equations, there exist so-called exact Riemannsolvers
* 
*  -  meaning that the states from both sides are combined in a way thatis consistent with the Euler equations along a discontinuity
* 
*  -  andapproximate Riemann solvers, which violate some physical properties and relyon other mechanisms to render the scheme accurate overall. Approximate Riemannsolvers have the advantage of beging cheaper to compute. Most flux functionshave their origin in the finite volume community, which are similar to DGmethods with polynomial degree 0 within the cells (called volumes). As thevolume integral of the Euler operator  [2.x.34]  would disappear forconstant solution and test functions, the numerical flux must fully representthe physical operator, explaining why there has been a large body of researchin that community. For DG methods, consistency is guaranteed by higher orderpolynomials within the cells, making the numerical flux less of an issue andusually affecting only the convergence rate, e.g., whether the solutionconverges as  [2.x.35] ,  [2.x.36]  or  [2.x.37]  in the  [2.x.38]  norm for polynomials of degree  [2.x.39] . The numericalflux can thus be seen as a mechanism to select more advantageousdissipation/dispersion properties or regarding the extremal eigenvalue of thediscretized and linearized operator, which affect the maximal admissible timestep size in explicit time integrators.
* In this tutorial program, we implement two variants of fluxes that can becontrolled via a switch in the program (of course, it would be easy to makethem a run time parameter controlled via an input file). The first flux isthe local Lax--Friedrichs flux[1.x.46]
* In the original definition of the Lax--Friedrichs flux, a factor  [2.x.40]  is used(corresponding to the maximal speed at which information is moving onthe two sides of the interface), statingthat the difference between the two states,  [2.x.41]  is penalizedby the largest eigenvalue in the Euler flux, which is  [2.x.42] ,where  [2.x.43]  is the speed of sound. In the implementationbelow, we modify the penalty term somewhat, given that the penalty is ofapproximate nature anyway. We use
* [1.x.47]
* The additional factor  [2.x.44]  reduces the penalty strength (which resultsin a reduced negative real part of the eigenvalues, and thus increases theadmissible time step size). Using the squares within the sums allows us toreduce the number of expensive square root operations, which is 4 for theoriginal Lax--Friedrichs definition, to a single one.This simplification leads to at most a factor of2 in the reduction of the parameter  [2.x.45] , since  [2.x.46] , with the last inequality followingfrom Young's inequality.
* The second numerical flux is one proposed by Harten, Lax and van Leer, calledthe HLL flux. It takes the different directions of propagation of the Eulerequations into account, depending on the speed of sound. It utilizes someintermediate states  [2.x.47]  and  [2.x.48]  to define the twobranches  [2.x.49]  and  [2.x.50] . From these branches, one then defines the flux[1.x.48]Regarding the definition of the intermediate state  [2.x.51]  and [2.x.52] , several variants have been proposed. The variant originallyproposed uses a density-averaged definition of the velocity,  [2.x.53] . Since we consider the Euler equations without shocks, wesimply use arithmetic means,  [2.x.54]  and  [2.x.55] , with  [2.x.56] , in this tutorial program, and leave othervariants to a possible extension. We also note that the HLL flux has beenextended in the literature to the so-called HLLC flux, where C stands for theability to represent contact discontinuities.
* At the boundaries with no neighboring state  [2.x.57]  available, it iscommon practice to deduce suitable exterior values from the boundaryconditions (see the general literature on DG methods for details). In thistutorial program, we consider three types of boundary conditions, namely[1.x.49] where all components are prescribed,[1.x.50][1.x.51], where we do not prescribe exteriorsolutions as the flow field is leaving the domain and use the interior valuesinstead; we still need to prescribe the energy as there is one incomingcharacteristic left in the Euler flux,[1.x.52]and [1.x.53] which describe a no-penetrationconfiguration:[1.x.54]
* The polynomial expansion of the solution is finally inserted to the weak formand test functions are replaced by the basis functions. This gives a discretein space, continuous in time nonlinear system with a finite number of unknowncoefficient values  [2.x.58] ,  [2.x.59] . Regarding the choice ofthe polynomial degree in the DG method, there is no consensus in literature asof 2019 as to what polynomial degrees are most efficient and the decision isproblem-dependent. Higher order polynomials ensure better convergence ratesand are thus superior for moderate to high accuracy requirements for[1.x.55] solutions. At the same time, the volume-to-surface ratioof where degrees of freedom are located,increases with higher degrees, and this makes the effect of the numerical fluxweaker, typically reducing dissipation. However, in most of the cases thesolution is not smooth, at least not compared to the resolution that can beafforded. This is true for example in incompressible fluid dynamics,compressible fluid dynamics, and the related topic of wave propagation. In thispre-asymptotic regime, the error is approximately proportional to thenumerical resolution, and other factors such as dispersion errors or thedissipative behavior become more important. Very high order methods are oftenruled out because they come with more restrictive CFL conditions measuredagainst the number of unknowns, and they are also not as flexible when itcomes to representing complex geometries. Therefore, polynomial degreesbetween two and six are most popular in practice, see e.g. the efficiencyevaluation in  [2.x.60]  and references cited therein.
* [1.x.56][1.x.57]
* 

* To discretize in time, we slightly rearrange the weak form and sum over allcells:[1.x.58]where  [2.x.61]  runs through all basis functions with from 1 to [2.x.62] .
* We now denote by  [2.x.63]  the mass matrix with entries  [2.x.64] , and by[1.x.59]the operator evaluating the right-hand side of the Euler operator, given afunction  [2.x.65]  associated with a global vector of unknownsand the finite element in use. This function  [2.x.66]  is explicitly time-dependent as thenumerical flux evaluated at the boundary will involve time-dependent data [2.x.67] ,  [2.x.68] , and  [2.x.69]  on someparts of the boundary, depending on the assignment of boundaryconditions. With this notation, we can write the discrete in space, continuousin time system compactly as[1.x.60]where we have taken the liberty to also denote the global solutionvector by  [2.x.70]  (in addition to the the corresponding finiteelement function). Equivalently, the system above has the form[1.x.61]
* For hyperbolic systems discretized by high-order discontinuous Galerkinmethods, explicit time integration of this system is very popular. This is dueto the fact that the mass matrix  [2.x.71]  is block-diagonal (with eachblock corresponding to only variables of the same kind defined on the samecell) and thus easily inverted. In each time step
* 
*  -  or stage of aRunge--Kutta scheme
* 
*  -  one only needs to evaluate the differential operatoronce using the given data and subsequently apply the inverse of the massmatrix. For implicit time stepping, on the other hand, one would first have tolinearize the equations and then iteratively solve the linear system, whichinvolves several residual evaluations and at least a dozen applications ofthe linearized operator, as has been demonstrated in the  [2.x.72]  tutorialprogram.
* Of course, the simplicity of explicit time stepping comes with a price, namelyconditional stability due to the so-called Courant--Friedrichs--Lewy (CFL)condition. It states that the time step cannot be larger than the fastestpropagation of information by the discretized differential operator. In moremodern terms, the speed of propagation corresponds to the largest eigenvaluein the discretized operator, and in turn depends on the mesh size, thepolynomial degree  [2.x.73]  and the physics of the Euler operator, i.e., theeigenvalues of the linearization of  [2.x.74]  with respect to [2.x.75] . In this program, we set the time step as follows:[1.x.62]
* with the maximum taken over all quadrature points and all cells. Thedimensionless number  [2.x.76]  denotes the Courant number and can bechosen up to a maximally stable number  [2.x.77] , whose valuedepends on the selected time stepping method and its stability properties. Thepower  [2.x.78]  used for the polynomial scaling is heuristic and representsthe closest fit for polynomial degrees between 1 and 8, see e.g. [2.x.79] . In the limit of higher degrees,  [2.x.80] , a scaling of [2.x.81]  is more accurate, related to the inverse estimates typically used forinterior penalty methods. Regarding the [1.x.63] mesh sizes  [2.x.82]  and [2.x.83]  used in the formula, we note that the convective transport isdirectional. Thus an appropriate scaling is to use the element length in thedirection of the velocity  [2.x.84] . The code below derives this scalingfrom the inverse of the Jacobian from the reference to real cell, i.e., weapproximate  [2.x.85] . The acoustic waves, instead, are isotropic in character, whichis why we use the smallest feature size, represented by the smallest singularvalue of  [2.x.86] , for the acoustic scaling  [2.x.87] . Finally, we need to add theconvective and acoustic limits, as the Euler equations can transportinformation with speed  [2.x.88] .
* In this tutorial program, we use a specific variant of [1.x.64], which in general use the following update procedurefrom the state  [2.x.89]  at time  [2.x.90]  to the new time  [2.x.91]  with [2.x.92] :[1.x.65]The vectors  [2.x.93] ,  [2.x.94] , in an  [2.x.95] -stage scheme areevaluations of the operator at some intermediate state and used to define theend-of-step value  [2.x.96]  via some linear combination. The scalarcoefficients in this scheme,  [2.x.97] ,  [2.x.98] , and  [2.x.99] , are defined such thatcertain conditions are satisfied for higher order schemes, the most basic onebeing  [2.x.100] . The parameters are typically collected inthe form of a so-called [1.x.66] that collects all of the coefficients that define thescheme. For a five-stage scheme, it would look like this:[1.x.67]
* In this tutorial program, we use a subset of explicit Runge--Kutta methods,so-called low-storage Runge--Kutta methods (LSRK), which assume additionalstructure in the coefficients. In the variant used by reference [2.x.101] , the assumption is to use Butcher tableaus ofthe form[1.x.68]With such a definition, the update to  [2.x.102]  shares the storage withthe information for the intermediate values  [2.x.103] . Starting with [2.x.104]  and  [2.x.105] , the updatein each of the  [2.x.106]  stages simplifies to[1.x.69]Besides the vector  [2.x.107]  that is successively updated, this schemeonly needs two auxiliary vectors, namely the vector  [2.x.108]  to hold theevaluation of the differential operator, and the vector  [2.x.109]  thatholds the right-hand side for the differential operator application. Insubsequent stages  [2.x.110] , the values  [2.x.111]  and  [2.x.112]  can usethe same storage.
* The main advantages of low-storage variants are the reduced memory consumptionon the one hand (if a very large number of unknowns must be fit in memory,holding all  [2.x.113]  to compute subsequent updates can be a limitalready for  [2.x.114]  in between five and eight
* 
*  -  recall that we are usingan explicit scheme, so we do not need to store any matrices that aretypically much larger than a few vectors), and the reduced memory access onthe other. In this program, we are particularly interested in the latteraspect. Since cost of operator evaluation is only a small multiple of the costof simply streaming the input and output vector from memory with the optimizedmatrix-free methods of deal.II, we must consider the cost of vector updates,and low-storage variants can deliver up to twice the throughput ofconventional explicit Runge--Kutta methods for this reason, see e.g. theanalysis in  [2.x.115] .
* Besides three variants for third, fourth and fifth order accuracy from thereference  [2.x.116] , we also use a fourth-order accuratevariant with seven stages that was optimized for acoustics setups from [2.x.117] . Acoustic problems are one of the interesting aspects ofthe subsonic regime of the Euler equations where compressibility leads to thetransmission of sound waves; often, one uses further simplifications of thelinearized Euler equations around a background state or the acoustic waveequation around a fixed frame.
* 

* [1.x.70][1.x.71]
* 

* The major ingredients used in this program are the fast matrix-free techniqueswe use to evaluate the operator  [2.x.118]  and the inverse mass matrix [2.x.119] . Actually, the term [1.x.72] is a slight misnomer,because we are working with a nonlinear operator and do not linearize theoperator that in turn could be represented by a matrix. However, fastevaluation of integrals has become popular as a replacement of sparsematrix-vector products, as shown in  [2.x.120]  and  [2.x.121] , and we have coinedthis infrastructure [1.x.73] in deal.II for thisreason. Furthermore, the inverse mass matrix is indeed applied in amatrix-free way, as detailed below.
* The matrix-free infrastructure allows us to quickly evaluate the integrals inthe weak forms. The ingredients are the fast interpolation from solutioncoefficients into values and derivatives at quadrature points, point-wiseoperations at quadrature points (where we implement the differential operatoras derived above), as well as multiplication by all test functions andsummation over quadrature points. The first and third component make use ofsum factorization and have been extensively discussed in the  [2.x.122]  tutorialprogram for the cell integrals and  [2.x.123]  for the face integrals. The onlydifference is that we now deal with a system of  [2.x.124]  components, rather thanthe scalar systems in previous tutorial programs. In the code, all thatchanges is a template argument of the FEEvaluation and FEFaceEvaluationclasses, the one to set the number of components. The access to the vector isthe same as before, all handled transparently by the evaluator. We also notethat the variant with a single evaluator chosen in the code below is not theonly choice
* 
*  -  we could also have used separate evalators for the separatecomponents  [2.x.125] ,  [2.x.126] , and  [2.x.127] ; given that we treat allcomponents similarly (also reflected in the way we state the equation as avector system), this would be more complicated here. As before, theFEEvaluation class provides explicit vectorization by combining the operationson several cells (and faces), involving data types calledVectorizedArray. Since the arithmetic operations are overloaded for this type,we do not have to bother with it all that much, except for the evaluation offunctions through the Function interface, where we need to provide particular[1.x.74] evaluations for several quadrature point locations at once.
* A more substantial change in this program is the operation at quadraturepoints: Here, the multi-component evaluators provide us with return types notdiscussed before. Whereas  [2.x.128]  would return a scalar(more precisely, a VectorizedArray type due to vectorization across cells) forthe Laplacian of  [2.x.129] , it now returns a type that is`Tensor<1,dim+2,VectorizedArray<Number>>`. Likewise, the gradient type is now`Tensor<1,dim+2,Tensor<1,dim,VectorizedArray<Number>>>`, where the outertensor collects the `dim+2` components of the Euler system, and the innertensor the partial derivatives in the various directions. For example, theflux  [2.x.130]  of the Euler system is of this type. In order to reduce the amount ofcode we have to write for spelling out these types, we use the C++ `auto`keyword where possible.
* From an implementation point of view, the nonlinearity is not a bigdifficulty: It is introduced naturally as we express the terms of the Eulerweak form, for example in the form of the momentum term  [2.x.131] . To obtain this expression, we first deduce the velocity [2.x.132]  from the momentum variable  [2.x.133] . Given that  [2.x.134]  is represented as a  [2.x.135] -degree polynomial, as is  [2.x.136] , thevelocity  [2.x.137]  is a rational expression in terms of the referencecoordinates  [2.x.138] . As we perform the multiplication  [2.x.139] , we obtain an expression that is theratio of two polynomials, with polynomial degree  [2.x.140]  in thenumerator and polynomial degree  [2.x.141]  in the denominator. Combined with thegradient of the test function, the integrand is of degree  [2.x.142]  in thenumerator and  [2.x.143]  in the denominator already for affine cells, i.e.,for parallelograms/ parallelepipeds.For curved cells, additional polynomial and rational expressionsappear when multiplying the integrand by the determinant of the Jacobian ofthe mapping. At this point, one usually needs to give up on insisting on exactintegration, and take whatever accuracy the Gaussian (more precisely,Gauss--Legrende) quadrature provides. The situation is then similar to the onefor the Laplace equation, where the integrand contains rational expressions onnon-affince cells and is also only integrated approximately. As these formulasonly integrate polynomials exactly, we have to live with the [1.x.75] in the form of an integration error.
* While inaccurate integration is usually tolerable for elliptic problems, forhyperbolic problems inexact integration causes some headache in the form of aneffect called [1.x.76]. The term comes from signal processing andexpresses the situation of inappropriate, too coarse sampling. In terms ofquadrature, the inappropriate sampling means that we use too few quadraturepoints compared to what would be required to accurately sample thevariable-coefficient integrand. It has been shown in the DG literature thataliasing errors can introduce unphysical oscillations in the numericalsolution for [1.x.77] resolved simulations. The fact that aliasing mostlyaffects coarse resolutions
* 
*  -  whereas finer meshes with the same schemework fine
* 
*  -  is not surprising because well-resolved simulationstend to be smooth on length-scales of a cell (i.e., they havesmall coefficients in the higher polynomial degrees that are missed bytoo few quadrature points, whereas the main solution contribution in the lowerpolynomial degrees is still well-captured
* 
*  -  this is simply a consequence of Taylor'stheorem). To address this topic, various approaches have been proposed in theDG literature. One technique is filtering which damps the solution componentspertaining to higher polynomial degrees. As the chosen nodal basis is nothierarchical, this would mean to transform from the nodal basis into ahierarchical one (e.g., a modal one based on Legendre polynomials) where thecontributions within a cell are split by polynomial degrees. In that basis,one could then multiply the solution coefficients associated with higherdegrees by a small number, keep the lower ones intact (to not destroy consistency), andthen transform back to the nodal basis. However, filters reduce the accuracy of themethod. Another, in some sense simpler, strategy is to use more quadraturepoints to capture non-linear terms more accurately. Using more than  [2.x.144] quadrature points per coordinate directions is sometimes calledover-integration or consistent integration. The latter name is most common inthe context of the incompressible Navier-Stokes equations, where the [2.x.145]  nonlinearity results in polynomial integrandsof degree  [2.x.146]  (when also considering the test function), which can beintegrated exactly with  [2.x.147]  quadraturepoints per direction as long as the element geometry is affine. In the contextof the Euler equations with non-polynomial integrands, the choice is lessclear. Depending on the variation in the various variables both [2.x.148]  or  [2.x.149]  points (integratingexactly polynomials of degree  [2.x.150]  or  [2.x.151] , respectively) are common.
* To reflect this variability in the choice of quadrature in the program, wekeep the number of quadrature points a variable to be specified just as thepolynomial degree, and note that one would make different choices dependingalso on the flow configuration. The default choice is  [2.x.152]  points
* 
*  -  a bitmore than the minimum possible of  [2.x.153]  points. The FEEvaluation andFEFaceEvaluation classes allow to seamlessly change the number of points by atemplate parameter, such that the program does not get more complicatedbecause of that.
* 

* [1.x.78][1.x.79]
* 

* The last ingredient is the evaluation of the inverse mass matrix  [2.x.154] . In DG methods with explicit time integration, mass matrices areblock-diagonal and thus easily inverted
* 
*  -  one only needs to invert thediagonal blocks. However, given the fact that matrix-free evaluation ofintegrals is closer in cost to the access of the vectors only, even theapplication of a block-diagonal matrix (e.g. via an array of LU factors) wouldbe several times more expensive than evaluation of  [2.x.155] simply because just storing and loading matrices of size`dofs_per_cell` times `dofs_per_cell` for higher order finite elementsrepeatedly is expensive. As this isclearly undesirable, part of the community has moved to bases where the massmatrix is diagonal, for example the [1.x.80]-orthogonal Legendre basis usinghierarchical polynomials or Lagrange polynomials on the points of the Gaussianquadrature (which is just another way of utilizing Legendreinformation). While the diagonal property breaks down for deformed elements,the error made by taking a diagonal mass matrix and ignoring the rest (avariant of mass lumping, though not the one with an additional integrationerror as utilized in  [2.x.156] ) has been shown to not alter discretizationaccuracy. The Lagrange basis in the points of Gaussian quadrature is sometimesalso referred to as a collocation setup, as the nodal points of thepolynomials coincide (= are "co-located") with the points of quadrature, obviating someinterpolation operations. Given the fact that we want to use more quadraturepoints for nonlinear terms in  [2.x.157] , however, the collocationproperty is lost. (More precisely, it is still used in FEEvaluation andFEFaceEvaluation after a change of basis, see the matrix-free paper [2.x.158] .)
* In this tutorial program, we use the collocation idea for the application ofthe inverse mass matrix, but with a slight twist. Rather than using thecollocation via Lagrange polynomials in the points of Gaussian quadrature, weprefer a conventional Lagrange basis in Gauss-Lobatto points as those make theevaluation of face integrals cheap. This is because for Gauss-Lobattopoints, some of the node points are located on the faces of the celland it is not difficult to show that on any given face, the only shapefunctions with non-zero values are exactly the ones whose node pointsare in fact located on that face. One could of course also use theGauss-Lobatto quadrature (with some additional integration error) as was donein  [2.x.159] , but we do not want to sacrifice accuracy as thesequadrature formulas are generally of lower order than the generalGauss quadrature formulas. Instead, we use an idea described in the reference [2.x.160]  where it was proposed to change the basis for thesake of applying the inverse mass matrix. Let us denote by  [2.x.161]  the matrix ofshape functions evaluated at quadrature points, with shape functions in the rowof the matrix and quadrature points in columns. Then, the mass matrix on a cell [2.x.162]  is given by[1.x.81]Here,  [2.x.163]  is the diagonal matrix with the determinant of the Jacobian timesthe quadrature weight (JxW) as entries. The matrix  [2.x.164]  is constructed as theKronecker product (tensor product) of one-dimensional matrices, e.g. in 3D as[1.x.82]which is the result of the basis functions being a tensor product ofone-dimensional shape functions and the quadrature formula being the tensorproduct of 1D quadrature formulas. For the case that the number of polynomialsequals the number of quadrature points, all matrices in  [2.x.165] are square, and also the ingredients to  [2.x.166]  in the Kronecker product aresquare. Thus, one can invert each matrix to form the overall inverse,[1.x.83]This formula is of exactly the same structure as the steps in the forwardevaluation of integrals with sum factorization techniques (i.e., theFEEvaluation and MatrixFree framework of deal.II). Hence, we can utilize thesame code paths with a different interpolation matrix, [2.x.167]  rather than  [2.x.168] .
* The class  [2.x.169]  implements thisoperation: It changes from the basis contained in the finite element (in thiscase, FE_DGQ) to the Lagrange basis in Gaussian quadrature points. Here, theinverse of a diagonal mass matrix can be evaluated, which is simply the inverseof the `JxW` factors (i.e., the quadrature weight times the determinant of theJacobian from reference to real coordinates). Once this is done, we can changeback to the standard nodal Gauss-Lobatto basis.
* The advantage of this particular way of applying the inverse mass matrix isa cost similar to the forward application of a mass matrix, which is cheaperthan the evaluation of the spatial operator  [2.x.170] with over-integration and face integrals. (Wewill demonstrate this with detailed timing information in the[1.x.84].) In fact, itis so cheap that it is limited by the bandwidth of reading the source vector,reading the diagonal, and writing into the destination vector on most modernarchitectures. The hardware used for the result section allows to do thecomputations at least twice as fast as the streaming of the vectors frommemory.
* 

* [1.x.85][1.x.86]
* 

* In this tutorial program, we implement two test cases. The first case is aconvergence test limited to two space dimensions. It runs a so-calledisentropic vortex which is transported via a background flow field. The secondcase uses a more exciting setup: We start with a cylinder immersed in achannel, using the  [2.x.171]  function. Here, weimpose a subsonic initial field at Mach number of  [2.x.172]  with aconstant velocity in  [2.x.173]  direction. At the top and bottom walls as well as atthe cylinder, we impose a no-penetration (i.e., tangential flow)condition. This setup forces the flow to re-orient as compared to the initialcondition, which results in a big sound wave propagating away from thecylinder. In upstream direction, the wave travels more slowly (as ithas to move against the oncoming gas), including adiscontinuity in density and pressure. In downstream direction, the transportis faster as sound propagation and fluid flow go in the same direction, which smearsout the discontinuity somewhat. Once the sound wave hits the upper and lowerwalls, the sound is reflected back, creating some nice shapes as illustratedin the [1.x.87] below.
* 

*  [1.x.88] [1.x.89]
*  The include files are similar to the previous matrix-free tutorial programs  [2.x.174] ,  [2.x.175] , and  [2.x.176] 
* 

* 
* [1.x.90]
* 
*  The following file includes the CellwiseInverseMassMatrix data structure that we will use for the mass matrix inversion, the only new include file for this tutorial program:
* 

* 
* [1.x.91]
* 
*  Similarly to the other matrix-free tutorial programs, we collect all parameters that control the execution of the program at the top of the file. Besides the dimension and polynomial degree we want to run with, we also specify a number of points in the Gaussian quadrature formula we want to use for the nonlinear terms in the Euler equations. Furthermore, we specify the time interval for the time-dependent problem, and implement two different test cases. The first one is an analytical solution in 2D, whereas the second is a channel flow around a cylinder as described in the introduction. Depending on the test case, we also change the final time up to which we run the simulation, and a variable `output_tick` that specifies in which intervals we want to write output (assuming that the tick is larger than the time step size).
* 

* 
* [1.x.92]
* 
*  Next off are some details of the time integrator, namely a Courant number that scales the time step size in terms of the formula  [2.x.177] , as well as a selection of a few low-storage Runge--Kutta methods. We specify the Courant number per stage of the Runge--Kutta scheme, as this gives a more realistic expression of the numerical cost for schemes of various numbers of stages.
* 

* 
* [1.x.93]
* 
*  Eventually, we select a detail of the spatial discretization, namely the numerical flux (Riemann solver) at the faces between cells. For this program, we have implemented a modified variant of the Lax--Friedrichs flux and the Harten--Lax--van Leer (HLL) flux.
* 

* 
* [1.x.94]
* 
*   [1.x.95]  [1.x.96]
* 

* 
*  We now define a class with the exact solution for the test case 0 and one with a background flow field for test case 1 of the channel. Given that the Euler equations are a problem with  [2.x.178]  equations in  [2.x.179]  dimensions, we need to tell the Function base class about the correct number of components.
* 

* 
* [1.x.97]
* 
*  As far as the actual function implemented is concerned, the analytical test case is an isentropic vortex case (see e.g. the book by Hesthaven and Warburton, Example 6.1 in Section 6.6 on page 209) which fulfills the Euler equations with zero force term on the right hand side. Given that definition, we return either the density, the momentum, or the energy depending on which component is requested. Note that the original definition of the density involves the  [2.x.180] -th power of some expression. Since  [2.x.181]  has pretty slow implementations on some systems, we replace it by logarithm followed by exponentiation (of base 2), which is mathematically equivalent but usually much better optimized. This formula might lose accuracy in the last digits for very small numbers compared to  [2.x.182]  but we are happy with it anyway, since small numbers map to data close to 1.   
*   For the channel test case, we simply select a density of 1, a velocity of 0.4 in  [2.x.183]  direction and zero in the other directions, and an energy that corresponds to a speed of sound of 1.3 measured against the background velocity field, computed from the relation  [2.x.184] .
* 

* 
* [1.x.98]
* 
*   [1.x.99]  [1.x.100]
* 

* 
*  The next few lines implement a few low-storage variants of Runge--Kutta methods. These methods have specific Butcher tableaux with coefficients  [2.x.185]  and  [2.x.186]  as shown in the introduction. As usual in Runge--Kutta method, we can deduce time steps,  [2.x.187]  from those coefficients. The main advantage of this kind of scheme is the fact that only two vectors are needed per stage, namely the accumulated part of the solution  [2.x.188]  (that will hold the solution  [2.x.189]  at the new time  [2.x.190]  after the last stage), the update vector  [2.x.191]  that gets evaluated during the stages, plus one vector  [2.x.192]  to hold the evaluation of the operator. Such a Runge--Kutta setup reduces the memory storage and memory access. As the memory bandwidth is often the performance-limiting factor on modern hardware when the evaluation of the differential operator is well-optimized, performance can be improved over standard time integrators. This is true also when taking into account that a conventional Runge--Kutta scheme might allow for slightly larger time steps as more free parameters allow for better stability properties.   
*   In this tutorial programs, we concentrate on a few variants of low-storage schemes defined in the article by Kennedy, Carpenter, and Lewis (2000), as well as one variant described by Tselios and Simos (2007). There is a large series of other schemes available, which could be addressed by additional sets of coefficients or slightly different update formulas.   
*   We define a single class for the four integrators, distinguished by the enum described above. To each scheme, we then fill the vectors for the  [2.x.193]  and  [2.x.194]  to the given variables in the class.
* 

* 
* [1.x.101]
* 
*  First comes the three-stage scheme of order three by Kennedy et al. (2000). While its stability region is significantly smaller than for the other schemes, it only involves three stages, so it is very competitive in terms of the work per stage.
* 

* 
* [1.x.102]
* 
*  The next scheme is a five-stage scheme of order four, again defined in the paper by Kennedy et al. (2000).
* 

* 
* [1.x.103]
* 
*  The following scheme of seven stages and order four has been explicitly derived for acoustics problems. It is a balance of accuracy for imaginary eigenvalues among fourth order schemes, combined with a large stability region. Since DG schemes are dissipative among the highest frequencies, this does not necessarily translate to the highest possible time step per stage. In the context of the present tutorial program, the numerical flux plays a crucial role in the dissipation and thus also the maximal stable time step size. For the modified Lax--Friedrichs flux, this scheme is similar to the `stage_5_order_4` scheme in terms of step size per stage if only stability is considered, but somewhat less efficient for the HLL flux.
* 

* 
* [1.x.104]
* 
*  The last scheme included here is the nine-stage scheme of order five from Kennedy et al. (2000). It is the most accurate among the schemes used here, but the higher order of accuracy sacrifices some stability, so the step length normalized per stage is less than for the fourth order schemes.
* 

* 
* [1.x.105]
* 
*  The main function of the time integrator is to go through the stages, evaluate the operator, prepare the  [2.x.195]  vector for the next evaluation, and update the solution vector  [2.x.196] . We hand off the work to the `pde_operator` involved in order to be able to merge the vector operations of the Runge--Kutta setup with the evaluation of the differential operator for better performance, so all we do here is to delegate the vectors and coefficients.     
*   We separately call the operator for the first stage because we need slightly modified arguments there: We evaluate the solution from the old solution  [2.x.197]  rather than a  [2.x.198]  vector, so the first argument is `solution`. We here let the stage vector  [2.x.199]  also hold the temporary result of the evaluation, as it is not used otherwise. For all subsequent stages, we use the vector `vec_ki` as the second vector argument to store the result of the operator evaluation. Finally, when we are at the last stage, we must skip the computation of the vector  [2.x.200]  as there is no coefficient  [2.x.201]  available (nor will it be used).
* 

* 
* [1.x.106]
* 
*   [1.x.107]  [1.x.108]
* 

* 
*  In the following functions, we implement the various problem-specific operators pertaining to the Euler equations. Each function acts on the vector of conserved variables  [2.x.202]  that we hold in the solution vectors, and computes various derived quantities.   
*   First out is the computation of the velocity, that we derive from the momentum variable  [2.x.203]  by division by  [2.x.204] . One thing to note here is that we decorate all those functions with the keyword `DEAL_II_ALWAYS_INLINE`. This is a special macro that maps to a compiler-specific keyword that tells the compiler to never create a function call for any of those functions, and instead move the implementation [1.x.109] to where they are called. This is critical for performance because we call into some of those functions millions or billions of times: For example, we both use the velocity for the computation of the flux further down, but also for the computation of the pressure, and both of these places are evaluated at every quadrature point of every cell. Making sure these functions are inlined ensures not only that the processor does not have to execute a jump instruction into the function (and the corresponding return jump), but also that the compiler can re-use intermediate information from one function's context in code that comes after the place where the function was called. (We note that compilers are generally quite good at figuring out which functions to inline by themselves. Here is a place where compilers may or may not have figured it out by themselves but where we know for sure that inlining is a win.)   
*   Another trick we apply is a separate variable for the inverse density  [2.x.205] . This enables the compiler to only perform a single division for the flux, despite the division being used at several places. As divisions are around ten to twenty times as expensive as multiplications or additions, avoiding redundant divisions is crucial for performance. We note that taking the inverse first and later multiplying with it is not equivalent to a division in floating point arithmetic due to roundoff effects, so the compiler is not allowed to exchange one way by the other with standard optimization flags. However, it is also not particularly difficult to write the code in the right way.   
*   To summarize, the chosen strategy of always inlining and careful definition of expensive arithmetic operations allows us to write compact code without passing all intermediate results around, despite making sure that the code maps to excellent machine code.
* 

* 
* [1.x.110]
* 
*  The next function computes the pressure from the vector of conserved variables, using the formula  [2.x.206] . As explained above, we use the velocity from the `euler_velocity()` function. Note that we need to specify the first template argument `dim` here because the compiler is not able to deduce it from the arguments of the tensor, whereas the second argument (number type) can be automatically deduced.
* 

* 
* [1.x.111]
* 
*  Here is the definition of the Euler flux function, i.e., the definition of the actual equation. Given the velocity and pressure (that the compiler optimization will make sure are done only once), this is straight-forward given the equation stated in the introduction.
* 

* 
* [1.x.112]
* 
*  This next function is a helper to simplify the implementation of the numerical flux, implementing the action of a tensor of tensors (with non-standard outer dimension of size `dim + 2`, so the standard overloads provided by deal.II's tensor classes do not apply here) with another tensor of the same inner dimension, i.e., a matrix-vector product.
* 

* 
* [1.x.113]
* 
*  This function implements the numerical flux (Riemann solver). It gets the state from the two sides of an interface and the normal vector, oriented from the side of the solution  [2.x.207]  towards the solution  [2.x.208] . In finite volume methods which rely on piece-wise constant data, the numerical flux is the central ingredient as it is the only place where the physical information is entered. In DG methods, the numerical flux is less central due to the polynomials within the elements and the physical flux used there. As a result of higher-degree interpolation with consistent values from both sides in the limit of a continuous solution, the numerical flux can be seen as a control of the jump of the solution from both sides to weakly impose continuity. It is important to realize that a numerical flux alone cannot stabilize a high-order DG method in the presence of shocks, and thus any DG method must be combined with further shock-capturing techniques to handle those cases. In this tutorial, we focus on wave-like solutions of the Euler equations in the subsonic regime without strong discontinuities where our basic scheme is sufficient.   
*   Nonetheless, the numerical flux is decisive in terms of the numerical dissipation of the overall scheme and influences the admissible time step size with explicit Runge--Kutta methods. We consider two choices, a modified Lax--Friedrichs scheme and the widely used Harten--Lax--van Leer (HLL) flux. For both variants, we first need to get the velocities and pressures from both sides of the interface and evaluate the physical Euler flux.   
*   For the local Lax--Friedrichs flux, the definition is  [2.x.209] , where the factor  [2.x.210]  gives the maximal wave speed and  [2.x.211]  is the speed of sound. Here, we choose two modifications of that expression for reasons of computational efficiency, given the small impact of the flux on the solution. For the above definition of the factor  [2.x.212] , we would need to take four square roots, two for the two velocity norms and two for the speed of sound on either side. The first modification is hence to rather use  [2.x.213]  as an estimate of the maximal speed (which is at most a factor of 2 away from the actual maximum, as shown in the introduction). This allows us to pull the square root out of the maximum and get away with a single square root computation. The second modification is to further relax on the parameter  [2.x.214] ---the smaller it is, the smaller the dissipation factor (which is multiplied by the jump in  [2.x.215] , which might result in a smaller or bigger dissipation in the end). This allows us to fit the spectrum into the stability region of the explicit Runge--Kutta integrator with bigger time steps. However, we cannot make dissipation too small because otherwise imaginary eigenvalues grow larger. Finally, the current conservative formulation is not energy-stable in the limit of  [2.x.216]  as it is not skew-symmetric, and would need additional measures such as split-form DG schemes in that case.   
*   For the HLL flux, we follow the formula from literature, introducing an additional weighting of the two states from Lax--Friedrichs by a parameter  [2.x.217] . It is derived from the physical transport directions of the Euler equations in terms of the current direction of velocity and sound speed. For the velocity, we here choose a simple arithmetic average which is sufficient for DG scenarios and moderate jumps in material parameters.   
*   Since the numerical flux is multiplied by the normal vector in the weak form, we multiply by the result by the normal vector for all terms in the equation. In these multiplications, the `operator*` defined above enables a compact notation similar to the mathematical definition.   
*   In this and the following functions, we use variable suffixes `_m` and `_p` to indicate quantities derived from  [2.x.218]  and  [2.x.219] , i.e., values "here" and "there" relative to the current cell when looking at a neighbor cell.
* 

* 
* [1.x.114]
* 
*  This and the next function are helper functions to provide compact evaluation calls as multiple points get batched together via a VectorizedArray argument (see the  [2.x.220]  tutorial for details). This function is used for the subsonic outflow boundary conditions where we need to set the energy component to a prescribed value. The next one requests the solution on all components and is used for inflow boundaries where all components of the solution are set.
* 

* 
* [1.x.115]
* 
*   [1.x.116]  [1.x.117]
* 

* 
*  This class implements the evaluators for the Euler problem, in analogy to the `LaplaceOperator` class of  [2.x.221]  or  [2.x.222] . Since the present operator is non-linear and does not require a matrix interface (to be handed over to preconditioners), we skip the various `vmult` functions otherwise present in matrix-free operators and only implement an `apply` function as well as the combination of `apply` with the required vector updates for the low-storage Runge--Kutta time integrator mentioned above (called `perform_stage`). Furthermore, we have added three additional functions involving matrix-free routines, namely one to compute an estimate of the time step scaling (that is combined with the Courant number for the actual time step size) based on the velocity and speed of sound in the elements, one for the projection of solutions (specializing  [2.x.223]  for the DG case), and one to compute the errors against a possible analytical solution or norms against some background state.   
*   The rest of the class is similar to other matrix-free tutorials. As discussed in the introduction, we provide a few functions to allow a user to pass in various forms of boundary conditions on different parts of the domain boundary marked by  [2.x.224]  variables, as well as possible body forces.
* 

* 
* [1.x.118]
* 
*  For the initialization of the Euler operator, we set up the MatrixFree variable contained in the class. This can be done given a mapping to describe possible curved boundaries as well as a DoFHandler object describing the degrees of freedom. Since we use a discontinuous Galerkin discretization in this tutorial program where no constraints are imposed strongly on the solution field, we do not need to pass in an AffineConstraints object and rather use a dummy for the construction. With respect to quadrature, we want to select two different ways of computing the underlying integrals: The first is a flexible one, based on a template parameter `n_points_1d` (that will be assigned the `n_q_points_1d` value specified at the top of this file). More accurate integration is necessary to avoid the aliasing problem due to the variable coefficients in the Euler operator. The second less accurate quadrature formula is a tight one based on `fe_degree+1` and needed for the inverse mass matrix. While that formula provides an exact inverse only on affine element shapes and not on deformed elements, it enables the fast inversion of the mass matrix by tensor product techniques, necessary to ensure optimal computational efficiency overall.
* 

* 
* [1.x.119]
* 
*  The subsequent four member functions are the ones that must be called from outside to specify the various types of boundaries. For an inflow boundary, we must specify all components in terms of density  [2.x.225] , momentum  [2.x.226]  and energy  [2.x.227] . Given this information, we then store the function alongside the respective boundary id in a map member variable of this class. Likewise, we proceed for the subsonic outflow boundaries (where we request a function as well, which we use to retrieve the energy) and for wall (no-penetration) boundaries where we impose zero normal velocity (no function necessary, so we only request the boundary id). For the present DG code where boundary conditions are solely applied as part of the weak form (during time integration), the call to set the boundary conditions can appear both before or after the `reinit()` call to this class. This is different from continuous finite element codes where the boundary conditions determine the content of the AffineConstraints object that is sent into MatrixFree for initialization, thus requiring to be set before the initialization of the matrix-free data structures.   
*   The checks added in each of the four function are used to ensure that boundary conditions are mutually exclusive on the various parts of the boundary, i.e., that a user does not accidentally designate a boundary as both an inflow and say a subsonic outflow boundary.
* 

* 
* [1.x.120]
* 
*   [1.x.121]  [1.x.122]
* 

* 
*  Now we proceed to the local evaluators for the Euler problem. The evaluators are relatively simple and follow what has been presented in  [2.x.228] ,  [2.x.229] , or  [2.x.230] . The first notable difference is the fact that we use an FEEvaluation with a non-standard number of quadrature points. Whereas we previously always set the number of quadrature points to equal the polynomial degree plus one (ensuring exact integration on affine element shapes), we now set the number quadrature points as a separate variable (e.g. the polynomial degree plus two or three halves of the polynomial degree) to more accurately handle nonlinear terms. Since the evaluator is fed with the appropriate loop lengths via the template argument and keeps the number of quadrature points in the whole cell in the variable  [2.x.231]  we now automatically operate on the more accurate formula without further changes.   
*   The second difference is due to the fact that we are now evaluating a multi-component system, as opposed to the scalar systems considered previously. The matrix-free framework provides several ways to handle the multi-component case. The variant shown here utilizes an FEEvaluation object with multiple components embedded into it, specified by the fourth template argument `dim + 2` for the components in the Euler system. As a consequence, the return type of  [2.x.232]  is not a scalar any more (that would return a VectorizedArray type, collecting data from several elements), but a Tensor of `dim+2` components. The functionality is otherwise similar to the scalar case; it is handled by a template specialization of a base class, called FEEvaluationAccess. An alternative variant would have been to use several FEEvaluation objects, a scalar one for the density, a vector-valued one with `dim` components for the momentum, and another scalar evaluator for the energy. To ensure that those components point to the correct part of the solution, the constructor of FEEvaluation takes three optional integer arguments after the required MatrixFree field, namely the number of the DoFHandler for multi-DoFHandler systems (taking the first by default), the number of the quadrature point in case there are multiple Quadrature objects (see more below), and as a third argument the component within a vector system. As we have a single vector for all components, we would go with the third argument, and set it to `0` for the density, `1` for the vector-valued momentum, and `dim+1` for the energy slot. FEEvaluation then picks the appropriate subrange of the solution vector during  [2.x.233]  and  [2.x.234]  or the more compact  [2.x.235]  and  [2.x.236]  calls.   
*   When it comes to the evaluation of the body force vector, we distinguish between two cases for efficiency reasons: In case we have a constant function (derived from  [2.x.237]  we can precompute the value outside the loop over quadrature points and simply use the value everywhere. For a more general function, we instead need to call the `evaluate_function()` method we provided above; this path is more expensive because we need to access the memory associated with the quadrature point data.   
*   The rest follows the other tutorial programs. Since we have implemented all physics for the Euler equations in the separate `euler_flux()` function, all we have to do here is to call this function given the current solution evaluated at quadrature points, returned by `phi.get_value(q)`, and tell the FEEvaluation object to queue the flux for testing it by the gradients of the shape functions (which is a Tensor of outer `dim+2` components, each holding a tensor of `dim` components for the  [2.x.238]  component of the Euler flux). One final thing worth mentioning is the order in which we queue the data for testing by the value of the test function, `phi.submit_value()`, in case we are given an external function: We must do this after calling `phi.get_value(q)`, because `get_value()` (reading the solution) and `submit_value()` (queuing the value for multiplication by the test function and summation over quadrature points) access the same underlying data field. Here it would be easy to achieve also without temporary variable `w_q` since there is no mixing between values and gradients. For more complicated setups, one has to first copy out e.g. both the value and gradient at a quadrature point and then queue results again by  [2.x.239]  and  [2.x.240]    
*   As a final note, we mention that we do not use the first MatrixFree argument of this function, which is a call-back from  [2.x.241]  The interfaces imposes the present list of arguments, but since we are in a member function where the MatrixFree object is already available as the `data` variable, we stick with that to avoid confusion.
* 

* 
* [1.x.123]
* 
*  The next function concerns the computation of integrals on interior faces, where we need evaluators from both cells adjacent to the face. We associate the variable `phi_m` with the solution component  [2.x.242]  and the variable `phi_p` with the solution component  [2.x.243] . We distinguish the two sides in the constructor of FEFaceEvaluation by the second argument, with `true` for the interior side and `false` for the exterior side, with interior and exterior denoting the orientation with respect to the normal vector.   
*   Note that the calls  [2.x.244]  and  [2.x.245]  combine the access to the vectors and the sum factorization parts. This combined operation not only saves a line of code, but also contains an important optimization: Given that we use a nodal basis in terms of the Lagrange polynomials in the points of the Gauss-Lobatto quadrature formula, only  [2.x.246]  out of the  [2.x.247]  basis functions evaluate to non-zero on each face. Thus, the evaluator only accesses the necessary data in the vector and skips the parts which are multiplied by zero. If we had first read the vector, we would have needed to load all data from the vector, as the call in isolation would not know what data is required in subsequent operations. If the subsequent  [2.x.248]  call requests values and derivatives, indeed all  [2.x.249]  vector entries for each component are needed, as the normal derivative is nonzero for all basis functions.   
*   The arguments to the evaluators as well as the procedure is similar to the cell evaluation. We again use the more accurate (over-)integration scheme due to the nonlinear terms, specified as the third template argument in the list. At the quadrature points, we then go to our free-standing function for the numerical flux. It receives the solution evaluated at quadrature points from both sides (i.e.,  [2.x.250]  and  [2.x.251] ), as well as the normal vector onto the minus side. As explained above, the numerical flux is already multiplied by the normal vector from the minus side. We need to switch the sign because the boundary term comes with a minus sign in the weak form derived in the introduction. The flux is then queued for testing both on the minus sign and on the plus sign, with switched sign as the normal vector from the plus side is exactly opposed to the one from the minus side.
* 

* 
* [1.x.124]
* 
*  For faces located at the boundary, we need to impose the appropriate boundary conditions. In this tutorial program, we implement four cases as mentioned above. (A fifth case, for supersonic outflow conditions is discussed in the "Results" section below.) The discontinuous Galerkin method imposes boundary conditions not as constraints, but only weakly. Thus, the various conditions are imposed by finding an appropriate [1.x.125] quantity  [2.x.252]  that is then handed to the numerical flux function also used for the interior faces. In essence, we "pretend" a state on the outside of the domain in such a way that if that were reality, the solution of the PDE would satisfy the boundary conditions we want.   
*   For wall boundaries, we need to impose a no-normal-flux condition on the momentum variable, whereas we use a Neumann condition for the density and energy with  [2.x.253]  and  [2.x.254] . To achieve the no-normal flux condition, we set the exterior values to the interior values and subtract two times the velocity in wall-normal direction, i.e., in the direction of the normal vector.   
*   For inflow boundaries, we simply set the given Dirichlet data  [2.x.255]  as a boundary value. An alternative would have been to use  [2.x.256] , the so-called mirror principle.   
*   The imposition of outflow is essentially a Neumann condition, i.e., setting  [2.x.257] . For the case of subsonic outflow, we still need to impose a value for the energy, which we derive from the respective function. A special step is needed for the case of [1.x.126], i.e., the case where there is a momentum flux into the domain on the Neumann portion. According to the literature (a fact that can be derived by appropriate energy arguments), we must switch to another variant of the flux on inflow parts, see Gravemeier, Comerford, Yoshihara, Ismail, Wall, "A novel formulation for Neumann inflow conditions in biomechanics", Int. J. Numer. Meth. Biomed. Eng., vol. 28 (2012). Here, the momentum term needs to be added once again, which corresponds to removing the flux contribution on the momentum variables. We do this in a post-processing step, and only for the case when we both are at an outflow boundary and the dot product between the normal vector and the momentum (or, equivalently, velocity) is negative. As we work on data of several quadrature points at once for SIMD vectorizations, we here need to explicitly loop over the array entries of the SIMD array.   
*   In the implementation below, we check for the various types of boundaries at the level of quadrature points. Of course, we could also have moved the decision out of the quadrature point loop and treat entire faces as of the same kind, which avoids some map/set lookups in the inner loop over quadrature points. However, the loss of efficiency is hardly noticeable, so we opt for the simpler code here. Also note that the final `else` clause will catch the case when some part of the boundary was not assigned any boundary condition via  [2.x.258] 
* 

* 
* [1.x.127]
* 
*  The next function implements the inverse mass matrix operation. The algorithms and rationale have been discussed extensively in the introduction, so we here limit ourselves to the technicalities of the  [2.x.259]  class. It does similar operations as the forward evaluation of the mass matrix, except with a different interpolation matrix, representing the inverse  [2.x.260]  factors. These represent a change of basis from the specified basis (in this case, the Lagrange basis in the points of the Gauss--Lobatto quadrature formula) to the Lagrange basis in the points of the Gauss quadrature formula. In the latter basis, we can apply the inverse of the point-wise `JxW` factor, i.e., the quadrature weight times the determinant of the Jacobian of the mapping from reference to real coordinates. Once this is done, the basis is changed back to the nodal Gauss-Lobatto basis again. All of these operations are done by the `apply()` function below. What we need to provide is the local fields to operate on (which we extract from the global vector by an FEEvaluation object) and write the results back to the destination vector of the mass matrix operation.   
*   One thing to note is that we added two integer arguments (that are optional) to the constructor of FEEvaluation, the first being 0 (selecting among the DoFHandler in multi-DoFHandler systems; here, we only have one) and the second being 1 to make the quadrature formula selection. As we use the quadrature formula 0 for the over-integration of nonlinear terms, we use the formula 1 with the default  [2.x.261]  (or `fe_degree+1` in terms of the variable name) points for the mass matrix. This leads to square contributions to the mass matrix and ensures exact integration, as explained in the introduction.
* 

* 
* [1.x.128]
* 
*   [1.x.129]  [1.x.130]
* 

* 
*  We now come to the function which implements the evaluation of the Euler operator as a whole, i.e.,  [2.x.262] , calling into the local evaluators presented above. The steps should be clear from the previous code. One thing to note is that we need to adjust the time in the functions we have associated with the various parts of the boundary, in order to be consistent with the equation in case the boundary data is time-dependent. Then, we call  [2.x.263]  to perform the cell and face integrals, including the necessary ghost data exchange in the `src` vector. The seventh argument to the function, `true`, specifies that we want to zero the `dst` vector as part of the loop, before we start accumulating integrals into it. This variant is preferred over explicitly calling `dst = 0.;` before the loop as the zeroing operation is done on a subrange of the vector in parts that are written by the integrals nearby. This enhances data locality and allows for caching, saving one roundtrip of vector data to main memory and enhancing performance. The last two arguments to the loop determine which data is exchanged: Since we only access the values of the shape functions one faces, typical of first-order hyperbolic problems, and since we have a nodal basis with nodes at the reference element surface, we only need to exchange those parts. This again saves precious memory bandwidth.   
*   Once the spatial operator  [2.x.264]  is applied, we need to make a second round and apply the inverse mass matrix. Here, we call  [2.x.265]  since only cell integrals appear. The cell loop is cheaper than the full loop as access only goes to the degrees of freedom associated with the locally owned cells, which is simply the locally owned degrees of freedom for DG discretizations. Thus, no ghost exchange is needed here.   
*   Around all these functions, we put timer scopes to record the computational time for statistics about the contributions of the various parts.
* 

* 
* [1.x.131]
* 
*  Let us move to the function that does an entire stage of a Runge--Kutta update. It calls  [2.x.266]  followed by some updates to the vectors, namely `next_ri = solution + factor_ai k_i` and `solution += factor_solution k_i`. Rather than performing these steps through the vector interfaces, we here present an alternative strategy that is faster on cache-based architectures. As the memory consumed by the vectors is often much larger than what fits into caches, the data has to effectively come from the slow RAM memory. The situation can be improved by loop fusion, i.e., performing both the updates to `next_ki` and `solution` within a single sweep. In that case, we would read the two vectors `rhs` and `solution` and write into `next_ki` and `solution`, compared to at least 4 reads and two writes in the baseline case. Here, we go one step further and perform the loop immediately when the mass matrix inversion has finished on a part of the vector.  [2.x.267]  provides a mechanism to attach an  [2.x.268]  both before the loop over cells first touches a vector entry (which we do not use here, but is e.g. used for zeroing the vector) and a second  [2.x.269]  to be called after the loop last touches an entry. The callback is in form of a range over the given vector (in terms of the local index numbering in the MPI universe) that can be addressed by `local_element()` functions.   
*   For this second callback, we create a lambda that works on a range and write the respective update on this range. Ideally, we would add the `DEAL_II_OPENMP_SIMD_PRAGMA` before the local loop to suggest to the compiler to SIMD parallelize this loop (which means in practice that we ensure that there is no overlap, also called aliasing, between the index ranges of the pointers we use inside the loops). It turns out that at the time of this writing, GCC 7.2 fails to compile an OpenMP pragma inside a lambda function, so we comment this pragma out below. If your compiler is newer, you should be able to uncomment these lines again.   
*   Note that we select a different code path for the last Runge--Kutta stage when we do not need to update the `next_ri` vector. This strategy gives a considerable speedup. Whereas the inverse mass matrix and vector updates take more than 60% of the computational time with default vector updates on a 40-core machine, the percentage is around 35% with the more optimized variant. In other words, this is a speedup of around a third.
* 

* 
* [1.x.132]
* 
*  Having discussed the implementation of the functions that deal with advancing the solution by one time step, let us now move to functions that implement other, ancillary operations. Specifically, these are functions that compute projections, evaluate errors, and compute the speed of information transport on a cell.   
*   The first of these functions is essentially equivalent to  [2.x.270]  just much faster because it is specialized for DG elements where there is no need to set up and solve a linear system, as each element has independent basis functions. The reason why we show the code here, besides a small speedup of this non-critical operation, is that it shows additional functionality provided by  [2.x.271]    
*   The projection operation works as follows: If we denote the matrix of shape functions evaluated at quadrature points by  [2.x.272] , the projection on cell  [2.x.273]  is an operation of the form  [2.x.274] , where  [2.x.275]  is the diagonal matrix containing the determinant of the Jacobian times the quadrature weight (JxW),  [2.x.276]  is the cell-wise mass matrix, and  [2.x.277]  is the evaluation of the field to be projected onto quadrature points. (In reality the matrix  [2.x.278]  has additional structure through the tensor product, as explained in the introduction.) This system can now equivalently be written as  [2.x.279] . Now, the term  [2.x.280]  and then  [2.x.281]  cancel, resulting in the final expression  [2.x.282] . This operation is implemented by  [2.x.283]  The name is derived from the fact that this projection is simply the multiplication by  [2.x.284] , a basis change from the nodal basis in the points of the Gaussian quadrature to the given finite element basis. Note that we call  [2.x.285]  to write the result into the vector, overwriting previous content, rather than accumulating the results as typical in integration tasks
* 
*  -  we can do this because every vector entry has contributions from only a single cell for discontinuous Galerkin discretizations.
* 

* 
* [1.x.133]
* 
*  The next function again repeats functionality also provided by the deal.II library, namely  [2.x.286]  We here show the explicit code to highlight how the vectorization across several cells works and how to accumulate results via that interface: Recall that each [1.x.134] of the vectorized array holds data from a different cell. By the loop over all cell batches that are owned by the current MPI process, we could then fill a VectorizedArray of results; to obtain a global sum, we would need to further go on and sum across the entries in the SIMD array. However, such a procedure is not stable as the SIMD array could in fact not hold valid data for all its lanes. This happens when the number of locally owned cells is not a multiple of the SIMD width. To avoid invalid data, we must explicitly skip those invalid lanes when accessing the data. While one could imagine that we could make it work by simply setting the empty lanes to zero (and thus, not contribute to a sum), the situation is more complicated than that: What if we were to compute a velocity out of the momentum? Then, we would need to divide by the density, which is zero
* 
*  -  the result would consequently be NaN and contaminate the result. This trap is avoided by accumulating the results from the valid SIMD range as we loop through the cell batches, using the function  [2.x.287]  to give us the number of lanes with valid data. It equals  [2.x.288]  on most cells, but can be less on the last cell batch if the number of cells has a remainder compared to the SIMD width.
* 

* 
* [1.x.135]
* 
*  This final function of the EulerOperator class is used to estimate the transport speed, scaled by the mesh size, that is relevant for setting the time step size in the explicit time integrator. In the Euler equations, there are two speeds of transport, namely the convective velocity  [2.x.289]  and the propagation of sound waves with sound speed  [2.x.290]  relative to the medium moving at velocity  [2.x.291] .   
*   In the formula for the time step size, we are interested not by these absolute speeds, but by the amount of time it takes for information to cross a single cell. For information transported along with the medium,  [2.x.292]  is scaled by the mesh size, so an estimate of the maximal velocity can be obtained by computing  [2.x.293] , where  [2.x.294]  is the Jacobian of the transformation from real to the reference domain. Note that  [2.x.295]  returns the inverse and transpose Jacobian, representing the metric term from real to reference coordinates, so we do not need to transpose it again. We store this limit in the variable `convective_limit` in the code below.   
*   The sound propagation is isotropic, so we need to take mesh sizes in any direction into account. The appropriate mesh size scaling is then given by the minimal singular value of  [2.x.296]  or, equivalently, the maximal singular value of  [2.x.297] . Note that one could approximate this quantity by the minimal distance between vertices of a cell when ignoring curved cells. To get the maximal singular value of the Jacobian, the general strategy would be some LAPACK function. Since all we need here is an estimate, we can avoid the hassle of decomposing a tensor of VectorizedArray numbers into several matrices and go into an (expensive) eigenvalue function without vectorization, and instead use a few iterations (five in the code below) of the power method applied to  [2.x.298] . The speed of convergence of this method depends on the ratio of the largest to the next largest eigenvalue and the initial guess, which is the vector of all ones. This might suggest that we get slow convergence on cells close to a cube shape where all lengths are almost the same. However, this slow convergence means that the result will sit between the two largest singular values, which both are close to the maximal value anyway. In all other cases, convergence will be quick. Thus, we can merely hardcode 5 iterations here and be confident that the result is good.
* 

* 
* [1.x.136]
* 
*  Similarly to the previous function, we must make sure to accumulate speed only on the valid cells of a cell batch.
* 

* 
* [1.x.137]
* 
*   [1.x.138]  [1.x.139]
* 

* 
*  This class combines the EulerOperator class with the time integrator and the usual global data structures such as FiniteElement and DoFHandler, to actually run the simulations of the Euler problem.   
*   The member variables are a triangulation, a finite element, a mapping (to create high-order curved surfaces, see e.g.  [2.x.299] ), and a DoFHandler to describe the degrees of freedom. In addition, we keep an instance of the EulerOperator described above around, which will do all heavy lifting in terms of integrals, and some parameters for time integration like the current time or the time step size.   
*   Furthermore, we use a PostProcessor instance to write some additional information to the output file, in similarity to what was done in  [2.x.300] . The interface of the DataPostprocessor class is intuitive, requiring us to provide information about what needs to be evaluated (typically only the values of the solution, except for the Schlieren plot that we only enable in 2D where it makes sense), and the names of what gets evaluated. Note that it would also be possible to extract most information by calculator tools within visualization programs such as ParaView, but it is so much more convenient to do it already when writing the output.
* 

* 
* [1.x.140]
* 
*  For the main evaluation of the field variables, we first check that the lengths of the arrays equal the expected values (the lengths `2*dim+4` or `2*dim+5` are derived from the sizes of the names we specify in the get_names() function below). Then we loop over all evaluation points and fill the respective information: First we fill the primal solution variables of density  [2.x.301] , momentum  [2.x.302]  and energy  [2.x.303] , then we compute the derived velocity  [2.x.304] , the pressure  [2.x.305] , the speed of sound  [2.x.306] , as well as the Schlieren plot showing  [2.x.307]  in case it is enabled. (See  [2.x.308]  for another example where we create a Schlieren plot.)
* 

* 
* [1.x.141]
* 
*  For the interpretation of quantities, we have scalar density, energy, pressure, speed of sound, and the Schlieren plot, and vectors for the momentum and the velocity.
* 

* 
* [1.x.142]
* 
*  With respect to the necessary update flags, we only need the values for all quantities but the Schlieren plot, which is based on the density gradient.
* 

* 
* [1.x.143]
* 
*  The constructor for this class is unsurprising: We set up a parallel triangulation based on the `MPI_COMM_WORLD` communicator, a vector finite element with `dim+2` components for density, momentum, and energy, a high-order mapping of the same degree as the underlying finite element, and initialize the time and time step to zero.
* 

* 
* [1.x.144]
* 
*  As a mesh, this tutorial program implements two options, depending on the global variable `testcase`: For the analytical variant (`testcase==0`), the domain is  [2.x.309] , with Dirichlet boundary conditions (inflow) all around the domain. For `testcase==1`, we set the domain to a cylinder in a rectangular box, derived from the flow past cylinder testcase for incompressible viscous flow by Sch&auml;fer and Turek (1996). Here, we have a larger variety of boundaries. The inflow part at the left of the channel is given the inflow type, for which we choose a constant inflow profile, whereas we set a subsonic outflow at the right. For the boundary around the cylinder (boundary id equal to 2) as well as the channel walls (boundary id equal to 3) we use the wall boundary type, which is no-normal flow. Furthermore, for the 3D cylinder we also add a gravity force in vertical direction. Having the base mesh in place (including the manifolds set by  [2.x.310]  we can then perform the specified number of global refinements, create the unknown numbering from the DoFHandler, and hand the DoFHandler and Mapping objects to the initialization of the EulerOperator.
* 

* 
* [1.x.145]
* 
*  In the following, we output some statistics about the problem. Because we often end up with quite large numbers of cells or degrees of freedom, we would like to print them with a comma to separate each set of three digits. This can be done via "locales", although the way this works is not particularly intuitive.  [2.x.311]  explains this in slightly more detail.
* 

* 
* [1.x.146]
* 
*  For output, we first let the Euler operator compute the errors of the numerical results. More precisely, we compute the error against the analytical result for the analytical solution case, whereas we compute the deviation against the background field with constant density and energy and constant velocity in  [2.x.312]  direction for the second test case.   
*   The next step is to create output. This is similar to what is done in  [2.x.313] : We let the postprocessor defined above control most of the output, except for the primal field that we write directly. For the analytical solution test case, we also perform another projection of the analytical solution and print the difference between that field and the numerical solution. Once we have defined all quantities to be written, we build the patches for output. Similarly to  [2.x.314] , we create a high-order VTK output by setting the appropriate flag, which enables us to visualize fields of high polynomial degrees. Finally, we call the  [2.x.315]  function to write the result to the given file name. This function uses special MPI parallel write facilities, which are typically more optimized for parallel file systems than the standard library's  [2.x.316]  variants used in most other tutorial programs. A particularly nice feature of the `write_vtu_in_parallel()` function is the fact that it can combine output from all MPI ranks into a single file, making it unnecessary to have a central record of all such files (namely, the "pvtu" file).   
*   For parallel programs, it is often instructive to look at the partitioning of cells among processors. To this end, one can pass a vector of numbers to  [2.x.317]  that contains as many entries as the current processor has active cells; these numbers should then be the rank of the processor that owns each of these cells. Such a vector could, for example, be obtained from  [2.x.318]  On the other hand, on each MPI process, DataOut will only read those entries that correspond to locally owned cells, and these of course all have the same value: namely, the rank of the current process. What is in the remaining entries of the vector doesn't actually matter, and so we can just get away with a cheap trick: We just fillall* values of the vector we give to  [2.x.319]  with the rank of the current MPI process. The key is that on each process, only the entries corresponding to the locally owned cells will be read, ignoring the (wrong) values in other entries. The fact that every process submits a vector in which the correct subset of entries is correct is all that is necessary.
* 

* 
* [1.x.147]
* 
*  The  [2.x.320]  function puts all pieces together. It starts off by calling the function that creates the mesh and sets up data structures, and then initializing the time integrator and the two temporary vectors of the low-storage integrator. We call these vectors `rk_register_1` and `rk_register_2`, and use the first vector to represent the quantity  [2.x.321]  and the second one for  [2.x.322]  in the formulas for the Runge--Kutta scheme outlined in the introduction. Before we start the time loop, we compute the time step size by the  [2.x.323]  function. For reasons of comparison, we compare the result obtained there with the minimal mesh size and print them to screen. For velocities and speeds of sound close to unity as in this tutorial program, the predicted effective mesh size will be close, but they could vary if scaling were different.
* 

* 
* [1.x.148]
* 
*  Now we are ready to start the time loop, which we run until the time has reached the desired end time. Every 5 time steps, we compute a new estimate for the time step
* 
*  -  since the solution is nonlinear, it is most effective to adapt the value during the course of the simulation. In case the Courant number was chosen too aggressively, the simulation will typically blow up with time step NaN, so that is easy to detect here. One thing to note is that roundoff errors might propagate to the leading digits due to an interaction of slightly different time step selections that in turn lead to slightly different solutions. To decrease this sensitivity, it is common practice to round or truncate the time step size to a few digits, e.g. 3 in this case. In case the current time is near the prescribed 'tick' value for output (e.g. 0.02), we also write the output. After the end of the time loop, we summarize the computation by printing some statistics, which is mostly done by the  [2.x.324]  function.
* 

* 
* [1.x.149]
* 
*  The main() function is not surprising and follows what was done in all previous MPI programs: As we run an MPI program, we need to call `MPI_Init()` and `MPI_Finalize()`, which we do through the  [2.x.325]  data structure. Note that we run the program only with MPI, and set the thread count to 1.
* 

* 
* [1.x.150]
* [1.x.151][1.x.152]
* 

* [1.x.153][1.x.154]
* 

* Running the program with the default settings on a machine with 40 processesproduces the following output:
* [1.x.155]
* 
* The program output shows that all errors are small. This is due to the factthat we use a relatively fine mesh of  [2.x.326]  cells with polynomials of degree5 for a solution that is smooth. An interesting pattern shows for the timestep size: whereas it is 0.0069 up to time 5, it increases to 0.0096 for latertimes. The step size increases once the vortex with some motion on top of thespeed of sound (and thus faster propagation) leaves the computational domainbetween times 5 and 6.5. After that point, the flow is simply uniformin the same direction, and the maximum velocity of the gas is reducedcompared to the previous state where the uniform velocity was overlaidby the vortex. Our time step formula recognizes this effect.
* The final block of output shows detailed information about the timingof individual parts of the programs; it breaks this down by showingthe time taken by the fastest and the slowest processor, and theaverage time
* 
*  -  this is often useful in very large computations tofind whether there are processors that are consistently overheated(and consequently are throttling their clock speed) or consistentlyslow for other reasons.The summary shows that 1283 time steps have been performedin 1.02 seconds (looking at the average time among all MPI processes), whilethe output of 11 files has taken additional 0.96 seconds. Broken down per timestep and into the five Runge--Kutta stages, the compute time per evaluation is0.16 milliseconds. This high performance is typical of matrix-free evaluatorsand a reason why explicit time integration is very competitive againstimplicit solvers, especially for large-scale simulations. The breakdown ofcomputational times at the end of the program run shows that the evaluation ofintegrals in  [2.x.327]  contributes with around 0.92 seconds and theapplication of the inverse mass matrix with 0.06 seconds. Furthermore, theestimation of the transport speed for the time step size computationcontributes with another 0.05 seconds of compute time.
* If we use three more levels of global refinement and 9.4 million DoFs in total,the final statistics are as follows (for the modified Lax--Friedrichs flux, [2.x.328] , and the same system of 40 cores of dual-socket Intel Xeon Gold 6230):
* [1.x.156]
* 
* Per time step, the solver now takes 0.02 seconds, about 25 times as long asfor the small problem with 147k unknowns. Given that the problem involves 64times as many unknowns, the increase in computing time is notsurprising. Since we also do 8 times as many time steps, the compute timeshould in theory increase by a factor of 512. The actual increase is 205 s /1.02 s = 202. This is because the small problem size cannot fully utilize the40 cores due to communication overhead. This becomes clear if we look into thedetails of the operations done per time step. The evaluation of thedifferential operator  [2.x.329]  with nearest neighbor communication goesfrom 0.92 seconds to 127 seconds, i.e., it increases with a factor of 138. Onthe other hand, the cost for application of the inverse mass matrix and thevector updates, which do not need to communicate between the MPI processes atall, has increased by a factor of 1195. The increase is more than thetheoretical factor of 512 because the operation is limited by the bandwidthfrom RAM memory for the larger size while for the smaller size, all vectorsfit into the caches of the CPU. The numbers show that the mass matrixevaluation and vector update part consume almost 40% of the time spent by theRunge--Kutta stages
* 
*  -  despite using a low-storage Runge--Kutta integrator andmerging of vector operations! And despite using over-integration for the [2.x.330]  operator. For simpler differential operators and more expensivetime integrators, the proportion spent in the mass matrix and vector updatepart can also reach 70%. If we compute a throughput number in terms of DoFsprocessed per second and Runge--Kutta stage, we obtain [1.x.157] This throughput number isvery high, given that simply copying one vector to another one runs atonly around 10,000 MDoFs/s.
* If we go to the next-larger size with 37.7 million DoFs, the overallsimulation time is 2196 seconds, with 1978 seconds spent in the timestepping. The increase in run time is a factor of 9.3 for the L_h operator(1179 versus 127 seconds) and a factor of 10.3 for the inverse mass matrix andvector updates (797 vs 77.5 seconds). The reason for this non-optimal increasein run time can be traced back to cache effects on the given hardware (with 40MB of L2 cache and 55 MB of L3 cache): While not all of the relevant data fitsinto caches for 9.4 million DoFs (one vector takes 75 MB and we have threevectors plus some additional data in MatrixFree), there is capacity for one anda half vector nonetheless. Given that modern caches are more sophisticated thanthe naive least-recently-used strategy (where we would have little re-use asthe data is used in a streaming-like fashion), we can assume that a sizeablefraction of data can indeed be delivered from caches for the 9.4 million DoFscase. For the larger case, even with optimal caching less than 10 percent ofdata would fit into caches, with an associated loss in performance.
* 

* [1.x.158][1.x.159]
* 

* For the modified Lax--Friedrichs flux and measuring the error in the momentumvariable, we obtain the following convergence table (the rates are verysimilar for the density and energy variables):
*  [2.x.331] 
* If we switch to the Harten-Lax-van Leer flux, the results are as follows: [2.x.332] 
* The tables show that we get optimal  [2.x.333] convergence rates for both numerical fluxes. The errors are slightly smallerfor the Lax--Friedrichs flux for  [2.x.334] , but the picture is reversed for [2.x.335] ; in any case, the differences on this testcase are relativelysmall.
* For  [2.x.336] , we reach the roundoff accuracy of  [2.x.337]  with bothfluxes on the finest grids. Also note that the errors are absolute with adomain length of  [2.x.338] , so relative errors are below  [2.x.339] . The HLL fluxis somewhat better for the highest degree, which is due to a slight inaccuracyof the Lax--Friedrichs flux: The Lax--Friedrichs flux sets a Dirichletcondition on the solution that leaves the domain, which results in a smallartificial reflection, which is accentuated for the Lax--Friedrichsflux. Apart from that, we see that the influence of the numerical flux isminor, as the polynomial part inside elements is the main driver of theaccucary. The limited influence of the flux also has consequences when tryingto approach more challenging setups with the higher-order DG setup: Taking forexample the parameters and grid of  [2.x.340] , we get oscillations (which in turnmake density negative and make the solution explode) with both fluxes once thehigh-mass part comes near the boundary, as opposed to the low-order finitevolume case ( [2.x.341] ). Thus, any case that leads to shocks in the solutionnecessitates some form of limiting or artificial dissipation. For anotheralternative, see the  [2.x.342]  tutorial program.
* 

* [1.x.166][1.x.167]
* 

* For the test case of the flow around a cylinder in a channel, we need tochange the first code line to
* [1.x.168]
* This test case starts with a background field of a constant velocityof Mach number 0.31 and a constant initial density; the flow will haveto go around an obstacle in the form of a cylinder. Since we impose ano-penetration condition on the cylinder walls, the flow thatinitially impinges head-on onto to cylinder has to rearrange,which creates a big sound wave. The following pictures show the pressure attimes 0.1, 0.25, 0.5, and 1.0 (top left to bottom right) for the 2D case with5 levels of global refinement, using 102,400 cells with polynomial degree of5 and 14.7 million degrees of freedom over all 4 solution variables.We clearly see the discontinuity thatpropagates slowly in the upstream direction and more quickly in downstreamdirection in the first snapshot at time 0.1. At time 0.25, the sound wave hasreached the top and bottom walls and reflected back to the interior. From thedifferent distances of the reflected waves from lower and upper walls we cansee the slight asymmetry of the Sch&auml;fer-Turek test case represented by [2.x.343]  with somewhat more space above thecylinder compared to below. At later times, the picture is more chaotic withmany sound waves all over the place.
*  [2.x.344] 
* The next picture shows an elevation plot of the pressure at time 1.0 lookingfrom the channel inlet towards the outlet at the same resolution
* 
*  -  here,we can see the large numberof reflections. In the figure, two types of waves are visible. Thelarger-amplitude waves correspond to various reflections that happened as theinitial discontinuity hit the walls, whereas the small-amplitude waves ofsize similar to the elements correspond to numerical artifacts. They have theirorigin in the finite resolution of the scheme and appear as the discontinuitytravels through elements with high-order polynomials. This effect can be curedby increasing resolution. Apart from this effect, the rich wave structure isthe result of the transport accuracy of the high-order DG method.
*  [2.x.345] 
* With 2 levels of global refinement with 1,600 cells, the mesh and itspartitioning on 40 MPI processes looks as follows:
*  [2.x.346] 
* When we run the code with 4 levels of global refinements on 40 cores, we getthe following output:
* [1.x.169]
* 
* The norms shown here for the various quantities are the deviations [2.x.347] ,  [2.x.348] , and  [2.x.349]  against the background field (namely, theinitial condition). The distribution of run time is overall similar as in theprevious test case. The only slight difference is the larger proportion oftime spent in  [2.x.350]  as compared to the inverse mass matrix and vectorupdates. This is because the geometry is deformed and the matrix-freeframework needs to load additional arrays for the geometry from memory thatare compressed in the affine mesh case.
* Increasing the number of global refinements to 5, the output becomes:
* [1.x.170]
* 
* The effect on performance is similar to the analytical test case
* 
*  -  intheory, computation times should increase by a factor of 8, but we actuallysee an increase by a factor of 11 for the time steps (219.5 seconds versus2450 seconds). This can be traced back to caches, with the small case mostlyfitting in caches. An interesting effect, typical of programs with a mix oflocal communication (integrals  [2.x.351] ) and global communication (computation oftransport speed) with some load imbalance, can be observed by looking at theMPI ranks that encounter the minimal and maximal time of different phases,respectively. Rank 0 reports the fastest throughput for the "rk time steppingtotal" part. At the same time, it appears to be slowest for the "computetransport speed" part, almost a factor of 2 slower than theaverage and almost a factor of 4 compared to the faster rank.Since the latter involves global communication, we can attribute theslowness in this part to the fact that the local Runge--Kutta stages haveadvanced more quickly on this rank and need to wait until the other processorscatch up. At this point, one can wonder about the reason for this imbalance:The number of cells is almost the same on all MPI processes.However, the matrix-free framework is faster on affine and Cartesiancells located towards the outlet of the channel, to which the lower MPI ranksare assigned. On the other hand, rank 32, which reports the highest run timefor the Runga--Kutta stages, owns the curved cells near the cylinder, forwhich no data compression is possible. To improve throughput, we could assigndifferent weights to different cell types when partitioning the [2.x.352]  object, or even measure the run time for afew time steps and try to rebalance then.
* The throughput per Runge--Kutta stage can be computed to 2085 MDoFs/s for the14.7 million DoFs test case over the 346,000 Runge--Kutta stages, slightly slowerthan the Cartesian mesh throughput of 2360 MDoFs/s reported above.
* Finally, if we add one additional refinement, we record the following output:
* [1.x.171]
* 
* The "rk time stepping total" part corresponds to a throughput of 2010 MDoFs/s. Theoverall run time to perform 139k time steps is 20k seconds (5.7 hours) or 7time steps per second
* 
*  -  not so bad for having nearly 60 millionunknowns. More throughput can be achieved by adding more cores tothe computation.
* 

* [1.x.172][1.x.173]
* 

* Switching the channel test case to 3D with 3 global refinements, the output is
* [1.x.174]
* 
* The physics are similar to the 2D case, with a slight motion in the zdirection due to the gravitational force. The throughput per Runge--Kuttastage in this case is[1.x.175]
* The throughput is lower than in 2D because the computation of the  [2.x.353]  termis more expensive. This is due to over-integration with `degree+2` points andthe larger fraction of face integrals (worse volume-to-surface ratio) withmore expensive flux computations. If we only consider the inverse mass matrixand vector update part, we record a throughput of 4857 MDoFs/s for the 2D caseof the isentropic vortex with 37.7 million unknowns, whereas the 3D caseruns with 4535 MDoFs/s. The performance is similar because both cases are infact limited by the memory bandwidth.
* If we go to four levels of global refinement, we need to increase the numberof processes to fit everything in memory
* 
*  -  the computation needs around 350GB of RAM memory in this case. Also, the time it takes to complete 35k timesteps becomes more tolerable by adding additional resources. We therefore use6 nodes with 40 cores each, resulting in a computation with 240 MPI processes:
* [1.x.176]
* This simulation had nearly 2 billion unknowns
* 
*  -  quite a largecomputation indeed, and still only needed around 1.5 seconds per timestep.
* 

* [1.x.177][1.x.178]
* 

* The code presented here straight-forwardly extends to adaptive meshes, givenappropriate indicators for setting the refinement flags. Large-scaleadaptivity of a similar solver in the context of the acoustic wave equationhas been achieved by the [1.x.179]. However, in the present context, the benefits of adaptivity are oftenlimited to early times and effects close to the origin of sound waves, as thewaves eventually reflect and diffract. This leads to steep gradients all overthe place, similar to turbulent flow, and a more or less globallyrefined mesh.
* Another topic that we did not discuss in the results section is a comparisonof different time integration schemes. The program provides four variants oflow-storage Runga--Kutta integrators that each have slightly differentaccuracy and stability behavior. Among the schemes implemented here, thehigher-order ones provide additional accuracy but come with slightly lowerefficiency in terms of step size per stage before they violate the CFLcondition. An interesting extension would be to compare the low-storagevariants proposed here with standard Runge--Kutta integrators or to use vectoroperations that are run separate from the mass matrix operation and compareperformance.
* 

* [1.x.180][1.x.181]
* 

* As mentioned in the introduction, the modified Lax--Friedrichs flux and theHLL flux employed in this program are only two variants of a large body ofnumerical fluxes available in the literature on the Euler equations. Oneexample is the HLLC flux (Harten-Lax-van Leer-Contact) flux which adds theeffect of rarefaction waves missing in the HLL flux, or the Roe flux. Asmentioned in the introduction, the effect of numerical fluxes on high-order DGschemes is debatable (unlike for the case of low-order discretizations).
* A related improvement to increase the stability of the solver is to alsoconsider the spatial integral terms. A shortcoming in the rather naiveimplementation used above is the fact that the energy conservation of theoriginal Euler equations (in the absence of shocks) only holds up to adiscretization error. If the solution is under-resolved, the discretizationerror can give rise to an increase in the numerical energy and eventuallyrender the discretization unstable. This is because of the inexact numericalintegration of the terms in the Euler equations, which both contain rationalnonlinearities and higher-degree content from curved cells. A way out of thisdilemma are so-called skew-symmetric formulations, see  [2.x.354]  for asimple variant. Skew symmetry means that switching the role of the solution [2.x.355]  and test functions  [2.x.356]  in the weak form produces theexact negative of the original quantity, apart from some boundary terms. Inthe discrete setting, the challenge is to keep this skew symmetry also whenthe integrals are only computed approximately (in the continuous case,skew-symmetry is a consequence of integration by parts). Skew-symmetricnumerical schemes balance spatial derivatives in the conservative form [2.x.357]  with contributions in theconvective form  [2.x.358]  for some  [2.x.359] . The precise terms depend onthe equation and the integration formula, and can in some cases by understoodby special skew-symmetric finite difference schemes.
* To get started, interested readers could take a look athttps://github.com/kronbichler/advection_miniapp, where askew-symmetric DG formulation is implemented with deal.II for a simple advectionequation.
* [1.x.182][1.x.183]
* 

* As mentioned in the introduction, the solution to the Euler equations developsshocks as the Mach number increases, which require additional mechanisms tostabilize the scheme, e.g. in the form of limiters. The main challenge besidesactually implementing the limiter or artificial viscosity approach would be toload-balance the computations, as the additional computations involved forlimiting the oscillations in troubled cells would make them more expensive than theplain DG cells without limiting. Furthermore, additional numerical fluxes thatbetter cope with the discontinuities would also be an option.
* One ingredient also necessary for supersonic flows are appropriate boundaryconditions. As opposed to the subsonic outflow boundaries discussed in theintroduction and implemented in the program, all characteristics are outgoingfor supersonic outflow boundaries, so we do not want to prescribe any externaldata,[1.x.184]
* In the code, we would simply add the additional statement
* [1.x.185]
* in the `local_apply_boundary_face()` function.
* [1.x.186][1.x.187]
* 

* When the interest with an Euler solution is mostly in the propagation of soundwaves, it often makes sense to linearize the Euler equations around abackground state, i.e., a given density, velocity and energy (or pressure)field, and only compute the change against these fields. This is the settingof the wide field of aeroacoustics. Even though the resolution requirementsare sometimes considerably reduced, implementation gets somewhat morecomplicated as the linearization gives rise to additional terms. From a codeperspective, in the operator evaluation we also need to equip the code withthe state to linearize against. This information can be provided either byanalytical functions (that are evaluated in terms of the position of thequadrature points) or by a vector similar to the solution. Based on thatvector, we would create an additional FEEvaluation object to read from it andprovide the values of the field at quadrature points. If the backgroundvelocity is zero and the density is constant, the linearized Euler equationsfurther simplify and can equivalently be written in the form of theacoustic wave equation.
* A challenge in the context of sound propagation is often the definition ofboundary conditions, as the computational domain needs to be of finite size,whereas the actual simulation often spans an infinite (or at least muchlarger) physical domain. Conventional Dirichlet or Neumann boundary conditionsgive rise to reflections of the sound waves that eventually propagate back tothe region of interest and spoil the solution. Therefore, various variants ofnon-reflecting boundary conditions or sponge layers, often in the form of[1.x.188]
* 
*  -  where the solution is damped without reflection
* 
*  -  are common.
* 

* [1.x.189][1.x.190]
* 

* The solver presented in this tutorial program can also be extended to thecompressible Navier--Stokes equations by adding viscous terms, as described in [2.x.360] . To keep as much of the performance obtainedhere despite the additional cost of elliptic terms, e.g. via an interiorpenalty method, one can switch the basis from FE_DGQ to FE_DGQHermite like inthe  [2.x.361]  tutorial program.
* 

* [1.x.191][1.x.192]
* 

* In this tutorial, we used face-centric loops. Here, cell and face integralsare treated in separate loops, resulting in multiple writing accesses into theresult vector, which is relatively expensive on modern hardware since writingoperations generally result also in an implicit read operation. Element-centricloops, on the other hand, are processing a cell and in direct successionprocessing all its 2d faces. Although this kind of loop implies that fluxes haveto be computed twice (for each side of an interior face), the fact that theresult vector has to accessed only once might
* 
*  - and the fact that the resultingalgorithm is free of race-conditions and as such perfectly suitable forshared memory
* 
*  - already give a performance boost. If you are interested in theseadvanced topics, you can take a look at  [2.x.362]  where we take the presenttutorial and modify it so that we can use these features.
* 

* [1.x.193][1.x.194] [2.x.363] 
* [0.x.1]