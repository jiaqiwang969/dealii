[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22]
* [1.x.23][1.x.24][1.x.25]
* 

*  [2.x.2] 
* This is the first of a number of tutorial programs that will finallycover "real" time-dependent problems, not the slightly odd form of timedependence found in  [2.x.3]  or the DAE model of  [2.x.4] . In particular, this program introducesthe wave equation in a bounded domain. Later,  [2.x.5] will consider an example of absorbing boundary conditions, and  [2.x.6]  " [2.x.7] " a kind of nonlinear wave equation producingsolutions called solitons.
* The wave equation in its prototypical form reads as follows: find [2.x.8]  that satisfies[1.x.26]
* Note that since this is an equation with second-order timederivatives, we need to pose two initial conditions, one for the valueand one for the time derivative of the solution.
* Physically, the equation describes the motion of an elastic medium. In2-d, one can think of how a membrane moves if subjected to aforce. The Dirichlet boundary conditions above indicate that themembrane is clamped at the boundary at a height  [2.x.9]  (this heightmight be moving as well &mdash; think of people holding a blanket andshaking it up and down). The first initial condition equals theinitial deflection of the membrane, whereas the second one gives itsvelocity. For example, one could think of pushing the membrane downwith a finger and then letting it go at  [2.x.10]  (nonzero deflection butzero initial velocity), or hitting it with a hammer at  [2.x.11]  (zerodeflection but nonzero velocity). Both cases would induce motion inthe membrane.
* 

* [1.x.27][1.x.28]
* 

* [1.x.29][1.x.30]
* There is a long-standing debate in the numerical analysis communityover whether a discretization of time dependent equations shouldinvolve first discretizing the time variable leading to a stationaryPDE at each time step that is then solved using standard finiteelement techniques (this is called the Rothe method), or whetherone should first discretize the spatial variables, leading to a largesystem of ordinary differential equations that can then be handled byone of the usual ODE solvers (this is called the method of lines).
* Both of these methods have advantages and disadvantages.Traditionally, people have preferred the method of lines, since itallows to use the very well developed machinery of high-order ODEsolvers available for the rather stiff ODEs resulting from thisapproach, including step length control and estimation of the temporalerror.
* On the other hand, Rothe's method becomes awkward when usinghigher-order time stepping method, since one then has to write down aPDE that couples the solution of the present time step not only withthat at the previous time step, but possibly also even earliersolutions, leading to a significant number of terms.
* For these reasons, the method of lines was the method of choice for along time. However, it has one big drawback: if we discretize thespatial variable first, leading to a large ODE system, we have tochoose a mesh once and for all. If we are willing to do this, thenthis is a legitimate and probably superior approach.
* If, on the other hand, we are looking at the wave equation and manyother time dependent problems, we find that the character of asolution changes as time progresses. For example, for the waveequation, we may have a single wave travelling through the domain,where the solution is smooth or even constant in front of and behindthe wave &mdash; adaptivity would be really useful for such cases, but thekey is that the area where we need to refine the mesh changes fromtime step to time step!
* If we intend to go that way, i.e. choose a different mesh for eachtime step (or set of time steps), then the method of lines is notappropriate any more: instead of getting one ODE system with a numberof variables equal to the number of unknowns in the finite elementmesh, our number of unknowns now changes all the time, a fact thatstandard ODE solvers are certainly not prepared to deal with atall. On the other hand, for the Rothe method, we just get a PDE foreach time step that we may choose to discretize independently of themesh used for the previous time step; this approach is not withoutperils and difficulties, but at least is a sensible and well-definedprocedure.
* For all these reasons, for the present program, we choose to use theRothe method for discretization, i.e. we first discretize in time andthen in space. We will not actually use adaptive meshes at all, sincethis involves a large amount of additional code, but we will commenton this some more in the [1.x.31].
* 

* [1.x.32][1.x.33]
* 

* Given these considerations, here is how we will proceed: let us firstdefine a simple time stepping method for this second order problem,and then in a second step do the spatial discretization, i.e. we willfollow Rothe's approach.
* For the first step, let us take a little detour first: in order todiscretize a second time derivative, we can either discretize itdirectly, or we can introduce an additional variable and transform thesystem into a first order system. In many cases, this turns out to beequivalent, but dealing with first order systems is often simpler. Tothis end, let us introduce[1.x.34]and call this variable the [1.x.35] for obvious reasons. We canthen reformulate the original wave equation as follows:[1.x.36]
* The advantage of this formulation is that it now only contains firsttime derivatives for both variables, for which it is simple to writedown time stepping schemes. Note that we do not have boundaryconditions for  [2.x.12]  at first. However, we could enforce  [2.x.13]  on the boundary. It turns out in numerical examples that thisis actually necessary: without doing so the solution doesn't look particularlywrong, but the Crank-Nicolson scheme does not conserve energy if one doesn'tenforce these boundary conditions.
* With this formulation, let us introduce the following timediscretization where a superscript  [2.x.14]  indicates the number of a timestep and  [2.x.15]  is the length of the present time step:[1.x.37]Note how we introduced a parameter  [2.x.16]  here. If we chose [2.x.17] , for example, the first equation would reduce to [2.x.18] , which is well-known as theforward or explicit Euler method. On the other hand, if we set [2.x.19] , then we would get [2.x.20] , which corresponds to thebackward or implicit Euler method. Both these methods are first orderaccurate methods. They are simple to implement, but they are notreally very accurate.
* The third case would be to choose  [2.x.21] . The first of theequations above would then read  [2.x.22] . This method is known asthe Crank-Nicolson method and has the advantage that it is secondorder accurate. In addition, it has the nice property that itpreserves the energy in the solution (physically, the energy is thesum of the kinetic energy of the particles in the membrane plus thepotential energy present due to the fact that it is locally stretched;this quantity is a conserved one in the continuous equation, but mosttime stepping schemes do not conserve it after timediscretization). Since  [2.x.23]  also appears in the equation for  [2.x.24] ,the Crank-Nicolson scheme is also implicit.
* In the program, we will leave  [2.x.25]  as a parameter, so that it willbe easy to play with it. The results section will show some numericalevidence comparing the different schemes.
* The equations above (called the [1.x.38] equationsbecause we have only discretized the time, but not space), can besimplified a bit by eliminating  [2.x.26]  from the first equation andrearranging terms. We then get[1.x.39]In this form, we see that if we are given the solution [2.x.27]  of the previous timestep, that we can then solve forthe variables  [2.x.28]  separately, i.e. one at a time. This isconvenient. In addition, we recognize that the operator in the firstequation is positive definite, and the second equation looksparticularly simple.
* 

* [1.x.40][1.x.41]
* 

* We have now derived equations that relate the approximate(semi-discrete) solution  [2.x.29]  and its time derivative  [2.x.30]  attime  [2.x.31]  with the solutions  [2.x.32]  of the previoustime step at  [2.x.33] . The next step is to also discretize thespatial variable using the usual finite element methodology. To thisend, we multiply each equation with a test function, integrate overthe entire domain, and integrate by parts where necessary. This leadsto[1.x.42]
* It is then customary to approximate  [2.x.34] , where  [2.x.35]  are the shape functions usedfor the discretization of the  [2.x.36] -th time step and  [2.x.37]  are theunknown nodal values of the solution. Similarly,  [2.x.38] . Finally, we have the solutions ofthe previous time step,  [2.x.39]  and  [2.x.40] . Note that since the solution of the previoustime step has already been computed by the time we get to time step [2.x.41] ,  [2.x.42]  are known. Furthermore, note that the solutionsof the previous step may have been computed on a different mesh, sowe have to use shape functions  [2.x.43] .
* If we plug these expansions into above equations and test with thetest functions from the present mesh, we get the following linearsystem:[1.x.43]where[1.x.44]
* 
* If we solve these two equations, we can move the solution one stepforward and go on to the next time step.
* It is worth noting that if we choose the same mesh on each time step(as we will in fact do in the program below), then we have the sameshape functions on time step  [2.x.44]  and  [2.x.45] ,i.e.  [2.x.46] . Consequently, we get [2.x.47]  and  [2.x.48] . On the other hand, if we hadused different shape functions, then we would have to computeintegrals that contain shape functions defined on two meshes. This is asomewhat messy process that we omit here, but that is treated in somedetail in  [2.x.49] .
* Under these conditions (i.e. a mesh that doesn't change), one can optimize thesolution procedure a bit by basically eliminating the solution of the secondlinear system. We will discuss this in the introduction of the  [2.x.50] " [2.x.51] " program.
* [1.x.45][1.x.46]
* 

* One way to compare the quality of a time stepping scheme is to see whether thenumerical approximation preserves conservation properties of the continuousequation. For the wave equation, the natural quantity to look at is theenergy. By multiplying the wave equation by  [2.x.52] , integrating over  [2.x.53] ,and integrating by parts where necessary, we find that[1.x.47]By consequence, in absence of body forces and constant boundary values, we getthat[1.x.48]is a conserved quantity, i.e. one that doesn't change with time. Wewill compute this quantity after each timestep. It is straightforward to see that if we replace  [2.x.54]  by its finiteelement approximation, and  [2.x.55]  by the finiteelement approximation of the velocity  [2.x.56] , then[1.x.49]As we will see in the results section, the Crank-Nicolson scheme does indeedconserve the energy, whereas neither the forward nor the backward Euler schemedo.
* 

* [1.x.50][1.x.51]
* 

* One of the reasons why the wave equation is nasty to solve numerically is thatexplicit time discretizations are only stable if the time step is smallenough. In particular, it is coupled to the spatial mesh width  [2.x.57] . For thelowest order discretization we use here, the relationship reads[1.x.52]where  [2.x.58]  is the wave speed, which in our formulation of the wave equation hasbeen normalized to one. Consequently, unless we use the implicit schemes with [2.x.59] , our solutions will not be numerically stable if we violate thisrestriction. Implicit schemes do not have this restriction for stability, butthey become inaccurate if the time step is too large.
* This condition was first recognized by Courant, Friedrichs, and Lewy &mdash;in 1928, long before computers became available for numericalcomputations! (This result appeared in the German language articleR. Courant, K. Friedrichs and H. Lewy: [1.x.53], MathematischeAnnalen, vol. 100, no. 1, pages 32-74, 1928.)This condition on the time step is most frequently just referredto as the [1.x.54] condition. Intuitively, the CFL condition saysthat the time step must not be larger than the time it takes a wave tocross a single cell.
* In the program, we will refine the square [2.x.60]  seven times uniformly, giving a mesh size of  [2.x.61] , whichis what we set the time step to. The fact that we set the time step and meshsize individually in two different places is error prone: it is too easy torefine the mesh once more but forget to also adjust the time step.  [2.x.62]  " [2.x.63] " shows a better way how to keep these things in sync.
* 

* [1.x.55][1.x.56]
* 

* Although the program has all the hooks to deal with nonzero initial andboundary conditions and body forces, we take a simple case where the domain isa square  [2.x.64]  and[1.x.57]
* This corresponds to a membrane initially at rest and clamped all around, wheresomeone is waving a part of the clamped boundary once up and down, therebyshooting a wave into the domain.
* 

*  [1.x.58] [1.x.59]
*   [1.x.60]  [1.x.61]
* 

* 
*  We start with the usual assortment of include files that we've seen in so many of the previous tests:
* 

* 
* [1.x.62]
* 
*  Here are the only three include files of some new interest: The first one is already used, for example, for the  [2.x.65]  and  [2.x.66]  functions. However, we here use another function in that class,  [2.x.67]  to compute our initial values as the  [2.x.68]  projection of the continuous initial values. Furthermore, we use  [2.x.69]  to generate the integrals  [2.x.70] . These were previously always generated by hand in  [2.x.71]  or similar functions in application code. However, we're too lazy to do that here, so simply use a library function:
* 

* 
* [1.x.63]
* 
*  In a very similar vein, we are also too lazy to write the code to assemble mass and Laplace matrices, although it would have only taken copying the relevant code from any number of previous tutorial programs. Rather, we want to focus on the things that are truly new to this program and therefore use the  [2.x.72]  and  [2.x.73]  functions. They are declared here:
* 

* 
* [1.x.64]
* 
*  Finally, here is an include file that contains all sorts of tool functions that one sometimes needs. In particular, we need the  [2.x.74]  class that, given an integer argument, returns a string representation of it. It is particularly useful since it allows for a second parameter indicating the number of digits to which we want the result padded with leading zeros. We will use this to write output files that have the form  [2.x.75]  denotes the number of the time step and always consists of three digits even if we are still in the single or double digit time steps.
* 

* 
* [1.x.65]
* 
*  The last step is as in all previous programs:
* 

* 
* [1.x.66]
* 
*   [1.x.67]  [1.x.68]
* 

* 
*  Next comes the declaration of the main class. It's public interface of functions is like in most of the other tutorial programs. Worth mentioning is that we now have to store four matrices instead of one: the mass matrix  [2.x.76] , the Laplace matrix  [2.x.77] , the matrix  [2.x.78]  used for solving for  [2.x.79] , and a copy of the mass matrix with boundary conditions applied used for solving for  [2.x.80] . Note that it is a bit wasteful to have an additional copy of the mass matrix around. We will discuss strategies for how to avoid this in the section on possible improvements.   
*   Likewise, we need solution vectors for  [2.x.81]  as well as for the corresponding vectors at the previous time step,  [2.x.82] . The  [2.x.83]  will be used for whatever right hand side vector we have when solving one of the two linear systems in each time step. These will be solved in the two functions  [2.x.84]  and  [2.x.85] .   
*   Finally, the variable  [2.x.86]  is used to indicate the parameter  [2.x.87]  that is used to define which time stepping scheme to use, as explained in the introduction. The rest is self-explanatory.
* 

* 
* [1.x.69]
* 
*   [1.x.70]  [1.x.71]
* 

* 
*  Before we go on filling in the details of the main class, let us define the equation data corresponding to the problem, i.e. initial and boundary values for both the solution  [2.x.88]  and its time derivative  [2.x.89] , as well as a right hand side class. We do so using classes derived from the Function class template that has been used many times before, so the following should not be a surprise.   
*   Let's start with initial values and choose zero for both the value  [2.x.90]  as well as its time derivative, the velocity  [2.x.91] :
* 

* 
* [1.x.72]
* 
*  Secondly, we have the right hand side forcing term. Boring as we are, we choose zero here as well:
* 

* 
* [1.x.73]
* 
*  Finally, we have boundary values for  [2.x.92]  and  [2.x.93] . They are as described in the introduction, one being the time derivative of the other:
* 

* 
* [1.x.74]
* 
*   [1.x.75]  [1.x.76]
* 

* 
*  The implementation of the actual logic is actually fairly short, since we relegate things like assembling the matrices and right hand side vectors to the library. The rest boils down to not much more than 130 lines of actual code, a significant fraction of which is boilerplate code that can be taken from previous example programs (e.g. the functions that solve linear systems, or that generate output).   
*   Let's start with the constructor (for an explanation of the choice of time step, see the section on Courant, Friedrichs, and Lewy in the introduction):
* 

* 
* [1.x.77]
* 
*   [1.x.78]  [1.x.79]
* 

* 
*  The next function is the one that sets up the mesh, DoFHandler, and matrices and vectors at the beginning of the program, i.e. before the first time step. The first few lines are pretty much standard if you've read through the tutorial programs at least up to  [2.x.94] :
* 

* 
* [1.x.80]
* 
*  Then comes a block where we have to initialize the 3 matrices we need in the course of the program: the mass matrix, the Laplace matrix, and the matrix  [2.x.95]  used when solving for  [2.x.96]  in each time step.     
*   When setting up these matrices, note that they all make use of the same sparsity pattern object. Finally, the reason why matrices and sparsity patterns are separate objects in deal.II (unlike in many other finite element or linear algebra classes) becomes clear: in a significant fraction of applications, one has to hold several matrices that happen to have the same sparsity pattern, and there is no reason for them not to share this information, rather than re-building and wasting memory on it several times.     
*   After initializing all of these matrices, we call library functions that build the Laplace and mass matrices. All they need is a DoFHandler object and a quadrature formula object that is to be used for numerical integration. Note that in many respects these functions are better than what we would usually do in application programs, for example because they automatically parallelize building the matrices if multiple processors are available in a machine: for more information see the documentation of WorkStream or the  [2.x.97]  "Parallel computing with multiple processors" module. The matrices for solving linear systems will be filled in the run() method because we need to re-apply boundary conditions every time step.
* 

* 
* [1.x.81]
* 
*  The rest of the function is spent on setting vector sizes to the correct value. The final line closes the hanging node constraints object. Since we work on a uniformly refined mesh, no constraints exist or have been computed (i.e. there was no need to call  [2.x.98]  as in other programs), but we need a constraints object in one place further down below anyway.
* 

* 
* [1.x.82]
* 
*   [1.x.83]  [1.x.84]
* 

* 
*  The next two functions deal with solving the linear systems associated with the equations for  [2.x.99]  and  [2.x.100] . Both are not particularly interesting as they pretty much follow the scheme used in all the previous tutorial programs.   
*   One can make little experiments with preconditioners for the two matrices we have to invert. As it turns out, however, for the matrices at hand here, using Jacobi or SSOR preconditioners reduces the number of iterations necessary to solve the linear system slightly, but due to the cost of applying the preconditioner it is no win in terms of run-time. It is not much of a loss either, but let's keep it simple and just do without:
* 

* 
* [1.x.85]
* 
*   [1.x.86]  [1.x.87]
* 

* 
*  Likewise, the following function is pretty much what we've done before. The only thing worth mentioning is how here we generate a string representation of the time step number padded with leading zeros to 3 character length using the  [2.x.101]  function's second argument.
* 

* 
* [1.x.88]
* 
*  Like  [2.x.102] , since we write output at every time step (and the system we have to solve is relatively easy), we instruct DataOut to use the zlib compression algorithm that is optimized for speed instead of disk usage since otherwise plotting the output becomes a bottleneck:
* 

* 
* [1.x.89]
* 
*   [1.x.90]  [1.x.91]
* 

* 
*  The following is really the only interesting function of the program. It contains the loop over all time steps, but before we get to that we have to set up the grid, DoFHandler, and matrices. In addition, we have to somehow get started with initial values. To this end, we use the  [2.x.103]  function that takes an object that describes a continuous function and computes the  [2.x.104]  projection of this function onto the finite element space described by the DoFHandler object. Can't be any simpler than that:
* 

* 
* [1.x.92]
* 
*  The next thing is to loop over all the time steps until we reach the end time ( [2.x.105]  in this case). In each time step, we first have to solve for  [2.x.106] , using the equation  [2.x.107]   [2.x.108]   [2.x.109] . Note that we use the same mesh for all time steps, so that  [2.x.110]  and  [2.x.111] . What we therefore have to do first is to add up  [2.x.112]  and the forcing terms, and put the result into the  [2.x.113]  vector. (For these additions, we need a temporary vector that we declare before the loop to avoid repeated memory allocations in each time step.)     
*   The one thing to realize here is how we communicate the time variable to the object describing the right hand side: each object derived from the Function class has a time field that can be set using the  [2.x.114]  and read by  [2.x.115]  In essence, using this mechanism, all functions of space and time are therefore considered functions of space evaluated at a particular time. This matches well what we typically need in finite element programs, where we almost always work on a single time step at a time, and where it never happens that, for example, one would like to evaluate a space-time function for all times at any given spatial location.
* 

* 
* [1.x.93]
* 
*  After so constructing the right hand side vector of the first equation, all we have to do is apply the correct boundary values. As for the right hand side, this is a space-time function evaluated at a particular time, which we interpolate at boundary nodes and then use the result to apply boundary values as we usually do. The result is then handed off to the solve_u() function:
* 

* 
* [1.x.94]
* 
*  The matrix for solve_u() is the same in every time steps, so one could think that it is enough to do this only once at the beginning of the simulation. However, since we need to apply boundary values to the linear system (which eliminate some matrix rows and columns and give contributions to the right hand side), we have to refill the matrix in every time steps before we actually apply boundary data. The actual content is very simple: it is the sum of the mass matrix and a weighted Laplace matrix:
* 

* 
* [1.x.95]
* 
*  The second step, i.e. solving for  [2.x.116] , works similarly, except that this time the matrix on the left is the mass matrix (which we copy again in order to be able to apply boundary conditions, and the right hand side is  [2.x.117]  plus forcing terms. Boundary values are applied in the same way as before, except that now we have to use the BoundaryValuesV class:
* 

* 
* [1.x.96]
* 
*  Finally, after both solution components have been computed, we output the result, compute the energy in the solution, and go on to the next time step after shifting the present solution into the vectors that hold the solution at the previous time step. Note the function  [2.x.118]  that can compute  [2.x.119]  and  [2.x.120]  in one step, saving us the expense of a temporary vector and several lines of code:
* 

* 
* [1.x.97]
* 
*   [1.x.98]  [1.x.99]
* 

* 
*  What remains is the main function of the program. There is nothing here that hasn't been shown in several of the previous programs:
* 

* 
* [1.x.100]
* [1.x.101][1.x.102]
* 

* When the program is run, it produces the following output:
* [1.x.103]
* 
* What we see immediately is that the energy is a constant at least after [2.x.121]  (until which the boundary source term  [2.x.122]  is nonzero, injectingenergy into the system).
* In addition to the screen output, the program writes the solution of each timestep to an output file. If we process them adequately and paste them into amovie, we get the following:
*  [2.x.123] 
* The movie shows the generated wave nice traveling through the domain and back,being reflected at the clamped boundary. Some numerical noise is trailing thewave, an artifact of a too-large mesh size that can be reduced by reducing themesh width and the time step.
* 

* [1.x.104][1.x.105][1.x.106]
* 

* If you want to explore a bit, try out some of the following things: [2.x.124]    [2.x.125] Varying  [2.x.126] . This gives different time stepping schemes, some of  which are stable while others are not. Take a look at how the energy  evolves.
*    [2.x.127] Different initial and boundary conditions, right hand sides.
*    [2.x.128] More complicated domains or more refined meshes. Remember that the time  step needs to be bounded by the mesh width, so changing the mesh should  always involve also changing the time step. We will come back to this issue  in  [2.x.129] .
*    [2.x.130] Variable coefficients: In real media, the wave speed is often  variable. In particular, the "real" wave equation in realistic media would  read  [1.x.107]  where  [2.x.131]  is the density of the material, and  [2.x.132]  is related to the  stiffness coefficient. The wave speed is then  [2.x.133] .
*   To make such a change, we would have to compute the mass and Laplace  matrices with a variable coefficient. Fortunately, this isn't too hard: the  functions  [2.x.134]  and   [2.x.135]  have additional default parameters that can  be used to pass non-constant coefficient functions to them. The required  changes are therefore relatively small. On the other hand, care must be  taken again to make sure the time step is within the allowed range.
*    [2.x.136] In the in-code comments, we discussed the fact that the matrices for  solving for  [2.x.137]  and  [2.x.138]  need to be reset in every time because of  boundary conditions, even though the actual content does not change. It is  possible to avoid copying by not eliminating columns in the linear systems,  which is implemented by appending a  [2.x.139]  argument to the call: 
* [1.x.108]
* 
*    [2.x.140] deal.II being a library that supports adaptive meshes it would of course be  nice if this program supported change the mesh every few time steps. Given the  structure of the solution &mdash; a wave that travels through the domain &mdash;  it would seem appropriate if we only refined the mesh where the wave currently is,  and not simply everywhere. It is intuitively clear that we should be able to  save a significant amount of cells this way. (Though upon further thought one  realizes that this is really only the case in the initial stages of the simulation.  After some time, for wave phenomena, the domain is filled with reflections of  the initial wave going in every direction and filling every corner of the domain.  At this point, there is in general little one can gain using local mesh  refinement.)
*   To make adaptively changing meshes possible, there are basically two routes.  The "correct" way would be to go back to the weak form we get using Rothe's  method. For example, the first of the two equations to be solved in each time  step looked like this:  [1.x.109]  Now, note that we solve for  [2.x.141]  on mesh  [2.x.142] , and  consequently the test functions  [2.x.143]  have to be from the space   [2.x.144]  as well. As discussed in the introduction, terms like   [2.x.145]  then require us to integrate the solution of the  previous step (which may have been computed on a different mesh   [2.x.146] ) against the test functions of the current mesh,  leading to a matrix  [2.x.147] . This process of integrating shape  functions from different meshes is, at best, awkward. It can be done  but because it is difficult to ensure that  [2.x.148]  and   [2.x.149]  differ by at most one level of refinement, one  has to recursively match cells from both meshes. It is feasible to  do this, but it leads to lengthy and not entirely obvious code.
*   The second approach is the following: whenever we change the mesh,  we simply interpolate the solution from the last time step on the old  mesh to the new mesh, using the SolutionTransfer class. In other words,  instead of the equation above, we would solve  [1.x.110]  where  [2.x.150]  interpolates a given function onto mesh  [2.x.151] .  This is a much simpler approach because, in each time step, we no  longer have to worry whether  [2.x.152]  were computed on the  same mesh as we are using now or on a different mesh. Consequently,  the only changes to the code necessary are the addition of a function  that computes the error, marks cells for refinement, sets up a  SolutionTransfer object, transfers the solution to the new mesh, and  rebuilds matrices and right hand side vectors on the new mesh. Neither  the functions building the matrices and right hand sides, nor the  solvers need to be changed.
*   While this second approach is, strictly speaking,  not quite correct in the Rothe framework (it introduces an addition source  of error, namely the interpolation), it is nevertheless what  almost everyone solving time dependent equations does. We will use this  method in  [2.x.153] , for example. [2.x.154] 
* 

* [1.x.111][1.x.112] [2.x.155] 
* [0.x.1]